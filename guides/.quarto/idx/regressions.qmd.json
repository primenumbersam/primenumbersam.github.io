{"title":"From Linear to Curved Structures to Fit the Data","markdown":{"yaml":{"title":"From Linear to Curved Structures to Fit the Data","subtitle":"Inner Products, Projection, and the Role of Positive Definite Kernels","author":"gitSAM","date":"2025-03-21","abstract":"This seminar note explores the geometric foundations of OLS, GLS, and GMM estimators through the lens of inner product spaces. We compare their respective objective functions as quadratic forms, analyze their level sets, and demonstrate how positive definite kernels reshape the geometry of estimation. By drawing parallels to Einstein's field equations, we highlight how the structure of estimation reflects deeper principles of information weighting and spatial deformation.","keywords":"OLS, GLS, GMM, inner product space, positive definite kernel, Mahalanobis norm, projection geometry, quadratic form, estimation theory, level set visualization, Einstein field equations, metric tensor, heteroskedasticity, moment conditions","format":{"html":{"code-fold":true,"toc":true}},"jupyter":"python3"},"headingText":"1. OLS as Orthogonal Projection in Euclidean Space","containsRefs":false,"markdown":"\n\n\nOLS solves the following problem:\n\n$$\n\\hat{\\beta}_{OLS} = \\arg\\min_\\beta \\| y - X\\beta \\|_2^2 = (X^\\top X)^{-1} X^\\top y\n$$\n\n- This corresponds to projecting $y$ orthogonally onto the column space of $X$.\n- The residual $\\varepsilon = y - X\\hat{\\beta}$ satisfies:\n\n$$\nX^\\top \\varepsilon = 0\n$$\n\n### Inner Product and Geometry\n\n- The $L^2$ norm used in OLS is induced by the **standard Euclidean inner product**:\n\n$$\n\\langle u, v \\rangle = u^\\top v\n$$\n\n- The distance function becomes:\n\n$$\n\\| u \\|_2 = \\sqrt{u^\\top u}\n$$\n\n- The set of parameter values yielding equal error defines a level set (isocurve):\n\n$$\n\\{ \\beta \\mid \\| y - X\\beta \\|_2^2 = c \\} \\Rightarrow \\text{spheres in parameter space}\n$$\n\n- This reflects **Pythagorean geometry** — the isocurves are **circles (in 2D), spheres (in 3D), or hyperspheres** in higher dimensions when $X^\\top X = I$; otherwise, elliptical.\n- In this sense, **OLS performs Euclidean projection** onto a linear subspace using the standard inner product.\n\n\n## 2. Generalized Least Squares (GLS)\n\nGLS extends OLS by accounting for non-spherical error covariance structure:\n\n$$\n\\hat{\\beta}_{GLS} = \\arg\\min_\\beta (y - X\\beta)^\\top \\Sigma^{-1} (y - X\\beta)\n$$\n\n- Here, $\\Sigma$ is the covariance matrix of the errors.\n- This defines a new inner product over the residual space:\n\n$$\n\\langle u, v \\rangle_\\Sigma = u^\\top \\Sigma^{-1} v\n$$\n\n- The level sets:\n\n$$\n\\{ \\beta \\mid (y - X\\beta)^\\top \\Sigma^{-1} (y - X\\beta) = c \\} \\Rightarrow \\text{ellipsoids in parameter space}\n$$\n\n- This reflects how different directions in parameter space are penalized differently depending on the **variance and correlation structure of the errors**.\n\n> The **ellipsoidal level sets** visualized earlier correspond precisely to this GLS structure.\n\n\n## 3. GMM as Orthogonality of Moment Conditions (Not Projection)\n\nGMM generalizes the estimation framework by relying on **moment conditions**, not residuals:\n\n$$\n\\hat{\\theta}_{GMM} = \\arg\\min_\\theta \\left[ \\bar{g}_n(\\theta)^\\top W \\bar{g}_n(\\theta) \\right]\n$$\n\nwhere:\n\n- $\\bar{g}_n(\\theta) = \\frac{1}{n} \\sum_{i=1}^n g(Z_i, \\theta)$ is the sample mean of moment functions\n- $W$ is a **positive definite weighting matrix**, usually the inverse of the estimated variance of $\\bar{g}_n(\\theta)$\n\n### Clarifying the Inner Product\n\nAlthough GMM also defines a quadratic form like GLS, it operates in a fundamentally different space:\n\n- In GLS: $u, v$ are residual vectors\n- In GMM: $u, v$ are **moment function evaluations**\n\nThus, while one can write:\n\n$$\n\\langle g_i(\\theta), g_j(\\theta) \\rangle_W = g_i(\\theta)^\\top W g_j(\\theta)\n$$\n\nit is important to emphasize:\n\n> **GMM is not a projection-based method**. It minimizes the violation of moment orthogonality conditions in expectation.\n\n- The isocurves:\n\n$$\n\\{ \\theta \\mid \\bar{g}_n(\\theta)^\\top W \\bar{g}_n(\\theta) = c \\}\n$$\n\nmay also appear elliptical — but they are not projections, and should not be conflated with GLS geometrically.\n\n### Local Approximation and Connection to GLS\n\nIn some cases, GMM can be viewed as a **local GLS approximation**. If moment conditions are close to linear in $\\theta$, and $g(Z_i, \\theta) \\approx A(Z_i)(\\theta - \\theta_0)$, then the GMM objective becomes:\n\n$$\n(\\theta - \\theta_0)^\\top A^\\top W A (\\theta - \\theta_0)\n$$\n\nwhich resembles GLS — but only **locally and approximately**.\n\n> Therefore, the geometric intuition of ellipsoids applies more precisely to GLS. For GMM, it’s a useful visual aid — but structurally distinct.\n\n---\n\n## 4. Einstein Field Equations and GMM: Structural Analogy\n\nEinstein’s Field Equations (EFE) in general relativity:\n\n$$\nG_{\\mu\\nu} = R_{\\mu\\nu} - \\frac{1}{2} R g_{\\mu\\nu} = 8\\pi T_{\\mu\\nu}\n$$\n\nwhere:\n\n- $T_{\\mu\\nu}$: Stress-energy tensor, representing energy and matter (matter-energy distribution)\n- $G_{\\mu\\nu}$: Einstein tensor, encoding the curvature of spacetime (curvature)\n  - $R_{\\mu\\nu}$: Ricci curvature tensor\n  - $R$: Scalar curvature\n  - $g_{\\mu\\nu}$: Metric tensor, defining the inner product in spacetime and governing geodesics\n\n\n### Analogy to Estimation Frameworks\n\n| Feature | OLS | GLS | GMM | EFE (Physics) |\n|--------|-----|-----|-----|----------------|\n| Space | Euclidean | Covariance-weighted | Moment function space | Curved spacetime |\n| Inner product | $I$ | $\\Sigma^{-1}$ | $W$ (moment-weighted) | $g_{\\mu\\nu}$ (metric) |\n| Optimization | Projection | Weighted projection | Moment orthogonality | Energy-curvature balance |\n| Level sets | Circles | Ellipses | Ellipsoid-like | Lightcones / geodesics |\n\n### Lightcones and Degenerate Conics\n\nIn EFE, the metric tensor $g_{\\mu\\nu}$ defines how distances and angles are measured — it is the analogue of a positive definite kernel in GMM. The field equations determine how the geometry (curvature) of spacetime reacts to matter and energy. In this sense, **spacetime is optimized or shaped in response to external inputs**, just as GMM shapes its estimation space based on the kernel $W$ and moment functions.\n\nThe level sets in general relativity are often visualized as **lightcones** — the surface separating causal influence from spacelike separation. Geometrically, a lightcone can be interpreted as the **degenerate case of a conic section**, where the quadric form:\n\n$$\nQ(x) = x^\\top g_{\\mu\\nu} x = 0\n$$\n\nresults in a **pair of intersecting lines**: this represents all null (light-like) directions emanating from a point. These are the boundary cases between time-like and space-like intervals, analogous to the way ellipsoids in GMM collapse into degenerate forms under singular kernel matrices.\n\nThus, in both GMM and EFE, the **shape and degeneracy of level sets** encode deep information about the underlying structure — whether it is a statistical model or the geometry of spacetime.\n\n\n## 5. Conclusion\n\n- OLS performs **Euclidean projection** in a space endowed with the standard inner product.\n- GLS generalizes this using a **positive definite kernel** over the residual space, yielding ellipsoidal level sets.\n- GMM further generalizes the concept by applying positive definite weighting over **moment function space**, not residuals.\n- Geometric analogies (spheres and ellipsoids) are structurally valid for OLS and GLS, and visually helpful — but must be used with care in GMM.\n- All three frameworks share a unifying theme: **geometry induced by positive definite structure**.\n\n> “OLS and GLS estimate by projection. GMM estimates by aligning empirical moments. Einstein’s theory bends space to match mass — estimation theory bends geometry to match data.”\n\n\n## Visuals\n\n### OLS vs. GLS: Different Projections Under the Same Linear Model\n\nThis section visualizes how **OLS** and **GLS** (with heteroskedastic weighting) yield different estimators even under the same linear model structure. The key distinction lies in how each method defines **distance and importance** via its respective inner product.\n\n#### Simulation Setup\n\n- **Data-generating process**:\n  $$\n  y = \\beta_0 + \\beta_1 x + \\varepsilon\n  $$\n  where the noise term $\\varepsilon$ is **heteroskedastic** — its variance increases with $x$.  \n  That is, the reliability of observations decreases toward the right end of the domain.\n\n- **OLS estimation**:\n  - Minimizes the unweighted squared residuals:\n    $$\n    \\min_\\beta \\| y - X\\beta \\|_2^2\n    $$\n  - Assumes all observations are equally informative.\n  - The projection balances the residuals across the entire domain, including high-noise regions.\n\n- **GLS estimation**:\n  - Minimizes a **precision-weighted** residual norm:\n    $$\n    \\min_\\beta (y - X\\beta)^\\top W (y - X\\beta)\n    $$\n    where $W$ is a **positive definite diagonal matrix** with weights inversely proportional to the noise variance.\n  - Observations with lower noise (on the left) are **more heavily weighted**.\n  - The resulting projection is **skewed toward the low-variance region**, yielding a steeper estimated slope.\n\n#### Interpretation\n\nEven though both methods fit a linear model, they differ fundamentally in the **geometry of projection**:\n- **OLS** projects onto the column space of $X$ using the **standard Euclidean inner product**.\n- **GLS** projects in a space where the residuals are measured using a **Mahalanobis-type norm**, effectively reshaping the loss geometry.\n\nThis difference becomes visually striking when we compare their fitted lines on heteroskedastic data — OLS follows the average trend, while GLS emphasizes the cleaner data and suppresses the noisy part.\n\n\n```{python}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulate data with heteroskedastic noise (to favor GMM adjustment)\nnp.random.seed(0)\nn = 100\nx = np.linspace(0, 10, n)\nX = np.vstack([np.ones(n), x]).T\n\n# True model\nbeta_true = np.array([1, 2])\n# Heteroskedastic noise: variance increases with x\nnoise_std = 0.5 + 1.5 * (x / x.max())  # ranges from 0.5 to 2.0\ny = X @ beta_true + np.random.normal(0, noise_std)\n\n# OLS estimation\nbeta_ols = np.linalg.inv(X.T @ X) @ X.T @ y\ny_hat_ols = X @ beta_ols\n\n# GMM weighting: inverse of variance (precision weighting)\nW = np.diag(1 / noise_std**2)\n\n# GMM estimation (optimal weighting under heteroskedasticity)\nXTWX = X.T @ W @ X\nXTWy = X.T @ W @ y\nbeta_gmm = np.linalg.inv(XTWX) @ XTWy\ny_hat_gmm = X @ beta_gmm\n\n# Plot with aspect ratio 1:1\nplt.figure(figsize=(8, 8))\nplt.scatter(x, y, color='lightgray', label='Observed data')\nplt.plot(x, y_hat_ols, label='OLS projection', color='blue', linewidth=2)\nplt.plot(x, y_hat_gmm, label='GMM projection (precision-weighted)', color='red', linestyle='--', linewidth=2)\nplt.title(\"OLS vs GMM Projection with Heteroskedastic Data\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.axis('equal')  # Set equal aspect ratio\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n```\n\n### Different Level set of objective function (quadratic form)\n\n| Method                | Objective Function                                                                 | Geometry of Norm         | Shape of Level Set         |\n|----------------------|--------------------------------------------------------------------------------------|---------------------------|-----------------------------|\n| **OLS (Unnormalized)** | $(y - X\\beta)^\\top (y - X\\beta)$                                              | Euclidean norm            | Ellipsoidal (if $X^\\top X \\neq I$) |\n| **OLS (Normalized)**   | $(\\beta - \\hat{\\beta})^\\top (X^\\top X)^{-1} (\\beta - \\hat{\\beta})$             | Mahalanobis norm          | Spherical               |\n| **GLS**               | $(y - X\\beta)^\\top W (y - X\\beta)$                                              | Mahalanobis norm on residuals | Ellipsoidal               |\n\n**Note**: \n\n- Although the GLS objective has the same mathematical form as GMM, its geometry is defined over the **residual space**, not over the space of moment conditions.  \n- The weighting matrix $W$ in GLS is typically $\\Sigma^{-1}$, the inverse of the error covariance matrix.\n\n\n```{python}\n# ===============================================\n# GLS vs OLS: Visualization of Quadratic Forms\n#\n# This code compares the level sets of:\n# (1) OLS objective (normalized via X'X)\n# (2) GLS objective with heteroskedastic weighting W\n#\n# Although GLS and GMM share a similar quadratic structure,\n# this code reflects GLS estimation:\n#   J(β) = (y - Xβ)' W (y - Xβ)\n# where W is a user-defined positive definite matrix (e.g., precision).\n#\n# The level sets visualize how the estimation geometry is altered\n# under different inner products: Euclidean vs. Mahalanobis.\n# ===============================================\n\n# Z-score normalization of x to improve XtX condition\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nx_normalized = scaler.fit_transform(x.reshape(-1, 1)).flatten()\nX_normalized = np.vstack([np.ones(n), x_normalized]).T\n\n# Recalculate OLS and GMM using normalized X\nbeta_ols_norm = np.linalg.inv(X_normalized.T @ X_normalized) @ X_normalized.T @ y\ny_hat_ols_norm = X_normalized @ beta_ols_norm\n\n# GMM estimation with same W\nXTWX_norm = X_normalized.T @ W @ X_normalized\nXTWy_norm = X_normalized.T @ W @ y\nbeta_gmm_norm = np.linalg.inv(XTWX_norm) @ XTWy_norm\ny_hat_gmm_norm = X_normalized @ beta_gmm_norm\n\n# New grid around beta_ols_norm\nb0_vals = np.linspace(beta_ols_norm[0] - 1, beta_ols_norm[0] + 1, 100)\nb1_vals = np.linspace(beta_ols_norm[1] - 1, beta_ols_norm[1] + 1, 100)\nB0, B1 = np.meshgrid(b0_vals, b1_vals)\nB_flat = np.vstack([B0.ravel(), B1.ravel()])\n\n# Normalized OLS objective (Mahalanobis)\nXtX_inv_norm = np.linalg.inv(X_normalized.T @ X_normalized)\ndelta_norm = B_flat - beta_ols_norm[:, None]\nJ_ols_normalized = np.einsum('ji,jk,ki->i', delta_norm, XtX_inv_norm, delta_norm).reshape(B0.shape)\n\n# GLS objective with normalized X\nJ_gmm_norm = []\nfor i in range(B_flat.shape[1]):\n    r = y - X_normalized @ B_flat[:, i]\n    obj = r.T @ W @ r\n    J_gmm_norm.append(obj)\nJ_gmm_norm = np.array(J_gmm_norm).reshape(B0.shape)\n\n# Plotting\nfig, axs = plt.subplots(1, 2, figsize=(14, 6))\n\n# Normalized OLS (should be spherical)\ncs1 = axs[0].contour(B0, B1, J_ols_normalized, levels=20, cmap='Blues')\naxs[0].plot(beta_ols_norm[0], beta_ols_norm[1], 'bo', label='OLS solution')\naxs[0].set_title(\"OLS (Normalized X): Spherical Level Sets\")\naxs[0].set_xlabel(r\"$\\beta_0$\")\naxs[0].set_ylabel(r\"$\\beta_1$\")\naxs[0].axis('equal')\naxs[0].legend()\naxs[0].grid(True)\n\n# GLS with normalized X\ncs2 = axs[1].contour(B0, B1, J_gmm_norm, levels=20, cmap='Reds')\naxs[1].plot(beta_gmm_norm[0], beta_gmm_norm[1], 'ro', label='GMM solution')\naxs[1].set_title(\"GMM (Normalized X): Ellipsoidal Level Sets\")\naxs[1].set_xlabel(r\"$\\beta_0$\")\naxs[1].set_ylabel(r\"$\\beta_1$\")\naxs[1].axis('equal')\naxs[1].legend()\naxs[1].grid(True)\n\nplt.tight_layout()\nplt.show()\n```\n\n\n\n","srcMarkdownNoYaml":"\n\n## 1. OLS as Orthogonal Projection in Euclidean Space\n\nOLS solves the following problem:\n\n$$\n\\hat{\\beta}_{OLS} = \\arg\\min_\\beta \\| y - X\\beta \\|_2^2 = (X^\\top X)^{-1} X^\\top y\n$$\n\n- This corresponds to projecting $y$ orthogonally onto the column space of $X$.\n- The residual $\\varepsilon = y - X\\hat{\\beta}$ satisfies:\n\n$$\nX^\\top \\varepsilon = 0\n$$\n\n### Inner Product and Geometry\n\n- The $L^2$ norm used in OLS is induced by the **standard Euclidean inner product**:\n\n$$\n\\langle u, v \\rangle = u^\\top v\n$$\n\n- The distance function becomes:\n\n$$\n\\| u \\|_2 = \\sqrt{u^\\top u}\n$$\n\n- The set of parameter values yielding equal error defines a level set (isocurve):\n\n$$\n\\{ \\beta \\mid \\| y - X\\beta \\|_2^2 = c \\} \\Rightarrow \\text{spheres in parameter space}\n$$\n\n- This reflects **Pythagorean geometry** — the isocurves are **circles (in 2D), spheres (in 3D), or hyperspheres** in higher dimensions when $X^\\top X = I$; otherwise, elliptical.\n- In this sense, **OLS performs Euclidean projection** onto a linear subspace using the standard inner product.\n\n\n## 2. Generalized Least Squares (GLS)\n\nGLS extends OLS by accounting for non-spherical error covariance structure:\n\n$$\n\\hat{\\beta}_{GLS} = \\arg\\min_\\beta (y - X\\beta)^\\top \\Sigma^{-1} (y - X\\beta)\n$$\n\n- Here, $\\Sigma$ is the covariance matrix of the errors.\n- This defines a new inner product over the residual space:\n\n$$\n\\langle u, v \\rangle_\\Sigma = u^\\top \\Sigma^{-1} v\n$$\n\n- The level sets:\n\n$$\n\\{ \\beta \\mid (y - X\\beta)^\\top \\Sigma^{-1} (y - X\\beta) = c \\} \\Rightarrow \\text{ellipsoids in parameter space}\n$$\n\n- This reflects how different directions in parameter space are penalized differently depending on the **variance and correlation structure of the errors**.\n\n> The **ellipsoidal level sets** visualized earlier correspond precisely to this GLS structure.\n\n\n## 3. GMM as Orthogonality of Moment Conditions (Not Projection)\n\nGMM generalizes the estimation framework by relying on **moment conditions**, not residuals:\n\n$$\n\\hat{\\theta}_{GMM} = \\arg\\min_\\theta \\left[ \\bar{g}_n(\\theta)^\\top W \\bar{g}_n(\\theta) \\right]\n$$\n\nwhere:\n\n- $\\bar{g}_n(\\theta) = \\frac{1}{n} \\sum_{i=1}^n g(Z_i, \\theta)$ is the sample mean of moment functions\n- $W$ is a **positive definite weighting matrix**, usually the inverse of the estimated variance of $\\bar{g}_n(\\theta)$\n\n### Clarifying the Inner Product\n\nAlthough GMM also defines a quadratic form like GLS, it operates in a fundamentally different space:\n\n- In GLS: $u, v$ are residual vectors\n- In GMM: $u, v$ are **moment function evaluations**\n\nThus, while one can write:\n\n$$\n\\langle g_i(\\theta), g_j(\\theta) \\rangle_W = g_i(\\theta)^\\top W g_j(\\theta)\n$$\n\nit is important to emphasize:\n\n> **GMM is not a projection-based method**. It minimizes the violation of moment orthogonality conditions in expectation.\n\n- The isocurves:\n\n$$\n\\{ \\theta \\mid \\bar{g}_n(\\theta)^\\top W \\bar{g}_n(\\theta) = c \\}\n$$\n\nmay also appear elliptical — but they are not projections, and should not be conflated with GLS geometrically.\n\n### Local Approximation and Connection to GLS\n\nIn some cases, GMM can be viewed as a **local GLS approximation**. If moment conditions are close to linear in $\\theta$, and $g(Z_i, \\theta) \\approx A(Z_i)(\\theta - \\theta_0)$, then the GMM objective becomes:\n\n$$\n(\\theta - \\theta_0)^\\top A^\\top W A (\\theta - \\theta_0)\n$$\n\nwhich resembles GLS — but only **locally and approximately**.\n\n> Therefore, the geometric intuition of ellipsoids applies more precisely to GLS. For GMM, it’s a useful visual aid — but structurally distinct.\n\n---\n\n## 4. Einstein Field Equations and GMM: Structural Analogy\n\nEinstein’s Field Equations (EFE) in general relativity:\n\n$$\nG_{\\mu\\nu} = R_{\\mu\\nu} - \\frac{1}{2} R g_{\\mu\\nu} = 8\\pi T_{\\mu\\nu}\n$$\n\nwhere:\n\n- $T_{\\mu\\nu}$: Stress-energy tensor, representing energy and matter (matter-energy distribution)\n- $G_{\\mu\\nu}$: Einstein tensor, encoding the curvature of spacetime (curvature)\n  - $R_{\\mu\\nu}$: Ricci curvature tensor\n  - $R$: Scalar curvature\n  - $g_{\\mu\\nu}$: Metric tensor, defining the inner product in spacetime and governing geodesics\n\n\n### Analogy to Estimation Frameworks\n\n| Feature | OLS | GLS | GMM | EFE (Physics) |\n|--------|-----|-----|-----|----------------|\n| Space | Euclidean | Covariance-weighted | Moment function space | Curved spacetime |\n| Inner product | $I$ | $\\Sigma^{-1}$ | $W$ (moment-weighted) | $g_{\\mu\\nu}$ (metric) |\n| Optimization | Projection | Weighted projection | Moment orthogonality | Energy-curvature balance |\n| Level sets | Circles | Ellipses | Ellipsoid-like | Lightcones / geodesics |\n\n### Lightcones and Degenerate Conics\n\nIn EFE, the metric tensor $g_{\\mu\\nu}$ defines how distances and angles are measured — it is the analogue of a positive definite kernel in GMM. The field equations determine how the geometry (curvature) of spacetime reacts to matter and energy. In this sense, **spacetime is optimized or shaped in response to external inputs**, just as GMM shapes its estimation space based on the kernel $W$ and moment functions.\n\nThe level sets in general relativity are often visualized as **lightcones** — the surface separating causal influence from spacelike separation. Geometrically, a lightcone can be interpreted as the **degenerate case of a conic section**, where the quadric form:\n\n$$\nQ(x) = x^\\top g_{\\mu\\nu} x = 0\n$$\n\nresults in a **pair of intersecting lines**: this represents all null (light-like) directions emanating from a point. These are the boundary cases between time-like and space-like intervals, analogous to the way ellipsoids in GMM collapse into degenerate forms under singular kernel matrices.\n\nThus, in both GMM and EFE, the **shape and degeneracy of level sets** encode deep information about the underlying structure — whether it is a statistical model or the geometry of spacetime.\n\n\n## 5. Conclusion\n\n- OLS performs **Euclidean projection** in a space endowed with the standard inner product.\n- GLS generalizes this using a **positive definite kernel** over the residual space, yielding ellipsoidal level sets.\n- GMM further generalizes the concept by applying positive definite weighting over **moment function space**, not residuals.\n- Geometric analogies (spheres and ellipsoids) are structurally valid for OLS and GLS, and visually helpful — but must be used with care in GMM.\n- All three frameworks share a unifying theme: **geometry induced by positive definite structure**.\n\n> “OLS and GLS estimate by projection. GMM estimates by aligning empirical moments. Einstein’s theory bends space to match mass — estimation theory bends geometry to match data.”\n\n\n## Visuals\n\n### OLS vs. GLS: Different Projections Under the Same Linear Model\n\nThis section visualizes how **OLS** and **GLS** (with heteroskedastic weighting) yield different estimators even under the same linear model structure. The key distinction lies in how each method defines **distance and importance** via its respective inner product.\n\n#### Simulation Setup\n\n- **Data-generating process**:\n  $$\n  y = \\beta_0 + \\beta_1 x + \\varepsilon\n  $$\n  where the noise term $\\varepsilon$ is **heteroskedastic** — its variance increases with $x$.  \n  That is, the reliability of observations decreases toward the right end of the domain.\n\n- **OLS estimation**:\n  - Minimizes the unweighted squared residuals:\n    $$\n    \\min_\\beta \\| y - X\\beta \\|_2^2\n    $$\n  - Assumes all observations are equally informative.\n  - The projection balances the residuals across the entire domain, including high-noise regions.\n\n- **GLS estimation**:\n  - Minimizes a **precision-weighted** residual norm:\n    $$\n    \\min_\\beta (y - X\\beta)^\\top W (y - X\\beta)\n    $$\n    where $W$ is a **positive definite diagonal matrix** with weights inversely proportional to the noise variance.\n  - Observations with lower noise (on the left) are **more heavily weighted**.\n  - The resulting projection is **skewed toward the low-variance region**, yielding a steeper estimated slope.\n\n#### Interpretation\n\nEven though both methods fit a linear model, they differ fundamentally in the **geometry of projection**:\n- **OLS** projects onto the column space of $X$ using the **standard Euclidean inner product**.\n- **GLS** projects in a space where the residuals are measured using a **Mahalanobis-type norm**, effectively reshaping the loss geometry.\n\nThis difference becomes visually striking when we compare their fitted lines on heteroskedastic data — OLS follows the average trend, while GLS emphasizes the cleaner data and suppresses the noisy part.\n\n\n```{python}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulate data with heteroskedastic noise (to favor GMM adjustment)\nnp.random.seed(0)\nn = 100\nx = np.linspace(0, 10, n)\nX = np.vstack([np.ones(n), x]).T\n\n# True model\nbeta_true = np.array([1, 2])\n# Heteroskedastic noise: variance increases with x\nnoise_std = 0.5 + 1.5 * (x / x.max())  # ranges from 0.5 to 2.0\ny = X @ beta_true + np.random.normal(0, noise_std)\n\n# OLS estimation\nbeta_ols = np.linalg.inv(X.T @ X) @ X.T @ y\ny_hat_ols = X @ beta_ols\n\n# GMM weighting: inverse of variance (precision weighting)\nW = np.diag(1 / noise_std**2)\n\n# GMM estimation (optimal weighting under heteroskedasticity)\nXTWX = X.T @ W @ X\nXTWy = X.T @ W @ y\nbeta_gmm = np.linalg.inv(XTWX) @ XTWy\ny_hat_gmm = X @ beta_gmm\n\n# Plot with aspect ratio 1:1\nplt.figure(figsize=(8, 8))\nplt.scatter(x, y, color='lightgray', label='Observed data')\nplt.plot(x, y_hat_ols, label='OLS projection', color='blue', linewidth=2)\nplt.plot(x, y_hat_gmm, label='GMM projection (precision-weighted)', color='red', linestyle='--', linewidth=2)\nplt.title(\"OLS vs GMM Projection with Heteroskedastic Data\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.axis('equal')  # Set equal aspect ratio\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n```\n\n### Different Level set of objective function (quadratic form)\n\n| Method                | Objective Function                                                                 | Geometry of Norm         | Shape of Level Set         |\n|----------------------|--------------------------------------------------------------------------------------|---------------------------|-----------------------------|\n| **OLS (Unnormalized)** | $(y - X\\beta)^\\top (y - X\\beta)$                                              | Euclidean norm            | Ellipsoidal (if $X^\\top X \\neq I$) |\n| **OLS (Normalized)**   | $(\\beta - \\hat{\\beta})^\\top (X^\\top X)^{-1} (\\beta - \\hat{\\beta})$             | Mahalanobis norm          | Spherical               |\n| **GLS**               | $(y - X\\beta)^\\top W (y - X\\beta)$                                              | Mahalanobis norm on residuals | Ellipsoidal               |\n\n**Note**: \n\n- Although the GLS objective has the same mathematical form as GMM, its geometry is defined over the **residual space**, not over the space of moment conditions.  \n- The weighting matrix $W$ in GLS is typically $\\Sigma^{-1}$, the inverse of the error covariance matrix.\n\n\n```{python}\n# ===============================================\n# GLS vs OLS: Visualization of Quadratic Forms\n#\n# This code compares the level sets of:\n# (1) OLS objective (normalized via X'X)\n# (2) GLS objective with heteroskedastic weighting W\n#\n# Although GLS and GMM share a similar quadratic structure,\n# this code reflects GLS estimation:\n#   J(β) = (y - Xβ)' W (y - Xβ)\n# where W is a user-defined positive definite matrix (e.g., precision).\n#\n# The level sets visualize how the estimation geometry is altered\n# under different inner products: Euclidean vs. Mahalanobis.\n# ===============================================\n\n# Z-score normalization of x to improve XtX condition\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nx_normalized = scaler.fit_transform(x.reshape(-1, 1)).flatten()\nX_normalized = np.vstack([np.ones(n), x_normalized]).T\n\n# Recalculate OLS and GMM using normalized X\nbeta_ols_norm = np.linalg.inv(X_normalized.T @ X_normalized) @ X_normalized.T @ y\ny_hat_ols_norm = X_normalized @ beta_ols_norm\n\n# GMM estimation with same W\nXTWX_norm = X_normalized.T @ W @ X_normalized\nXTWy_norm = X_normalized.T @ W @ y\nbeta_gmm_norm = np.linalg.inv(XTWX_norm) @ XTWy_norm\ny_hat_gmm_norm = X_normalized @ beta_gmm_norm\n\n# New grid around beta_ols_norm\nb0_vals = np.linspace(beta_ols_norm[0] - 1, beta_ols_norm[0] + 1, 100)\nb1_vals = np.linspace(beta_ols_norm[1] - 1, beta_ols_norm[1] + 1, 100)\nB0, B1 = np.meshgrid(b0_vals, b1_vals)\nB_flat = np.vstack([B0.ravel(), B1.ravel()])\n\n# Normalized OLS objective (Mahalanobis)\nXtX_inv_norm = np.linalg.inv(X_normalized.T @ X_normalized)\ndelta_norm = B_flat - beta_ols_norm[:, None]\nJ_ols_normalized = np.einsum('ji,jk,ki->i', delta_norm, XtX_inv_norm, delta_norm).reshape(B0.shape)\n\n# GLS objective with normalized X\nJ_gmm_norm = []\nfor i in range(B_flat.shape[1]):\n    r = y - X_normalized @ B_flat[:, i]\n    obj = r.T @ W @ r\n    J_gmm_norm.append(obj)\nJ_gmm_norm = np.array(J_gmm_norm).reshape(B0.shape)\n\n# Plotting\nfig, axs = plt.subplots(1, 2, figsize=(14, 6))\n\n# Normalized OLS (should be spherical)\ncs1 = axs[0].contour(B0, B1, J_ols_normalized, levels=20, cmap='Blues')\naxs[0].plot(beta_ols_norm[0], beta_ols_norm[1], 'bo', label='OLS solution')\naxs[0].set_title(\"OLS (Normalized X): Spherical Level Sets\")\naxs[0].set_xlabel(r\"$\\beta_0$\")\naxs[0].set_ylabel(r\"$\\beta_1$\")\naxs[0].axis('equal')\naxs[0].legend()\naxs[0].grid(True)\n\n# GLS with normalized X\ncs2 = axs[1].contour(B0, B1, J_gmm_norm, levels=20, cmap='Reds')\naxs[1].plot(beta_gmm_norm[0], beta_gmm_norm[1], 'ro', label='GMM solution')\naxs[1].set_title(\"GMM (Normalized X): Ellipsoidal Level Sets\")\naxs[1].set_xlabel(r\"$\\beta_0$\")\naxs[1].set_ylabel(r\"$\\beta_1$\")\naxs[1].axis('equal')\naxs[1].legend()\naxs[1].grid(True)\n\nplt.tight_layout()\nplt.show()\n```\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"number-sections":false,"output-file":"regressions.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.42","author":"gitSAM","jupyter":"python3","theme":"cosmo","title":"From Linear to Curved Structures to Fit the Data","subtitle":"Inner Products, Projection, and the Role of Positive Definite Kernels","date":"2025-03-21","abstract":"This seminar note explores the geometric foundations of OLS, GLS, and GMM estimators through the lens of inner product spaces. We compare their respective objective functions as quadratic forms, analyze their level sets, and demonstrate how positive definite kernels reshape the geometry of estimation. By drawing parallels to Einstein's field equations, we highlight how the structure of estimation reflects deeper principles of information weighting and spatial deformation.","keywords":"OLS, GLS, GMM, inner product space, positive definite kernel, Mahalanobis norm, projection geometry, quadratic form, estimation theory, level set visualization, Einstein field equations, metric tensor, heteroskedasticity, moment conditions"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}