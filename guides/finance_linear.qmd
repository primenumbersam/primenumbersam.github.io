---
title: Linear Models for Asset Management
subtitle: Wrong and Bad
author: gitSAM
date: '2025-03-10'
format:
  html:
    code-fold: true
    toc: true
---

## Overview

The failure to rationally explain the excess returns for dominant firms reflects not merely model misspecification but a systematic failure to capture the fundamental nature of contemporary capitalism, where returns increasingly flow not as compensation for risk-bearing but as rents extracted through structural market advantages. This insight challenges the core assumptions of traditional asset management, which relies heavily on models like the Capital Asset Pricing Model (CAPM) or multi-factor frameworks to guide investment decisions. In an post-2008 era of quantitative easing (QE), ETF dominance, and mega-cap firms leveraging network effects, regulatory capture, and institutional protection to secure outsized profits, these models lead to suboptimal portfolio allocations. This study delves into the implications of this disconnect, critiques the limitations of conventional approaches, and proposes a data-driven alternative using stochastic dominance to better align asset management with the realities of modern markets.

The evolution of portfolio optimization and asset pricing reflects a tension between theory and empirics:

- **Markowitz (1952)**: Introduced mean-variance optimization, assuming normal returns and quadratic utility—elegant but empirically fragile.
- **CAPM (1964)**: Linked returns to market risk, but anomalies (e.g., size, value) prompted multi-factor models.
- **Fama-French (1992)**: Added factors, yet struggled with momentum and low-volatility effects.
- **Behavioral Finance**: Addressed inefficiencies but lacked a cohesive framework.

c.f. Tested Linear Models

- **Challenging Linear Factor Models**  
  - **Factor Selection**: Highlight multicollinearity (mkt_excess, macro_bm) and limited explanatory power.  
  - **Lasso Regression**: Apply regularization to select predictors for FF 10 industry portfolio excess returns.  
- **Challenging Betting Against Beta**  
  - **Out-of-Sample Performance**: Optimal assets have betas near 1, not high or low (SIC 10 portfolios).  
- **Challenging the traditional Mean-Variance weight Optimization**  
  - **Extreme Weights**: Unconstrained MV yields impractical results; constraints (e.g., no short sales) added.  
  - **Passive vs. Active**: Passive value-weighted portfolios outperform MV with lower costs.  
- **Challenging the Parametric Mean-Variance Weight Optimization**  
  - **Bayesian update** : Black and Litterman (1992)
  - **Momentum and Size Tilting**: Brandt et al. (2009) approach underperforms passive benchmarks.  


## Dual Approaches to Asset Management

In economics, a limited resource allocation involves relative price vectors ($P$) and quantity vectors ($Q$), often constrained by a structural inner product:

$$P \cdot Q = \text{constant}$$

With a convex or concave scalar function of $P$ or $Q$ which are complete and compact Euclidean spaces, this structure enables convex optimization tools. Duality in convex optimization posits that solving for $Q$ given $P$ (primal problem) or $P$ given $Q$ (dual problem) yields equivalent results under linear constraints, reflecting the Hahn-Banach theorem’s guarantee of consistent linear functionals.

1. **Asset Weight Optimization**:
   - **Goal**: Given $P$ (joint distribution of returns), find $Q$ (asset weights) to optimize an objective (e.g., maximize Sharpe ratio).
   - **Critique**: Modern Portfolio Theory (MPT) assumes elliptical distributions and time-separable utility (e.g., CRRA), which are unrealistic. Non-stationary or fat-tailed distributions (e.g., Cauchy) render moment estimates unreliable, and utility may not reflect diminishing marginal returns for future wealth.

2. **Asset Pricing Optimization**:
   - **Goal**: Given $Q$ (future cash flows), find $P$ (state prices) to satisfy no-arbitrage conditions via the Euler equation.
   - **Critique**: If risk-return tradeoffs break down (as with mega-caps), linear factor models derived from local approximations fail, undermining theoretical predictions.


## Limitations of Linear Asset Pricing Models

Traditional asset pricing theories operate on the fundamental premise that expected returns directly compensate for systematic risk exposure. However, the empirical reality of top-performing mega-cap stocks presents a profound contradiction: these entities often display disproportionate returns relative to their risk profiles. Consider Apple, which despite having relatively high beta compared to other mega-caps, has generated returns that consistently exceed what would be predicted by its systematic risk exposure alone.

### Statistical Limitations

The conventional approach to addressing anomalies involves extending traditional asset pricing models by incorporating additional factors. These extensions represent a fundamentally inadequate methodology as they merely perpetuate the linear structure of existing frameworks while appending explanatory variables. The standard multi-factor model takes the form:

$$E(R_i) = R_f + \beta_{i,1}F_1 + \beta_{i,2}F_2 + ... + \beta_{i,n}F_n + \alpha_i$$

Where $E(R_i)$ represents expected returns, $R_f$ is the risk-free rate, $\beta_{i,j}$ are factor loadings, and $F_j$ are risk factors. The proposed extensions for capturing phenomena such as network effects, winner-take-all dynamics, and regulatory capture simply add terms to this equation.

This approach has several critical flaws related to both model specification and parameter estimation.

Linear additive models inherently assume factor orthogonality, normal distribution of returns, and stable risk premiums—assumptions violated by the complex, non-linear relationships between institutional protection, market power, and returns. These models rely on the core statistical assumptions:

1. $E(\varepsilon_i) = 0$ (zero-mean residuals)
2. $Cov(F_j, \varepsilon_i) = 0$ (factors uncorrelated with residuals)
3. $Cov(F_j, F_k) = 0$ for $j \neq k$ (orthogonal factors)
4. $\varepsilon_i \sim N(0, \sigma^2)$ (normally distributed residuals)

Yet empirical tests would consistently reject these assumptions for mega-cap returns. A particularly problematic issue is multicollinearity between factors. When adding variables such as institutional protection, regulatory capture, and network effects to explain mega-cap performance, these factors often exhibit high correlation amongst themselves. This multicollinearity leads to unstable coefficient estimates, inflated standard errors, and ultimately, unreliable inference. For instance, a firm's market power is often highly correlated with its regulatory influence, making it difficult to disentangle their individual effects.

Moreover, adding qualitative variables like "network effects" to statistical models is particularly vulnerable to p-hacking, as these constructs can be operationalized in numerous ways. The improvement in in-sample fitting ($R^2$) often comes at the expense of out-of-sample prediction accuracy—a fundamental manifestation of the bias-variance tradeoff in mean squared error:

$$MSE(prediction) = Bias^2 + Variance + Irreducible\_Error$$

Adding complex factors to capture mega-cap outperformance typically increases model complexity, which may reduce in-sample bias but simultaneously increases estimation variance. This increased variance occurs because more complex models with additional parameters require more data for stable estimation and are more prone to overfitting noise rather than signal. This relationship is particularly problematic for stock returns, which violate ergodicity assumptions necessary for consistent parameter estimation.

Furthermore, traditional frameworks remain epistemologically constrained by their reliance on backward-looking statistical relationships. Consider the standard time-series approach to estimating factor models:

$$R_{it} = \alpha_i + \beta_{i,MKT}R_{mt} + \beta_{i,SMB}SMB_t + \beta_{i,HML}HML_t + \varepsilon_{it}$$

This specification assumes parameter stability over time ($\beta_{i,j,t} = \beta_{i,j,t+1}$), an assumption violated when institutional dynamics create structural breaks. Microsoft's evolving regulatory landscape demonstrates this challenge—its systematic risk profile has fundamentally changed as its market and political power have increased, rendering historical beta estimates increasingly irrelevant for forward-looking predictions.

### The Inverse Relationship Between Risk and Return for Dominant Firms

The conventional understanding of systematic risk requires fundamental reconceptualization. The Capital Asset Pricing Model posits:

$$E(R_i) = R_f + \beta_i(E(R_m) - R_f)$$

Where $\beta_i = \frac{Cov(R_i, R_m)}{Var(R_m)} = \rho_{i,m}\frac{\sigma_i}{\sigma_m}$ measures systematic risk exposure. This can be decomposed into correlation ($\rho_{i,m}$) and relative volatility components ($\frac{\sigma_i}{\sigma_m}$).

An empirical analysis of market data reveals a critical paradox for mega-cap stocks versus non-mega-cap stocks. Both categories typically exhibit similar correlation coefficients with the market ($\rho_{i,m} \approx 1$), indicating strong co-movement with overall market trends. Similarly, their expected returns ($E(R_i)$) are often comparable or even favor mega-caps. However, the crucial distinction emerges in their volatility profiles: mega-cap stocks consistently demonstrate lower idiosyncratic volatility ($\sigma_i^{mega} < \sigma_i^{non-mega}$) due to their institutional protection, diversified revenue streams, and market power.

This creates a fundamental contradiction in the risk-return paradigm. When calculating the Sharpe ratio or other risk-adjusted return metrics:

$$Sharpe_i = \frac{E(R_i) - R_f}{\sigma_i}$$

Mega-cap stocks consistently outperform on a risk-adjusted basis. This superior risk-adjusted performance cannot be reconciled with traditional asset pricing theory, which predicts that lower risk should be associated with lower returns. [The empirical reality](#a-big-small) suggests the opposite for market dominant stocks:

$$\frac{E(R_i^{mega}) - R_f}{\sigma_i^{mega}} > \frac{E(R_i^{non-mega}) - R_f}{\sigma_i^{non-mega}}$$

Despite comparable raw returns and market correlations, the risk-adjusted outperformance of mega-cap stocks directly contradicts the foundational risk-return trade-off of asset pricing theory. Amazon exemplifies this phenomenon—its volatility has decreased relative to smaller competitors as its market dominance has increased, yet its returns have remained exceptional, generating superior risk-adjusted performance that cannot be explained by traditional models.

### Market Efficiency and Structural Advantages

Market efficiency assumptions underlying traditional models presuppose:

$$E[R_{i,t+1} | \Omega_t] := E_t[R_{i,t+1}]$$

Where $\Omega_t$ represents the information set available at time $t$, and $E_t[\cdot]$ denotes expectation conditional on information at time $t$. However, the persistence of abnormal returns for dominant firms suggests either:

1. Information about institutional advantages is not fully incorporated in prices
2. These advantages create structural market inefficiencies that cannot be arbitraged away

Google's digital advertising dominance illustrates these dynamics. Its network effects create increasing returns to scale that can be modeled as:

$$Profit_t = f(MarketShare_{t-1})^{\gamma}$$

Where $\gamma > 1$ indicates increasing returns to scale. Traditional asset pricing models assume competitive markets where $\gamma \leq 1$, making them structurally incapable of capturing the valuation implications of dominant market positions.

The fundamental inadequacy of traditional models becomes evident when examining their predictive accuracy for top market-cap stocks. A comparison of realized returns versus CAPM-predicted returns for top 10 stocks from 2010-2020 shows systematic underestimation:

$$\alpha_i:= E(R_i) - [R_f + \beta_i(R_m - R_f)] > 0 \text{ for top 10 stocks}$$

This persistent alpha cannot be explained as compensation for omitted risk factors, as these firms typically enjoy reduced risk through institutional protection and market dominance.

This theoretical crisis demands not incremental additions to existing models but a comprehensive reconceptualization of capital market dynamics. The failure to accurately predict returns for dominant firms reflects not merely model misspecification but a systematic failure to capture the fundamental nature of contemporary capitalism, where returns increasingly flow not as compensation for risk-bearing but as rents extracted through structural market advantages.

A more appropriate specification might acknowledge the non-linear relationship between institutional position and returns:

$$E(R_i) = R_f + \beta_i(R_m - R_f) + g(MarketPower_i, InstitutionalProtection_i)$$

Where $g(\cdot)$ is a non-linear function capturing the complex relationship between market power, institutional protection, and returns—a relationship that fundamentally contradicts the risk-return paradigm underlying traditional asset pricing theory.

## Practical Asset Management

### Implications of Model Failure for Asset Management

Asset management involves making investment decisions to achieve financial objectives, such as maximizing returns or minimizing risk, typically by constructing portfolios based on expected returns and risk estimates derived from asset pricing models. However, the inadequacy of traditional models when applied to dominant firms—those "superstar" companies that capture disproportionate market value—creates significant practical challenges:

1. **Breakdown of Market Efficiency Assumptions**  
   Traditional models assume markets are efficient, with asset prices reflecting all available information. Yet, dominant firms consistently generate abnormal returns that persist over time, suggesting inefficiencies tied to structural advantages—like brand loyalty or data monopolies—that markets fail to fully price in. For asset managers, this means traditional models underestimate the value of these firms, leading to underweighting in portfolios.

2. **Inverse Risk-Return Dynamics**  
   Models like the CAPM posit a linear relationship where higher risk (measured as beta) yields higher expected returns. For dominant firms, however, the opposite often holds: their entrenched positions reduce idiosyncratic risk—through diversified revenue streams or regulatory buffers—while delivering superior returns. This inversion misleads asset managers into overestimating the risk of mega-cap stocks and underestimating their return potential, skewing portfolio optimization.

3. **Factor Instability and Omission**  
   Multi-factor models, such as those based on Fama-French factors, depend on stable relationships between risk factors (e.g., size, value) and returns. Yet, the market power of dominant firms evolves dynamically—through technological innovation or policy shifts—causing factor loadings to fluctuate. Moreover, these models lack factors that explicitly capture rents from structural advantages, leaving asset managers with incomplete tools to assess true risk-adjusted performance.

4. **Inability to Model Non-Linear Effects**  
   The linear structure of traditional models cannot accommodate the non-linear dynamics of contemporary capitalism, such as increasing returns to scale or tipping points in market dominance. For example, a firm like Amazon benefits from self-reinforcing network effects that amplify its returns beyond what risk-based models predict, rendering traditional allocations ineffective.

These shortcomings result in portfolios that miss out on the concentrated returns of dominant firms, which increasingly drive market performance. Asset managers relying on outdated frameworks risk underperformance in a landscape where structural advantages, not risk exposure, dictate success.


### A Data-Driven Alternative: Stochastic Dominance

To address these challenges, asset management must shift toward empirical, data-driven approaches that eschew the restrictive assumptions of traditional models. One such method is **stochastic dominance**, which evaluates assets by comparing their entire return distributions rather than relying on summary statistics like mean and variance. This approach offers a practical way to identify investments that align with the empirical realities of contemporary markets.

- **How It Works**  
  - **First-Order Stochastic Dominance (FSD)**: Asset A dominates Asset B if its cumulative distribution function (CDF) lies below B’s for all return levels, meaning A offers higher returns across all outcomes.  
  - **Second-Order Stochastic Dominance (SSD)**: For risk-averse investors, A dominates B if the area under A’s CDF (its integral) is less than B’s up to any point, indicating a better risk-return tradeoff.  

- **Advantages**  
  - **Robustness**: Unlike mean-variance optimization, stochastic dominance requires no assumptions about return distributions (e.g., normality), making it suitable for markets with fat tails or skewness—common in mega-cap stock performance.  
  - **Empirical Focus**: By analyzing historical return distributions, it captures the real-world impact of structural advantages, such as the persistent outperformance of firms like Apple or Microsoft.  
  - **Flexibility**: It accommodates diverse investor risk preferences without specifying a utility function, broadening its applicability.  

- **Practical Implementation**  
  Asset managers can apply stochastic dominance by:  
  - **Historical Simulations**: Using past return data to estimate CDFs and calculate the likelihood of one asset dominating another.  
  - **Resampling**: Employing techniques like bootstrapping to test dominance under varying market conditions, enhancing robustness.  

- **Challenges**  
  - **Data Requirements**: Accurate CDF estimation demands extensive historical data, which may be scarce for newer firms or asset classes.  
  - **Non-Stationarity**: Return distributions shift over time—due to regulatory changes or competitive disruptions—limiting the reliability of historical dominance.  
  - **Portfolio Complexity**: Extending stochastic dominance to multi-asset portfolios is computationally intensive, often requiring simplifications or heuristic rules.  

Despite these hurdles, stochastic dominance provides a rigorous framework that sidesteps the theoretical pitfalls of traditional models. It allows asset managers to focus on observable performance, directly addressing the rent extraction that defines dominant firms.

### Complementary Tools

While stochastic dominance serves as a strong foundation, asset managers can bolster their strategies with additional tools to capture the nuances of contemporary markets:

- **Fundamental Analysis**  
   Detailed assessments of a firm’s financials, competitive moat (e.g., patents, scale), and industry trends can reveal structural advantages missed by quantitative models. For instance, analyzing Tesla’s innovation pipeline or Alphabet’s data ecosystem offers insights into their sustained dominance.

- **Machine Learning**  
   Advanced algorithms can detect non-linear patterns in returns or predict shifts in market power, though they require careful tuning to avoid overfitting and ensure interpretability.

- **Liquidity Considerations**  
   Mega-cap stocks typically offer high liquidity, reducing transaction costs and making them practical choices for large portfolios—a factor stochastic dominance alone might not prioritize.

Moreover, the concentration of returns in a few dominant firms suggests that a focused investment strategy—overweighting current mega-caps or scouting emerging leaders—may outperform broad diversification. However, identifying future dominants demands foresight into technological, regulatory, and economic trends, adding complexity to the process.

### Conclusion

In a world where returns stem from structural market advantages rather than risk-bearing, traditional asset pricing models fail to guide effective asset management. Their inability to predict the performance of dominant firms—rooted in flawed assumptions about efficiency, risk, and linearity—leads to misallocated portfolios that undervalue mega-cap opportunities. A data-driven approach using stochastic dominance offers a compelling alternative, leveraging empirical return distributions to identify superior investments without theoretical baggage. While challenges like data demands and non-stationarity persist, combining this method with fundamental analysis and modern tools equips asset managers to navigate contemporary capitalism. By embracing flexibility and empiricism, asset management can better capture the rents that define today’s markets, delivering superior outcomes in an era of concentrated dominance.
