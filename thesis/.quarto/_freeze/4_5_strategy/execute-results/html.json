{
  "hash": "1ca7856e6fc0384429073d327f375b8f",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 05 Strategy\nexecute:\n  enabled: true\n---\n\n\n\n\n\n## Motivation and Market Philosophy\n\nTraditional asset pricing frameworks rest on no-arbitrage principles and risk-return tradeoffs, assuming that all assets exist to offer compensation for exposure to priced risks. Under such models, individual asset returns are interpreted as linear functions of sensitivities to systematic risk factors.\n\nIn contrast, the TBTF (Too Big To Fail) strategy is motivated by a fundamentally different view of financial markets under late-stage capitalism. We argue that the secondary stock market reflects persistent dominance by a small subset of firms whose market power is self-reinforcing. These firms attract new capital not due to risk-efficiency, but due to narrative-driven legitimacy and path-dependent concentration of initial endowment.\n\nThis motivates a strategy grounded not in diversification or risk exposure, but in capital persistence, market lock-in, and capital hierarchy.\n\n\n## Portfolio Selection Rule\n\nThe TBTF strategy is constructed under When-What–How framework:\n\n- **When**: look-back window (in-sample estimation) & look-forward window (rebalancing frequency)\n- **What**: asset universe (e.g. all US-listed stocks) & selection rule (e.g. top-n market cap)\n- **How**: asset weighting scheme (e.g. convex capital concentration fit)\n\n### Asset Universe\n\nOur strategy operates on the full set of U.S. listed common stocks traded on NYSE, NASDAQ, and AMEX. Stocks with non-positive market capitalization are excluded to avoid bankrupt or illiquid firms.\n\n::: {#daea5db7 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport sqlite3\ncon = sqlite3.connect(database=\"../../tbtf.sqlite\")\n\ncrsp = pd.read_sql_query(\n  sql=\"SELECT * FROM crsp\",\n  con=con,\n  parse_dates={\"date\"}\n)\n```\n:::\n\n\n### Ranking and State Formation\n\nEach month, firms are ranked by market capitalization. These rankings define discrete capital states (percentile bins). We focus on the top-decile (state = 10), selecting the top-$n$ firms at time $t$ based on market-cap rank. The baseline is $n=10$, with robustness checks for $n \\in \\{5, 7, 10, 15, 20\\}$.\n\nA Markovian state transition framework is imposed to capture the temporal dynamics of capital flow between ranked states. The top state exhibits persistent and asymmetric capital retention, supporting the capital lock-in hypothesis.\n\n## Asset Weighting Schemes\n\nWe evaluate several portfolio weighting methods:\n\n- **TBTF (Convex Structural Weighting)**:  \n  Capital weights are determined by in-sample estimates of the convex relationship between market-cap rank and capital share. For example, specifications can be :\n\n  - *Quadratic Form*: $w_i \\propto \\alpha + \\beta r_i + \\gamma r_i^2$\n  - *Exponential Form*: $w_i \\propto \\alpha e^{\\beta r_i}$\n  - see @sec-appendix for more\n  \n  Note that the exponential form ensures structural monotonicity.\n\n- **Value-Weighted (VW)**:  \n  Proportional to each firm’s market capitalization.\n\n- **Equal-Weighted (EQ)**:  \n  Uniform allocation across all selected assets.\n\n::: {#493368e5 .cell execution_count=2}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 현재 경로 기준으로 상위 디렉토리로 경로 추가\nimport sys, os\nsys.path.append(os.path.abspath('../..'))\n\nimport tbtf\n```\n:::\n\n\nTo empirically validate the structural weighting functions, we fit several models to the in-sample relationship between rank and capital share. The following plots show the cross-sectional fitting results based on a representative rebalance date.\n\n::: {#fe9f5636 .cell execution_count=3}\n``` {.python .cell-code}\nin_end = '2009-12-31'\nin_sample_months = 48\n\nprint('Snapshot at:', in_end)\nprint('Look-back period:', in_sample_months, 'months')\n\ndf_in_sample, _ = tbtf.split_in_out_sample(crsp, in_end, in_sample_months)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSnapshot at: 2009-12-31\nLook-back period: 48 months\n```\n:::\n:::\n\n\n::: {#ab0d8004 .cell execution_count=4}\n``` {.python .cell-code}\nfig, axes = plt.subplots(3, 2, figsize=(14, 15))\ntbtf.plot_quadratic_fit(df_in_sample, n=10, state=10, ax=axes[0, 0])\ntbtf.plot_exponential_fit(df_in_sample, n=10, state=10, ax=axes[0, 1])\ntbtf.plot_mean_fit(df_in_sample, n=10, state=10, ax=axes[1, 0])\ntbtf.plot_loess_fit(df_in_sample, n=10, state=10, ax=axes[1, 1])\ntbtf.plot_bayes_fit(df_in_sample, n=10, state=10, ax=axes[2, 0])\ntbtf.plot_spline_fit(df_in_sample, n=10, state=10, ax=axes[2, 1])\nfig.suptitle(f\"Fitted Capital Share Functions at {in_end}\", fontsize=16)\nplt.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![Empirical Fit at 2009-12-31](4_5_strategy_files/figure-html/cell-5-output-1.png){}\n:::\n:::\n\n\n::: {#67449f4b .cell execution_count=5}\n``` {.python .cell-code}\nin_end = '2023-12-31'\nin_sample_months = 48\n\nprint('Snapshot at:', in_end)\nprint('Look-back period:', in_sample_months, 'months')\n\ndf_in_sample, _ = tbtf.split_in_out_sample(crsp, in_end, in_sample_months)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSnapshot at: 2023-12-31\nLook-back period: 48 months\n```\n:::\n:::\n\n\n::: {#06c1d4f6 .cell execution_count=6}\n``` {.python .cell-code}\nfig, axes = plt.subplots(3, 2, figsize=(14, 15))\ntbtf.plot_quadratic_fit(df_in_sample, n=10, state=10, ax=axes[0, 0])\ntbtf.plot_exponential_fit(df_in_sample, n=10, state=10, ax=axes[0, 1])\ntbtf.plot_mean_fit(df_in_sample, n=10, state=10, ax=axes[1, 0])\ntbtf.plot_loess_fit(df_in_sample, n=10, state=10, ax=axes[1, 1])\ntbtf.plot_bayes_fit(df_in_sample, n=10, state=10, ax=axes[2, 0])\ntbtf.plot_spline_fit(df_in_sample, n=10, state=10, ax=axes[2, 1])\nfig.suptitle(f\"Fitted Capital Share Functions at {in_end}\", fontsize=16)\nplt.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![Empirical Fit at 2023-12-31](4_5_strategy_files/figure-html/cell-7-output-1.png){}\n:::\n:::\n\n\n## Rebalancing Frequency\n\nWe test fixed-interval rebalancing schemes:\n\n- Monthly (default)\n- Quarterly\n- Semiannual\n- Annual\n\nRebalancing frequency directly affects turnover and transaction cost implications. Monthly rebalancing is selected as the baseline to balance responsiveness with frictions.\n\n\n## Benchmark Portfolios\n\nFor external comparison, we use:\n\n- **Pre-2010 Index Benchmarks**: Dow Jones Industrial Average (^DJI), Nasdaq-100 (^NDX)\n- **Post-2010 Index ETFs**: \n  - DIA (ETF version of DJIA, State Street Corporation ETF. Fund inception: 1998/01/14), \n  - QQQ (ETF version of NDX. Nasdaq-100 ticker. the Invesco QQQ Trust. Fund inception: 1999/03/10), \n  - SPY (The SPDR S&P 500 ETF, Fund inception: 1993/01/22), \n  - VTI (Vanguard Total Stock Market ETF, Fund inception: 2001년 5월 24일), \n- **Post-2010 Style Portfolios**: Fama-French ME5 × PRIOR1/3/5, constructed with rolling monthly value- or equal-weighting\n\n\n## Strategic Rationale\n\nThe TBTF strategy is not merely a rule-based selection method. It reflects a structural argument that allocative efficiency in modern markets is compromised by persistent capital concentration. Instead of diversifying away idiosyncratic risk, markets are increasingly dominated by capital lock-in, hierarchy reinforcement, and narrative legitimacy.\n\nThis framework reconceptualizes the role of financial assets from carriers of risk to vehicles of structural dominance, and evaluates portfolio construction in light of this altered paradigm.\n\n\n## TBTF Strategy Pipeline\n\nThis appendix section outlines the modular structure of the TBTF portfolio strategy, from input specification to out-of-sample return generation and performance evaluation. The pipeline is designed to be flexible across different weighting schemes, rebalance frequencies, and strategy parameters.\n\n### Inputs (Arguments)\n\n| Argument | Description |\n|----------|-------------|\n| `df` | Main input DataFrame (`crsp`) including fields such as `date`, `permno`, `mktcap`, `ret`, `state`, `mktcap_lag`, etc. |\n| `state_level` | Target capital state to define the asset universe, typically the top decile (e.g., `10`). |\n| `top_n` | Number of assets to be selected at each rebalance point (e.g., `n ∈ {5, 10, 20, 30, 50}`). |\n| `rebalance_freq` | Rebalancing interval (e.g., `'1M'`, `'3M'`, `'6M'`, `'12M'`). |\n| `weighting_method` | One of `'equal'`, `'value'`, `'quadratic'`, or `'exponential'`. |\n| `in_sample_period` | Estimation window for in-sample weight calibration (e.g., `'2010-01-01'` to `'2013-12-31'`). |\n| `out_sample_period` | Evaluation window for out-of-sample backtesting (e.g., `'2014-01-01'` to `'2023-12-31'`). |\n| `eta`, `p` | Parameters for CRRA utility (`eta`) and Omega ratio threshold (`p`). |\n\n\n### Strategy Pipeline (Pseudo-code Logic)\n\n#### Step 1: Data Partitioning & Filtering\n\n```python\nin_sample = df[(df['date'] >= in_sample_start) & (df['date'] <= in_sample_end)]\nout_sample = df[(df['date'] > in_sample_end) & (df['date'] <= out_sample_end)]\nuniverse = df[df['state'] == state_level]\n```\n\n- Restrict selection universe to target capital state (e.g., top decile).\n\n\n#### Step 2: In-sample Weight Estimation\n\nIf `weighting_method` is `'quadratic'` or `'exponential'`:\n\n- Estimate the relationship between within-state rank and capital share using in-sample data:\n  - Quadratic:  \n    $$\\text{CapShare}_i = \\alpha + \\beta \\cdot \\text{Rank}_i + \\gamma \\cdot \\text{Rank}_i^2$$\n  - Exponential:  \n    $$\\text{CapShare}_i = \\alpha \\cdot e^{\\beta \\cdot \\text{Rank}_i}$$\n- Save estimated coefficients:\n  ```python\n  coefficients = {'alpha': ..., 'beta': ..., 'gamma': ...}\n  ```\n\n#### Step 3: Out-of-sample Portfolio Construction\n\nFor each rebalance date in the out-sample period:\n\n1. Filter to target state (`state == state_level`)\n2. Rank by market cap, select top `n`\n3. Assign weights:\n   - Equal: $w_i = 1/n$\n   - Value: $w_i = \\text{mktcap}_i / \\sum \\text{mktcap}$\n   - Quadratic: apply $\\hat{\\alpha}, \\hat{\\beta}, \\hat{\\gamma}$ to rank\n   - Exponential: apply $\\hat{\\alpha}, \\hat{\\beta}$ to rank\n4. Store `[date, permno, weight]` for forward return application\n\n\n#### Step 4: Portfolio Return Computation\n\nAt each evaluation date after rebalance:\n\n- Merge forward returns of selected assets\n- Compute portfolio return:\n  $$R_t^{\\text{portfolio}} = \\sum_i w_{i,t} \\cdot r_{i,t}$$\n- Construct time series of out-of-sample portfolio returns\n\n\n#### Step 5: Turnover Calculation\n\nFor each rebalance window:\n\n- Calculate:\n  $$\\text{Turnover}_t = \\sum_i |w_{i,t} - w_{i,t-1}|$$\n- Useful for assessing transaction costs and liquidity requirements\n\n\n#### Step 6: Performance Evaluation\n\nUsing out-of-sample return series, compute:\n\n- **Risk-Adjusted Metrics**:\n  - Sharpe ratio, Sortino ratio\n  - Omega ratio (threshold = `p`)\n  - Calmar ratio, maximum drawdown (MDD)\n  - CRRA utility with risk aversion parameter `eta`\n\n- Return summary as dictionary:\n  ```python\n  metrics_dict = {\n      'Sharpe': ...,\n      'Sortino': ...,\n      'Omega': ...,\n      'CRRA': ...,\n      'CAGR': ...,\n      'MDD': ...\n  }\n  ```\n\n\n### Outputs\n\n| Output | Description |\n|--------|-------------|\n| `returns_df` | Time series of out-of-sample portfolio returns |\n| `weights_df` | Portfolio composition (weights per date and permno) |\n| `turnover_df` | Time series of turnover at each rebalance point |\n| `metrics_dict` | Dictionary of strategy performance metrics |\n\n### Module Lists\n\n```python\n#| label: TBTF Strategy Full Module\n#| warning: false\n#| message: false\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\n\n# ----------------------------\n# 1. Data Partitioning\n# ----------------------------\ndef split_in_out_sample(df, in_end, in_sample_months=36, out_end=None):\n    in_sample = df[(df['date'] <= in_end)].copy()\n    if out_end:\n        out_sample = df[(df['date'] > in_end) & (df['date'] <= out_end)].copy()\n    else:\n        out_sample = df[(df['date'] > in_end)].copy()\n    return in_sample, out_sample\n\n# ----------------------------\n# 2. Weight Estimation\n# ----------------------------\ndef estimate_exponential_weights(df_in_sample, n=10, state=10):\n    # Placeholder for full implementation\n    pass\n\ndef estimate_quadratic_weights(df_in_sample, n=10, state=10):\n    # Placeholder for full implementation\n    pass\n# ----------------------------\n# 3. Portfolio Construction\n# ----------------------------\ndef construct_tbtf_exponential(crsp_df, target_state, top_n, params):\n    # Placeholder for full implementation\n    pass\n\ndef construct_tbtf_quadratic(crsp_df, target_state, top_n, params):\n    # Placeholder for full implementation\n    pass\n\n# ----------------------------\n# 4. Return and Turnover Calculation\n# ----------------------------\ndef compute_return_tbtf(crsp, rebalance_dates, weighting_method='exponential', top_n=10, state=10, in_sample_months=36):\n    # Placeholder for full implementation\n    pass\n\ndef compute_return_pfo(crsp, rebalance_dates, weighting_method='vw', top_n=10):\n    # Placeholder for traditional VW or EW method\n    pass\n\ndef compute_turnover(weights_df, rebalance_dates):\n    # Placeholder for full implementation\n    pass\n\n# ----------------------------\n# 5. Performance Evaluation\n# ----------------------------\ndef evaluate_performance(returns, eta=3, p=0.01, periods_per_year=12):\n    # Placeholder for full implementation\n    pass\n\n# ----------------------------\n# 6. Rebalance Date Utility\n# ----------------------------\ndef get_rebalance_offset(rebalance_freq):\n    # Placeholder for full implementation\n    pass\n\n# ----------------------------\n# 7. Backtest Pipeline\n# ----------------------------\ndef backtest_pipeline(crsp, in_end, out_end, in_sample_months, rebalance_freq, weighting_method, top_n, state, eta, p):\n    # Placeholder for full backtest execution logic\n    pass\n\n# ----------------------------\n# 8. Visualization (optional)\n# ----------------------------\ndef plot_quadratic_fit(df_in_sample, n=10, state=10):\n    # Placeholder for plot generation\n    pass\n\ndef plot_exponential_fit(df_in_sample, n=10, state=10):\n    # Placeholder for plot generation\n    pass\n\n```\n\n\n## Appendix: Alternative Fitting Methods for Capital Share Functions {#sec-appendix}\n\n\n| Method        | Functional Form           | Statistical Interpretation        | Flexibility | Stability | Interpretability | Complexity |\n|---------------|---------------------------|-----------------------------------|-------------|-----------|------------------|------------|\n| Mean-Based    | Stepwise function          | Group-wise Sample Mean            | Low         | Low       | High             | Very Low   |\n| LOESS         | Local smoothing            | Nonparametric Local Regression    | Very High   | Medium    | Medium           | Moderate   |\n| Bayesian      | Global shrinkage estimator | Hierarchical Bayesian Model       | Medium      | High      | Medium           | Moderate–High |\n| Spline        | Smooth piecewise polynomial| Semi-parametric Regression        | High        | Medium    | Medium           | Moderate   |\n\n\n### Mean-Based Estimation (1st Moment per Rank)\n\n**Definition**  \nFor each rank $r \\in \\{1,\\dots,10\\}$, we estimate the average capital share over the in-sample period:\n$$\n\\hat{w}_r = \\frac{1}{T} \\sum_{t=1}^{T} w_{r,t}\n$$\n\n**Statistical Structure**\n\n- A fully nonparametric approach, treating each rank as a discrete category.\n- Equivalent to a **fixed effect model without regressors**, estimating the group-wise sample mean.\n\n**Theoretical Interpretation**\n\n- This is a form of **empirical histogram fitting**, relying only on the sample mean of observed capital shares.\n- The method does not require distributional assumptions and treats the sample average as representative, regardless of time variation or non-stationarity.\n\n**Advantages**\n\n- Simple, intuitive, and computationally efficient.\n- Highly flexible due to the absence of structural assumptions.\n\n**Limitations**\n\n- Very sensitive to variance and outliers in the capital share distribution at each rank.\n- No extrapolation or smoothing is applied across ranks.\n\n### LOESS (Locally Weighted Scatterplot Smoothing)\n\n**Definition**  \nCapital share is estimated via local regression, using a kernel-weighted average of nearby observations:\n$$\n\\hat{w}_r = \\sum_{j} K\\left(\\frac{r - j}{h}\\right) w_j\n$$\n- $K(\\cdot)$ is a kernel function (e.g., tricube); $h$ controls the bandwidth and smoothness.\n\n**Statistical Structure**\n\n- A classic **nonparametric regression** method.\n- LOESS typically uses local polynomial regressions (order 0, 1, or 2) for smoothing.\n\n**Theoretical Interpretation**\n\n- Constructs the overall rank–capshare curve as a composition of **local approximations**, rather than a single global function.\n- Highly sensitive to the bias-variance tradeoff depending on bandwidth $h$.\n\n**Advantages**\n\n- Effectively captures local irregularities in the rank–capshare mapping.\n- Very flexible, with no functional form assumptions.\n\n**Limitations**\n\n- Poor extrapolation performance at boundary ranks (e.g., rank 1 and 10).\n- Bandwidth selection has a significant effect on smoothing behavior.\n\n\n### Bayesian Smoothing (Hierarchical Empirical Bayes)\n\n**Definition**  \nEach rank-specific capital share $w_r$ is modeled as a noisy observation from a latent distribution:\n$$\nw_r \\sim \\mathcal{N}(\\theta_r, \\sigma_r^2), \\quad \\theta_r \\sim \\mathcal{N}(\\mu, \\tau^2)\n$$\n\n**Statistical Structure**\n\n- A **hierarchical Bayesian model** that allows ranks to share strength through a common prior.\n- In the empirical Bayes setting, the hyperparameters $(\\mu, \\tau^2)$ are estimated from data.\n\n**Theoretical Interpretation**\n\n- Stabilizes estimates for ranks with high variance by **shrinking** them toward the global prior.\n- Ranks near the center are barely adjusted, while outlier ranks are pulled closer to the mean.\n\n**Advantages**\n\n- Produces stable estimates even under noisy or volatile capital share data.\n- Allows for uncertainty quantification via posterior distributions.\n\n**Limitations**\n\n- Sensitive to prior or hyperparameter specification.\n- Implementation can be more complex than standard nonparametric methods.\n\n\n### Spline Smoothing (Piecewise Polynomial with Smooth Joints)\n\n**Definition**  \nThe rank–capshare function is approximated as a linear combination of spline basis functions:\n$$\n\\hat{w}_r = \\sum_{k} \\beta_k B_k(r)\n$$\n- $B_k(r)$ are spline basis functions (e.g., B-splines or natural splines).\n\n**Statistical Structure**\n\n- A **semi-parametric regression** approach.\n- The model is similar to linear regression but extended via basis transformation.\n\n**Theoretical Interpretation**\n\n- Balances **global fitting** and **local flexibility** by using piecewise polynomial segments with smooth joints.\n- The number and placement of knots (and the degree of polynomials) determine flexibility.\n\n**Advantages**\n\n- Produces smooth and differentiable fits across the rank domain.\n- Extrapolation is partially feasible, especially when using natural splines.\n\n**Limitations**\n\n- Performance is highly dependent on the selection of knots.\n- Can overfit if too many basis functions are used.\n\n",
    "supporting": [
      "4_5_strategy_files"
    ],
    "filters": [],
    "includes": {}
  }
}