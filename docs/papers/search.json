[
  {
    "objectID": "yield_curve.html",
    "href": "yield_curve.html",
    "title": "Yield curve",
    "section": "",
    "text": "채권 가격 (bond price)과 시장 금리(yield rate, 채권 수익률)는 역의 관계(inverse relationship)\n\n금리가 오르면 채권 가격은 하락하고,\n금리가 내리면 채권 가격은 상승한다.\n\n왜?\n\n시장 금리가 상승하면 기존의 낮은 금리를 가진 채권의 상대적 매력이 떨어지므로 가격이 하락하고,\n시장 금리가 하락하면 기존의 높은 금리를 가진 채권이 더 매력적으로 보이므로 가격이 상승한다.\n\n채권의 현재 가치(PV)를 수식으로 표현하면 다음과 같다.\n\\[\nP = \\sum_{t=1}^{T} \\frac{C}{(1 + r)^t} + \\frac{F}{(1 + r)^T}\n\\]\n여기서,\n- \\(P\\) : 채권 가격 (Present Value)\n- \\(C\\) : 매년 지급되는 쿠폰 이자 (Coupon Payment)\n- \\(F\\) : 만기 시 원금(Face Value)\n- \\(r\\) : 시장 금리 (Yield Rate)\n- \\(T\\) : 잔존 만기\n시장금리(\\(r\\))가 증가하면 할인율이 커져서 내가 가진 채권의 현재 가치(\\(P\\))가 낮아짐\n\n\n가정:\n\n채권의 액면가: 1,000달러\n쿠폰 이자율: 5% (연간)\n만기: 3년\n현재 시장 금리: 5% → 2% (3% 하락)\n\n경기불황 때문에 기준금리가 내려가서 시장 금리가 하락하면,\n새로 발행되는 2% 금리의 채권보다 기존 5%의 쿠폰을 지급하는 채권이 더 매력적으로 보이므로,\n기존 채권의 가격이 상승한다.\n\n\n\n채권 금리는 단순히 하나의 고정된 값이 아니라, 만기에 따라 다르게 형성됨.\n\n단기 채권(Short-term bonds, 1년 이하): 단기 금리는 중앙은행의 정책금리(예: 연준의 FFR, 한국은행의 기준금리)와 직접적으로 연결됨.\n중기 채권(Mid-term bonds, 2~10년): 경제 성장률, 기대 인플레이션, 신용 위험 등에 영향을 받음.\n장기 채권(Long-term bonds, 10년 이상): 장기 기대 인플레이션, 국가 신용, 경기 사이클 등에 따라 수익률이 결정됨.\n\n\n\n\n채권은 듀레이션(Duration)이 길수록 금리 변화에 대한 가격 민감도가 커진다.\n- 장기 채권: 금리 변화에 더 민감\n- 단기 채권: 금리 변화에 덜 민감\n\n\n\nTerm Structure는 채권 시장에서 만기(term)에 따라 형성되는 금리 구조\nTerm Structure는 주로 Yield Curve(수익률 곡선)의 형태로 시각화된다.\n\n\n\n\n금리 상승 (Yield 상승) 예상 → 채권 가격 하락 → 기존 장기 채권 보유자는 손해. 기존 장기 채권을 매도하고, 새로운 금리가 반영된 채권을 매수. 기준금리 상승때문에 단기 채권은 괜찮음.\n금리 하락 (Yield 하락) 에상 → 채권 가격 상승 → 기존 장기 채권 보유자는 이득. 기존 채권을 보유하여 가격 상승에 따른 자본 이득(capital gain) 확보. 장기 채권을 보유하는 것이 유리.",
    "crumbs": [
      "Apps",
      "금융",
      "Yield curve"
    ]
  },
  {
    "objectID": "yield_curve.html#채권-가격과-시장-금리의-관계",
    "href": "yield_curve.html#채권-가격과-시장-금리의-관계",
    "title": "Yield curve",
    "section": "",
    "text": "채권 가격 (bond price)과 시장 금리(yield rate, 채권 수익률)는 역의 관계(inverse relationship)\n\n금리가 오르면 채권 가격은 하락하고,\n금리가 내리면 채권 가격은 상승한다.\n\n왜?\n\n시장 금리가 상승하면 기존의 낮은 금리를 가진 채권의 상대적 매력이 떨어지므로 가격이 하락하고,\n시장 금리가 하락하면 기존의 높은 금리를 가진 채권이 더 매력적으로 보이므로 가격이 상승한다.\n\n채권의 현재 가치(PV)를 수식으로 표현하면 다음과 같다.\n\\[\nP = \\sum_{t=1}^{T} \\frac{C}{(1 + r)^t} + \\frac{F}{(1 + r)^T}\n\\]\n여기서,\n- \\(P\\) : 채권 가격 (Present Value)\n- \\(C\\) : 매년 지급되는 쿠폰 이자 (Coupon Payment)\n- \\(F\\) : 만기 시 원금(Face Value)\n- \\(r\\) : 시장 금리 (Yield Rate)\n- \\(T\\) : 잔존 만기\n시장금리(\\(r\\))가 증가하면 할인율이 커져서 내가 가진 채권의 현재 가치(\\(P\\))가 낮아짐\n\n\n가정:\n\n채권의 액면가: 1,000달러\n쿠폰 이자율: 5% (연간)\n만기: 3년\n현재 시장 금리: 5% → 2% (3% 하락)\n\n경기불황 때문에 기준금리가 내려가서 시장 금리가 하락하면,\n새로 발행되는 2% 금리의 채권보다 기존 5%의 쿠폰을 지급하는 채권이 더 매력적으로 보이므로,\n기존 채권의 가격이 상승한다.\n\n\n\n채권 금리는 단순히 하나의 고정된 값이 아니라, 만기에 따라 다르게 형성됨.\n\n단기 채권(Short-term bonds, 1년 이하): 단기 금리는 중앙은행의 정책금리(예: 연준의 FFR, 한국은행의 기준금리)와 직접적으로 연결됨.\n중기 채권(Mid-term bonds, 2~10년): 경제 성장률, 기대 인플레이션, 신용 위험 등에 영향을 받음.\n장기 채권(Long-term bonds, 10년 이상): 장기 기대 인플레이션, 국가 신용, 경기 사이클 등에 따라 수익률이 결정됨.\n\n\n\n\n채권은 듀레이션(Duration)이 길수록 금리 변화에 대한 가격 민감도가 커진다.\n- 장기 채권: 금리 변화에 더 민감\n- 단기 채권: 금리 변화에 덜 민감\n\n\n\nTerm Structure는 채권 시장에서 만기(term)에 따라 형성되는 금리 구조\nTerm Structure는 주로 Yield Curve(수익률 곡선)의 형태로 시각화된다.\n\n\n\n\n금리 상승 (Yield 상승) 예상 → 채권 가격 하락 → 기존 장기 채권 보유자는 손해. 기존 장기 채권을 매도하고, 새로운 금리가 반영된 채권을 매수. 기준금리 상승때문에 단기 채권은 괜찮음.\n금리 하락 (Yield 하락) 에상 → 채권 가격 상승 → 기존 장기 채권 보유자는 이득. 기존 채권을 보유하여 가격 상승에 따른 자본 이득(capital gain) 확보. 장기 채권을 보유하는 것이 유리.",
    "crumbs": [
      "Apps",
      "금융",
      "Yield curve"
    ]
  },
  {
    "objectID": "yield_curve.html#yield-curve수익률-곡선",
    "href": "yield_curve.html#yield-curve수익률-곡선",
    "title": "Yield curve",
    "section": "2 Yield Curve(수익률 곡선)",
    "text": "2 Yield Curve(수익률 곡선)\nYield Curve는 만기별 수익률을 연결한 곡선으로, 주요 형태는 3가지:\n\n정상적인 형태(Normal Yield Curve): 장기 금리가 단기 금리보다 높음 → 경제 성장이 기대되는 경우\n역전된 형태(Inverted Yield Curve): 장기 금리가 단기 금리보다 낮음 → 경기 침체 신호\n평평한 형태(Flat Yield Curve): 단기와 장기 금리 차이가 거의 없음 → 경기 전환기\n\n이렇게 다양한 형태가 나타나는 이유를 설명하는 이론들에는, 금리 기대이론(Expectations Hypothesis), 유동성 프리미엄 이론(Liquidity Premium Theory), 시장 분할 이론(Market Segmentation Theory) 등이 있음.\nYield Curve(수익률 곡선)는 본질적으로 \\((\\text{만기}, \\text{yield})\\) 쌍에 대한 Conditional Expectation Model을 적용한 결과라고 볼 수 있다.\n채권 시장에서 다양한 만기의 국채(예: 1년, 2년, 5년, 10년, 30년 등)의 수익률 데이터를 수집하면, 기본적으로 (만기, yield) 쌍의 산점도(scatter plot)를 얻을 수 있다.\n이후, Yield Curve는 이러한 데이터를 조건부 기대값 모형 (Conditional Expectation model)을 사용하여 스무딩(Smoothing)하거나 추정(Fitting)한 것이라고 볼 수 있다.\n수익률 곡선을 추정하는 대표적인 방법으로 다음과 같은 모델들이 있다.\n\n2.1 Polynomial Regression\n\n가장 기본적인 방법은 2차 또는 3차 다항식을 사용하여 스무딩한 곡선을 추정.\n\\[\n\\mathbb{E}[Y | T] = \\beta_0 + \\beta_1 T + \\beta_2 T^2 + \\beta_3 T^3 + \\epsilon\n\\] 여기서:\n\n\\(T\\)는 채권의 만기,\n\\(Y\\)는 수익률(Yield),\n\\(\\mathbb{E}[Y | T]\\)는 조건부 기대값.\n\n\n\n\n2.2 Spline Regression\n\nCubic Spline 또는 B-spline을 사용하여 여러 구간에서 스무딩된 Yield Curve를 만듬.\n\n\n\n2.3 Nelson-Siegel & Svensson model\n\n실무에서 많이 사용하는 Nelson-Siegel Model의 기본 식 \\[\nY(T) = \\beta_0 + \\beta_1 \\frac{1 - e^{-T/\\tau}}{T/\\tau} + \\beta_2 \\left(\\frac{1 - e^{-T/\\tau}}{T/\\tau} - e^{-T/\\tau} \\right)\n\\]\n여기서:\n\n\\(\\beta_0, \\beta_1, \\beta_2\\)는 모델의 파라미터\n\\(\\tau\\)는 시간 척도 조정 파라미터\n\\(T\\)는 만기 (term)\n\n\n\n\n2.4 Gaussian Process, Neural Networks\n\nGaussian Process Regression (GPR) 또는 딥러닝 모델(Neural Networks)을 활용하여 Yield Curve를 추정하는 방법",
    "crumbs": [
      "Apps",
      "금융",
      "Yield curve"
    ]
  },
  {
    "objectID": "yield_curve.html#code-scatter-plot-yield-curve",
    "href": "yield_curve.html#code-scatter-plot-yield-curve",
    "title": "Yield curve",
    "section": "3 Code: Scatter Plot + Yield Curve",
    "text": "3 Code: Scatter Plot + Yield Curve\n(만기, 수익률) 데이터 산점도를 그린 후, 스무딩된 Yield Curve를 적용.\n\n\nCode\n# Yield Curve(수익률 곡선)**는 본질적으로 (term,yield) 쌍에 대한 Conditional Expectation Model을 적용한 결과\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\n\n# 예제 데이터 (만기, 수익률)\nmaturity = np.array([1, 2, 3, 5, 7, 10, 20, 30])  # 만기 (years)\nyield_rates = np.array([3.2, 3.4, 3.5, 3.7, 3.8, 4.0, 4.2, 4.3])  # 수익률 (%)\n\n# Nelson-Siegel 모델 함수 정의\ndef nelson_siegel(T, beta0, beta1, beta2, tau):\n    return beta0 + beta1 * (1 - np.exp(-T/tau)) / (T/tau) + beta2 * ((1 - np.exp(-T/tau)) / (T/tau) - np.exp(-T/tau))\n\n# 초기값 설정 및 최적화\npopt, _ = curve_fit(nelson_siegel, maturity, yield_rates, p0=[4, -1, 1, 2])\n\n# 스무딩된 곡선 생성\nT_fit = np.linspace(0.5, 30, 100)  # 연속적인 만기 값\nY_fit = nelson_siegel(T_fit, *popt)\n\n# 산점도 및 수익률 곡선 그래프\nplt.figure(figsize=(10, 6))\nplt.scatter(maturity, yield_rates, color='blue', label=\"Observed Data (Scatter)\")\nplt.plot(T_fit, Y_fit, color='red', linestyle='-', label=\"Fitted Yield Curve (Nelson-Siegel)\")\nplt.xlabel(\"Maturity (Years)\")\nplt.ylabel(\"Yield (%)\")\nplt.title(\"Yield Curve: Scatter Plot with Nelson-Siegel Fit\")\nplt.legend()\nplt.grid(True)\nplt.show()",
    "crumbs": [
      "Apps",
      "금융",
      "Yield curve"
    ]
  },
  {
    "objectID": "structure_cointegration.html",
    "href": "structure_cointegration.html",
    "title": "Time-Varying Cointegration",
    "section": "",
    "text": "This study investigates whether key U.S. economic indicators exhibit time-varying cointegration and how structural breaks affect their long-run relationships. The analysis aims to:\n\nIdentify which variables are cointegrated in the long term.\nTrack how these relationships evolve over time.\nDetect and interpret structural breaks in equilibrium relationships.\nExtend beyond traditional models by using fractional and threshold cointegration frameworks.",
    "crumbs": [
      "Apps",
      "경제",
      "Time-Varying Cointegration"
    ]
  },
  {
    "objectID": "structure_cointegration.html#research-overview",
    "href": "structure_cointegration.html#research-overview",
    "title": "Time-Varying Cointegration",
    "section": "",
    "text": "This study investigates whether key U.S. economic indicators exhibit time-varying cointegration and how structural breaks affect their long-run relationships. The analysis aims to:\n\nIdentify which variables are cointegrated in the long term.\nTrack how these relationships evolve over time.\nDetect and interpret structural breaks in equilibrium relationships.\nExtend beyond traditional models by using fractional and threshold cointegration frameworks.",
    "crumbs": [
      "Apps",
      "경제",
      "Time-Varying Cointegration"
    ]
  },
  {
    "objectID": "structure_cointegration.html#preliminaries",
    "href": "structure_cointegration.html#preliminaries",
    "title": "Time-Varying Cointegration",
    "section": "2 Preliminaries",
    "text": "2 Preliminaries\n\nSpurious correlation vs. Cointegration: High correlation between non-stationary variables may be misleading unless the variables are cointegrated.\nCointegration: A linear combination of I(1) variables that is stationary (I(0)) implies a stable long-term equilibrium.\nInterpretation: Variables sharing a cointegration relationship tend to move together over time, despite individual stochastic trends.\nEmpirical Rule: In long-term data (e.g., &gt;10 years), a correlation coefficient &gt; 0.7 between I(1) series may suggest stable equilibrium.\nNelson & Plosser (1982): U.S. macroeconomic series often follow stochastic trends, cautioning against naive regression without testing for cointegration.",
    "crumbs": [
      "Apps",
      "경제",
      "Time-Varying Cointegration"
    ]
  },
  {
    "objectID": "structure_cointegration.html#research-questions",
    "href": "structure_cointegration.html#research-questions",
    "title": "Time-Varying Cointegration",
    "section": "3 Research Questions",
    "text": "3 Research Questions\n\nWhich economic indicators form long-term relationships?\nHow have these relationships changed over time?\nDo identified structural breaks correspond to major economic shocks (e.g., 2008, COVID-19, inflation)?",
    "crumbs": [
      "Apps",
      "경제",
      "Time-Varying Cointegration"
    ]
  },
  {
    "objectID": "structure_cointegration.html#data-and-preprocessing",
    "href": "structure_cointegration.html#data-and-preprocessing",
    "title": "Time-Varying Cointegration",
    "section": "4 Data and Preprocessing",
    "text": "4 Data and Preprocessing\nPeriod: January 1990 – December 2024 (34 years)\nFrequency: Monthly\n\n\n\n\n\n\n\n\n\n\nCategory\nVariable\nSource\nStart Year\nNotes\n\n\n\n\nEquity\nSPY, NDX\nYahoo Finance\n1993, 1985\nDaily/Monthly\n\n\nCurrency\nDXY\nFRED/Yahoo\n1973\nDaily/Monthly\n\n\nBonds\nFed Funds Rate, 10Y Treasury Yield\nFRED\n1954, 1953\nMonthly\n\n\nMoney Supply\nM2\nFRED\n1959\nMonthly\n\n\nCommodities\nGold Price\nYahoo\n1975\nDaily/Monthly\n\n\nInflation\nCPI\nFRED\n1947\nMonthly\n\n\nConsumption\nConsumer Sentiment\nFRED\n1978\nMonthly\n\n\nInvestment\nReal GPDIC1\nFRED\n1960\nOriginally quarterly; interpolated monthly",
    "crumbs": [
      "Apps",
      "경제",
      "Time-Varying Cointegration"
    ]
  },
  {
    "objectID": "structure_cointegration.html#methodology",
    "href": "structure_cointegration.html#methodology",
    "title": "Time-Varying Cointegration",
    "section": "5 Methodology",
    "text": "5 Methodology\n\n5.1 Testing for Cointegration\n\n5.1.1 Step 1: Unit Root Testing\n\nEnsure variables are I(1)\nMethods:\n\nADF Test\nPhillips-Perron Test\nADF-GLS (ERS)\nKPSS\n\n\n\n\n5.1.2 Step 2: Cointegration Existence\n\nApply only if variables are I(1)\nMethods:\n\nJohansen Test (multivariate)\nEngle-Granger Test (pairwise)\n\nIf cointegration fails: consider VAR or short-run models\n\n\n\n\n5.2 Detecting Structural Breaks\n\n5.2.1 Step 1: Break Detection in Traditional Cointegration\n\nMethods:\n\nBai-Perron Test\nQuandt-Andrews Test\nRolling Johansen Test\nCUSUM & CUSUMSQ Tests\n\n\n\n\n5.2.2 Step 2: Qualitative Mapping to Events\n\n\n\nBreakpoint\nLikely Cause\n\n\n\n\n2008-Q3\nGlobal Financial Crisis\n\n\n2011-Q3\nEuropean Debt Crisis\n\n\n2020-Q1\nCOVID-19 Shock\n\n\n2022-Q1\nInflation & Fed Rate Hikes\n\n\n\nOverlay structural breaks with macroeconomic shocks, policy shifts, and global market events.\n\n\n\n5.3 Fractional Cointegration Extension\n\n5.3.1 Step 1: Testing\n\nEstimate fractional differencing order (\\(d\\)) via:\n\nGPH Test\nRobinson Test\n\n\n\n\n5.3.2 Step 2: Detecting Breaks\n\nUse methods for long-memory models:\n\nRolling estimates of \\(d\\)\nWavelet-based structural break detection\nRolling Hurst exponent analysis\n\n\n\n\n\n5.4 Comparison of Breakpoints (Traditional vs. Fractional)\n\nCommon breakpoints strengthen the validity of structural shifts.\nTraditional: discrete shifts; Fractional: gradual long-memory transitions.\n\n\n\n5.5 Threshold Cointegration Models\n\n5.5.1 Step 1: Apply TECM\n\nEstimate threshold level (\\(\\gamma\\))\nModel: \\[\n\\Delta Y_t =\n\\begin{cases}\n\\alpha_1 (Y_{t-1} - \\beta X_{t-1}) + \\epsilon_t, & \\text{if } |Y_{t-1} - \\beta X_{t-1}| &gt; \\gamma \\\\\n\\alpha_2 (Y_{t-1} - \\beta X_{t-1}) + \\epsilon_t, & \\text{otherwise}\n\\end{cases}\n\\]\n\n\n\n5.5.2 Step 2: Interpret Regime-dependent Adjustments\n\nUse Sup-Wald test for significance\nEvaluate asymmetry in adjustment speeds (\\(\\alpha_1 \\ne \\alpha_2\\))\n\n\n\n\n5.6 Threshold Fractional Cointegration (TFECM)\n\n5.6.1 Step 1: Estimation\n\nCombine fractional differencing with threshold effects: \\[\n\\Delta Y_t =\n\\begin{cases}\n(1 - L)^{d_1} X_t + \\epsilon_t, & \\text{if } |X_t - \\beta Y_t| &gt; \\gamma \\\\\n(1 - L)^{d_2} Y_t + \\eta_t, & \\text{otherwise}\n\\end{cases}\n\\]\n\n\n\n5.6.2 Step 2: Interpretation\n\nCaptures memory-driven and threshold-based nonlinearity\nUse Sup LM test for threshold significance",
    "crumbs": [
      "Apps",
      "경제",
      "Time-Varying Cointegration"
    ]
  },
  {
    "objectID": "structure_cointegration.html#summary-evaluation",
    "href": "structure_cointegration.html#summary-evaluation",
    "title": "Time-Varying Cointegration",
    "section": "6 Summary Evaluation",
    "text": "6 Summary Evaluation\nStrengths: - Systematic, step-by-step progression from standard to advanced models - Combination of linear and nonlinear, short-memory and long-memory models - Identifies persistent shifts and gradual regime changes\nChallenges: - Data-driven thresholds may introduce bias - Advanced methods require substantial computational resources - Fractional and nonlinear models need theoretical grounding for interpretation\nConclusion: This framework offers a rigorous, flexible, and empirically grounded approach to studying evolving long-run relationships in macroeconomic data, with wide applications in investment strategy, economic forecasting, and policy evaluation.",
    "crumbs": [
      "Apps",
      "경제",
      "Time-Varying Cointegration"
    ]
  },
  {
    "objectID": "pricing_equal.html",
    "href": "pricing_equal.html",
    "title": "Market Price of Education",
    "section": "",
    "text": "In economically stratified societies, opportunities presented as equal—such as access to education or standardized qualifications—often yield vastly unequal outcomes across social classes. This paradox arises not from individual differences in merit, but from structural disparities in access to capital, networks, and strategic opportunities. As a result, the same formal credential may function as a gateway to ownership and control for one class, and to employment and dependency for another.\nWhile much of financial economics has focused on mispricings in asset markets, we consider an analogous but underexplored domain: the misvaluation of social opportunities under conditions of inherited inequality. In particular, we examine how intergenerational capital endowments determine the realized value of ostensibly equal qualifications.\nTo formalize this idea, we introduce a stylized model in which society is divided into two economic classes: the proletariat (P) and the capitalist (C). Members of both classes must meet an identical qualification standard (e.g., educational attainment) to realize their economic inheritance. However, the assets they stand to inherit differ fundamentally. While the capitalist class passes down productive capital and exclusive access to high-value opportunities, the proletariat transfers only general access to labor markets. In this framework, the qualification standard operates analogously to a strike price (\\(K\\)) in a call option: it must be met before the holder can claim the underlying asset.\nWe argue that this metaphor captures a critical structural asymmetry. Even when the qualification threshold is formally equal, the option value of meeting that threshold is class-contingent. The resulting inequality is not a failure of access, but a mispricing of opportunity rooted in structural capital segmentation. Our model offers a formal interpretation of this mechanism, drawing from asset pricing logic—specifically the Equivalent Martingale Measure (EMM)—to quantify how inherited capital shapes the payoff structure of educational credentials.",
    "crumbs": [
      "Apps",
      "경제",
      "Market Price of Education"
    ]
  },
  {
    "objectID": "pricing_equal.html#introduction",
    "href": "pricing_equal.html#introduction",
    "title": "Market Price of Education",
    "section": "",
    "text": "In economically stratified societies, opportunities presented as equal—such as access to education or standardized qualifications—often yield vastly unequal outcomes across social classes. This paradox arises not from individual differences in merit, but from structural disparities in access to capital, networks, and strategic opportunities. As a result, the same formal credential may function as a gateway to ownership and control for one class, and to employment and dependency for another.\nWhile much of financial economics has focused on mispricings in asset markets, we consider an analogous but underexplored domain: the misvaluation of social opportunities under conditions of inherited inequality. In particular, we examine how intergenerational capital endowments determine the realized value of ostensibly equal qualifications.\nTo formalize this idea, we introduce a stylized model in which society is divided into two economic classes: the proletariat (P) and the capitalist (C). Members of both classes must meet an identical qualification standard (e.g., educational attainment) to realize their economic inheritance. However, the assets they stand to inherit differ fundamentally. While the capitalist class passes down productive capital and exclusive access to high-value opportunities, the proletariat transfers only general access to labor markets. In this framework, the qualification standard operates analogously to a strike price (\\(K\\)) in a call option: it must be met before the holder can claim the underlying asset.\nWe argue that this metaphor captures a critical structural asymmetry. Even when the qualification threshold is formally equal, the option value of meeting that threshold is class-contingent. The resulting inequality is not a failure of access, but a mispricing of opportunity rooted in structural capital segmentation. Our model offers a formal interpretation of this mechanism, drawing from asset pricing logic—specifically the Equivalent Martingale Measure (EMM)—to quantify how inherited capital shapes the payoff structure of educational credentials.",
    "crumbs": [
      "Apps",
      "경제",
      "Market Price of Education"
    ]
  },
  {
    "objectID": "pricing_equal.html#literature-review",
    "href": "pricing_equal.html#literature-review",
    "title": "Market Price of Education",
    "section": "2 Literature Review",
    "text": "2 Literature Review\nThe theoretical foundation of this study builds on the limits to arbitrage framework articulated by Shleifer and Vishny (1997), which departs from textbook descriptions of riskless, capital-free arbitrage. In reality, most arbitrage is conducted by specialized professionals who manage other people’s capital. This agency relationship introduces a key fragility: interim losses—even in positions with positive expected returns—can lead principals to withdraw funding, forcing premature liquidation. As a result, arbitrage fails not simply due to capital constraints, but because of institutional frictions, reputational risk, and moral hazard embedded in the principal-agent structure. Gromb and Vayanos (2002) further formalize how capital-constrained arbitrageurs under agency costs produce persistent mispricings with adverse welfare implications. Complementary work by Xiong (2001) emphasizes that convergence trades can be destabilized when arbitrageurs face redemption risks and cannot extend their positions during periods of amplified mispricing.\nGeanakoplos (2010) contributes the concept of the leverage cycle, in which shifts in capital availability and margin requirements not only amplify financial volatility, but also entrench inequality. These dynamics resonate with the structural mechanisms examined in this paper, where economic opportunity is a function of both capital access and intertemporal credibility.\nBehavioral finance research, as surveyed by Barberis and Thaler (2003), highlights how cognitive biases and non-Bayesian updating further limit arbitrage effectiveness. These behavioral frictions—when coupled with institutional rigidities—exacerbate price anomalies and contribute to enduring structural barriers in economic mobility.\nBeyond finance, sociological theories and applied economic studies shed light on the broader mechanisms of structural inequality. Bourdieu’s theory of social reproduction Bourdieu (1973) posits that cultural capital—acquired through family and class—perpetuates unequal opportunity across generations. Empirical work by Chetty et al. (2014) establishes a strong linkage between parental income and children’s educational and labor market outcomes, confirming that equal qualifications yield vastly unequal economic returns depending on class background. Piketty (2014) emphasizes that capital accumulation and intergenerational wealth transfers dominate earned income in determining long-term economic trajectories.\nRecent reports by the UK’s Social Mobility Commission Social Mobility Commission (2023) and the OECD OECD (2018) corroborate this view, showing that economic mobility has stagnated despite rising access to education. Reeves (2017) introduces the concept of the “glass floor”, where the upper class protects its status not by accelerating upward mobility but by shielding itself from downward mobility—often through mechanisms such as inherited networks, cultural signaling, and capital buffers.\nTogether, these literatures converge on a central insight: nominally equal qualifications can mask radically unequal opportunity valuations, shaped by capital endowments, behavioral constraints, and institutional structures. This study extends the literature by providing a formal asset pricing model in which educational qualifications function as strike prices in class-contingent call options on inherited capital. The framework quantifies how structurally identical credentials yield asymmetric economic outcomes, offering a new perspective on the market valuation of education under inequality.",
    "crumbs": [
      "Apps",
      "경제",
      "Market Price of Education"
    ]
  },
  {
    "objectID": "pricing_equal.html#structural-model-of-opportunity-valuation",
    "href": "pricing_equal.html#structural-model-of-opportunity-valuation",
    "title": "Market Price of Education",
    "section": "3 Structural Model of Opportunity Valuation",
    "text": "3 Structural Model of Opportunity Valuation\n\n3.1 Social Stratification and Inherited Access\nWe consider a stylized economy consisting of two economically segregated classes:\n\nProletariat (P): Access only to general economic opportunities.\nCapitalist (C): Access to both general and exclusive special opportunities.\n\nWithin each class, opportunities are traded in perfectly competitive markets that satisfy the no-arbitrage condition internally. However, capital constraints prevent members of the proletariat from accessing the exclusive opportunities available to the capitalist class. This segmentation effectively enforces a limit to arbitrage between classes, rendering superior opportunities inaccessible to the working class.\nFrom a pricing perspective, while special opportunities may have equal or higher intrinsic value than general ones, their inaccessibility to the proletariat implies a market inefficiency rooted in capital immobility. In theory, the capitalist class might profit by arbitraging—shorting general opportunities and longing special ones. However, such trades require the existence of a counterparty market for general opportunities. In this stratified economy, such a market is absent: general opportunities, often tied to future labor or positional access, are not formally tradable. Hence, arbitrage fails not due to capital constraints, but due to market incompleteness.\nBoth classes transmit economic outcomes across generations. However, children must meet a common qualification threshold (e.g., educational degree) before they can claim their inherited assets. This threshold functions as a strike price (\\(K\\)) in a call option: until it is met, the offspring hold a contingent claim—a call option—on their parents’ accumulated capital.\nCrucially, although the qualification standard is uniform, its realized economic payoff differs dramatically. A graduate from the capitalist class inherits and operates substantial capital, while a similarly qualified graduate from the proletariat class becomes an employee within capitalist-owned institutions.\nThis section formalizes the option-like structure of intergenerational mobility, where equal credentials mask deeply unequal access to capital. The goal is to quantify this asymmetry through a structural valuation framework based on asset pricing theory.\n\n\n3.2 EMM and the Qualification as a Call Option\nWe consider a stylized one-period binomial framework with two possible future states—good (up) and bad (down). This minimal setting allows us to derive the present value of a call option under the Equivalent Martingale Measure (EMM). Under EMM, asset prices discounted at the risk-free rate evolve as martingales in a risk-neutral world, ensuring no-arbitrage pricing.\nLet \\(S_0\\) denote the current value of an asset and \\(R\\) the gross risk-free return over the period. Then the value of a European call option with strike price \\(K\\) at time 0, denoted \\(\\hat{f}_0\\), satisfies:\n\\[\n\\hat{f}_0 \\cdot R = \\mathbb{E}^q[f_1]\n\\]\nwhere the terminal payoff is:\n\\[\nf_1 = \\max(S_0 \\cdot U - K, 0)\n\\]\nwith \\(U\\) representing the up-state return. The EMM condition for the underlying asset implies:\n\\[\nS_0 \\cdot R = \\mathbb{E}^q[S_1]\n\\]\nSolving for the state price density (risk-neutral probability) \\(q\\) yields:\n\\[\nq = \\frac{R - D}{U - D}\n\\]\nAssuming the following normalization and notation:\n\nInitial Asset Price: \\(S_0 = 1\\)\nStrike Price (qualification threshold): \\(K = 1\\)\nUp-state return: \\(U\\)\nDown-state return: \\(D\\)\nGross risk-free return: \\(R\\)\nMaturity: 18 years (quarterly steps = 72 periods)\n\nwe derive the fair value of the option as:\n\\[\n\\hat{f}_0 = \\frac{(R - D)(U - 1)}{R(U - D)}\n\\]\nThis expression quantifies the present value of a future qualification-based claim, where the payoff depends on surpassing a common educational or institutional threshold. Crucially, while the strike price \\(K\\) is identical across individuals, the valuation of the option varies across economic classes, depending on the underlying asset and pricing measure.\nIn our broader framework, the underlying asset differs by class:\n\nFor the proletariat class (P), the risky asset is interpreted as human capital, with its return distribution proxied by wage growth rate. Their risk-free rate is taken to be the growth rate of real GDP per capita, reflecting limited access to financial instruments.\nFor the capitalist class (C), the risky asset corresponds to equity (e.g., stock ownership), and the risk-free asset is approximated by long-term bond yields. Unlike P class, the C class has access to capital markets and can realize financial returns conditional on qualifications.\n\nAlthough a continuous-time limit (e.g., Black-Scholes under \\(U \\cdot D = 1\\)) could be adopted, we retain the discrete binomial structure for its interpretability and ability to directly compare inter-class valuation asymmetries. The model makes transparent how seemingly equal qualifications can yield divergent option values, driven by class-specific capital endowments and asset accessibility.\n\n\n3.3 Empirical Valuation Across Classes\nTo quantify the asymmetry in opportunity valuation across social classes, we empirically estimate the EMM-based call option values for the proletariat and capitalist classes using historical U.S. data from Q1 1982 to Q4 2019 (152 quarterly observations). For each class, we identify distinct risky and risk-free asset proxies, reflecting their differential access to economic instruments.\nFor Proletariat (P) Class Children:\n\nReturn on Risky Asset: growth rate of U.S. Median usual weekly real earnings (LES1252881600Q)\nReturn on Risk-Free Asset: growth rate of U.S. Real GDP per capita (A939RX0Q048SBEA)\nEstimated Parameters:\n\n\\(U_p\\): 75th percentile of quarterly wage growth\n\\(D_p\\): 25th percentile of quarterly wage growth\n\\(R_p\\): Median quarterly growth rate of real GDP per capita\n\n\nFor Capitalist (C) Class Children:\n\nReturn on Risky Asset: growth rate of S&P 500 Total Return Index (SPX)\nReturn on Risk-Free Asset: 10-Year U.S. Treasury Bond Yield (DGS10)\nEstimated Parameters:\n\n\\(U_c\\): 75th percentile of equity return\n\\(D_c\\): 25th percentile of equity return\n\\(R_c\\):= Median of quarterly US 10-Year Treasury Bond Yield\n\n\nWe now compute and compare \\(\\hat{f}_0\\) for each class.\n\n\nYF.download() has changed argument auto_adjust default to True\n\n\nProletariat Class Children Parameters:\nU_p: 1.0060\nD_p: 0.9968\nR_p: 1.0051\n\nCapitalist Class Children Parameters:\nU_c: 1.0711\nD_c: 0.9916\nR_c: 1.0514\n\nFair price of Call Option, held by Proletariat Class Children:\n 0.3043\nFair price of Call Option, held by Capitalist Class Children:\n 0.9728\n\n\n\n\n\n\n\n\n\n\n\nAs \\(U\\) rises, improvements in \\(D\\) drive option values faster, reflecting how convex opportunity structures magnify marginal safety. This highlights the asymmetry in asset-based opportunity structures—capitalist-class children benefit not only from better average returns, but also from a sharper convex payoff structure that amplifies improvements across all economic states.\n\n\n3.4 Structural Implications\nEmpirical estimation reveals a stark asymmetry in the valuation of identical qualification thresholds across classes. The EMM-based fair value of the call option for proletariat-class children is approximately 0.3043, while for capitalist-class children it is 0.9728 — a more than threefold difference.\nThis disparity does not stem from differences in effort or merit, but from structurally unequal asset endowments and access to opportunity. In our framework, the same strike price \\(K = 1\\) results in class-specific valuations due to differing \\((U, D, R)\\) parameters. The qualification thus operates not as a universal gateway, but as a contingent claim whose value is conditioned by class.\nThis structural valuation gap challenges the premise of equal opportunity and underscores how asset-based class differentiation shapes the realized value of seemingly identical credentials. Our framework calls for a reassessment of how economic policy, educational equity, and social mobility are understood and modeled under persistent inequality.\n\n\n3.5 Conclusion\nThis paper reframes education not as an equalizing force, but as an option contract on inherited capital—one whose value varies sharply across economic classes. When access to assets and opportunity is structurally segmented, qualifications lose their universal function and become class-contingent claims. The findings highlight the need to rethink educational fairness, not merely in terms of formal access, but through the lens of the payoff structures such access yields in unequal economic environments.",
    "crumbs": [
      "Apps",
      "경제",
      "Market Price of Education"
    ]
  },
  {
    "objectID": "modern_matthew.html",
    "href": "modern_matthew.html",
    "title": "Modern Matthew Effect",
    "section": "",
    "text": "This report extends my empirical findings from the ME5 PRIOR research portfolios (of Fama and French) into a broader philosophical and socioeconomic interpretation. I draw connections between large-cap momentum dynamics in capital markets and foundational ideas from ethics, social choice theory, and political philosophy.\nThe core phenomenon observed—persistent outperformance among firms that are already the largest and most successful—bears striking resemblance to the biblical and sociological concept of the Matthew Effect:\n\n“For to every one who has, more will be given…” — Matthew 25:29\n\nIn financial markets, this is not merely a parable—it is measurable, structural, and persistent. It reveals that what appears to be a free, fair, and competitive economy may in fact be a self-reinforcing regime of privilege, cloaked under the language of freedom, equality, fairness, and consent—the founding pillars of liberal democracy.",
    "crumbs": [
      "Apps",
      "경제",
      "Modern Matthew Effect"
    ]
  },
  {
    "objectID": "modern_matthew.html#introduction",
    "href": "modern_matthew.html#introduction",
    "title": "Modern Matthew Effect",
    "section": "",
    "text": "This report extends my empirical findings from the ME5 PRIOR research portfolios (of Fama and French) into a broader philosophical and socioeconomic interpretation. I draw connections between large-cap momentum dynamics in capital markets and foundational ideas from ethics, social choice theory, and political philosophy.\nThe core phenomenon observed—persistent outperformance among firms that are already the largest and most successful—bears striking resemblance to the biblical and sociological concept of the Matthew Effect:\n\n“For to every one who has, more will be given…” — Matthew 25:29\n\nIn financial markets, this is not merely a parable—it is measurable, structural, and persistent. It reveals that what appears to be a free, fair, and competitive economy may in fact be a self-reinforcing regime of privilege, cloaked under the language of freedom, equality, fairness, and consent—the founding pillars of liberal democracy.",
    "crumbs": [
      "Apps",
      "경제",
      "Modern Matthew Effect"
    ]
  },
  {
    "objectID": "modern_matthew.html#structural-inequality-at-the-firm-level",
    "href": "modern_matthew.html#structural-inequality-at-the-firm-level",
    "title": "Modern Matthew Effect",
    "section": "Structural Inequality at the Firm level",
    "text": "Structural Inequality at the Firm level\nMy analysis of the ME5 PRIOR portfolios revealed the following:\n\nEven within the top 20% of firms by market capitalization (ME5), those that performed best in the prior 11 months (PRIOR5) tend to continue outperforming in the subsequent period.\nThis effect is particularly strong post-2010, aligning with macroeconomic changes such as quantitative easing, the rise of passive investing, and capital concentration.\nThis implies a reinforcing feedback loop: prior success draws more capital, more capital enables further success, and performance becomes increasingly decoupled from diversification.\n\nThe analogy to social mobility is immediate: those who have, get more—not just relatively, but absolutely and increasingly.",
    "crumbs": [
      "Apps",
      "경제",
      "Modern Matthew Effect"
    ]
  },
  {
    "objectID": "modern_matthew.html#the-matthew-effect-at-the-household-level",
    "href": "modern_matthew.html#the-matthew-effect-at-the-household-level",
    "title": "Modern Matthew Effect",
    "section": "The Matthew Effect at the Household Level",
    "text": "The Matthew Effect at the Household Level\nIf capital lock-in dynamics operate not only at the firm level but also across households—a plausible and empirically supported assumption—the implications are far-reaching:\n\nWealthy households benefit disproportionately from capital gains and passive income\nLower- and middle-income households rely predominantly on labor income, with limited upside\nThe result is a declining probability of upward mobility, and increasing wealth stratification\n\nThis structure undermines the ideal of a meritocratic liberal democracy where opportunity is equitably distributed and fairness is institutionalized. In reality, the capital system begins to resemble a rigged equilibrium, where the initial balance is illusionary—a symbolic counterweight with no real effect.",
    "crumbs": [
      "Apps",
      "경제",
      "Modern Matthew Effect"
    ]
  },
  {
    "objectID": "modern_matthew.html#the-philosophical-lens-arrow-spinoza-and-pascal",
    "href": "modern_matthew.html#the-philosophical-lens-arrow-spinoza-and-pascal",
    "title": "Modern Matthew Effect",
    "section": "The Philosophical Lens: Arrow, Spinoza, and Pascal",
    "text": "The Philosophical Lens: Arrow, Spinoza, and Pascal\nArrow’s utilitarian social welfare framework assumes at least some degree of fairness and fluidity among agents. But in a setting of structural inequality and declining mobility:\n\nThe expected utility of society does not necessarily increase, even if total capital grows\nThe bottom half of society may experience static or declining utility, overwhelmed by the persistent rise of the elite\n\nFrom a Spinozist viewpoint, this is a failure of a rational, self-sustaining society—it lacks balance and cohesion. For Pascal, it raises the existential irony of a system designed for all, yet serving few.\nAnd if we apply Rawls’ difference principle, we see an institutional failure to justify inequality based on benefit to the least advantaged. Liberal democracy, viewed from the perspective of financial capitalism, begins to fracture: the ideals of liberty, fairness, and equality become ceremonial rather than structural.",
    "crumbs": [
      "Apps",
      "경제",
      "Modern Matthew Effect"
    ]
  },
  {
    "objectID": "modern_matthew.html#toward-a-new-interpretation-of-market-performance",
    "href": "modern_matthew.html#toward-a-new-interpretation-of-market-performance",
    "title": "Modern Matthew Effect",
    "section": "Toward a New Interpretation of Market Performance",
    "text": "Toward a New Interpretation of Market Performance\nIn light of this, the ME5 PRIOR5 outperformance is no longer just a quantitative anomaly or a smart strategy. It is a signal—a structural symptom of capitalistic reinforcement, not unlike a planetary orbit deepening into a gravity well.\nThus, our empirical findings do more than guide portfolio construction. They challenge how we: - Model efficiency in capital markets - Measure welfare in society - Justify inequality under modern capitalism",
    "crumbs": [
      "Apps",
      "경제",
      "Modern Matthew Effect"
    ]
  },
  {
    "objectID": "modern_matthew.html#conclusion",
    "href": "modern_matthew.html#conclusion",
    "title": "Modern Matthew Effect",
    "section": "Conclusion",
    "text": "Conclusion\n\nThe Matthew Effect is not just a biblical principle—it is a lived, structural condition of our financial and social reality.\n\nThe capital markets amplify this dynamic, and unless counterbalanced by social, policy, or philosophical intervention, it threatens not only equity, but long-term stability and collective welfare.",
    "crumbs": [
      "Apps",
      "경제",
      "Modern Matthew Effect"
    ]
  },
  {
    "objectID": "modern_matthew.html#appendix-me5-prior-portfolios",
    "href": "modern_matthew.html#appendix-me5-prior-portfolios",
    "title": "Modern Matthew Effect",
    "section": "Appendix: ME5 PRIOR Portfolios",
    "text": "Appendix: ME5 PRIOR Portfolios\nIn this report, I examined the persistence of momentum within the top market-cap group (ME5) using the Fama-French 25 Portfolios sorted by size and prior return. Specifically, I analyzed how the performance of five portfolios (PRIOR1 to PRIOR5), sorted by prior 11-month returns within the ME5 group, has evolved between two distinct market regimes: pre-2010 and post-2010.\nThe results are striking.\n\nEven among the largest companies (top 20% by market cap), those that performed best over the prior year continued to outperform in the next month—both in the pre and post-2010 market regime.\n\nWhat initially seemed like a behavioral or statistical curiosity turns out to reflect a deeper transformation of the market’s internal structure—a transformation where capital flows, visibility, and narrative dominance now reinforce past winners among the already-rich. In this setting, the language of freedom, fairness, and opportunity—so essential to liberal democracy—becomes less about equal access and more about locked-in advantage. The market becomes a mirror, not of fair competition, but of Matthew Effect dynamics, where “to those who have, more shall be given.”\nFor risk-aware investors engaged in short-term, monthly rebalancing, the implications are concrete:\n\nPRIOR3 offers exceptional consistency with lower volatility and downside exposure\nPRIOR5 delivers superior absolute return with tolerable risk\nBoth are based on highly liquid, mega-cap stocks, making them operationally feasible and institutionally scalable\n\n\nIn short: Even among the richest firms, past winners tend to keep winning.\nMomentum is no longer a temporary anomaly. It is part of the architecture.\n\nThis realization challenges equilibrium-based pricing models and calls into question our normative assumptions about economic fairness. If capital accumulates in a non-reverting way among the already-powerful, mobility collapses—and with it, the utilitarian social welfare that undergirds Arrow’s idealized world of dynamic fairness. What remains is a hollow pendulum: a financial system that swings, but never truly balances.\n\nMethodology\nWe compute a comprehensive set of performance metrics (e.g., annualized return, Sharpe, Sortino, Calmar, Omega, expected CRRA utility) for each ME5 PRIOR portfolio across two structural periods:\n\nPre-2010: January 1996 to December 2009\nPost-2010: January 2010 to December 2023\n\nAll returns are monthly and expressed as decimals (net of compounding). Metrics are calculated for value-weighted portfolios of NYSE-listed firms within ME5, using prior 11-month cumulative returns (excluding the most recent month) to define momentum quintiles.\n\n\nEmpirical Summary and Interpretation\n\nPre-2010:\n\nPRIOR4 (moderate momentum) was most effective for risk-adjusted returns (Sharpe 0.66, Sortino 0.94)\n\nPRIOR5 had higher return, but more volatility and negative skew\n\nPRIOR1 severely underperformed (negative average return, deep drawdowns)\n\nPost-2010:\n\nMomentum dominance intensifies\n\nPRIOR5 exhibits top raw returns (13.7%), Sortino (1.41), and CRRA utility\n\nPRIOR3 shows best Sharpe (0.91) and second lowest volatility (15%)\n\nBoth dominate the risk-return landscape with resilience and predictability\n\n\nThe market is no longer cyclical, but directional; no longer egalitarian, but reinforcing. Passive capital flows, QE-induced yield suppression, and index-based exposure collectively sustain a regime where “winners win more, losers lag longer.” If the ideal of equal opportunity was once the balancing pendulum of a free market society, the modern financial system now swings in only one direction.\n\n\nME5 PRIOR Performance Comparison\n\n\nCode\nimport pandas as pd\nimport pandas_datareader.data as pdr\nimport numpy as np\nfrom scipy.stats import skew, kurtosis\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning) # FutureWarning 제거\npd.set_option('display.width', None)\npd.set_option('display.max_columns', 15)\n\n# (1) Load Fama-French data\nstart_date = '1996-01-01'\nend_date = '2023-12-31'\n\n# Define evaluation functions\np = 0.005 # 0.5% 월간 수익률 threshold\neta = 3 # CRRA risk-aversion parameter\n\nff = pdr.DataReader(\"25_Portfolios_ME_Prior_12_2\", \"famafrench\", start_date, end_date)\nff_df = ff[0]\n# print(ff[0].columns.tolist())\n\n# (2) Extract ME5 PRIOR portfolios and convert to net return\nme5_prior = pd.DataFrame({\n    'PRIOR1': ff_df['BIG LoPRIOR'],\n    'PRIOR2': ff_df['ME5 PRIOR2'],\n    'PRIOR3': ff_df['ME5 PRIOR3'],\n    'PRIOR4': ff_df['ME5 PRIOR4'],\n    'PRIOR5': ff_df['BIG HiPRIOR'],\n}) / 100  # convert % to net return\n\nme5_prior.index = me5_prior.index.to_timestamp()\n\n# (3) Split: pre-2010 vs post-2010\nsplit_date = pd.to_datetime(\"2010-01-01\")\nme5_prior_pre = me5_prior[me5_prior.index &lt; split_date]\nme5_prior_post = me5_prior[me5_prior.index &gt;= split_date]\n\n# (4) Metric definitions\ndef annualized_return(r): return (1 + r).prod()**(12 / len(r)) - 1\ndef annualized_volatility(r): return r.std() * np.sqrt(12)\ndef sharpe_ratio(r): return (r.mean() / r.std()) * np.sqrt(12)\ndef sortino_ratio(r): return (r.mean() / r[r &lt; 0].std()) * np.sqrt(12)\ndef omega_ratio(r, threshold=p):\n    gains = r[r &gt; threshold] - threshold\n    losses = threshold - r[r &lt; threshold]\n    return gains.sum() / losses.sum() if losses.sum() &gt; 0 else np.nan\ndef max_drawdown(r):\n    cum = (1 + r).cumprod()\n    peak = cum.cummax()\n    dd = (cum - peak) / peak\n    return dd.min()\ndef calmar_ratio(r):\n    ar = annualized_return(r)\n    mdd = max_drawdown(r)\n    return ar / abs(mdd) if mdd &lt; 0 else np.nan\ndef expected_crra(r, eta=eta):\n    x = 1 + r\n    util = (x**(1 - eta) - 1) / (1 - eta)\n    return util.mean()\ndef fisher_skewness(r): return skew(r, bias=False)\ndef excess_kurtosis(r): return kurtosis(r, fisher=True, bias=False)\n\nmetrics = {\n    \"Annualized Return\": annualized_return,\n    \"Annualized Volatility\": annualized_volatility,\n    \"Sharpe Ratio\": sharpe_ratio,\n    \"Sortino Ratio\": sortino_ratio,\n    \"Omega Ratio\": omega_ratio,\n    \"Calmar Ratio\": calmar_ratio,\n    \"Expected CRRA\": expected_crra,\n    \"Fisher Skewness\": fisher_skewness,\n    \"Excess Kurtosis\": excess_kurtosis\n}\n\n# (5) Evaluate for each period\ndef evaluate_metrics(df):\n    return pd.DataFrame({\n        port: {name: func(df[port]) for name, func in metrics.items()}\n        for port in df.columns\n    }).T\n\nresult_pre = evaluate_metrics(me5_prior_pre)\nresult_post = evaluate_metrics(me5_prior_post)\n\n\n\nPerformance Metrics for ME5 PRIOR Portfolios\n\n\n\n\n\n\nPerformance Metrics (Pre-2010)\n\n\n\nAnnualized Return\nAnnualized Volatility\nSharpe Ratio\nSortino Ratio\nOmega Ratio\nCalmar Ratio\nExpected CRRA\nFisher Skewness\nExcess Kurtosis\n\n\n\n\nPRIOR1\n-0.0138\n0.3050\n0.1064\n0.1498\n0.9271\n-0.0166\n-0.0092\n0.2258\n2.3524\n\n\nPRIOR2\n0.0604\n0.2042\n0.3896\n0.5606\n1.0790\n0.1015\n0.0014\n-0.0223\n2.0126\n\n\nPRIOR3\n0.0598\n0.1584\n0.4472\n0.5899\n1.0570\n0.1265\n0.0027\n-0.3123\n2.4509\n\n\nPRIOR4\n0.0877\n0.1435\n0.6603\n0.9439\n1.1981\n0.2205\n0.0052\n-0.5270\n0.9767\n\n\nPRIOR5\n0.0835\n0.1851\n0.5283\n0.7321\n1.1629\n0.1670\n0.0037\n-0.5999\n0.9929\n\n\n\n\n\n\n\n\n\n\n\n\n\nPerformance Metrics (Post-2010)\n\n\n\nAnnualized Return\nAnnualized Volatility\nSharpe Ratio\nSortino Ratio\nOmega Ratio\nCalmar Ratio\nExpected CRRA\nFisher Skewness\nExcess Kurtosis\n\n\n\n\nPRIOR1\n0.0954\n0.2640\n0.4775\n0.7371\n1.2210\n0.2337\n0.0016\n0.1463\n2.9558\n\n\nPRIOR2\n0.1378\n0.1727\n0.8370\n1.2415\n1.4502\n0.5904\n0.0082\n-0.1295\n1.3204\n\n\nPRIOR3\n0.1342\n0.1512\n0.9129\n1.3653\n1.4582\n0.5231\n0.0085\n-0.3910\n0.4723\n\n\nPRIOR4\n0.1232\n0.1500\n0.8526\n1.3902\n1.4047\n0.5853\n0.0078\n-0.0354\n0.4905\n\n\nPRIOR5\n0.1372\n0.1633\n0.8721\n1.4108\n1.4592\n0.5302\n0.0084\n-0.0665\n0.2892\n\n\n\n\n\n\n\nRisk-adjusted metrics highlight structural momentum persistence.\n\n\nPerformance comparison of ME5 PRIOR portfolios\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnnualized return, Sharpe ratio (r_f=0), and Expected CRRA (eta=3) show consistent momentum even among the top 20% in size.",
    "crumbs": [
      "Apps",
      "경제",
      "Modern Matthew Effect"
    ]
  },
  {
    "objectID": "market_conditions.html",
    "href": "market_conditions.html",
    "title": "Free Market Conditions",
    "section": "",
    "text": "This appendix examines the interdependence of key theoretical conditions in economics and finance: Perfectly Competitive Markets (PCM), the Capital Asset Pricing Model (CAPM), the Efficient Market Hypothesis (EMH), and the No-Arbitrage (NA) condition. While these concepts share common foundations in equilibrium and efficiency, they serve distinct purposes. Their structural assumptions determine how prices are formed, how risk is allocated, and whether arbitrage opportunities persist in the long run.",
    "crumbs": [
      "Apps",
      "경제",
      "Free Market Conditions"
    ]
  },
  {
    "objectID": "indicator_growth.html",
    "href": "indicator_growth.html",
    "title": "경제성장 대표지표?",
    "section": "",
    "text": "중앙은행이 대기업(‘고래’ 기업)들에게만 몰빵으로 현금 유동성을 공급하고, 이 기업들은 새로운 회사를 설립하고 그 자금을 투자에 사용한다. 투자된 돈이 중위 소득 상승과 소비로 연결되지 않고 다시 주식시장으로 유입되어 주가가 상승하면, 명목 GDP(Nominal GDP)는 증가할 수 있다. 하지만 이러한 명목 GDP 증가는 실제 경제 성장(real growth)을 반영하는 것일까?\n\n\n\n현금 유동성 공급으로 새롭게 만들어진 기업들이 소비자물가지수(CPI) 산정에 포함되지 않는 사업(B2B, 금융, AI, 데이터센터 등)을 한다면, CPI는 거의 변하지 않을 가능성이 매우 높다. 결과적으로, 명목 GDP 증가가 실질 GDP(Real GDP) 증가로 보이는 착시가 발생할 수 있다. 즉, 통계적으로 경제가 성장한 것처럼 보일 수 있지만, 실질적으로 대다수의 생활 수준은 나아지지 않아졌을 가능성이 매우 높다.\n\n\n\n공리주의 사회에서 경제 성장의 핵심 목표는 단순한 GDP 증가가 아니라 대다수 국민의 실질적 생활 수준 향상이다. 이를 더 잘 반영하는 지표는 Real GDP per capita 보다 Median of Real hourly Wage (시간당 실질 임금의 중위값) 이 될 수 있다.\n\nReal GDP per capita는 평균적인 경제 생산량 증가를 보여주지만, 실제로 국민 개개인의 소득 분배를 고려하지 않는다.\n\nReal Median hourly Wage는 국민 대다수가 체감하는 실질 소득의 변화를 나타내므로, 보다 정확한 경제 성장 지표가 될 수 있다.\n\n만약 명목 GDP가 증가하지만 시간당 중위임금이 정체되거나 하락한다면, 이는 대기업과 자산가 중심의 성장일 뿐, 일반 국민들에게 혜택이 돌아가지 않는 성장일 가능성이 크다.\n\n\n\n\n경제 성장의 지속 가능성을 예측하는 데 있어, 현재 사용되는 Natural GDP Growth Rate(자연 경제 성장률) 모델은 구조적인 문제를 가질 수 있다.\n- 이런 모델들은 과거 데이터를 기반으로 경제 성장률을 예측하지만, 금융시장 및 소비자 행동의 비선형적 변화를 제대로 반영하지 못한다.\n- 반면 Consumer Sentiment Index(소비자 심리 지수) 는 실시간으로 소비자들의 기대와 시장 심리를 측정한다.\n- 즉, 미래의 경제 성장 가능성을 예측하는 데 있어, 소비자들이 체감하는 경제 신뢰도와 구매 의향이 더 실질적인 지표가 될 수 있다.\n\n\n\n\n명목 GDP와 실질 GDP만으로 경제 성장을 평가하는 것은 한계가 있다.\n\nReal Median Wage가 상승하지 않는다면, GDP 성장은 일부 계층에만 혜택을 주는 비대칭적 성장일 가능성이 크다.\n경제 전망을 판단할 때, Consumer Sentiment Index와 같은 체감 지표가 전통적인 성장률 모델보다 더 현실적인 통찰을 제공할 수 있다.\n\n결국, 우리가 고민해야 할 것은 단순한 GDP 증가가 아니라 ’국민 개개인이 체감하는 실질적 경제 성장’이다.",
    "crumbs": [
      "Apps",
      "경제",
      "경제성장 대표지표?"
    ]
  },
  {
    "objectID": "indicator_growth.html#real-gdp-vs.-real-median-hourly-wage",
    "href": "indicator_growth.html#real-gdp-vs.-real-median-hourly-wage",
    "title": "경제성장 대표지표?",
    "section": "",
    "text": "중앙은행이 대기업(‘고래’ 기업)들에게만 몰빵으로 현금 유동성을 공급하고, 이 기업들은 새로운 회사를 설립하고 그 자금을 투자에 사용한다. 투자된 돈이 중위 소득 상승과 소비로 연결되지 않고 다시 주식시장으로 유입되어 주가가 상승하면, 명목 GDP(Nominal GDP)는 증가할 수 있다. 하지만 이러한 명목 GDP 증가는 실제 경제 성장(real growth)을 반영하는 것일까?\n\n\n\n현금 유동성 공급으로 새롭게 만들어진 기업들이 소비자물가지수(CPI) 산정에 포함되지 않는 사업(B2B, 금융, AI, 데이터센터 등)을 한다면, CPI는 거의 변하지 않을 가능성이 매우 높다. 결과적으로, 명목 GDP 증가가 실질 GDP(Real GDP) 증가로 보이는 착시가 발생할 수 있다. 즉, 통계적으로 경제가 성장한 것처럼 보일 수 있지만, 실질적으로 대다수의 생활 수준은 나아지지 않아졌을 가능성이 매우 높다.\n\n\n\n공리주의 사회에서 경제 성장의 핵심 목표는 단순한 GDP 증가가 아니라 대다수 국민의 실질적 생활 수준 향상이다. 이를 더 잘 반영하는 지표는 Real GDP per capita 보다 Median of Real hourly Wage (시간당 실질 임금의 중위값) 이 될 수 있다.\n\nReal GDP per capita는 평균적인 경제 생산량 증가를 보여주지만, 실제로 국민 개개인의 소득 분배를 고려하지 않는다.\n\nReal Median hourly Wage는 국민 대다수가 체감하는 실질 소득의 변화를 나타내므로, 보다 정확한 경제 성장 지표가 될 수 있다.\n\n만약 명목 GDP가 증가하지만 시간당 중위임금이 정체되거나 하락한다면, 이는 대기업과 자산가 중심의 성장일 뿐, 일반 국민들에게 혜택이 돌아가지 않는 성장일 가능성이 크다.\n\n\n\n\n경제 성장의 지속 가능성을 예측하는 데 있어, 현재 사용되는 Natural GDP Growth Rate(자연 경제 성장률) 모델은 구조적인 문제를 가질 수 있다.\n- 이런 모델들은 과거 데이터를 기반으로 경제 성장률을 예측하지만, 금융시장 및 소비자 행동의 비선형적 변화를 제대로 반영하지 못한다.\n- 반면 Consumer Sentiment Index(소비자 심리 지수) 는 실시간으로 소비자들의 기대와 시장 심리를 측정한다.\n- 즉, 미래의 경제 성장 가능성을 예측하는 데 있어, 소비자들이 체감하는 경제 신뢰도와 구매 의향이 더 실질적인 지표가 될 수 있다.\n\n\n\n\n명목 GDP와 실질 GDP만으로 경제 성장을 평가하는 것은 한계가 있다.\n\nReal Median Wage가 상승하지 않는다면, GDP 성장은 일부 계층에만 혜택을 주는 비대칭적 성장일 가능성이 크다.\n경제 전망을 판단할 때, Consumer Sentiment Index와 같은 체감 지표가 전통적인 성장률 모델보다 더 현실적인 통찰을 제공할 수 있다.\n\n결국, 우리가 고민해야 할 것은 단순한 GDP 증가가 아니라 ’국민 개개인이 체감하는 실질적 경제 성장’이다.",
    "crumbs": [
      "Apps",
      "경제",
      "경제성장 대표지표?"
    ]
  },
  {
    "objectID": "indicator_growth.html#부록-gdp-성장률의-한계와-대안적-경제지표",
    "href": "indicator_growth.html#부록-gdp-성장률의-한계와-대안적-경제지표",
    "title": "경제성장 대표지표?",
    "section": "2 부록: GDP 성장률의 한계와 대안적 경제지표",
    "text": "2 부록: GDP 성장률의 한계와 대안적 경제지표\n\n2.1 GDP 성장률의 측정과 한계\nGross Domestic Product(GDP)는 전통적으로 국가 경제 성장을 나타내는 핵심 지표로 활용되어 왔다. 그러나 최근 학계에서는 GDP 중심의 성장률 평가가 실제 국민의 삶의 질을 정확히 반영하지 못할 수 있다는 우려가 꾸준히 제기되고 있다(Kubiszewski et al., 2013; Stiglitz et al., 2009). 특히 금융 및 자산시장 중심의 성장 패턴이 강해질수록, GDP가 증가함에도 불구하고 소득 격차가 벌어지고 중산층의 생활 수준은 오히려 하락할 수 있다(Piketty, 2014).\n\nStiglitz, Sen, and Fitoussi (2009) 는 ’GDP가 단순한 생산물의 양적 증가만을 측정할 뿐, 불평등 심화와 실제 국민들이 체감하는 경제적 안녕(well-being)을 제대로 반영하지 못한다’는 문제를 공식적으로 제기한 바 있다. 이들은 GDP의 한계를 극복할 새로운 경제지표가 필요하다고 주장했다.\nCoyle (2014) 역시 GDP 수치 자체가 금융시장의 과열 현상으로 인해 실질적 경제 생산성 및 개개인의 복지 향상과 크게 괴리될 가능성을 지적했다. 그는 특히 금융시장의 투자 활동이 명목 GDP를 상승시키지만, 이러한 상승이 곧 일반 대중의 삶의 질 향상과 연결되지 않을 수 있음을 명시했다.\n\n\n\n2.2 대안적 경제지표로서의 실질 중위임금(Real Median Wage)\n경제학자들은 경제 성과 측정의 대안으로 소득분포의 중위값(median) 또는 중간층의 실질 소득 변화에 주목할 필요가 있다고 강조한다(Saez & Zucman, 2016; Chetty et al., 2017). 특히 중위임금(Median Wage)은 경제 성장이 국민 개개인의 생활 수준에 실제로 기여하는지를 명확히 나타내는 핵심적 지표로 주목받는다.\n\nChetty et al. (2017) 의 연구에 따르면, 미국 경제에서 GDP의 꾸준한 상승에도 불구하고 최근 30년간 실질 중위 소득이 정체된 사례가 발견되었으며, 이는 GDP 중심의 경제 성장이 반드시 대다수의 국민에게 이득을 가져다주지 않는다는 사실을 보여준다. 이 연구는 중위 소득을 핵심적 지표로 삼아 경제 정책의 우선순위를 재조정할 필요가 있다고 제안한다.\nSaez and Zucman (2016) 은 소득불평등의 증가가 GDP 성장률과 무관하게 진행되며, 중위 소득의 정체 또는 감소가 발생하면 경제 성장은 지속가능성을 잃게 된다고 경고한다. 그들은 중위 소득과 실질 생활 수준을 함께 고려하는 새로운 정책 프레임워크의 필요성을 강조한다.\n\n\n\n2.3 경제적 기대지표: Consumer Sentiment Index의 우월성\n경제 성장의 미래 전망을 평가할 때, 일반적으로 Central Bank 및 국제기구는 자연 경제 성장률(natural GDP rate)과 같은 구조적 모형 기반의 지표를 선호해왔다. 하지만 최근 연구들은 구조적 모형이 금융시장과 실물경제의 복잡한 상호작용을 잘 반영하지 못할 가능성이 있다고 지적한다(Sims, 2010; Coibion & Gorodnichenko, 2015). 이에 따라 소비자심리지수(Consumer Sentiment Index)가 경제 전망의 보다 정확한 선행지표로 주목받기 시작했다.\n\nCoibion and Gorodnichenko (2015) 는 소비자심리지수가 GDP 성장률, 특히 실질 소비지출의 미래 경로를 잘 예측하는 능력을 가졌음을 실증적으로 증명했다. 이 연구는 특히 불확실성이 높은 시기일수록 소비자심리지수가 공식적인 성장률 모형보다 경제 예측력에서 뛰어남을 보였다.\nSims (2010) 는 구조적 경제 예측 모형이 과거 데이터의 패턴에 과도하게 의존하여 금융 위기와 같은 비선형적 충격을 놓치는 경향이 있다고 지적했다. 반면 소비자 기대감은 그러한 경제적 충격을 보다 신속히 반영하며, 실물경제의 미래 변화에 대한 중요한 통찰력을 제공한다.\n\n\n\n2.4 시사점\n위 연구들의 공통된 결론은 명확하다. 전통적 GDP 성장률 측정 방식은 경제 성장과 국민 생활 수준 향상을 반드시 보장하지 않으며, 때로는 현실과 심각한 괴리를 일으킬 수 있다. 실질 중위임금(Real Median Wage)과 시간당 실질 중위임금 (Hourly Real Median Wage)은 국민들의 실제 생활수준 개선 여부를 평가하는 데 있어 GDP보다 더욱 정확한 지표이며, 미래 경제에 대한 소비자의 심리를 측정하는 소비자심리지수는 구조적 예측모델보다 현실적 통찰력을 제공한다.\n경제 정책은 GDP라는 숫자 증가에 매몰되지 않고, 개개인의 실질 생활수준과 소비자의 체감적 경제 기대감을 더 정확히 반영하는 지표 중심으로 전환해야 한다.",
    "crumbs": [
      "Apps",
      "경제",
      "경제성장 대표지표?"
    ]
  },
  {
    "objectID": "indicator_growth.html#참고문헌",
    "href": "indicator_growth.html#참고문헌",
    "title": "경제성장 대표지표?",
    "section": "3 참고문헌",
    "text": "3 참고문헌\n\nChetty, R., Grusky, D., Hell, M., Hendren, N., Manduca, R., & Narang, J. (2017). The fading American dream: Trends in absolute income mobility since 1940. Science, 356(6336), 398-406.\nCoibion, O., & Gorodnichenko, Y. (2015). Information Rigidity and the Expectations Formation Process: A Simple Framework and New Facts. American Economic Review, 105(8), 2644-2678.\nCoyle, D. (2014). GDP: A brief but affectionate history. Princeton University Press.\nKubiszewski, I., Costanza, R., Franco, C., Lawn, P., Talberth, J., Jackson, T., & Aylmer, C. (2013). Beyond GDP: Measuring and achieving global genuine progress. Ecological Economics, 93, 57-68.\nPiketty, T. (2014). Capital in the Twenty-First Century. Harvard University Press.\nSaez, E., & Zucman, G. (2016). Wealth inequality in the United States since 1913: Evidence from capitalized income tax data. Quarterly Journal of Economics, 131(2), 519-578.\nSims, C. A. (2010). Rational Inattention and Monetary Economics. Handbook of Monetary Economics, 3, 155-181.\nStiglitz, J., Sen, A., & Fitoussi, J. P. (2009). Report by the commission on the measurement of economic performance and social progress. Paris: Commission on the Measurement of Economic Performance and Social Progress.",
    "crumbs": [
      "Apps",
      "경제",
      "경제성장 대표지표?"
    ]
  },
  {
    "objectID": "gitsam_reading.html",
    "href": "gitsam_reading.html",
    "title": "gitsam 독후감",
    "section": "",
    "text": "성장과 분배의 균형? 1.",
    "crumbs": [
      "Apps",
      "경제",
      "gitsam 독후감"
    ]
  },
  {
    "objectID": "gitsam_reading.html#책들의-요점",
    "href": "gitsam_reading.html#책들의-요점",
    "title": "gitsam 독후감",
    "section": "책들의 요점",
    "text": "책들의 요점\n\nPascal (1670): “Human nature is full of contradictions; reason alone cannot guide us—faith and intuition must complement our understanding.”\n\nVoltaire (1759): “Unconditional optimism is dangerous; do not fall into the trap of blind optimism, but instead cultivate your own reality.”\n\nTwain (1881): “Social status is arbitrary; true worth is not determined by birth but by one’s character and actions.”\n\nFriedman (1980): “Economic freedom is essential for political freedom; individuals should have the power to make their own choices in the marketplace.”",
    "crumbs": [
      "Apps",
      "경제",
      "gitsam 독후감"
    ]
  },
  {
    "objectID": "gitsam_reading.html#the-dual-foundations-of-freedom",
    "href": "gitsam_reading.html#the-dual-foundations-of-freedom",
    "title": "gitsam 독후감",
    "section": "The Dual Foundations of Freedom",
    "text": "The Dual Foundations of Freedom\nPascal (1670) famously wrote, “Justice without power is inefficient; power without justice is tyranny.” Centuries later, Friedman (1980) asserted, “Economic freedom is essential for political freedom; individuals should have the power to make their own choices in the marketplace.”\nEven earlier, Voltaire (1759) warned: “Unconditional optimism is dangerous; do not fall into the trap of blind optimism, but instead cultivate your own reality.” And Mark Twain, in The Gilded Age (1873), portrayed an America full of opportunity, corruption, inequality, and performative democracy—a society gilded, not golden.\nAlthough these statements emerged from distinct historical contexts—Pascal’s Christian moralism, Voltaire’s skeptical humanism, Twain’s satirical realism, and Friedman’s classical liberal economics—they converge on a central insight: power without a just foundation leads not to flourishing but to systemic dysfunction. In this short essay I will try to unify their ideas and shows how modern capitalist democracies might fail when justice is disempowered and power is left unjustified. Unequal capital redistribution, I argue, is not merely a moral preference but a structural imperative.\nJustice is not merely a legal norm; it is the internal compass of action. Power, in contrast, is the external capacity to act. Justice gives purpose; power enables that purpose to manifest.\nPascal’s formulation remains as relevant today: justice without the means to enforce or embody it results in futility; power without ethical direction leads to exploitation. Efficiency, then, is not simply about optimal resource use but the moral alignment of intent and capacity. Only when both are present can human systems function with legitimacy.",
    "crumbs": [
      "Apps",
      "경제",
      "gitsam 독후감"
    ]
  },
  {
    "objectID": "gitsam_reading.html#the-economic-freedom",
    "href": "gitsam_reading.html#the-economic-freedom",
    "title": "gitsam 독후감",
    "section": "The Economic Freedom",
    "text": "The Economic Freedom\nFriedman anchors power in the economy. He contends that economic freedom—the ability to choose where to work, what to buy, and how to invest—is the foundation of meaningful political freedom. Political rights without economic power are ceremonial.\nIn his view, economic power must not be monopolized. Broad distribution of economic agency allows individuals to actualize their moral intentions in the marketplace. If wealth becomes too concentrated, democracy becomes formal but hollow. Here, Pascal and Friedman align: without justice, economic power fuels exclusion and control rather than autonomy and participation.",
    "crumbs": [
      "Apps",
      "경제",
      "gitsam 독후감"
    ]
  },
  {
    "objectID": "gitsam_reading.html#the-seesaw-of-justice",
    "href": "gitsam_reading.html#the-seesaw-of-justice",
    "title": "gitsam 독후감",
    "section": "The Seesaw of Justice",
    "text": "The Seesaw of Justice\nImagine a seesaw. The pivot point represents justice. Those historically deprived of economic opportunity sit close to the fulcrum and rise little, even as they strive. Those positioned far from it, endowed with capital and legacy, rise effortlessly.\nNature teaches that balance in systems often requires asymmetry. In physics, mechanical leverage works through inverse proportionality. Likewise, in society, fairness often demands unequal compensation to restore functional balance.\nModern capitalism has fixed the pivot unfairly, amplifying the advantages of the already-advantaged. Rebalancing the seesaw by repositioning the pivot is not artificial or utopian; it is a natural act of justice. Redistribution is thus not charity but structural correction.\nJustice is not neutral. It is inherently corrective. Treating unequals more unequally sustains more inequality. True justice restructures economic power to enable all individuals to exercise freedom meaningfully.",
    "crumbs": [
      "Apps",
      "경제",
      "gitsam 독후감"
    ]
  },
  {
    "objectID": "gitsam_reading.html#the-myth-of-meritocracy",
    "href": "gitsam_reading.html#the-myth-of-meritocracy",
    "title": "gitsam 독후감",
    "section": "The Myth of Meritocracy",
    "text": "The Myth of Meritocracy\nToday’s democracies too often wear the mask of equality while tolerating extreme inequality in capital ownership. Since markets allocate resources based on capital, those with more capital inherently wield more power—economic and political.\nThis gives rise to plutocratic freedom: a distorted system where only the wealthy enjoy actual choice. The ideal of freedom becomes a cover for entrenched hierarchy. The seesaw is tilted, the pivot artificially fixed, and justice reduced to ornamental language.\nHistory does not show that capital-concentrated growth naturally leads to widespread development. What it reveals, time and again, is regression—that trickle-down systems fail not through accident but by structural design. Blind optimism in such mechanisms is dangerous, as Voltaire cautioned.",
    "crumbs": [
      "Apps",
      "경제",
      "gitsam 독후감"
    ]
  },
  {
    "objectID": "gitsam_reading.html#toward-a-just-democracy",
    "href": "gitsam_reading.html#toward-a-just-democracy",
    "title": "gitsam 독후감",
    "section": "Toward a Just Democracy",
    "text": "Toward a Just Democracy\nWhat is needed is not utopian equality, but uneven yet fair redistribution. The point is not to flatten all outcomes but to raise the floor. Redistribution repositions the pivot on the seesaw to empower those structurally disadvantaged.\nIf freedom requires both justice and power, and if power today flows through capital, then capital redistribution becomes necessary for the moral function of democracy. Without it, freedom becomes a privilege of the few, and society drifts into gilded tyranny.\nJustice without power is impotent. Power without justice is violent. To preserve democratic integrity, power must be distributed as broadly as the moral will to act.\nPascal, Voltaire, Twain, and Friedman each warned, in their own language, against systems that confuse appearance with substance, and privilege with freedom. Their lessons remain urgent.\nJustice must be empowered, and power must be justified. Without both, efficiency collapses into inequality, and freedom decays into performance. The democratic project does not fail from misaligned ideology but from misaligned structure. And sometimes, all it takes to begin restoring balance is the courage to move the pivot.",
    "crumbs": [
      "Apps",
      "경제",
      "gitsam 독후감"
    ]
  },
  {
    "objectID": "gitsam_reading.html#footnotes",
    "href": "gitsam_reading.html#footnotes",
    "title": "gitsam 독후감",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n균형 좋아하시네. 낙수효과? 참을 만큼 참았습니다. 이제는 명확히 말해야 할 때입니다. 성장은 면죄부가 아닙니다. 오히려 지난 수십 년간의 자본집중적 성장은 분배의 실패를 구조화했고, 경제적 자유를 극소수의 특권으로 전락시켰습니다. 정의(Justice) 없는 권력(Power)은 폭정이고, 권력 없는 정의는 무능입니다. 지금 필요한 것은 ‘모두를 위한 성장’ 같은 착한 구호가 아니라, 기울어진 시소의 중심축을 강제로라도 옮기는 구조적 재분배입니다. 지금 이 사회에 필요한 건 성장이 아니라, 정의로운 힘의 재구성입니다.↩︎",
    "crumbs": [
      "Apps",
      "경제",
      "gitsam 독후감"
    ]
  },
  {
    "objectID": "finance_linear.html",
    "href": "finance_linear.html",
    "title": "Linear Models of Finance",
    "section": "",
    "text": "The failure to rationally explain the excess returns for dominant firms reflects not merely model misspecification but a systematic failure to capture the fundamental nature of contemporary capitalism, where returns increasingly flow not as compensation for risk-bearing but as rents extracted through structural market advantages. This insight challenges the core assumptions of traditional asset management, which relies heavily on models like the Capital Asset Pricing Model (CAPM) or multi-factor frameworks to guide investment decisions. In an post-2008 era of quantitative easing (QE), ETF dominance, and mega-cap firms leveraging network effects, regulatory capture, and institutional protection to secure outsized profits, these models lead to suboptimal portfolio allocations. This study delves into the implications of this disconnect, critiques the limitations of conventional approaches, and proposes a data-driven alternative using stochastic dominance to better align asset management with the realities of modern markets.\nThe evolution of portfolio optimization and asset pricing reflects a tension between theory and empirics:\n\nMarkowitz (1952): Introduced mean-variance optimization, assuming normal returns and quadratic utility—elegant but empirically fragile.\nCAPM (1964): Linked returns to market risk, but anomalies (e.g., size, value) prompted multi-factor models.\nFama-French (1992): Added factors, yet struggled with momentum and low-volatility effects.\nBehavioral Finance: Addressed inefficiencies but lacked a cohesive framework.\n\nc.f. Tested Linear Models\n\nChallenging Linear Factor Models\n\nFactor Selection: Highlight multicollinearity (mkt_excess, macro_bm) and limited explanatory power.\n\nLasso Regression: Apply regularization to select predictors for FF 10 industry portfolio excess returns.\n\n\nChallenging Betting Against Beta\n\nOut-of-Sample Performance: Optimal assets have betas near 1, not high or low (SIC 10 portfolios).\n\n\nChallenging the traditional Mean-Variance weight Optimization\n\nExtreme Weights: Unconstrained MV yields impractical results; constraints (e.g., no short sales) added.\n\nPassive vs. Active: Passive value-weighted portfolios outperform MV with lower costs.\n\n\nChallenging the Parametric Mean-Variance Weight Optimization\n\nBayesian update : Black and Litterman (1992)\nMomentum and Size Tilting: Brandt et al. (2009) approach underperforms passive benchmarks.",
    "crumbs": [
      "Apps",
      "금융",
      "Linear Models of Finance"
    ]
  },
  {
    "objectID": "finance_linear.html#overview",
    "href": "finance_linear.html#overview",
    "title": "Linear Models of Finance",
    "section": "",
    "text": "The failure to rationally explain the excess returns for dominant firms reflects not merely model misspecification but a systematic failure to capture the fundamental nature of contemporary capitalism, where returns increasingly flow not as compensation for risk-bearing but as rents extracted through structural market advantages. This insight challenges the core assumptions of traditional asset management, which relies heavily on models like the Capital Asset Pricing Model (CAPM) or multi-factor frameworks to guide investment decisions. In an post-2008 era of quantitative easing (QE), ETF dominance, and mega-cap firms leveraging network effects, regulatory capture, and institutional protection to secure outsized profits, these models lead to suboptimal portfolio allocations. This study delves into the implications of this disconnect, critiques the limitations of conventional approaches, and proposes a data-driven alternative using stochastic dominance to better align asset management with the realities of modern markets.\nThe evolution of portfolio optimization and asset pricing reflects a tension between theory and empirics:\n\nMarkowitz (1952): Introduced mean-variance optimization, assuming normal returns and quadratic utility—elegant but empirically fragile.\nCAPM (1964): Linked returns to market risk, but anomalies (e.g., size, value) prompted multi-factor models.\nFama-French (1992): Added factors, yet struggled with momentum and low-volatility effects.\nBehavioral Finance: Addressed inefficiencies but lacked a cohesive framework.\n\nc.f. Tested Linear Models\n\nChallenging Linear Factor Models\n\nFactor Selection: Highlight multicollinearity (mkt_excess, macro_bm) and limited explanatory power.\n\nLasso Regression: Apply regularization to select predictors for FF 10 industry portfolio excess returns.\n\n\nChallenging Betting Against Beta\n\nOut-of-Sample Performance: Optimal assets have betas near 1, not high or low (SIC 10 portfolios).\n\n\nChallenging the traditional Mean-Variance weight Optimization\n\nExtreme Weights: Unconstrained MV yields impractical results; constraints (e.g., no short sales) added.\n\nPassive vs. Active: Passive value-weighted portfolios outperform MV with lower costs.\n\n\nChallenging the Parametric Mean-Variance Weight Optimization\n\nBayesian update : Black and Litterman (1992)\nMomentum and Size Tilting: Brandt et al. (2009) approach underperforms passive benchmarks.",
    "crumbs": [
      "Apps",
      "금융",
      "Linear Models of Finance"
    ]
  },
  {
    "objectID": "finance_linear.html#dual-approaches-to-asset-management",
    "href": "finance_linear.html#dual-approaches-to-asset-management",
    "title": "Linear Models of Finance",
    "section": "2 Dual Approaches to Asset Management",
    "text": "2 Dual Approaches to Asset Management\nIn economics, a limited resource allocation involves relative price vectors (\\(P\\)) and quantity vectors (\\(Q\\)), often constrained by a structural inner product:\n\\[P \\cdot Q = \\text{constant}\\]\nWith a convex or concave scalar function of \\(P\\) or \\(Q\\) which are complete and compact Euclidean spaces, this structure enables convex optimization tools. Duality in convex optimization posits that solving for \\(Q\\) given \\(P\\) (primal problem) or \\(P\\) given \\(Q\\) (dual problem) yields equivalent results under linear constraints, reflecting the Hahn-Banach theorem’s guarantee of consistent linear functionals.\n\nAsset Weight Optimization:\n\nGoal: Given \\(P\\) (joint distribution of returns), find \\(Q\\) (asset weights) to optimize an objective (e.g., maximize Sharpe ratio).\nCritique: Modern Portfolio Theory (MPT) assumes elliptical distributions and time-separable utility (e.g., CRRA), which are unrealistic. Non-stationary or fat-tailed distributions (e.g., Cauchy) render moment estimates unreliable, and utility may not reflect diminishing marginal returns for future wealth.\n\nAsset Pricing Optimization:\n\nGoal: Given \\(Q\\) (future cash flows), find \\(P\\) (state prices) to satisfy no-arbitrage conditions via the Euler equation.\nCritique: If risk-return tradeoffs break down (as with mega-caps), linear factor models derived from local approximations fail, undermining theoretical predictions.",
    "crumbs": [
      "Apps",
      "금융",
      "Linear Models of Finance"
    ]
  },
  {
    "objectID": "finance_linear.html#limitations-of-linear-asset-pricing-models",
    "href": "finance_linear.html#limitations-of-linear-asset-pricing-models",
    "title": "Linear Models of Finance",
    "section": "3 Limitations of Linear Asset Pricing Models",
    "text": "3 Limitations of Linear Asset Pricing Models\nTraditional asset pricing theories operate on the fundamental premise that expected returns directly compensate for systematic risk exposure. However, the empirical reality of top-performing mega-cap stocks presents a profound contradiction: these entities often display disproportionate returns relative to their risk profiles. Consider Apple, which despite having relatively high beta compared to other mega-caps, has generated returns that consistently exceed what would be predicted by its systematic risk exposure alone.\n\n3.1 Statistical Limitations\nThe conventional approach to addressing anomalies involves extending traditional asset pricing models by incorporating additional factors. These extensions represent a fundamentally inadequate methodology as they merely perpetuate the linear structure of existing frameworks while appending explanatory variables. The standard multi-factor model takes the form:\n\\[E(R_i) = R_f + \\beta_{i,1}F_1 + \\beta_{i,2}F_2 + ... + \\beta_{i,n}F_n + \\alpha_i\\]\nWhere \\(E(R_i)\\) represents expected returns, \\(R_f\\) is the risk-free rate, \\(\\beta_{i,j}\\) are factor loadings, and \\(F_j\\) are risk factors. The proposed extensions for capturing phenomena such as network effects, winner-take-all dynamics, and regulatory capture simply add terms to this equation.\nThis approach has several critical flaws related to both model specification and parameter estimation.\nLinear additive models inherently assume factor orthogonality, normal distribution of returns, and stable risk premiums—assumptions violated by the complex, non-linear relationships between institutional protection, market power, and returns. These models rely on the core statistical assumptions:\n\n\\(E(\\varepsilon_i) = 0\\) (zero-mean residuals)\n\\(Cov(F_j, \\varepsilon_i) = 0\\) (factors uncorrelated with residuals)\n\\(Cov(F_j, F_k) = 0\\) for \\(j \\neq k\\) (orthogonal factors)\n\\(\\varepsilon_i \\sim N(0, \\sigma^2)\\) (normally distributed residuals)\n\nYet empirical tests would consistently reject these assumptions for mega-cap returns. A particularly problematic issue is multicollinearity between factors. When adding variables such as institutional protection, regulatory capture, and network effects to explain mega-cap performance, these factors often exhibit high correlation amongst themselves. This multicollinearity leads to unstable coefficient estimates, inflated standard errors, and ultimately, unreliable inference. For instance, a firm’s market power is often highly correlated with its regulatory influence, making it difficult to disentangle their individual effects.\nMoreover, adding qualitative variables like “network effects” to statistical models is particularly vulnerable to p-hacking, as these constructs can be operationalized in numerous ways. The improvement in in-sample fitting (\\(R^2\\)) often comes at the expense of out-of-sample prediction accuracy—a fundamental manifestation of the bias-variance tradeoff in mean squared error:\n\\[MSE(prediction) = Bias^2 + Variance + Irreducible\\_Error\\]\nAdding complex factors to capture mega-cap outperformance typically increases model complexity, which may reduce in-sample bias but simultaneously increases estimation variance. This increased variance occurs because more complex models with additional parameters require more data for stable estimation and are more prone to overfitting noise rather than signal. This relationship is particularly problematic for stock returns, which violate ergodicity assumptions necessary for consistent parameter estimation.\nFurthermore, traditional frameworks remain epistemologically constrained by their reliance on backward-looking statistical relationships. Consider the standard time-series approach to estimating factor models:\n\\[R_{it} = \\alpha_i + \\beta_{i,MKT}R_{mt} + \\beta_{i,SMB}SMB_t + \\beta_{i,HML}HML_t + \\varepsilon_{it}\\]\nThis specification assumes parameter stability over time (\\(\\beta_{i,j,t} = \\beta_{i,j,t+1}\\)), an assumption violated when institutional dynamics create structural breaks. Microsoft’s evolving regulatory landscape demonstrates this challenge—its systematic risk profile has fundamentally changed as its market and political power have increased, rendering historical beta estimates increasingly irrelevant for forward-looking predictions.\n\n\n3.2 The Inverse Relationship Between Risk and Return for Dominant Firms\nThe conventional understanding of systematic risk requires fundamental reconceptualization. The Capital Asset Pricing Model posits:\n\\[E(R_i) = R_f + \\beta_i(E(R_m) - R_f)\\]\nWhere \\(\\beta_i = \\frac{Cov(R_i, R_m)}{Var(R_m)} = \\rho_{i,m}\\frac{\\sigma_i}{\\sigma_m}\\) measures systematic risk exposure. This can be decomposed into correlation (\\(\\rho_{i,m}\\)) and relative volatility components (\\(\\frac{\\sigma_i}{\\sigma_m}\\)).\nAn empirical analysis of market data reveals a critical paradox for mega-cap stocks versus non-mega-cap stocks. Both categories typically exhibit similar correlation coefficients with the market (\\(\\rho_{i,m} \\approx 1\\)), indicating strong co-movement with overall market trends. Similarly, their expected returns (\\(E(R_i)\\)) are often comparable or even favor mega-caps. However, the crucial distinction emerges in their volatility profiles: mega-cap stocks consistently demonstrate lower idiosyncratic volatility (\\(\\sigma_i^{mega} &lt; \\sigma_i^{non-mega}\\)) due to their institutional protection, diversified revenue streams, and market power.\nThis creates a fundamental contradiction in the risk-return paradigm. When calculating the Sharpe ratio or other risk-adjusted return metrics:\n\\[Sharpe_i = \\frac{E(R_i) - R_f}{\\sigma_i}\\]\nMega-cap stocks consistently outperform on a risk-adjusted basis. This superior risk-adjusted performance cannot be reconciled with traditional asset pricing theory, which predicts that lower risk should be associated with lower returns. The empirical reality suggests the opposite for market dominant stocks:\n\\[\\frac{E(R_i^{mega}) - R_f}{\\sigma_i^{mega}} &gt; \\frac{E(R_i^{non-mega}) - R_f}{\\sigma_i^{non-mega}}\\]\nDespite comparable raw returns and market correlations, the risk-adjusted outperformance of mega-cap stocks directly contradicts the foundational risk-return trade-off of asset pricing theory. Amazon exemplifies this phenomenon—its volatility has decreased relative to smaller competitors as its market dominance has increased, yet its returns have remained exceptional, generating superior risk-adjusted performance that cannot be explained by traditional models.\n\n\n3.3 Market Efficiency and Structural Advantages\nMarket efficiency assumptions underlying traditional models presuppose:\n\\[E[R_{i,t+1} | \\Omega_t] := E_t[R_{i,t+1}]\\]\nWhere \\(\\Omega_t\\) represents the information set available at time \\(t\\), and \\(E_t[\\cdot]\\) denotes expectation conditional on information at time \\(t\\). However, the persistence of abnormal returns for dominant firms suggests either:\n\nInformation about institutional advantages is not fully incorporated in prices\nThese advantages create structural market inefficiencies that cannot be arbitraged away\n\nGoogle’s digital advertising dominance illustrates these dynamics. Its network effects create increasing returns to scale that can be modeled as:\n\\[Profit_t = f(MarketShare_{t-1})^{\\gamma}\\]\nWhere \\(\\gamma &gt; 1\\) indicates increasing returns to scale. Traditional asset pricing models assume competitive markets where \\(\\gamma \\leq 1\\), making them structurally incapable of capturing the valuation implications of dominant market positions.\nThe fundamental inadequacy of traditional models becomes evident when examining their predictive accuracy for top market-cap stocks. A comparison of realized returns versus CAPM-predicted returns for top 10 stocks from 2010-2020 shows systematic underestimation:\n\\[\\alpha_i:= E(R_i) - [R_f + \\beta_i(R_m - R_f)] &gt; 0 \\text{ for top 10 stocks}\\]\nThis persistent alpha cannot be explained as compensation for omitted risk factors, as these firms typically enjoy reduced risk through institutional protection and market dominance.\nThis theoretical crisis demands not incremental additions to existing models but a comprehensive reconceptualization of capital market dynamics. The failure to accurately predict returns for dominant firms reflects not merely model misspecification but a systematic failure to capture the fundamental nature of contemporary capitalism, where returns increasingly flow not as compensation for risk-bearing but as rents extracted through structural market advantages.\nA more appropriate specification might acknowledge the non-linear relationship between institutional position and returns:\n\\[E(R_i) = R_f + \\beta_i(R_m - R_f) + g(MarketPower_i, InstitutionalProtection_i)\\]\nWhere \\(g(\\cdot)\\) is a non-linear function capturing the complex relationship between market power, institutional protection, and returns—a relationship that fundamentally contradicts the risk-return paradigm underlying traditional asset pricing theory.",
    "crumbs": [
      "Apps",
      "금융",
      "Linear Models of Finance"
    ]
  },
  {
    "objectID": "finance_linear.html#practical-asset-management",
    "href": "finance_linear.html#practical-asset-management",
    "title": "Linear Models of Finance",
    "section": "4 Practical Asset Management",
    "text": "4 Practical Asset Management\n\n4.1 Implications of Model Failure for Asset Management\nAsset management involves making investment decisions to achieve financial objectives, such as maximizing returns or minimizing risk, typically by constructing portfolios based on expected returns and risk estimates derived from asset pricing models. However, the inadequacy of traditional models when applied to dominant firms—those “superstar” companies that capture disproportionate market value—creates significant practical challenges:\n\nBreakdown of Market Efficiency Assumptions\nTraditional models assume markets are efficient, with asset prices reflecting all available information. Yet, dominant firms consistently generate abnormal returns that persist over time, suggesting inefficiencies tied to structural advantages—like brand loyalty or data monopolies—that markets fail to fully price in. For asset managers, this means traditional models underestimate the value of these firms, leading to underweighting in portfolios.\nInverse Risk-Return Dynamics\nModels like the CAPM posit a linear relationship where higher risk (measured as beta) yields higher expected returns. For dominant firms, however, the opposite often holds: their entrenched positions reduce idiosyncratic risk—through diversified revenue streams or regulatory buffers—while delivering superior returns. This inversion misleads asset managers into overestimating the risk of mega-cap stocks and underestimating their return potential, skewing portfolio optimization.\nFactor Instability and Omission\nMulti-factor models, such as those based on Fama-French factors, depend on stable relationships between risk factors (e.g., size, value) and returns. Yet, the market power of dominant firms evolves dynamically—through technological innovation or policy shifts—causing factor loadings to fluctuate. Moreover, these models lack factors that explicitly capture rents from structural advantages, leaving asset managers with incomplete tools to assess true risk-adjusted performance.\nInability to Model Non-Linear Effects\nThe linear structure of traditional models cannot accommodate the non-linear dynamics of contemporary capitalism, such as increasing returns to scale or tipping points in market dominance. For example, a firm like Amazon benefits from self-reinforcing network effects that amplify its returns beyond what risk-based models predict, rendering traditional allocations ineffective.\n\nThese shortcomings result in portfolios that miss out on the concentrated returns of dominant firms, which increasingly drive market performance. Asset managers relying on outdated frameworks risk underperformance in a landscape where structural advantages, not risk exposure, dictate success.\n\n\n4.2 A Data-Driven Alternative: Stochastic Dominance\nTo address these challenges, asset management must shift toward empirical, data-driven approaches that eschew the restrictive assumptions of traditional models. One such method is stochastic dominance, which evaluates assets by comparing their entire return distributions rather than relying on summary statistics like mean and variance. This approach offers a practical way to identify investments that align with the empirical realities of contemporary markets.\n\nHow It Works\n\nFirst-Order Stochastic Dominance (FSD): Asset A dominates Asset B if its cumulative distribution function (CDF) lies below B’s for all return levels, meaning A offers higher returns across all outcomes.\n\nSecond-Order Stochastic Dominance (SSD): For risk-averse investors, A dominates B if the area under A’s CDF (its integral) is less than B’s up to any point, indicating a better risk-return tradeoff.\n\nAdvantages\n\nRobustness: Unlike mean-variance optimization, stochastic dominance requires no assumptions about return distributions (e.g., normality), making it suitable for markets with fat tails or skewness—common in mega-cap stock performance.\n\nEmpirical Focus: By analyzing historical return distributions, it captures the real-world impact of structural advantages, such as the persistent outperformance of firms like Apple or Microsoft.\n\nFlexibility: It accommodates diverse investor risk preferences without specifying a utility function, broadening its applicability.\n\nPractical Implementation\nAsset managers can apply stochastic dominance by:\n\nHistorical Simulations: Using past return data to estimate CDFs and calculate the likelihood of one asset dominating another.\n\nResampling: Employing techniques like bootstrapping to test dominance under varying market conditions, enhancing robustness.\n\nChallenges\n\nData Requirements: Accurate CDF estimation demands extensive historical data, which may be scarce for newer firms or asset classes.\n\nNon-Stationarity: Return distributions shift over time—due to regulatory changes or competitive disruptions—limiting the reliability of historical dominance.\n\nPortfolio Complexity: Extending stochastic dominance to multi-asset portfolios is computationally intensive, often requiring simplifications or heuristic rules.\n\n\nDespite these hurdles, stochastic dominance provides a rigorous framework that sidesteps the theoretical pitfalls of traditional models. It allows asset managers to focus on observable performance, directly addressing the rent extraction that defines dominant firms.\n\n\n4.3 Complementary Tools\nWhile stochastic dominance serves as a strong foundation, asset managers can bolster their strategies with additional tools to capture the nuances of contemporary markets:\n\nFundamental Analysis\nDetailed assessments of a firm’s financials, competitive moat (e.g., patents, scale), and industry trends can reveal structural advantages missed by quantitative models. For instance, analyzing Tesla’s innovation pipeline or Alphabet’s data ecosystem offers insights into their sustained dominance.\nMachine Learning\nAdvanced algorithms can detect non-linear patterns in returns or predict shifts in market power, though they require careful tuning to avoid overfitting and ensure interpretability.\nLiquidity Considerations\nMega-cap stocks typically offer high liquidity, reducing transaction costs and making them practical choices for large portfolios—a factor stochastic dominance alone might not prioritize.\n\nMoreover, the concentration of returns in a few dominant firms suggests that a focused investment strategy—overweighting current mega-caps or scouting emerging leaders—may outperform broad diversification. However, identifying future dominants demands foresight into technological, regulatory, and economic trends, adding complexity to the process.\n\n\n4.4 Conclusion\nIn a world where returns stem from structural market advantages rather than risk-bearing, traditional asset pricing models fail to guide effective asset management. Their inability to predict the performance of dominant firms—rooted in flawed assumptions about efficiency, risk, and linearity—leads to misallocated portfolios that undervalue mega-cap opportunities. A data-driven approach using stochastic dominance offers a compelling alternative, leveraging empirical return distributions to identify superior investments without theoretical baggage. While challenges like data demands and non-stationarity persist, combining this method with fundamental analysis and modern tools equips asset managers to navigate contemporary capitalism. By embracing flexibility and empiricism, asset management can better capture the rents that define today’s markets, delivering superior outcomes in an era of concentrated dominance.",
    "crumbs": [
      "Apps",
      "금융",
      "Linear Models of Finance"
    ]
  },
  {
    "objectID": "finance_asset_puzzle.html",
    "href": "finance_asset_puzzle.html",
    "title": "Asset Premium Puzzles",
    "section": "",
    "text": "Since the 1980s, financial economists have grappled with two so-called “puzzles” in asset pricing theory: the Equity Premium Puzzle (EPP) and the Risk-Free Rate Puzzle (RFRP). These puzzles refer to the pronounced gap between theoretical predictions derived from standard rational-expectations models with representative-agent utility maximization and the empirical observations on asset returns and risk-free rates. In particular, Mehra and Prescott (1985) and Weil (1989) spotlighted the disparity between observed excess returns on equities and the relatively tame variability of aggregate consumption, labeling the phenomenon a “puzzle” under the canonical equilibrium framework.\nBeneath both puzzles lies a single “master equation,” namely the Euler equation, which in its most general form states:\n\\[\n\\mathbb{E}_t \\big[m_t R_{t+1}\\big] = 1,\n\\]\nwhere \\(m_t\\) (the Stochastic Discount Factor) is closely tied to the marginal utility of consumption, and \\(R_{t+1}\\) represents the return on various assets. The equity premium and risk-free rate each follow from this broad formulation, yet empirical magnitudes diverge significantly from conventional theoretical predictions.",
    "crumbs": [
      "Apps",
      "금융",
      "Asset Premium Puzzles"
    ]
  },
  {
    "objectID": "finance_asset_puzzle.html#introduction",
    "href": "finance_asset_puzzle.html#introduction",
    "title": "Asset Premium Puzzles",
    "section": "",
    "text": "Since the 1980s, financial economists have grappled with two so-called “puzzles” in asset pricing theory: the Equity Premium Puzzle (EPP) and the Risk-Free Rate Puzzle (RFRP). These puzzles refer to the pronounced gap between theoretical predictions derived from standard rational-expectations models with representative-agent utility maximization and the empirical observations on asset returns and risk-free rates. In particular, Mehra and Prescott (1985) and Weil (1989) spotlighted the disparity between observed excess returns on equities and the relatively tame variability of aggregate consumption, labeling the phenomenon a “puzzle” under the canonical equilibrium framework.\nBeneath both puzzles lies a single “master equation,” namely the Euler equation, which in its most general form states:\n\\[\n\\mathbb{E}_t \\big[m_t R_{t+1}\\big] = 1,\n\\]\nwhere \\(m_t\\) (the Stochastic Discount Factor) is closely tied to the marginal utility of consumption, and \\(R_{t+1}\\) represents the return on various assets. The equity premium and risk-free rate each follow from this broad formulation, yet empirical magnitudes diverge significantly from conventional theoretical predictions.",
    "crumbs": [
      "Apps",
      "금융",
      "Asset Premium Puzzles"
    ]
  },
  {
    "objectID": "finance_asset_puzzle.html#theoretical-framework",
    "href": "finance_asset_puzzle.html#theoretical-framework",
    "title": "Asset Premium Puzzles",
    "section": "2 Theoretical Framework",
    "text": "2 Theoretical Framework\n\n2.1 Equity Premium Puzzle\nThe seminal work of Lucas (1978) and later Mehra and Prescott (1985) established the Equity Premium Puzzle within a representative-agent, general-equilibrium asset pricing model. The typical consumer (or representative agent) maximizes expected utility over time, commonly assumed to be CRRA (Constant Relative Risk Aversion):\n\\[\nU(C_t) = \\frac{C_t^{1-\\gamma}}{1-\\gamma}, \\quad \\gamma &gt; 0, \\,\\gamma \\neq 1,\n\\]\nwhere \\(\\gamma\\) is the coefficient of relative risk aversion (RRA). From the consumer’s intertemporal optimization, an Euler equation emerges:\n\\[\n\\mathbb{E}_t \\Big[\\beta \\Big(\\frac{C_{t+1}}{C_t}\\Big)^{-\\gamma} \\big(R_{e,t+1} - R_{f,t+1}\\big)\\Big] = 0,\n\\]\nwhere:\n\n\\(\\beta\\): subjective time discount factor (\\(0 &lt; \\beta &lt; 1\\)),\n\n\\(C_{t+1} / C_t\\): consumption growth rate,\n\n\\(R_{e,t+1}\\): equity return,\n\n\\(R_{f,t+1}\\): risk-free asset return.\n\nUnder some approximations (e.g., log-linearization of consumption growth), Mehra and Prescott (1985) famously derived:\n\\[\n\\mathbb{E}[R_e - R_f] \\approx \\gamma \\,\\sigma_c^2,\n\\]\nwhere \\(\\sigma_c^2\\) is the variance of consumption growth. If the variance of consumption growth is small (on the order of 1%–2% annually), the observed 6%–8% annual equity premium can only be reconciled by positing implausibly high risk aversion coefficients (often above 20). Since typical estimates of \\(\\gamma\\) in microeconomic or macroeconomic studies hover around 1–5, this gap forms the crux of the EPP.\n\n\n2.2 Risk-Free Rate Puzzle\nA closely related conundrum is the Risk-Free Rate Puzzle, initially highlighted by Weil (1989). Under the same CRRA framework and rational expectations, the Euler equation for the risk-free asset implies:\n\\[\n1 = \\mathbb{E}_t \\Big[\\beta \\Big(\\frac{C_{t+1}}{C_t}\\Big)^{-\\gamma} R_{f,t+1}\\Big].\n\\]\nApproximating in logs,\n\\[\nr_f \\approx \\delta + \\gamma \\,g_c - \\frac{1}{2}\\,\\gamma\\,(\\gamma + 1)\\,\\sigma_c^2,\n\\]\nwhere:\n\n\\(r_f = \\ln(R_f)\\): the log of the risk-free rate,\n\n\\(\\delta = -\\ln(\\beta)\\): time preference rate,\n\n\\(g_c = \\mathbb{E}[\\ln(C_{t+1}/C_t)]\\): average consumption growth rate.\n\nEmpirically, long-run real risk-free rates are typically in the 1%–3% range, whereas the above equation might predict rates of 4%–8% given plausible values for \\(\\gamma\\), \\(\\delta\\), and \\(g_c\\). Again, the severe mismatch between model forecasts and observed data has led researchers to classify it as a “puzzle.”\n\n\n2.3 Critical Assumptions\nTo preserve tractability, the standard model assumes:\n\nA representative agent — but who truly “represents” the market?\n\nTime-separability of the utility function — ensuring that period utilities add linearly over time.\n\nGlobal concavity of CRRA utility — guaranteeing diminishing marginal utility at all consumption levels.\n\nWhile these assumptions yield elegant closed-form solutions, they may excessively simplify real-world heterogeneity. Crucially, when the economy scales up over time, CRRA utility remains well-defined, but in practice this might obscure the role of vastly different consumption paths across distinct wealth brackets.",
    "crumbs": [
      "Apps",
      "금융",
      "Asset Premium Puzzles"
    ]
  },
  {
    "objectID": "finance_asset_puzzle.html#critical-perspective",
    "href": "finance_asset_puzzle.html#critical-perspective",
    "title": "Asset Premium Puzzles",
    "section": "3 Critical Perspective",
    "text": "3 Critical Perspective\n\n3.1 Rethinking ‘Rational Expectations’\nTraditionally, Rational Expectations is seen as a condition that agents use all available information efficiently, forming unbiased forecasts. However, from a purely mathematical standpoint, “expectations” and “covariances” are simply operators for dealing with means and correlations of random variables. Such operators—particularly bilinear forms and inner products—require specific algebraic properties (linearity, symmetry, Cauchy–Schwarz inequality, etc.). While these simplifications can be powerful in physics or engineering, in economics they might be overly restrictive when applied to highly heterogeneous populations and institutions.\n\n\n3.2 The Euler Equation\nBy construction, the Euler condition \\(\\mathbb{E}_t [m_{t+1} R_{t+1}] = 1\\) is mathematically akin to an inner product on a probability space. This yields a focus on second moments (variance, covariance) and often leads to elliptical distribution assumptions (e.g., normal, Student-\\(t\\)). Real-world wealth distributions and market participation, however, may be far from elliptical in their statistical properties—especially when only a small fraction of the population holds the majority of risky assets.\n\n\n3.3 Implications of Globally Concave CRRA Utility\nCRRA utility, with its global concavity, implies a declining marginal utility as consumption grows. If aggregate consumption (\\(C_t\\)) trends upward over time, the ratio of marginal utilities \\(\\bigl[u'(C_{t+1}) / u'(C_t)\\bigr]\\) naturally declines, ensuring an inverse relationship between the SDF (\\(m_{t+1}\\)) and any asset (or variable) with a long-term growth trend. In equity markets, returns \\(R_{t+1}\\) also tend to grow over time, so \\(m_t\\) and \\(R_{t+1}\\) end up negatively correlated by construction.\nIf one then chooses a suitably volatile variable (with sufficient high variance) to stand in for \\(m_{t+1}\\), one can reconcile observed excess returns with the theoretical predictions—effectively defusing the puzzle. In that sense, the puzzle may be an artifact of incomplete modeling of real-world heterogeneity.",
    "crumbs": [
      "Apps",
      "금융",
      "Asset Premium Puzzles"
    ]
  },
  {
    "objectID": "finance_asset_puzzle.html#explaining-asset-premiums",
    "href": "finance_asset_puzzle.html#explaining-asset-premiums",
    "title": "Asset Premium Puzzles",
    "section": "4 Explaining Asset Premiums",
    "text": "4 Explaining Asset Premiums\n\n4.1 Through Market Heterogeneity\nRather than focusing exclusively on heterogeneous preferences (e.g., habit formation, behavioral biases), the perspective here is that heterogeneous economic environments—in particular, vast differences in wealth and market participation—play a decisive role:\n\nConcentrated Stock Market Participation\n\nA small fraction of wealthy households hold the majority of equity. Their risk attitudes and consumption patterns have disproportionate influence on asset prices.\n\nWealth and Consumption Inequality\n\nHigh-net-worth individuals exhibit markedly different consumption patterns from the average household, more closely aligning with equity market fluctuations.\n\n\nFrom this viewpoint, the high observed equity premium is not a puzzle at all once one acknowledges that a very small sub-population—namely, the extremely wealthy—holds large, volatile wealth positions that effectively determine marginal prices in the stock market. According to Federal Reserve data (Board 2025), 93% of households’ stock market wealth (though not 93% of total market capitalization) belongs to the wealthiest 10%. This implies that most aggregate equity risk and returns accrue to a relatively narrow stratum. For the “representative household,” which holds only about 7% of equity, stock price movements have minimal impact on its marginal utility of consumption. Hence, standard representative-agent formulations fail to capture the truly relevant marginal investor.\nEmpirical evidence from the following studies supports this line of reasoning:\n\nBasak and Cuoco (1998): Demonstrates that limited stock market participation elevates risk premia and depresses risk-free rates.\n\nYogo (2006): Shows that consumption by wealthy households closely tracks equity returns, reinforcing the link between high-end consumption dynamics and asset prices.\n\nGomes and Michaelides (2008): Connects growing income inequality with stock market participation patterns, resulting in rising equity premia.\n\nLettau, Ludvigson, and Ma (2019): Shows that a single macroeconomic factor tied to capital share (reflecting wealthy shareholders’ consumption) can explain a broad range of cross-sectional stock return premia.\n\n\n\n4.2 Limitations\nDespite offering a plausible explanation, this heterogeneity-based view faces practical hurdles:\n\nLow frequency data: Due to infrequent reporting on high-net-worth wealth, applying short-term no-arbitrage principles in cross-sectional asset pricing is problematic.\n\nDifficulty of reconciling EMH: Efficient Markets Hypothesis (EMH) posits that arbitrage opportunities vanish quickly, but wealth data often lack the granularity or frequency to confirm this.\n\nLong-run identification: Changes in upper-tier wealth or consumption may be valid for a long-run SDF (\\(m\\)), yet verifying this for short-run asset pricing remains challenging.",
    "crumbs": [
      "Apps",
      "금융",
      "Asset Premium Puzzles"
    ]
  },
  {
    "objectID": "finance_asset_puzzle.html#conclusion",
    "href": "finance_asset_puzzle.html#conclusion",
    "title": "Asset Premium Puzzles",
    "section": "5 Conclusion",
    "text": "5 Conclusion\nThe Equity Premium Puzzle and Risk-Free Rate Puzzle have dominated discussions in asset pricing for decades. However, labeling them as genuine “puzzles” may reflect an artifact of restrictive models that hinge on a single representative agent, uniform preferences, and high-level assumptions about consumption growth. By introducing heterogeneous market participation, particularly the reality that a small fraction of wealthy agents holds the lion’s share of risky assets, one finds that what appears to be a puzzle for the average consumer is, in fact, quite explicable among those who actually drive stock prices.\nIn short, when empirical ownership and wealth concentration data are properly accounted for, the puzzling gaps between theory and observation can diminish or disappear. The challenge remains to integrate heterogeneous agent frameworks with accurate micro-level data on wealth and consumption in order to provide a more comprehensive understanding of asset prices—an endeavor that holds promise for reconciling the so-called “puzzles” with empirical reality.",
    "crumbs": [
      "Apps",
      "금융",
      "Asset Premium Puzzles"
    ]
  },
  {
    "objectID": "d_4_ethics.html",
    "href": "d_4_ethics.html",
    "title": "AI시대 경제윤리",
    "section": "",
    "text": "“Where Geometry Permits Freedom”\n\nThe equilibrium of an economic system has long been interpreted as a signal of resolution—a resting point of optimization, balance, or social efficiency. Yet our analysis reveals a different logic. In closed dynamical systems, especially those that model capital and labor under conditions of automation, equilibrium is not justice. It is a structural consequence.\nThe share-based dynamic system explored in this study, governed by:\n\\[\n\\frac{dc}{dt} = c(a_{2,2} - a_{2,4} c),\n\\]\ndoes not depict a market adjusting toward balance. It describes a phase-space curvature, a regime in which autonomous capital growth confronts internal saturation. When \\(a_{2,1}\\)—the coupling coefficient from labor to capital—is effectively zero, the system becomes structurally mutating: labor vanishes, and capital becomes decoupled, autopoietic, and unsustainable. Importantly, this is not a theoretical boundary case. It is the empirically dominant configuration across top 0.1%, 1%, and 10% U.S. wealth share trajectories (1989–2024).\nOur simulations show that when labor ceases to contribute to capital formation—even under high automation growth rates (\\(a_{2,2} &gt; a_{2,3}\\))—the entire system decays. Capital itself collapses. Dominance without anchoring becomes self-destructive. A capitalist system without labor is a closed loop of extraction without reproduction: it does not evolve—it consumes itself.\nIn contrast, calibrating \\(a_{2,1}\\) to a small but positive value (e.g., \\(0.0007\\)) transforms the trajectory. Capital shares stabilize at empirically plausible levels. The geometry of the system shifts. This demonstrates that structural persistence is governed not by equilibrium conditions, but by bifurcation thresholds in the feedback network. A minute coupling term becomes the difference between long-run stability and structural extinction.\nThis leads to a broader ethical insight: freedom in such systems emerges only where curvature permits deviation. Just as Milton Friedman once argued that economic freedom undergirds political liberty, we propose a parallel structural logic:\n&gt; Curvature is the precondition for meaningful intervention.\nWhere the system is flat, efforts dissipate. But where curvature is high—where local changes generate divergent long-term behavior—intervention becomes not only possible, but necessary.\nEthics in dynamical systems is not a matter of ex post redistribution. It is a matter of anticipatory geometry. If the system’s structure ensures the long-run disappearance of labor, then justice cannot be restored through compensatory policies. Once a group is eliminated from phase space, there is no feasible re-entry. Normative analysis must occur before the attractor forms.\nThis view casts doubt on the methodological neutrality of mainstream economic models. Formalism often equates stability with fairness. But as our system shows, stability may encode structural injustice. A model that predicts the disappearance of labor and yet interprets the result as an equilibrium has not merely erred—it has misunderstood the purpose of modeling.\n\nThe model does not fail because reality is messy.\nThe model fails because it permits no mess, and thus no future for those outside its logic.\n\nA just system, in this framing, is not one that converges. It is one that preserves structural space for all agents to persist. It is not a system that assumes meritocratic outcomes, but one that resists the topological elimination of any group.\nWe began with a structural question:\n&gt; What happens when capital reproduces without labor?\nWe conclude with a geometric truth:\n&gt; A system that eliminates its agents is not efficient—it is pathologically optimized.\nThe role of mathematics is not to prescribe solutions.\nMathematics reveals, but it does not repair.\nIt is our ethics that prescribe; it is our courage that cures.\nMathematics illuminates which futures are geometrically open—and which are already foreclosed.\n\nLet us not mistake convergence for harmony.\nLet us read equilibrium not as justice, but as a possible injustice.\n\nAnd let us recover, from the differential equations of capital and labor,\nan ethics that is not imposed from above,\nbut one that must emerge from within the structure of the system itself.",
    "crumbs": [
      "Apps",
      "돈의 동역학",
      "AI시대 경제윤리"
    ]
  },
  {
    "objectID": "d_2_elementary.html",
    "href": "d_2_elementary.html",
    "title": "동역학 기초수학",
    "section": "",
    "text": "Let \\(\\mathbf{x} = (x, y)\\) denote the population levels of two interacting species—prey (\\(x\\)) and predator (\\(y\\)). The classical Lotka–Volterra equations describe their nonlinear dynamics as:\n\\[\n\\begin{aligned}\n\\dot{x} &= x(a_{11} - a_{12} y), \\\\\n\\dot{y} &= y(a_{21} x - a_{22}),\n\\end{aligned}\n\\]\nwhere all parameters \\(a_{ij} &gt; 0\\) represent positive interaction or decay rates. This defines a closed, two-dimensional autonomous system on the positive orthant \\(\\mathbb{R}_+^2\\).\nThere are no external shocks, migrations, or species mutations—only endogenous interaction between predator and prey. The predator grows by consuming the prey; the prey reproduces in the absence of predation.\nThis system exhibits a distinctive dynamical feature: closed orbits in the phase plane. That is, solutions cycle indefinitely without converging to a fixed point or diverging to infinity. This reflects the conservative nature of the system—total “interaction energy” is preserved, and the system perpetually oscillates.\n\nRemarks: These closed cycles correspond, in the frequency domain, to dominant harmonic modes—i.e., a natural periodicity. From a systems-theoretic perspective, this allows interpretation via Fourier decomposition, where the cyclical behavior of populations maps to periodic oscillators in signal processing.\n\nThus, the Lotka–Volterra model offers a rich entry point for understanding non-equilibrium dynamics, nonlinear feedback, and structural cycles—core features of many biological and economic systems.\n\n\nCode\n# Simulation Plan\n# 1. Vector Field + Trajectory (Auto-Scaled)\n# 2. Time-Domain Dynamics (Population Levels)\n# 3. Normalized Time-Series (Prey vs. Predator Share)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.integrate import solve_ivp\n\n# 이 모형의 궤적은 보존계(conservative system)에서 closed orbit을 형성.\n# 시간 신호(time series)의 Fourier transform이 주파수 영역(frequency domain)에서 고유한 dominant cycle을 갖는다.\n\n# Lotka–Volterra Parameters\na11 = 1.0   # prey natural growth rate\na12 = 0.1   # prey loss due to predator\na21 = 0.075 # predator growth due to prey\na22 = 1.5   # predator natural decay\n\n# Define the ODE system\ndef lotka_volterra(t, y):\n    L, C = y\n    dLdt = a11 * L - a12 * L * C\n    dCdt = a21 * L * C - a22 * C\n    return [dLdt, dCdt]\n\n# Time grid and solve\nL0, C0 = 10, 5\nt_span = (0, 50)\nt_eval = np.linspace(t_span[0], t_span[1], 1000)\nsol = solve_ivp(lotka_volterra, t_span, [L0, C0], t_eval=t_eval)\nt, L, C = sol.t, sol.y[0], sol.y[1]\n\n# -------- 1. Vector Field + Trajectory (Auto-Scaled) --------\n\n# Dynamic range based on solution\nL_max = max(L) * 1.1  # add 10% margin\nC_max = max(C) * 1.1\n\nL_vals = np.linspace(0, L_max, 25)\nC_vals = np.linspace(0, C_max, 25)\nL_grid, C_grid = np.meshgrid(L_vals, C_vals)\n\n# Recompute vector field on new grid\ndL = a11 * L_grid - a12 * L_grid * C_grid\ndC = a21 * L_grid * C_grid - a22 * C_grid\nmag = np.sqrt(dL**2 + dC**2)\ndL_norm = dL / (mag + 1e-8)\ndC_norm = dC / (mag + 1e-8)\n\n# Plot\nplt.figure(figsize=(7, 6))\nplt.quiver(L_grid, C_grid, dL_norm, dC_norm, angles='xy', color='gray', alpha=0.6)\nplt.plot(L, C, 'r', linewidth=2, label=\"System Trajectory\")\nplt.xlabel(\"Prey (L)\")\nplt.ylabel(\"Predator (C)\")\nplt.title(\"Phase Space with Vector Field (Auto-Scaled)\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n# -------- 2. Time-Domain Dynamics (Level) --------\nplt.figure(figsize=(12, 5))\nplt.plot(t, L, label=\"Prey (L)\", linewidth=2)\nplt.plot(t, C, label=\"Predator (C)\", linewidth=2)\nplt.title(\"Time-Domain Dynamics (Lotka–Volterra)\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Population\")\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# -------- 3. Normalized Dynamics (Share) --------\nl_t = L / (L + C)\nc_t = C / (L + C)\n\nplt.figure(figsize=(10, 4))\nplt.plot(t, l_t, label=\"Prey share (L / (L+C))\", linewidth=2)\nplt.plot(t, c_t, label=\"Predator share (C / (L+C))\", linewidth=2)\nplt.title(\"Normalized Population Share over Time\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Normalized Share\")\nplt.ylim(0, 1)\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Apps",
      "돈의 동역학",
      "동역학 기초수학"
    ]
  },
  {
    "objectID": "d_2_elementary.html#lotkavolterra-model",
    "href": "d_2_elementary.html#lotkavolterra-model",
    "title": "동역학 기초수학",
    "section": "",
    "text": "Let \\(\\mathbf{x} = (x, y)\\) denote the population levels of two interacting species—prey (\\(x\\)) and predator (\\(y\\)). The classical Lotka–Volterra equations describe their nonlinear dynamics as:\n\\[\n\\begin{aligned}\n\\dot{x} &= x(a_{11} - a_{12} y), \\\\\n\\dot{y} &= y(a_{21} x - a_{22}),\n\\end{aligned}\n\\]\nwhere all parameters \\(a_{ij} &gt; 0\\) represent positive interaction or decay rates. This defines a closed, two-dimensional autonomous system on the positive orthant \\(\\mathbb{R}_+^2\\).\nThere are no external shocks, migrations, or species mutations—only endogenous interaction between predator and prey. The predator grows by consuming the prey; the prey reproduces in the absence of predation.\nThis system exhibits a distinctive dynamical feature: closed orbits in the phase plane. That is, solutions cycle indefinitely without converging to a fixed point or diverging to infinity. This reflects the conservative nature of the system—total “interaction energy” is preserved, and the system perpetually oscillates.\n\nRemarks: These closed cycles correspond, in the frequency domain, to dominant harmonic modes—i.e., a natural periodicity. From a systems-theoretic perspective, this allows interpretation via Fourier decomposition, where the cyclical behavior of populations maps to periodic oscillators in signal processing.\n\nThus, the Lotka–Volterra model offers a rich entry point for understanding non-equilibrium dynamics, nonlinear feedback, and structural cycles—core features of many biological and economic systems.\n\n\nCode\n# Simulation Plan\n# 1. Vector Field + Trajectory (Auto-Scaled)\n# 2. Time-Domain Dynamics (Population Levels)\n# 3. Normalized Time-Series (Prey vs. Predator Share)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.integrate import solve_ivp\n\n# 이 모형의 궤적은 보존계(conservative system)에서 closed orbit을 형성.\n# 시간 신호(time series)의 Fourier transform이 주파수 영역(frequency domain)에서 고유한 dominant cycle을 갖는다.\n\n# Lotka–Volterra Parameters\na11 = 1.0   # prey natural growth rate\na12 = 0.1   # prey loss due to predator\na21 = 0.075 # predator growth due to prey\na22 = 1.5   # predator natural decay\n\n# Define the ODE system\ndef lotka_volterra(t, y):\n    L, C = y\n    dLdt = a11 * L - a12 * L * C\n    dCdt = a21 * L * C - a22 * C\n    return [dLdt, dCdt]\n\n# Time grid and solve\nL0, C0 = 10, 5\nt_span = (0, 50)\nt_eval = np.linspace(t_span[0], t_span[1], 1000)\nsol = solve_ivp(lotka_volterra, t_span, [L0, C0], t_eval=t_eval)\nt, L, C = sol.t, sol.y[0], sol.y[1]\n\n# -------- 1. Vector Field + Trajectory (Auto-Scaled) --------\n\n# Dynamic range based on solution\nL_max = max(L) * 1.1  # add 10% margin\nC_max = max(C) * 1.1\n\nL_vals = np.linspace(0, L_max, 25)\nC_vals = np.linspace(0, C_max, 25)\nL_grid, C_grid = np.meshgrid(L_vals, C_vals)\n\n# Recompute vector field on new grid\ndL = a11 * L_grid - a12 * L_grid * C_grid\ndC = a21 * L_grid * C_grid - a22 * C_grid\nmag = np.sqrt(dL**2 + dC**2)\ndL_norm = dL / (mag + 1e-8)\ndC_norm = dC / (mag + 1e-8)\n\n# Plot\nplt.figure(figsize=(7, 6))\nplt.quiver(L_grid, C_grid, dL_norm, dC_norm, angles='xy', color='gray', alpha=0.6)\nplt.plot(L, C, 'r', linewidth=2, label=\"System Trajectory\")\nplt.xlabel(\"Prey (L)\")\nplt.ylabel(\"Predator (C)\")\nplt.title(\"Phase Space with Vector Field (Auto-Scaled)\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n# -------- 2. Time-Domain Dynamics (Level) --------\nplt.figure(figsize=(12, 5))\nplt.plot(t, L, label=\"Prey (L)\", linewidth=2)\nplt.plot(t, C, label=\"Predator (C)\", linewidth=2)\nplt.title(\"Time-Domain Dynamics (Lotka–Volterra)\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Population\")\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# -------- 3. Normalized Dynamics (Share) --------\nl_t = L / (L + C)\nc_t = C / (L + C)\n\nplt.figure(figsize=(10, 4))\nplt.plot(t, l_t, label=\"Prey share (L / (L+C))\", linewidth=2)\nplt.plot(t, c_t, label=\"Predator share (C / (L+C))\", linewidth=2)\nplt.title(\"Normalized Population Share over Time\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Normalized Share\")\nplt.ylim(0, 1)\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Apps",
      "돈의 동역학",
      "동역학 기초수학"
    ]
  },
  {
    "objectID": "d_2_elementary.html#closed-system-dynamics",
    "href": "d_2_elementary.html#closed-system-dynamics",
    "title": "동역학 기초수학",
    "section": "2 Closed System Dynamics",
    "text": "2 Closed System Dynamics\nThe guiding question of any dynamical system is deceptively simple:\n\n“Given enough time, where does the system go?”\n\nThe conventional mathematical answer is that it converges to a steady-state path—a trajectory in which the internal forces of the system reach dynamic balance. Yet in closed systems, this “steady-state” is often mistaken for an indicator of optimality or efficiency. In reality, it is simply an outcome permitted by the structure, not selected for its virtue.\nHistorically, equilibrium emerged in static contexts: balance of forces in classical mechanics, concentration in chemical equilibrium, or price equalization in Walrasian economics. These notions carried an implicit assumption of desirability: what is balanced is often assumed to be good. But in dynamical systems, particularly those expressed as vector fields, equilibrium must be reinterpreted. The long-run behavior may include fixed points (sinks, saddles, sources), or more complex attractors like limit cycles, quasi-periodic orbits, or chaotic regimes. The steady-state path is not a normative endpoint—it is a constraint-compatible attractor, shaped by the system’s geometry.\nIn our context, a steady-state is defined as a location in the system’s state space where the vector field vanishes: \\[\n\\dot{\\mathbf{x}} = f(\\mathbf{x}) = 0,\n\\] or, more broadly, where the system’s flow settles into a repeating or invariant configuration governed by internal dynamics alone.\nBut crucially, in social and economic systems, the existence of such a steady-state does not justify its moral legitimacy. A dynamical system may converge toward labor extinction or monopolized capital reproduction—not because these are efficient or just, but because they are structurally encoded. That is:\n\nPersistence is positional, not performative. Steady-states do not signal merit—they expose geometry.\n\nWe formalize our model within the general structure of an autonomous system:\n\nDefinition (Autonomous System):\nA system is autonomous if its evolution depends solely on state, not time: \\[\n\\dot{\\mathbf{x}} = f(\\mathbf{x}), \\quad \\mathbf{x}(0) = \\mathbf{x}_0,\n\\] where \\(f : \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\) is smooth.\n\nSuch systems are closed if no external forcing, structural shocks, or additional dimensions intervene. These assumptions imply that the system’s phase space is governed by internal logic alone.\n\nAssumption 1 (Autonomy): The vector field \\(f\\) is time-invariant: \\(\\partial f / \\partial t = 0\\).\nAssumption 2 (Closure): All evolution arises from the existing state; no new variables or information enter.\n\nThese assumptions are not cosmetic—they define a world in which only internally available motion is possible.\nAccordingly, the state space \\(\\mathcal{M}\\) is typically modeled as a smooth manifold, encoding both geometric constraints and domain-specific feasibility:\n\nIn ecology, \\(\\mathbb{R}_+^n\\) enforces non-negativity.\nIn replicator dynamics, the unit simplex \\(\\Delta^{n-1}\\) ensures probabilistic normalization: \\(\\sum x_i = 1\\).\nIn capital-labor systems, we may constrain motion to the set of feasible capital/labor ratios or normalized wealth shares.\n\n\nThe manifold does not just host the system—it constrains its fate.\nGeometry is not inert background; it is active architecture.\n\nAs a canonical illustration, consider a discrete-time Markov chain: \\[\n\\mathbf{x}_{t+1} = P \\mathbf{x}_t,\n\\] where \\(P\\) is a row-stochastic matrix and \\(\\mathbf{x}_t \\in \\Delta^{n-1}\\). The state evolves entirely within the simplex, preserving closure and total mass.\n\nThe invariant distribution \\(\\mathbf{x}^*\\) satisfies \\(P \\mathbf{x}^* = \\mathbf{x}^*\\).\nThe transition is internal, memoryless, and fully endogenous.\n\nThis analogy demonstrates that both deterministic continuous-time systems (e.g., ODEs) and stochastic discrete-time systems (e.g., Markov chains) operate under the same architectural logic:\n\nChange is allowed only within structure Innovation is not generated—it is assumed absent\n\nThus, when economic models use “equilibrium” to describe such systems, the term often smuggles in normative meaning under structural constraints. Our reformulation rejects this conflation. In our model:\n\nThe steady-state may represent labor extinction.\nCapital may persist by structural inertia.\nFeedbacks may reinforce inequality even without growth.\n\nThe central insight is that equilibrium is not a justification. It is a geometric consequence.\nThe system does not select for good outcomes—it selects what fits according to the system’s pre-existing structure. Equilibrium in a real dynamic model is not a name for a desirable or normatively fair path—as it often appears in unrealistic normative models—but rather the ideal limit of motion under realistically imposed constraints. It is where the system stops—not because it ought to—but because it can proceed no further within the model’s structure.",
    "crumbs": [
      "Apps",
      "돈의 동역학",
      "동역학 기초수학"
    ]
  },
  {
    "objectID": "d_2_elementary.html#normalization",
    "href": "d_2_elementary.html#normalization",
    "title": "동역학 기초수학",
    "section": "3 Normalization",
    "text": "3 Normalization\n\nfrom Levels to Shares: ODEs as Markov-like Systems\n\nIn many economic or ecological models, the system is initially formulated in absolute levels. Consider a general two-dimensional ODE system describing the interaction of two populations—such as labor (\\(L\\)) and capital (\\(C\\)), or prey and predator:\n\\[\n\\begin{aligned}\n\\dot{L}(t) &= f_L(L, C), \\\\\n\\dot{C}(t) &= f_C(L, C).\n\\end{aligned}\n\\]\nThis level-based description captures total quantity dynamics. However, in many contexts, what matters is not the absolute size of each group but their relative share in the system. For this, we define normalized states:\n\\[\n\\ell(t) = \\frac{L(t)}{L(t) + C(t)}, \\quad c(t) = \\frac{C(t)}{L(t) + C(t)},\n\\]\nso that \\(\\ell + c = 1\\). The state space becomes the 1-simplex \\(\\Delta^1 = \\{ (\\ell, c) \\in \\mathbb{R}^2 \\mid \\ell + c = 1, \\, \\ell, c \\ge 0 \\}\\).\nLet \\(S(t) = L(t) + C(t)\\) denote the total. Then using the quotient rule, the share dynamics are:\n\\[\n\\dot{\\ell} = \\frac{f_L \\cdot S - L \\cdot (f_L + f_C)}{S^2}.\n\\]\nRewriting in terms of \\(\\ell\\) and \\(c\\):\n\\[\n\\dot{\\ell} = \\ell (f_L - \\bar{f}), \\quad \\text{where } \\bar{f} = \\ell f_L + c f_C.\n\\]\nThis equation takes the exact form of replicator dynamics, familiar from evolutionary game theory. It describes the differential success of one component relative to the population-weighted average. Many nonlinear level-based systems can be recast as share-based, constrained systems evolving on a simplex.\nGeneralizing to \\(n\\)-dimensional systems: suppose we have \\(\\mathbf{x}(t) = (x_1, \\dots, x_n) \\in \\mathbb{R}_+^n\\), and define normalized shares:\n\\[\n\\tilde{x}_i(t) = \\frac{x_i(t)}{\\sum_{j=1}^n x_j(t)}.\n\\]\nThen the normalized dynamics become:\n\\[\n\\dot{\\tilde{x}}_i = \\tilde{x}_i \\left[ g_i(\\mathbf{x}) - \\sum_j \\tilde{x}_j g_j(\\mathbf{x}) \\right],\n\\]\nwhich is again replicator-type dynamics—a nonlinear but share-preserving flow over the simplex \\(\\Delta^{n-1}\\).\nNow consider the special case where the original system is linear:\n\\[\n\\dot{\\mathbf{x}} = A \\mathbf{x}, \\quad \\text{with } A \\text{ a non-negative matrix}.\n\\]\nAfter normalization, the system evolves as:\n\\[\n\\dot{\\tilde{\\mathbf{x}}} = \\tilde{\\mathbf{x}} \\cdot (A - \\bar{a} I),\n\\]\nwhere \\(\\bar{a} = \\tilde{\\mathbf{x}}^\\top A \\mathbf{1}\\) is the average growth rate. This dynamics mimics the evolution of a discrete-time Markov chain in continuous time—particularly when \\(A\\) is column-stochastic (or conserves total mass, i.e., \\(\\sum_i \\dot{x}_i = 0\\)).\nKey Structural Insight:\n\nLevel dynamics describe absolute accumulation or depletion.\n\nShare dynamics describe relative competition or reproduction.\n\nThe latter induces a geometry over the simplex, mirroring that of Markov chains or probabilistic flows.\n\nThis connection is more than formal. In applied settings, analyzing normalized dynamics often simplifies the structure, isolates systemic selection mechanisms, and aligns with probabilistic interpretations.\nA closed ODE system can often be interpreted in dual form:\n\nAs mass evolution in \\(\\mathbb{R}_+^n\\)\n\nAs probability evolution in \\(\\Delta^{n-1}\\)\n\nBoth views preserve internal consistency but emphasize different types of systemic constraint: conservation of total quantity vs. invariance of share structure.",
    "crumbs": [
      "Apps",
      "돈의 동역학",
      "동역학 기초수학"
    ]
  },
  {
    "objectID": "d_2_elementary.html#fixed-points-and-their-existence",
    "href": "d_2_elementary.html#fixed-points-and-their-existence",
    "title": "동역학 기초수학",
    "section": "4 Fixed Points and Their Existence",
    "text": "4 Fixed Points and Their Existence\nA central goal in the study of closed dynamical systems is to characterize their long-run behavior. The most fundamental concept in this regard is the fixed point—a state in which the system remains unchanged under its own dynamics.\n\nDefinition (Fixed Point):\nA point \\(\\mathbf{x}^* \\in \\mathbb{R}^n\\) is a fixed point of the system \\(\\dot{\\mathbf{x}} = f(\\mathbf{x})\\) if: \\[\nf(\\mathbf{x}^*) = 0.\n\\]\n\nAt such a point, the vector field vanishes; the system “comes to rest” in a dynamical sense. In closed systems, fixed points are endogenous attractors: they arise not by design but by the internal logic of the system itself.\n\nTheorem (Brouwer’s Fixed Point Theorem):\nIf \\(f\\) is continuous and maps a compact and convex subset \\(K \\subset \\mathbb{R}^n\\) into itself, then there exists at least one \\(\\mathbf{x}^* \\in K\\) such that \\(f(\\mathbf{x}^*) = 0\\).\n\nThis foundational result guarantees the existence of fixed points in many applied settings, including economics and evolutionary biology, especially when the state space is constrained—for example, to the positive orthant \\(\\mathbb{R}_+^n\\) or the unit simplex \\(\\Delta^{n-1}\\).\nHowever, existence does not imply uniqueness, stability, or optimality. There may be many fixed points, some of which are unstable or economically inefficient.\nIn the context of replicator dynamics, which often result from normalization of level-based ODEs (as shown in the previous section), fixed points have a natural interpretation:\n\nExample (Replicator Dynamics):\nLet \\(\\mathbf{x} = (x_1, x_2, x_3) \\in \\Delta^2\\) represent strategy shares in a 3-type population. The replicator equation is: \\[\n\\dot{x}_i = x_i\\left[(A \\mathbf{x})_i - \\mathbf{x}^\\top A \\mathbf{x}\\right].\n\\] Every Nash equilibrium corresponds to a fixed point of this system.\n\nIn this setting, the equilibrium condition says that any strategy with positive share must yield the same expected payoff as the population average. The resulting fixed point \\(\\mathbf{x}^*\\) reflects invariance under self-replicating dynamics: if every strategy earns the average return, no further adjustment occurs.\nMore broadly, fixed points serve as candidates for global asymptotic behavior—states toward which the system may converge over time. Whether or not this convergence occurs depends on their stability, a topic we will address in the next section. But even without stability, fixed points offer an essential structural insight: Fixed points are not always efficient. They are not guaranteed to maximize a welfare function, only to satisfy internal consistency.\nThis distinction is especially important in economic models. Market equilibria, for example, may persist due to structural constraints, not because they are Pareto optimal. In ecological models, predator-prey balance may be achieved at inefficient population levels, depending on the interaction parameters.\nFinally, fixed points are the bridge between nonlinear systems and their linear approximations. In order to study local stability, we must first locate these points. They form the anchors around which the system’s behavior can be analyzed geometrically.\nFixed points exist under mild conditions (Brouwer):\n\nThey are invariant under the system’s own dynamics.\n\nThey may correspond to equilibria in economic or evolutionary settings.\n\nTheir role is structural, not necessarily optimal.\n\nTheir stability and selection are determined by local geometry, but they are ultimately constrained by the global topology of the system.",
    "crumbs": [
      "Apps",
      "돈의 동역학",
      "동역학 기초수학"
    ]
  },
  {
    "objectID": "d_2_elementary.html#local-stability-and-linear-approximation",
    "href": "d_2_elementary.html#local-stability-and-linear-approximation",
    "title": "동역학 기초수학",
    "section": "5 Local Stability and Linear Approximation",
    "text": "5 Local Stability and Linear Approximation\nOnce fixed points have been identified, the next question is whether they are locally stable—that is, whether the system returns to the fixed point after a small perturbation or diverges away. This leads to a key analytical technique: linearization, which provides a geometric approximation of the nonlinear system near a fixed point.\n\nDefinition (Linearization and Jacobian Matrix):\nGiven a system \\(\\dot{\\mathbf{x}} = f(\\mathbf{x})\\) and a fixed point \\(\\mathbf{x}^*\\), the system near \\(\\mathbf{x}^*\\) is approximated by: \\[\n\\dot{\\mathbf{x}} \\approx J(\\mathbf{x}^*)(\\mathbf{x} - \\mathbf{x}^*),\n\\] where the Jacobian matrix \\(J(\\mathbf{x}^*)\\) is: \\[\nJ(\\mathbf{x}^*) = \\left[ \\frac{\\partial f_i}{\\partial x_j}(\\mathbf{x}^*) \\right]_{i,j}.\n\\]\n\nThis approximation turns the nonlinear system into a linear one locally, enabling the application of spectral methods. The eigenvalues of the Jacobian dictate how perturbations evolve:\n\nAll eigenvalues with negative real parts → locally asymptotically stable (exponential convergence to equilibrium).\nAny eigenvalue with a positive real part → locally unstable.\nAll eigenvalues with nonzero real parts → the fixed point is a hyperbolic steady state.\n\n\nTheorem (Hartman–Grobman):\nIf \\(J(\\mathbf{x}^*)\\) has no eigenvalues with zero real part, the nonlinear system near \\(\\mathbf{x}^*\\) is topologically conjugate to its linearization. That is, it behaves qualitatively like the linearized system in a neighborhood of \\(\\mathbf{x}^*\\).\n\n\nExample (Community Matrix in Ecology):\nFor a model such as \\(\\dot{\\mathbf{x}} = \\operatorname{diag}(\\mathbf{x}) A \\mathbf{x}\\), with fixed point satisfying \\(A \\mathbf{x}^* = 0\\), the Jacobian becomes: \\[\nJ(\\mathbf{x}^*) = \\operatorname{diag}(\\mathbf{x}^*) A.\n\\] This community matrix expresses how each population or strategy reacts to small changes in others.\n\nThe Jacobian spectrum captures the local geometry of the vector field, but this geometry does not emerge in isolation. It reflects a projection of the system’s global structure, shaped by its interaction rules, constraints, and feedback topology. In this sense, local geometry does not determine the system—it merely expresses, in differential form at the fixed point, the localized imprint of the global architecture.\n\nExample (Predator–Prey System):\nIn a Lotka–Volterra system, if \\(J(\\mathbf{x}^*)\\) has purely imaginary eigenvalues, the system exhibits neutral cycles.\nIf eigenvalues have negative real parts, trajectories spiral back to equilibrium.\n\nIn finance or economics, local stability is often interpreted as bounded response of prices, capital stocks, or returns. But such interpretations are frequently assumed, not derived from first principles.\nFor example:\n\nIn dynamic asset pricing, if gross return \\(R_t\\) is used, stability typically assumes \\(0 &lt; R_t &lt; 1\\) (i.e., contraction).\nWhen net return \\(r_t\\) is used, the “stable” range becomes \\(-1 &lt; r_t &lt; 1\\).\nThese assumptions echo the convergence criteria of geometric series, though models rarely state this explicitly.\n\nIn stochastic frameworks, such as intertemporal asset pricing, the Martingale Convergence Theorem is often invoked under a discounted summation condition:\n\\[\n\\mathbb{E}\\left[\\sum_{t=0}^\\infty \\beta^t R_t\\right] &lt; \\infty,\n\\]\nThis converges if and only if \\(\\beta R &lt; 1\\), which is again a condition for the convergence of an infinite geometric series. It corresponds, in spectral terms, to requiring that the spectral radius of the return operator be less than one.\nThus, across frameworks:\n\nNegative real eigenvalues (in continuous-time ODEs) imply contraction via exponential decay.\nSpectral radius &lt; 1 (in discrete-time or stochastic models) ensures convergence through bounded iteration.\nImaginary eigenvalues indicate cyclic or oscillatory local behavior.\n\n\nYet in mainstream economic models, these conditions are often treated as if structurally self-evident, when they are in fact mathematical assumptions—frequently unacknowledged and untested.\nDifferent parameterizations (e.g., gross vs. net returns) shift the meaning of “stability,” but this shift is rarely made explicit.\n\nWhat appears as “natural stability” is often a restatement of basic spectral convergence conditions. The deeper question is not whether the system is stable, but what structural mechanisms produce or undermine stability—and whether they correspond to economic reality or analytical convenience.\nLocal linearization thus acts as a microscope: it allows us to classify equilibrium behavior based on local differential structure, even if the global landscape remains nonlinear and complex. Local stability is not a measure of global optimality. It is a statement about local resilience, shaped by the system’s internal feedback at that point.",
    "crumbs": [
      "Apps",
      "돈의 동역학",
      "동역학 기초수학"
    ]
  },
  {
    "objectID": "d_2_elementary.html#replicator-dynamics-and-nash-equilibrium",
    "href": "d_2_elementary.html#replicator-dynamics-and-nash-equilibrium",
    "title": "동역학 기초수학",
    "section": "6 Replicator Dynamics and Nash Equilibrium",
    "text": "6 Replicator Dynamics and Nash Equilibrium\nReplicator dynamics is a canonical framework for modeling how the relative shares of competing strategies evolve over time under selection pressure. The dynamics are defined on the unit simplex \\(\\Delta^{n-1} = \\{ \\mathbf{x} \\in \\mathbb{R}^n \\mid \\sum x_i = 1,\\, x_i \\ge 0 \\}\\), which represents normalized population shares, probability distributions, or strategy proportions. The simplex is a smooth manifold with boundary, meaning that trajectories are constrained not only by dimensionality but also by the non-negativity and summation conditions.\nLet \\(A\\) be an \\(n \\times n\\) payoff matrix, and let \\(\\mathbf{x}(t) \\in \\Delta^{n-1}\\) be the population state. The replicator equation is given by:\n\\[\n\\dot{x}_i = x_i\\left[(A\\mathbf{x})_i - \\mathbf{x}^\\top A \\mathbf{x}\\right], \\quad i = 1, \\dots, n.\n\\]\nThis describes a nonlinear system where each type grows or declines based on its relative performance compared to the population average. The term \\((A\\mathbf{x})_i\\) represents the fitness or payoff of type \\(i\\), and \\(\\mathbf{x}^\\top A \\mathbf{x}\\) is the average fitness.\n\nIf \\((A\\mathbf{x})_i &gt; \\mathbf{x}^\\top A \\mathbf{x}\\), then \\(x_i\\) increases.\nIf \\((A\\mathbf{x})_i &lt; \\mathbf{x}^\\top A \\mathbf{x}\\), then \\(x_i\\) decreases.\n\n\nRemark: The geometry of the simplex ensures that the total sum \\(\\sum x_i = 1\\) is preserved over time. This is crucial—it implies that the vector field is always tangential to the simplex, never pointing outward or inward, thus reinforcing the idea of a closed system on a constrained manifold.\n\nEvery Nash equilibrium \\(\\mathbf{x}^*\\) of the game defined by payoff matrix \\(A\\) is a fixed point of the replicator dynamics. This means:\n\\[\nx_i^* &gt; 0 \\Rightarrow (A\\mathbf{x}^*)_i = \\mathbf{x}^{*\\top} A \\mathbf{x}^*, \\quad \\forall i.\n\\]\nAt such a point, no type has an incentive to grow or shrink—each performs identically in expectation, relative to the population average.\nThese fixed points reflect internal compatibility, not global optimality. Even suboptimal strategies can persist if they are structurally embedded or protected by the system’s interaction structure.\nTo analyze behavior near such equilibria, we compute the Jacobian matrix \\(J(\\mathbf{x}^*)\\) of the replicator dynamics. If none of its eigenvalues lie on the imaginary axis—i.e., if \\(\\mathbf{x}^*\\) is a hyperbolic steady state—then the local behavior is governed by the linearized system.\nThis is precisely where we invoke the Hartman–Grobman theorem (as previously discussed): local trajectories behave qualitatively like those of the linearized system near the fixed point.\n\nIf all eigenvalues have negative real parts, the Nash equilibrium is locally asymptotically stable.\nIf any eigenvalues have positive real parts, the point is locally unstable.\nA mixture of signs yields saddle-type behavior.\n\n\nExample (3-Strategy Game):\nIn a rock–paper–scissors setup with equal payoffs, the interior Nash equilibrium is neutrally stable: the Jacobian has purely imaginary eigenvalues, resulting in closed orbits. However, small deformations to the payoff structure can lead to damping (attracting focus) or amplifying spirals (repelling cycles), depending on the spectral shift.\n\nBoundary behavior: If the system approaches the boundary of the simplex, such as when one type goes extinct (\\(x_i \\to 0\\)), then the dynamics reduce in dimension and may settle into absorbing states—a hallmark of path dependence and structural lock-in, often observed in socioeconomic systems.\nThere exists a formal topological equivalence between replicator dynamics and Lotka–Volterra systems. Specifically, the 3-dimensional replicator system:\n\\[\n\\dot{x}_i = x_i\\left[(A\\mathbf{x})_i - \\mathbf{x}^\\top A \\mathbf{x}\\right], \\quad \\sum x_i = 1,\n\\]\nis diffeomorphic to a 2-dimensional generalized Lotka–Volterra system via coordinate transformations that map the simplex \\(\\Delta^{n-1}\\) to an open subset of \\(\\mathbb{R}^{n-1}\\) (cf. Hofbauer and Sigmund, 1998).\nThis correspondence highlights a shared logic of nonlinear self-interaction, feedback amplification, and constrained growth across domains:\n\nIn ecology, predator-prey cycles arise without optimization.\nIn economics, inefficient strategies or norms may persist by structural embedding.\nIn institutional dynamics, dominant behaviors may lock in without being globally optimal.\n\n\nTakeaway:\nA Nash equilibrium in replicator dynamics is not merely a rest point of strategic interaction. It is often a hyperbolic fixed point shaped and stabilized by the system’s internal structure.\nIts stability reflects not only the local geometry but the global constraints and feedback architecture of the system.",
    "crumbs": [
      "Apps",
      "돈의 동역학",
      "동역학 기초수학"
    ]
  },
  {
    "objectID": "d_2_elementary.html#perronfrobenius-and-dominant-states",
    "href": "d_2_elementary.html#perronfrobenius-and-dominant-states",
    "title": "동역학 기초수학",
    "section": "7 Perron–Frobenius and Dominant States",
    "text": "7 Perron–Frobenius and Dominant States\nIn both discrete-time Markov chains and linear continuous-time ODE systems, the question of long-run behavior often reduces to identifying dominant structures—not necessarily the best performing, but the most structurally entrenched. These are the states or directions that persist, absorb, and eventually dominate the system’s trajectory.\nLet us begin with the discrete-time case.\n\n7.1 Discrete-time: Markov Chains and Invariant Distributions\nConsider a finite-state Markov chain with transition matrix \\(P \\in \\mathbb{R}^{n \\times n}\\) satisfying:\n\n\\(P_{ij} \\ge 0\\) for all \\(i,j\\) (non-negativity),\n\\(\\sum_j P_{ij} = 1\\) for all \\(i\\) (row stochasticity).\n\nThis defines an evolution of probability distributions over \\(n\\) states:\n\\[\n\\mathbf{x}_{t+1} = P \\mathbf{x}_t.\n\\]\nThe long-run behavior is governed by the invariant distribution \\(\\boldsymbol{\\pi}\\) satisfying:\n\\[\nP \\boldsymbol{\\pi} = \\boldsymbol{\\pi}.\n\\]\nUnder the mild condition that \\(P\\) is irreducible and aperiodic, the Perron–Frobenius theorem guarantees the existence and uniqueness of a strictly positive invariant vector \\(\\boldsymbol{\\pi} &gt; 0\\). Regardless of the initial condition \\(\\mathbf{x}_0\\), the system converges:\n\\[\n\\lim_{t \\to \\infty} \\mathbf{x}_t = \\boldsymbol{\\pi}.\n\\]\n\nTheorem (Perron–Frobenius, stochastic form):\nFor an irreducible stochastic matrix \\(P\\), the largest eigenvalue is \\(\\lambda_1 = 1\\), with a unique strictly positive eigenvector \\(\\boldsymbol{\\pi}\\) satisfying \\(P\\boldsymbol{\\pi} = \\boldsymbol{\\pi}\\). All other eigenvalues satisfy \\(|\\lambda_i| &lt; 1\\).\n\nThis invariant distribution encodes the long-run frequency of each state—not because those states are inherently superior, but because they are structurally reinforced.\n\n\n7.2 Continuous-Time: Linear ODEs and Eigenvector Dominance\nNow consider a linear continuous-time system:\n\\[\n\\dot{\\mathbf{x}} = A \\mathbf{x},\n\\]\nwhere \\(A \\in \\mathbb{R}^{n \\times n}\\) is a non-negative, irreducible matrix. The solution is given by:\n\\[\n\\mathbf{x}(t) = e^{At} \\mathbf{x}_0.\n\\]\nIn this setting, Perron–Frobenius theory again applies: the matrix \\(A\\) has a dominant eigenvalue \\(\\lambda_1 \\in \\mathbb{R}\\) with the largest real part, and the corresponding eigenvector \\(\\mathbf{v}_1 &gt; 0\\) determines the direction of long-run growth:\n\\[\n\\lim_{t \\to \\infty} \\frac{\\mathbf{x}(t)}{\\|\\mathbf{x}(t)\\|} = \\frac{\\mathbf{v}_1}{\\|\\mathbf{v}_1\\|}, \\quad \\text{assuming } \\mathbf{x}_0 \\not\\perp \\mathbf{v}_1.\n\\]\nThat is, over time the trajectory aligns with \\(\\mathbf{v}_1\\), even if initial conditions pointed elsewhere. All other modes decay (or oscillate) relative to this dominant direction.\n\nSpectral Dominance Principle:\nThe eigenvector associated with the largest eigenvalue absorbs and aligns the system’s trajectory over time. It defines not just equilibrium, but the structural tendency of the system.\n\n\n\n7.3 Structural Interpretation\nIn both stochastic and linear dynamical settings, dominance may not arise from merit—it may emerges from network topology, feedback structure, and reinforcement logic. This has profound implications for economic and institutional systems.\n\nExample (Capital Allocation):\nSuppose a set of firms or sectors reallocates capital over time according to a stable Markov transition matrix \\(P\\). Even if capital initially diffuses broadly, over time it concentrates in the states corresponding to high values in the Perron vector \\(\\boldsymbol{\\pi}\\). These are not necessarily the most productive or innovative firms—they are the ones with persistent inflows, self-reinforcement, or structural centrality.\n\nThis reveals a critical insight:\n\nPersistence is not always driven by contribution. In many systems, it is driven by connectivity—a measure of how embedded a node is in the system’s transition logic.\n\nThis applies equally in:\n\nFinance (e.g., asset flows concentrating in TBTF institutions),\nSocial networks (e.g., attention concentrating on central nodes),\nPolitical systems (e.g., power concentrating in deeply institutionalized actors).\n\nThe Perron eigenvector is often invisible in short-run observations. It emerges over time—just as compound interest, reputation, or market share slowly diverge. It represents a kind of topological power: a state’s ability to maintain and attract flow simply by virtue of its location in the interaction structure. In spectral graph theory, centrality measures like PageRank are nothing but Perron vectors of stochastic adjacency matrices. The same may apply here—dominance is the outcome of structural centrality, not performance in any exogenous sense.\nThe Perron–Frobenius theorem is not merely an algebraic artifact—it provides a structural lens for understanding long-run dominance:\n\nIn Markov chains, the invariant distribution reflects probabilistic entrenchment.\nIn linear ODEs, the dominant eigenvector determines trajectory alignment.\nIn capital systems, the dominant state absorbs wealth flows without necessarily earning them.\n\nWhat persists is not necessarily what performs—it is what connects, absorbs, and aligns. Dominance in closed systems is often a function of topological position, not marginal productivity.\nThis sets the stage for the next section: to what extent do such structures reflect selection under constraint versus selection by innovation? And how can we distinguish structural lock-in from true competitive superiority?",
    "crumbs": [
      "Apps",
      "돈의 동역학",
      "동역학 기초수학"
    ]
  },
  {
    "objectID": "d_2_elementary.html#selection-under-constraint-in-quasi-closed-systems",
    "href": "d_2_elementary.html#selection-under-constraint-in-quasi-closed-systems",
    "title": "동역학 기초수학",
    "section": "8 Selection under Constraint in Quasi-Closed Systems",
    "text": "8 Selection under Constraint in Quasi-Closed Systems\nNot all persistence is evidence of superior performance. In many empirical systems, especially economic and institutional ones, long-run survival emerges not through innovation, but through structural embedding. These systems function as if they were closed, even when occasional shocks or entries occur. We refer to such regimes as quasi-closed systems: systems where the underlying architecture evolves slowly, and where structural inertia dominates strategic choice.\nQuasi-closed systems share several defining features. First, the entry and exit of agents is infrequent; new firms, products, or individuals rarely alter the overall configuration. Second, the transition structure—how states evolve or shift—remains relatively stable over time. Third, the definition of identities (such as firm types, classes, or asset categories) tends to be fixed ex ante, with little endogenous mutation. Finally, outcome divergence arises primarily through growth differentials, not fundamental innovation. Agents do not win because they change the game; they win because they are positioned advantageously within the existing one.\nThis is not evolutionary competition. It is fixation without adaptation—a dynamic in which the system converges toward a stratified hierarchy of outcomes, even in the absence of meaningful differentiation in performance.\nConventional economic narratives typically invoke a logic of selection by merit: those who persist must be those who perform better. But in quasi-closed systems, structural position outweighs competitive edge. Survival and dominance reflect where an agent begins, not necessarily how they behave. The analogy often drawn from biology—“survival of the fittest”—is more Spencerian than Darwinian. Spencer’s framework implies that survival evidences superiority. Darwin’s actual theory emphasized contextual fit: species survive when they are viable within given constraints, not because they are globally optimal. In this distinction lies a profound shift in explanatory logic: from performance-based selection to constraint-based persistence.\nEconomic systems often display the same structural logic. Capital tends to concentrate not due to constant outperformance, but due to preferential attachment: once capital accumulates disproportionately, future inflows tend to reinforce this advantage. Even in the absence of marginal superiority, historical position becomes a gravitational center. As shown in Perron–Frobenius-type dynamics, long-run dominance is determined not by productivity, but by connectivity within the transition structure—topological advantage, not economic value, dictates persistence.\nThis insight reveals why closed and quasi-closed models are so useful: they allow us to study how structure itself generates inequality. Within these frameworks, we can formally identify fixed points, hyperbolic steady states, and invariant sets. We can trace dominance to the eigenstructure of interaction matrices. We can classify local stability, and project global path dependence. But this analytical clarity comes with a cost. By design, such models exclude structural mutation. They assume that the architecture of the system is fixed—even as it generates asymmetry.\nReal-world economic history, however, is shaped as much by ruptures as by reinforcements. New technologies, regulatory collapses, and institutional realignments frequently destroy the closed logic of existing systems. These events cannot be interpreted as small shocks; they are structural mutations. When automation displaces labor, when digital platforms rewire capital flows, when financial regimes collapse—the topology of the system changes. These are not deviations within a closed phase space. They mark the birth of a new one.\nThe implication is clear. Persistence is not proof of merit. It is often the artifact of selection under structural constraint, reinforced by slowly evolving or frozen interaction rules. Such systems reveal how inequality can emerge and endure even when performance differences are minimal or absent. But they cannot explain how inequality transforms—how new forms emerge, how old structures collapse, how systems adapt or fail to adapt when their internal logics are broken.\nTo understand these transitions, we must move beyond the closed framework. The next chapter begins here: with the breakage of internal feedback loops, the introduction of non-conservative flows, and the rise of irreversible dynamics. In this new setting, structure itself becomes endogenous—subject to mutation, decay, and displacement. Closed systems explain how inequality persists; open systems must explain how inequality evolves.",
    "crumbs": [
      "Apps",
      "돈의 동역학",
      "동역학 기초수학"
    ]
  },
  {
    "objectID": "dichotomy.html",
    "href": "dichotomy.html",
    "title": "Identifying Dichotomy",
    "section": "",
    "text": "Wealth concentration and inequality have long been central issues in economics, attracting renewed attention as wealth disparities continue to widen. The well-documented Pareto Principle suggests that the top 20% holds approximately 80% of total wealth, but empirical studies indicate that this ratio has become even more skewed over time (Piketty and Saez 2003; Alvaredo et al. 2013; Saez and Zucman 2016). Such trends raise fundamental questions about the underlying dynamics of wealth distribution: Is wealth accumulation a zero-sum game? If so, what is the most evident dichotomy that best exposes this phenomenon?\nAt every moment, newly created capital is introduced into the economic system through production, subsequently distributed among economic agents, and then either dissipated through consumption or retained through investment. Unlike a static framework where total wealth is fixed, real-world economic systems operate as dynamic entities where total wealth evolves over time. The interplay between production, distribution, consumption, and investment determines how wealth is allocated and reallocated across different socioeconomic groups. If the system maintains a stable and proportional allocation of new wealth over time, the relative shares of net worth for different groups remain unchanged. However, if allocation mechanisms fluctuate, then the wealth share of one group necessarily increases at the expense of another—reinforcing a zero-sum dynamic.\nThe key question, therefore, is: How should we partition the population to most clearly expose this zero-sum characteristic? In other words, at which wealth cutoff percentile do we observe the highest absolute correlation between two complementary wealth groups? Should we divide the population into the top 10% versus the bottom 90%? Or is a more extreme division, such as top 0.1% versus bottom 99.9%, more effective in revealing these dynamics?",
    "crumbs": [
      "Apps",
      "경제",
      "Identifying Dichotomy"
    ]
  },
  {
    "objectID": "dichotomy.html#introduction",
    "href": "dichotomy.html#introduction",
    "title": "Identifying Dichotomy",
    "section": "",
    "text": "Wealth concentration and inequality have long been central issues in economics, attracting renewed attention as wealth disparities continue to widen. The well-documented Pareto Principle suggests that the top 20% holds approximately 80% of total wealth, but empirical studies indicate that this ratio has become even more skewed over time (Piketty and Saez 2003; Alvaredo et al. 2013; Saez and Zucman 2016). Such trends raise fundamental questions about the underlying dynamics of wealth distribution: Is wealth accumulation a zero-sum game? If so, what is the most evident dichotomy that best exposes this phenomenon?\nAt every moment, newly created capital is introduced into the economic system through production, subsequently distributed among economic agents, and then either dissipated through consumption or retained through investment. Unlike a static framework where total wealth is fixed, real-world economic systems operate as dynamic entities where total wealth evolves over time. The interplay between production, distribution, consumption, and investment determines how wealth is allocated and reallocated across different socioeconomic groups. If the system maintains a stable and proportional allocation of new wealth over time, the relative shares of net worth for different groups remain unchanged. However, if allocation mechanisms fluctuate, then the wealth share of one group necessarily increases at the expense of another—reinforcing a zero-sum dynamic.\nThe key question, therefore, is: How should we partition the population to most clearly expose this zero-sum characteristic? In other words, at which wealth cutoff percentile do we observe the highest absolute correlation between two complementary wealth groups? Should we divide the population into the top 10% versus the bottom 90%? Or is a more extreme division, such as top 0.1% versus bottom 99.9%, more effective in revealing these dynamics?",
    "crumbs": [
      "Apps",
      "경제",
      "Identifying Dichotomy"
    ]
  },
  {
    "objectID": "dichotomy.html#data",
    "href": "dichotomy.html#data",
    "title": "Identifying Dichotomy",
    "section": "2 Data",
    "text": "2 Data\nOur empirical analysis is based on FRED (Federal Reserve Economic Data), covering quarterly observations from 1989 to 2024. The dataset is structured into wealth brackets representing net wealth shares at different percentile levels:\nGroups (stars)\n\n\\(X_4(t)\\): Share of Net Worth Held by the Top 0.1% (99.9th to 100th Wealth Percentiles) (WFRBSTP1300)\n\\(X_3(t)\\): Share of Net Worth Held by the 99th to 99.9th Wealth Percentiles (WFRBS99T999273)\n\nc.f. Share of Net Worth Held by the Top 1% (99th to 100th Wealth Percentiles) (WFRBST01134)\n\n\\(X_2(t)\\): Share of Net Worth Held by the 90th to 99th Wealth Percentiles (WFRBSN09161)\n\\(X_1(t)\\): Share of Net Worth Held by the 50th to 90th Wealth Percentiles (WFRBSN40188)\n\\(X_0(t)\\): Share of Net Worth Held by the Bottom 50% (1st to 50th Wealth Percentiles) (WFRBSB50215)\n\nAdditionally, we reference wealth cutoff amount to identify the minimum level of wealth required to belong to specific top percentile groups:\nCutoff Levels (bins)\n\n\\(p_4\\) or 99.9th: Minimum Wealth Cutoff for the Top 0.1% (99.9th to 100th Wealth Percentiles) (WFRBLTP1311)\n\\(p_3\\) or 99th: Minimum Wealth Cutoff for the 99th to 99.9th Wealth Percentiles (WFRBL99T999309)\n\\(p_2\\) or 90th: Minimum Wealth Cutoff for the 90th to 99th Wealth Percentiles (WFRBLN09304)\n\\(p_1\\) or 50th: Minimum Wealth Cutoff for the 50th to 90th Wealth Percentiles (WFRBLN40302)",
    "crumbs": [
      "Apps",
      "경제",
      "Identifying Dichotomy"
    ]
  },
  {
    "objectID": "dichotomy.html#methodology",
    "href": "dichotomy.html#methodology",
    "title": "Identifying Dichotomy",
    "section": "3 Methodology",
    "text": "3 Methodology\nGiven that total share of net wealth must always sum to one, any partition of the population into two groups remains complementary: \\[\nX_0 + X_1 + X_2 + X_3 + X_4 = 1.\n\\]\nTo quantify the most evident dichotomy, we define two complementary wealth groups for different percentile cutoffs:\n\nWhen \\(p = p_1\\):\n\\[\n  a(t) = X_4(t)+X_3(t)+X_2(t)+X_1(t), \\quad b(t) = X_0(t).\n\\]\nWhen \\(p = p_2\\):\n\\[\n  a(t) = X_4(t)+X_3(t)+X_2(t), \\quad b(t) = X_1(t)+X_0(t).\n\\]\nWhen \\(p = p_3\\):\n\\[\n  a(t) = X_4(t)+X_3(t), \\quad b(t) = X_2(t)+X_1(t)+X_0(t).\n\\]\nWhen \\(p = p_4\\):\n\\[\n  a(t) = X_4(t), \\quad b(t) = X_3(t)+X_2(t)+X_1(t)+X_0(t).\n\\]\n\nFor each cutoff \\(p\\), we compute the correlation:\n\\[\n   y(p) = \\mathrm{corr}\\bigl(a(t),\\,b(t)\\bigr).\n\\]\nWe seek the wealth cutoff \\(p\\) that maximizes the absolute correlation \\(|y(p)|\\), revealing the strongest inverse relationship between the two resulting wealth groups. A high absolute correlation suggests that fluctuations in one group’s net worth share are systematically mirrored by the other, reinforcing the zero-sum nature of wealth accumulation. This dichotomy provides insight into how different capital accumulation mechanisms—through labor or capital investment—shape long-term wealth distribution.\nStrong inverse correlations at certain percentiles may indicate critical thresholds where redistribution policies—such as capital taxation or inheritance taxation—could have the most pronounced effects (Piketty 2011; Piketty, Saez, and Stantcheva 2014; Saez and Zucman 2016).",
    "crumbs": [
      "Apps",
      "경제",
      "Identifying Dichotomy"
    ]
  },
  {
    "objectID": "dichotomy.html#empirical-results",
    "href": "dichotomy.html#empirical-results",
    "title": "Identifying Dichotomy",
    "section": "4 Empirical Results",
    "text": "4 Empirical Results\nAfter processing the quarterly dataset (excluding missing values), we find that the 90th percentile cutoff (\\(p_2\\)) exhibits the highest absolute correlation between the two complementary wealth groups. Specifically, as of 2022-07-01, the minimum wealth required to be in the top 10% was approximately $2,152,788.\nThis suggests that dividing the population into top 10% vs. bottom 90% most effectively reveals the zero-sum nature of wealth redistribution, compared to other partitions such as top 50% vs. bottom 50% or top 0.1% vs. bottom 99.9%.\nThese findings imply that the most structurally significant wealth division occurs between the top 10% and the rest, rather than between the ultra-rich and lower percentiles. This observation aligns with broader discussions on wealth polarization, where the top 10% increasingly dominates capital ownership while the bottom 90% exhibits a more recessive trajectory.",
    "crumbs": [
      "Apps",
      "경제",
      "Identifying Dichotomy"
    ]
  },
  {
    "objectID": "dichotomy.html#conclusions",
    "href": "dichotomy.html#conclusions",
    "title": "Identifying Dichotomy",
    "section": "5 Conclusions",
    "text": "5 Conclusions\nThis study demonstrates that partitioning the population at the 90th wealth percentile provides the most evident dichotomy in revealing the zero-sum nature of capital accumulation. Over time, wealth redistribution mechanisms result in strong inverse correlations between net worth shares of different groups, underscoring the struggling aspects of capital accumulation. The analysis suggests that the structural division between the top 10% and the bottom 90% is more significant than commonly assumed top 1% vs. bottom 99% splits, reinforcing the notion that wealth concentration extends beyond the ultra-rich and affects broader socioeconomic strata.\nThese findings hold important implications for public policy, particularly in debates surrounding progressive taxation, capital gains policies, and inheritance tax structures. A strong inverse correlation at the 90th percentile threshold suggests that redistributive policies targeted at this level could have significant implications for long-term wealth dynamics. This aligns with prior research emphasizing the role of tax policy in shaping wealth accumulation patterns and mitigating excessive concentration of economic power (Piketty, Saez, and Stantcheva 2014; Saez and Zucman 2016).\nWhile this study primarily focuses on empirical correlation analysis, future research should explore additional macroeconomic variables to refine our understanding of wealth distribution dynamics. Incorporating GDP growth rates, investment patterns, labor market structures, and monetary policy changes may provide further insights into how systemic wealth flows evolve in response to economic shocks and policy interventions. Additionally, extending the dataset to include international comparisons could offer a broader perspective on whether the 90th percentile threshold serves as a critical inflection point for wealth inequality across different economies. Further research integrating both empirical and theoretical approaches will be essential in developing more effective strategies for addressing wealth concentration and economic mobility.\n\n\nCode\nimport pandas as pd\nimport pandas_datareader.data as web\nimport matplotlib.pyplot as plt\n\n# 데이터 기간 설정\nstart_date = '1989-07-01'\nend_date = '2024-07-01'\n\n# FRED 데이터 가져오기\nseries_ids = {\n    'X_4': 'WFRBSTP1300',\n    'X_3': 'WFRBS99T999273',\n    'X_2': 'WFRBSN09161',\n    'X_1': 'WFRBSN40188',\n    'X_0': 'WFRBSB50215'\n}\n\ndata = pd.DataFrame()\nfor name, series_id in series_ids.items():\n    data[name] = web.DataReader(series_id, 'fred', start_date, end_date)\n\n# 결측값 제거\ndata.dropna(inplace=True)\n\n\n# FRED에서 wealth level 데이터 가져오기\nwealth_level_ids = {\n    1: 'WFRBLN40302',\n    2: 'WFRBLN09304',\n    3: 'WFRBL99T999309',\n    4: 'WFRBLTP1311'\n}\n\nwealth_levels = {}\nfor p, series_id in wealth_level_ids.items():\n    latest_data = web.DataReader(series_id, 'fred', start_date, end_date).dropna().iloc[-1]\n    wealth_levels[p] = (latest_data.name, latest_data.iloc[0])  # 날짜와 값을 함께 저장\n    \n# 총 관측치 수 출력\nprint(f\"Total number of observations after removing NaN values: {len(data)}\")\n\n\nTotal number of observations after removing NaN values: 141\n\n\n\n\nCode\n# 상관관계 계산 함수\ndef calculate_correlation(a, b):\n    return a.corr(b)\n\n# 상관관계 계산\ncorrelations = {\n    1: calculate_correlation(data['X_4'] + data['X_3'] + data['X_2'] + data['X_1'], data['X_0']),\n    2: calculate_correlation(data['X_4'] + data['X_3'] + data['X_2'], data['X_1'] + data['X_0']),\n    3: calculate_correlation(data['X_4'] + data['X_3'], data['X_2'] + data['X_1'] + data['X_0']),\n    4: calculate_correlation(data['X_4'], data['X_3'] + data['X_2'] + data['X_1'] + data['X_0'])\n}\n\n# 결과 출력\nfor p, y in correlations.items():\n    print(f\"Correlation for p_{p}: {y:.4f}\")\n\n# 최소 상관관계를 가지는 wealth percentile 찾기\nmin_corr_p = min(correlations, key=correlations.get)\npercentile_map = {1: \"50th\", 2: \"90th\", 3: \"99th\", 4: \"99.9th\"}\nprint(f\"Minimum correlation is at p_{min_corr_p}: {percentile_map[min_corr_p]}\")\nwealth_date, wealth_level = wealth_levels[min_corr_p]\nprint(f\"Wealth level for {percentile_map[min_corr_p]} percentile on {wealth_date.date()}: ${wealth_level:,.0f}\")\n\n\n# 그래프 표현\np_values = list(correlations.keys())\ny_values = list(correlations.values())\n\nplt.plot(p_values, y_values, marker='o')\nplt.xlabel('Wealth Cutoff (p)')\nplt.ylabel('Correlation (y(p))')\nplt.title('Wealth Cutoff vs Correlation')\nplt.grid(True)\nplt.show()\n\n\nCorrelation for p_1: -0.9983\nCorrelation for p_2: -0.9998\nCorrelation for p_3: -0.9996\nCorrelation for p_4: -0.9989\nMinimum correlation is at p_2: 90th\nWealth level for 90th percentile on 2022-07-01: $2,153,046",
    "crumbs": [
      "Apps",
      "경제",
      "Identifying Dichotomy"
    ]
  },
  {
    "objectID": "corner_solution.html",
    "href": "corner_solution.html",
    "title": "Corner Solutions",
    "section": "",
    "text": "0.1 1. Introduction\nIn standard economic theory, both consumer preferences and production sets are generally assumed to exhibit convexity (Arrow and Debreu 1954; Debreu 1959). This assumption supports foundational results, including the existence and uniqueness of equilibrium and the efficiency of market allocations. In practice, however, features such as network externalities (Katz and Shapiro 1985; Rochet and Tirole 2003), rent-seeking (Shleifer and Vishny 1993), and multiple equilibria—often culminating in pronounced market dominance—can produce outcomes resembling non-convex preferences (Arthur 1994). In many cases, corner solutions and path-dependent equilibria emerge from winner-takes-all dynamics, concentrated economic power, and barriers to entry.\n\n\n0.2 2. Convexity in Economic Theory\n2.1 Convex Preferences and Production Sets\n\nConsumer preferences are typically modeled with quasi-concave utility functions, yielding convex (or “bowl-shaped”) indifference curves. This setup implies a preference for diversity in consumption, rather than extreme or corner solutions (Debreu 1959).\nProducers are often assumed to face diminishing marginal returns, reflected in a convex production possibility set. Under such conditions, output expansions follow a predictable pattern, and average costs rise eventually.\n\n2.2 Existence and Efficiency of Equilibrium\n\nWith convexity, free market entry, symmetric information, and price-taking behavior, perfectly competitive markets are shown to possess a stable equilibrium that is Pareto efficient (Arrow and Debreu 1954).\nThese results typically rely on fixed-point theorems and the properties of convex sets, ensuring both the existence of equilibrium prices and (in many cases) uniqueness or stability (Debreu 1959).\n\n2.3 Normative Implications\n\nConvexity underpins the normative stance that, absent significant market failures, competitive markets gravitate toward Pareto-efficient resource allocations.\nConsequently, government interventions usually aim to correct externalities, public goods issues, or information asymmetries within a broader context of largely convex preferences and production sets.\n\n\n\n0.3 3. Non-Convexities in Reality\n3.1 Network Externalities and Increasing Returns to Scale\n\nIn contrast to diminishing returns, many digital or platform-based markets exhibit network externalities, or increasing returns to scale (Katz and Shapiro 1985; Rochet and Tirole 2003). As additional users join a platform, its value to each user grows, often driving corner solutions in both production and consumption.\nInstead of smoothly concave utility or production functions, certain markets feature segments of increasing marginal returns, leading to “winner-takes-all” or “winner-take-most” dynamics.\n\n3.2 Coordination Games and Multiple Equilibria\n\nNetwork externalities commonly create coordination games, where each agent’s optimal choice depends on the choices of others. Small initial advantages or random shocks may tip the market toward a specific product or standard, resulting in lock-in (Arthur 1994).\nSuch scenarios can produce multiple Nash equilibria, for instance everyone choosing Product A or everyone choosing Product B, with potentially large welfare differences between them.\n\n3.3 Extreme or Corner Solutions in Consumption and Production\n\nWith robust network effects, consumers or producers may converge on a single brand, platform, or location, effectively marginalizing other options—even if those alternatives might have been preferred under purely convex preferences.\nThese corner solutions deviate from the classical idea that diversification in consumption and moderate scales in production yield optimal outcomes.\n\n3.4 Rent-Seeking and Incumbent Power\n\nDominant firms or groups can exploit political influence—through lobbying or regulatory capture—to fortify their positions, reinforcing non-convex outcomes by stifling competition (Tirole 1988; Shleifer and Vishny 1993).\nRent-seeking intensifies the misallocation of resources, as efforts are diverted to defending or reinforcing incumbents’ power, often via barriers to entry, reduced competition, and growing inequalities.\n\n\n\n0.4 4. Government Interventions\n4.1 Theoretical View: Correcting Market Imperfections\n\nTraditionally, policy interventions focus on addressing market failures, assuming that preferences and technologies remain fundamentally convex and that interventions are limited and transparent.\n\n4.2 Empirical Evidence: Policy Amplifies Non-Convexities\n\nIn reality, incumbents can wield outsized influence through lobbying and political capture, prompting policies that strengthen market concentration (Tirole 1988).\nInstead of fostering genuinely competitive markets, such policies may lock in non-convex outcomes, creating a vicious cycle of entrenched monopolistic power and limited competition.\n\n4.3 Lock-in and Path Dependence\n\nWhen policy-making aligns with incumbent interests, even minor advantages can become self-reinforcing (Arthur 1994).\nConsequently, once a market tips toward a specific firm, region, or product, effective competition may prove infeasible without sweeping policy reforms or disruptive innovation.\n\n\n\n0.5 5. Conclusion\nAlthough classical economic models lean on convex preferences and technologies to assert the existence of unique, efficient equilibria, real-world dynamics often revolve around non-convex phenomena. Network externalities, coordination failures, and rent-seeking can drive corner solutions, multiple equilibria, and lock-in that preserve incumbent advantages. Far from mitigating these issues, government policies sometimes exacerbate them through preferential treatment of dominant actors. Recognizing these non-convex realities is crucial for crafting policy frameworks that transcend purely theoretical assumptions of convexity and address the path-dependent complexity characterizing modern markets.\n\n\n0.6 Appendix: Utilitarian Objective function\n\n\nCode\n#@title Utilitarian objective function\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom scipy.optimize import minimize\n\n# 함수 정의\ndef z_function(x, y, a, b):\n  return y * (x**a) + (1 - y) * ((b - x)**a)\n\n# x, y 범위 및 매개변수 설정\na = 0.3  # 매개변수 a 값 (0과 1 사이)\nb = 20  # 매개변수 b 값\n\nx = np.linspace(0, b, 100)  # x 범위: 0부터 20까지 100개의 점\ny = np.linspace(0, 1, 100)  # y 범위: 0부터 1까지 100개의 점\nX, Y = np.meshgrid(x, y)  # x, y 좌표 격자 생성\n\n\n# Z 값 계산\nZ = z_function(X, Y, a, b)\n\n\ndef negative_z_function(params):\n    x, y = params\n    return -z_function(x, y, a, b)  # 최솟값을 찾기 위해 음수 값 반환\n\n# 초기값 설정 (interior 범위 내)\ninitial_guess = [b / 2, 0.5]\n\n# 경계 조건 설정\nbounds = [(0, b), (0, 1)]\n\n# 최적화 실행\nresult = minimize(negative_z_function, initial_guess, bounds=bounds)\n\n# 결과 추출\nextreme_point_x, extreme_point_y = result.x\nextreme_point_z = z_function(extreme_point_x, extreme_point_y, a, b)\n\nprint(\"Extreme Point (x, y, z):\", extreme_point_x, extreme_point_y, extreme_point_z)\n\n# Calculate Hessian matrix\ndef hessian_matrix(x, y, a, b):\n  \"\"\"Calculates the Hessian matrix of the z_function.\"\"\"\n  d2z_dx2 = a * (a - 1) * (y * (x**(a - 2)) + (1 - y) * ((b - x)**(a - 2)))\n  d2z_dy2 = 0  # Second derivative with respect to y is 0\n  d2z_dxdy = a * (x**(a - 1) - (b - x)**(a - 1))\n  d2z_dydx = d2z_dxdy  # Mixed partial derivatives are equal\n\n  return [[d2z_dx2, d2z_dxdy], [d2z_dydx, d2z_dy2]]\n\n# Determine the type of extreme point\nhessian = hessian_matrix(extreme_point_x, extreme_point_y, a, b)\ndeterminant = np.linalg.det(hessian)\n\nif determinant &gt; 0 and hessian[0][0] &gt; 0:\n  extreme_type = \"Minimum\"\nelif determinant &gt; 0 and hessian[0][0] &lt; 0:\n  extreme_type = \"Maximum\"\nelse:\n  extreme_type = \"Saddle\"\n\nprint(\"Extreme Point Type:\", extreme_type)\n\n\n# 3D 그래프 그리기\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(X, Y, Z)\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('z')\nplt.title('3D Graph of z = y*x^a + (1-y)(b-x)^a')\n\n\n# global interior extreme point 표시\nax.scatter(extreme_point_x, extreme_point_y, extreme_point_z, color='red', marker='o', s=100)\nax.text(extreme_point_x, extreme_point_y, extreme_point_z, f'Extreme Point ({extreme_type})', color='red')\n\nplt.show()\n\n# Contour Plot 그리기\nfig, ax = plt.subplots()\ncontour = ax.contour(X, Y, Z)\nax.set_xlabel('x')\nax.set_ylabel('y')\nplt.title('Contour Plot of z = y*x^a + (1-y)(b-x)^a')\nplt.clabel(contour, inline=1, fontsize=10)\n\n# global interior extreme point 표시\nax.scatter(extreme_point_x, extreme_point_y, color='red', marker='o', s=100)\nax.text(extreme_point_x, extreme_point_y, 'Extreme Point', color='red')\n\nplt.show()\n\n\nExtreme Point (x, y, z): 10.0 0.5 1.9952623149688795\nExtreme Point Type: Saddle\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.7 Appendix: Homogeneous function of degree 1\n\n\nCode\n# Define a return to scale\nscale = 1 # Constant return to scale, i.e. Homogeneous function of degree 1\n\n# Define parameter a\na = 1/4\n\n# total wealth of x\nk_x = 2\n# total wealth of y\nk_y = 2\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 경고 메시지 숨기기\nnp.seterr(invalid='ignore')\n\ndef numerical_derivative(f, X, Y, h=1e-5):\n    \"\"\" Compute numerical partial derivatives using central difference method.\"\"\"\n    dfdx = (f(X + h, Y) - f(X - h, Y)) / (2 * h)  # ∂f/∂x\n    dfdy = (f(X, Y + h) - f(X, Y - h)) / (2 * h)  # ∂f/∂y\n    return dfdx, dfdy\n\n# Define functions u_1(x,y) = x^a * y^(1-a) and u_2(x,y) = (2-x)(2-y)\ndef u1(x, y):\n    return x**(scale*a) * y**(scale*(1-a))\n\ndef u2(x, y):\n    return (k_x - x)**(scale*a) * (k_y - y)**(scale*(1-a))\n\n# Define the grid\nx = np.linspace(0, k_x, 15)\ny = np.linspace(0, k_y, 15)\nX, Y = np.meshgrid(x, y)\n\n# Compute the numerical derivatives (vector field components)\nU1, V1 = numerical_derivative(u1, X, Y)\nU2, V2 = numerical_derivative(u2, X, Y)\n\n# Reduce the density of vectors for better visualization\nx_sparse = np.linspace(0, k_x, 8)\ny_sparse = np.linspace(0, k_y, 8)\nX_sparse, Y_sparse = np.meshgrid(x_sparse, y_sparse)\nU1_sparse, V1_sparse = numerical_derivative(u1, X_sparse, Y_sparse)\nU2_sparse, V2_sparse = numerical_derivative(u2, X_sparse, Y_sparse)\n\n# Plot the combined vector fields and contour plots\n#plt.figure(figsize=(8, 8))\n\n# Contour plots of u_1 and u_2 (level curves only)\ncontour1 = plt.contour(X, Y, u1(X, Y), colors='blue', linestyles='solid', linewidths=1)\ncontour2 = plt.contour(X, Y, u2(X, Y), colors='red', linestyles='dashed', linewidths=1)\n\n# Overlay vector fields\nplt.quiver(X_sparse, Y_sparse, U1_sparse, V1_sparse, color='b', angles='xy', label='∇$u_1$')\nplt.quiver(X_sparse, Y_sparse, U2_sparse, V2_sparse, color='r', angles='xy', label='∇$u_2$')\n\n# Labels and grid\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Gradient Vector Fields & Level Curves of $u_1$ and $u_2$')\nplt.legend()\nplt.grid('scaled')\nplt.axis('square')\n\nplt.tight_layout()\n# Show the plot\nplt.show()\n\n# Compute the sum of gradients\nU_sum = U1 + U2\nV_sum = V1 + V2\n\n# Reduce the density of vectors for better visualization\nU_sum_sparse, V_sum_sparse = numerical_derivative(lambda x, y: u1(x, y) + u2(x, y), X_sparse, Y_sparse)\n\n# Plot the combined vector fields and contour plots\n#plt.figure(figsize=(8, 8))\n\n# Contour plots of u_1 and u_2 (level curves only)\ncontour1 = plt.contour(X, Y, u1(X, Y), colors='blue', linestyles='solid', linewidths=1)\ncontour2 = plt.contour(X, Y, u2(X, Y), colors='red', linestyles='dashed', linewidths=1)\n\n# Overlay sum of gradient vector fields\nplt.quiver(X_sparse, Y_sparse, U_sum_sparse, V_sum_sparse, color='g', angles='xy', label='∇($u_1 + u_2$)')\n\n# Labels and grid\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Sum of Gradient Vector Fields & Level Curves of $u_1$ and $u_2$')\nplt.legend()\n\nplt.grid('scaled')\nplt.axis('square')\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.8 Appendix: Sigmoid utility function\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Define constants\nkx = (np.pi**3 / 2) ** (1/3)\nky = (2**(1/2)) * ((np.pi**3 / 2) ** (1/3))\n\n# Define the grid\nx = np.linspace(0, kx, 1000)\ny = np.linspace(0, ky, 1000)\nX, Y = np.meshgrid(x, y)\n\n# Define the functions\nu1 = 1 - np.cos(X**(1/3) * Y**(2/3))\nu2 = 1 - np.cos((kx - X)**(1/3) * (ky - Y)**(2/3))\n\n# Find intersection points where u1 == u2\nthreshold = 1e-3  # Numerical tolerance for equality\nintersection_mask = np.abs(u1 - u2) &lt; threshold\nX_intersect = X[intersection_mask]\nY_intersect = Y[intersection_mask]\nZ_intersect = u1[intersection_mask]  # u1 and u2 are nearly equal\n\n# Create 3D plot\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection='3d')\n\n# Plot intersection line\nax.scatter(X_intersect, Y_intersect, Z_intersect, color='black', s=10, label='Intersection Line')\n\n# Surface plots for reference\nax.plot_surface(X, Y, u1, cmap='Blues', alpha=0.5)\nax.plot_surface(X, Y, u2, cmap='Reds', alpha=0.5)\n\n# Labels and title\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\nax.set_title('3D Intersection of $u_1$ and $u_2$')\nax.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nArrow, Kenneth J., and Gerard Debreu. 1954. “Existence of an Equilibrium for a Competitive Economy.” Econometrica 22 (3): 265–90.\n\n\nArthur, W. Brian. 1994. Increasing Returns and Path Dependence in the Economy. Ann Arbor, MI: University of Michigan Press.\n\n\nDebreu, Gerard. 1959. Theory of Value: An Axiomatic Analysis of Economic Equilibrium. New Haven, CT: Yale University Press.\n\n\nKatz, Michael L., and Carl Shapiro. 1985. “Network Externalities, Competition, and Compatibility.” The American Economic Review 75 (3): 424–40.\n\n\nRochet, Jean-Charles, and Jean Tirole. 2003. “Platform Competition in Two-Sided Markets.” Journal of the European Economic Association 1 (4): 990–1029.\n\n\nShleifer, Andrei, and Robert W. Vishny. 1993. “Corruption.” The Quarterly Journal of Economics 108 (3): 599–617.\n\n\nTirole, Jean. 1988. The Theory of Industrial Organization. Cambridge, MA: MIT Press.",
    "crumbs": [
      "Apps",
      "경제",
      "Corner Solutions"
    ]
  },
  {
    "objectID": "capm.html",
    "href": "capm.html",
    "title": "Revisiting the CAPM",
    "section": "",
    "text": "The Capital Asset Pricing Model (CAPM), developed by Sharpe (1964), Lintner (1965), and Mossin (1966), remains a cornerstone of modern finance, linking expected returns to risk. It classifies risk into two categories: systematic risk, which stems from market-wide factors and cannot be diversified away, and unsystematic risk, which is asset-specific and can be mitigated through diversification. The CAPM posits that only systematic risk, measured by beta, justifies a return premium, as investors can eliminate unsystematic risk by holding a diversified portfolio, ideally approximating the market portfolio. This principle aligns with broader linear factor models like the Arbitrage Pricing Theory (APT) (Ross 1976), which extends the CAPM by incorporating multiple systematic risk factors while similarly dismissing diversifiable risk under no-arbitrage conditions.\nHowever, the CAPM’s empirical validity has been contested. Banz (1981) documented the size effect, where small-cap stocks outperform CAPM predictions, while Basu (1977) identified the value effect, showing excess returns for stocks with high earnings-to-price ratios. These anomalies spurred the development of multifactor models, such as the Fama-French three-factor model (Fama and French 1993), which augment beta with size and value factors. Beyond these, market concentration has emerged as a critical lens for understanding asset pricing deviations. Hou and Robinson (2006) found that firms in concentrated industries earn higher returns, attributing this to economic rents from market power. Edmans (2009) linked ownership concentration to superior performance, while Choi et al. (2017) showed that institutional investors with concentrated portfolios outperform diversified ones. Neuhann and Sockin (2024) explored how financial market concentration distorts capital allocation, and Bustamante and Donangelo (2017) tied product market concentration to industry returns.\nFrom a theoretical perspective, Magill and Quinzii (1996) argued that incomplete markets—lacking sufficient contingent claims—prevent full risk hedging, challenging CAPM assumptions. Cochrane (1996) emphasized the role of investment-based pricing, while Campbell (1992) critiqued volatility as an incomplete risk measure. Socioeconomic analyses, such as Saez and Zucman (2016), further connect market concentration to wealth inequality, highlighting broader implications. This rich body of literature suggests that market structure and concentration significantly complicate the CAPM’s risk-return framework, necessitating a deeper examination.",
    "crumbs": [
      "Apps",
      "금융",
      "Revisiting the CAPM"
    ]
  },
  {
    "objectID": "capm_industry.html",
    "href": "capm_industry.html",
    "title": "Industry CAPM",
    "section": "",
    "text": "This study investigates the empirical validity of the single-factor Capital Asset Pricing Model (CAPM) when applied to value-weighted industry portfolios over a multi-decade horizon (1999–2023). While the CAPM remains a foundational model in asset pricing theory, its assumptions—such as static equilibrium and constant risk loadings (betas)—may be unrealistic in dynamic, evolving markets. In this context, we assess whether the model adequately explains cross-sectional return differences at the industry level.\nWe focus on three core empirical challenges: 1. The time-variation in industry-specific market betas, which undermines the model’s assumption of stable covariance structures. 2. The uncertainty in estimating the market risk premium, which can distort expected returns and cost-of-capital estimations. 3. The presence of persistent pricing errors (alphas) that suggest structural mispricing and potentially omitted risk factors.\nOur approach draws on the empirical framework pioneered by Fama and French (1997), extended with contemporary data and methods. Using firm-level CRSP data and SIC-based industry classification, we construct monthly value-weighted portfolios and implement rolling beta estimations, cross-sectional regressions, and alpha decomposition to evaluate the model’s performance.\nReferences:\n\nFama, Eugene F., and Kenneth R. French. “Industry costs of equity.” Journal of Financial Economics 43.2 (1997): 153–193.\n\nFama-French Data Library. CRSP and SIC Classification Methodologies.",
    "crumbs": [
      "Apps",
      "금융",
      "Industry CAPM"
    ]
  },
  {
    "objectID": "capm_industry.html#introduction",
    "href": "capm_industry.html#introduction",
    "title": "Industry CAPM",
    "section": "",
    "text": "This study investigates the empirical validity of the single-factor Capital Asset Pricing Model (CAPM) when applied to value-weighted industry portfolios over a multi-decade horizon (1999–2023). While the CAPM remains a foundational model in asset pricing theory, its assumptions—such as static equilibrium and constant risk loadings (betas)—may be unrealistic in dynamic, evolving markets. In this context, we assess whether the model adequately explains cross-sectional return differences at the industry level.\nWe focus on three core empirical challenges: 1. The time-variation in industry-specific market betas, which undermines the model’s assumption of stable covariance structures. 2. The uncertainty in estimating the market risk premium, which can distort expected returns and cost-of-capital estimations. 3. The presence of persistent pricing errors (alphas) that suggest structural mispricing and potentially omitted risk factors.\nOur approach draws on the empirical framework pioneered by Fama and French (1997), extended with contemporary data and methods. Using firm-level CRSP data and SIC-based industry classification, we construct monthly value-weighted portfolios and implement rolling beta estimations, cross-sectional regressions, and alpha decomposition to evaluate the model’s performance.\nReferences:\n\nFama, Eugene F., and Kenneth R. French. “Industry costs of equity.” Journal of Financial Economics 43.2 (1997): 153–193.\n\nFama-French Data Library. CRSP and SIC Classification Methodologies.",
    "crumbs": [
      "Apps",
      "금융",
      "Industry CAPM"
    ]
  },
  {
    "objectID": "capm_industry.html#data-and-methodology",
    "href": "capm_industry.html#data-and-methodology",
    "title": "Industry CAPM",
    "section": "2 Data and Methodology",
    "text": "2 Data and Methodology\n\nData Source:\n\nCRSP: Monthly time-series market capitalization data for public stocks listed on Nasdaq, NYSE, and AMEX from 1994-01 to 2023-12.\nFama-French Data Library: Market excess returns (Rm-Rf) and one-month Treasury bill rates as the proxy for the risk-free rate.\n\nTime Frequency and Period: Monthly, covering 30 years (1994-01 to 2023-12), utilizing 5-year rolling windows to estimate monthly industry-specific betas (initial estimation starts from 1999-02).\nIndustry Classification: Ten major industries defined based on SIC codes:\n\nAgriculture, Mining, Construction, Manufacturing, Transportation, Utilities, Wholesale, Retail, Finance, and Services.\nAdjustment1: Reclassified ‘Public’ to ‘Service’, excluded ‘Missing’.",
    "crumbs": [
      "Apps",
      "금융",
      "Industry CAPM"
    ]
  },
  {
    "objectID": "capm_industry.html#model-specification",
    "href": "capm_industry.html#model-specification",
    "title": "Industry CAPM",
    "section": "3 Model Specification",
    "text": "3 Model Specification\nThe single-factor CAPM model used:\n\\[\n(E[r_i] - r_f)=  \\alpha_i + \\beta_i (E[r_m] - r_f)\n\\]\nWhere:\n\n\\(E[r_i]\\): Time-averaged return (i.e., realized net growth rate) of industry portfolio ( i )\n\\(r_f\\): Time-averaged return of the risk-free asset (e.g., one-month T-bill)\n\\(E[r_m]\\): Time-averaged return of the market portfolio, defined as a value-weighted convex combination of all industry portfolios\n\\(\\beta_i\\): Industry-specific market beta, representing the linear projection coefficient onto the market excess return under a single-factor model. It is traditionally assumed to be constant under static equilibrium conditions.\n\\(\\alpha_i\\): Orthogonal component of the industry portfolio’s return relative to the market factor; equivalently, the mean residual from an orthogonal projection onto the market return. This term captures either pricing errors under CAPM or the effects of omitted risk factors.",
    "crumbs": [
      "Apps",
      "금융",
      "Industry CAPM"
    ]
  },
  {
    "objectID": "capm_industry.html#the-alpha",
    "href": "capm_industry.html#the-alpha",
    "title": "Industry CAPM",
    "section": "4 The Alpha",
    "text": "4 The Alpha\n\n4.1 Alpha as Structural Residual\nThe parameter \\(\\alpha_i\\) corresponds to the intercept term in the linear projection:\n\\[\nr_i - r_f = \\alpha_i + \\beta_i (r_m - r_f) + \\varepsilon_i\n\\]\nHere, \\(\\alpha_i\\) represents the expected component of the residual \\(\\varepsilon_i\\), which is orthogonal to the regressor \\((r_m - r_f)\\) by construction. Under the ideal assumptions of the Capital Asset Pricing Model (CAPM), \\(\\alpha_i = 0\\), implying no predictable deviation from the risk–return relationship. However, in empirical settings, \\(\\alpha_i\\) often deviates from zero—either due to omitted risk factors, model misspecification, or more fundamentally, due to structural failures in capital allocation across sectors.\n\n\n4.2 Logical Scenarios Illustrating CAPM’s Limitations\nTo explore how industry structure impacts the CAPM framework, consider two stylized market configurations:\n\nScenario 1: Single-Dominant Industry\nSuppose the Services sector accounts for 70% of total market capitalization. Since the market portfolio is value-weighted, the return on the market portfolio will be highly collinear with the Services sector return. In this case, the market portfolio fails to represent diversified systematic risk, as it is disproportionately driven by one sector’s performance.\nScenario 2: Perfectly Offsetting Dual Industries\nImagine two large industries each holding approximately 45% of market capitalization, and being perfectly negatively correlated. The market return would exhibit minimal volatility, as losses in one sector would be offset by gains in the other. This structure enhances diversification and represents the theoretical ideal behind the CAPM’s diversification assumptions.\n\nIn practice, however, dominant sectors such as Services and Manufacturing are positively correlated, which results in a market portfolio that is both concentrated and volatile. This undermines the CAPM assumption that the market portfolio is a well-diversified proxy for aggregate risk.\n\n\n4.3 Alpha as a Signal of Capital Misallocation**\nWithin CAPM, alpha is often labeled an “anomaly”—a deviation from the expected return under the no-arbitrage risk–return framework. However, when alpha consistently appears in specific industries, particularly in the form of persistent negative alpha, this points not to noise but to a deeper structural inefficiency in how capital is allocated.\nIf industry returns are highly correlated (i.e., \\(\\rho_{i,j} \\to 1\\)) and market capitalization is heavily skewed toward a few dominant sectors, the market portfolio—constructed as a convex combination of sector returns—becomes nearly collinear with those few dominant sectors. In this case, the Security Market Line (SML) appears linear not due to pricing efficiency but due to mechanical dependency built into the structure of capital allocation.\nFrom the CAPM identity:\n\\[\n\\beta_i = \\rho_{i,m} \\frac{\\sigma_i}{\\sigma_m}\n\\]\nIf \\(\\rho_{i,m} \\approx 1\\), then it follows that:\n\\[\nE[r_i] \\approx \\frac{\\sigma_i}{\\sigma_m} E[r_m]\n\\]\nThus, the expected return becomes a scaled function of asset volatility, making the SML appear quasi-deterministic. In such markets, CAPM beta may encode allocative redundancy rather than meaningful pricing information.\nIn a highly inter-dependent and concentrated market structure, a well-fitting SML does not necessarily imply informational efficiency. Instead, it may reflect:\n\nInefficient capital flows\nSystemic overexposure to correlated sectors\nA lack of effective diversification\n\nPersistent negative alpha—especially when concentrated in structurally flawed sectors like finance and transportation—should be interpreted not as statistical error, but as evidence of systemic mispricing and market design failure. These inefficiencies are not easily corrected through arbitrage and may persist due to institutional or regulatory frictions. In this view, the alpha in CAPM may serve as a diagnostic tool for understanding the structurally skewed capital markets.",
    "crumbs": [
      "Apps",
      "금융",
      "Industry CAPM"
    ]
  },
  {
    "objectID": "capm_industry.html#from-1999-to-2023",
    "href": "capm_industry.html#from-1999-to-2023",
    "title": "Industry CAPM",
    "section": "5 From 1999 to 2023",
    "text": "5 From 1999 to 2023\n\n5.1 Historical excess risk premiums of the US stock market\n\n\nCode\n# 30 years of crsp_monthly\n# start_date = \"1994-01-31\" # i.e. '1994-02-01'\n# end_date = \"2023-12-31\"\n\n# Because of 5 year rolling estimation of monthly beta\nstart_date = \"1999-01-31\"\nend_date = \"2023-12-31\"\n\nprint(f\"Start Date: {start_date}\")\nprint(f\"End Date: {end_date}\")\n\n\nStart Date: 1999-01-31\nEnd Date: 2023-12-31\n\n\n\n\nCode\n#@title Libraries and Time-window\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.formula.api as smf\n\nimport sqlite3\n\ntidy_finance = sqlite3.connect(\"../../tidy_finance_python.sqlite\")\n\nfactors_ff3_monthly = pd.read_sql_query(\n  sql=\"SELECT month, mkt_excess, rf FROM factors_ff3_monthly\",\n  con=tidy_finance,\n  parse_dates={\"month\"}\n)\n\n# 1994-01-01 indicates mktcap at 1994-01-31 which is the start date\n# the first return is calculated\ncrsp_monthly = pd.read_sql_query(\n  sql=\"SELECT permno, month, ret, ret_excess, mktcap, mktcap_lag, siccd, industry, exchange FROM crsp_monthly\",\n  con=tidy_finance,\n  parse_dates={\"month\"}\n)\n\n# 5 year rolling estimated beta is available from 1999-01-01\nbeta = (pd.read_sql_query(\n    sql=\"SELECT permno, month, beta_monthly FROM beta\",\n    con=tidy_finance,\n    parse_dates={\"month\"})\n)\n\nbeta_lag = (beta\n  .assign(month = lambda x: x[\"month\"] + pd.DateOffset(months=1))\n  .get([\"permno\", \"month\", \"beta_monthly\"])\n  .rename(columns={\"beta_monthly\": \"beta_lag\"})\n  .dropna()\n)\n\n# Calculate 12-month moving average\nfactors_ff3_monthly['mkt_excess_ma12'] = factors_ff3_monthly['mkt_excess'].rolling(window=12).mean()\n\n# Plot: Market Excess Return with 12-month Moving Average\nplt.figure(figsize=(12, 5))\nplt.plot(factors_ff3_monthly['month'], factors_ff3_monthly['mkt_excess'], label='Monthly Excess Return', color='lightsteelblue')\nplt.plot(factors_ff3_monthly['month'], factors_ff3_monthly['mkt_excess_ma12'], label='12-Month Moving Average', color='darkblue')\nplt.axhline(0, color='gray', linestyle='--', linewidth=1)\nplt.title('Monthly Market Excess Return with 12-Month Moving Average', fontsize=14)\nplt.xlabel('Date')\nplt.ylabel('Excess Return')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n5.2 Structural Shifts in Industry Concentration\n\nInter-Industry Divergence: Growing disparity in concentration levels across industries\nIntra-Industry Consolidation: Increasing dominance of top firms within each industry\n\n\n\nCode\n#@title Number of Firms in Industry Portfolios\n# First, create a dummy column for counting\ncrsp_monthly['count'] = 1\n\n# Create the pivot table\npfo_number = crsp_monthly.pivot_table(\n    values='count',  # The column to aggregate (count in this case)\n    index='month',    # The column to use as index\n    columns='industry', # The column to use as columns\n    aggfunc='sum',    # The aggregation function to use (sum in this case)\n    fill_value=0      # Fill NaN values with 0\n)\n\nsorted_columns = pfo_number.mean().sort_values(ascending=False).index\npfo_number[sorted_columns].plot(\n    kind='line',\n    xlabel='month',\n    ylabel='number of firms',\n    title='Number of Firms in Industry'\n)\nplt.legend(bbox_to_anchor=(1.0, 1.0)) # legend outside\nplt.show()\n\npfo_number[['Missing','Public']].plot(\n    kind='line',\n    xlabel='month',\n    ylabel='number of firms',\n    title='Number of Firms in Industry'\n)\nplt.show()\n\nprint('The average number of firms in Missing industry is', pfo_number['Missing'].mean().round(2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe average number of firms in Missing industry is 2.82\n\n\n\n\nCode\n#@title Industry Concentration Dynamics\n\n# Average Firm Size in Industry Portfolios (Public in Black)\n\npfo_size = crsp_monthly.pivot_table(\n    index='month',\n    columns='industry',\n    values='mktcap',\n    aggfunc='mean'\n)\n\nsorted_columns = pfo_size.mean().sort_values(ascending=False).index\n\nax = pfo_size[sorted_columns].plot(\n    kind='line',\n    xlabel='month',\n    ylabel='mktcap',\n    title='Average Firm Size in Industry Portfolios',\n    linewidth=1.5\n)\n\n# Set Public line to black\nfor line, col in zip(ax.get_lines(), sorted_columns):\n    if col == \"Public\":\n        line.set_color('black')\n        line.set_linewidth(2.0)\n\nplt.legend(bbox_to_anchor=(1.0, 1.0))  # legend outside\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n#@title 산업 내 HHI (Herfindahl-Hirschman Index)\n\n# Step 1: 각 월, 각 산업 내 기업별 시가총액 비중 계산\ncrsp_monthly['mktcap_share'] = (\n    crsp_monthly\n    .groupby(['month', 'industry'], group_keys=False)['mktcap']\n    .transform(lambda x: x / x.sum())\n)\n\n# Step 2: HHI 계산 (각 산업의 각 월에 대해)\nindustry_hhi = (\n    crsp_monthly\n    .assign(mktcap_share_sq=lambda x: x['mktcap_share'] ** 2)\n    .groupby(['month', 'industry'], group_keys=False)['mktcap_share_sq']\n    .sum()\n    .unstack()\n    .sort_index()\n)\n\n# 산업 내 Top-5 Market Cap Share 계산 \ndef top5_share_func(df):\n    # group에는 'month', 'industry'가 포함되므로 사용하지 않음\n    top5_sum = df.nlargest(5, 'mktcap')['mktcap'].sum()\n    total = df['mktcap'].sum()\n    return top5_sum / total if total != 0 else np.nan\n\n# Step: 그룹핑 컬럼을 index로 빼서 apply의 group에서 제거\ntop5_share = (\n    crsp_monthly\n    .sort_values(['month', 'industry', 'mktcap'], ascending=[True, True, False])\n    .set_index(['month', 'industry'])  # &lt;-- group에 포함되지 않게 index로 설정\n    .groupby(['month', 'industry'], group_keys=False)\n    .apply(top5_share_func)  # group에 month/industry 포함되지 않음\n    .unstack()  # 산업별 column\n    .sort_index()\n)\n\nselected_industries = ['Transportation', 'Utilities', 'Retail', 'Manufacturing', 'Services']\n\n# HHI plot\nindustry_hhi[selected_industries].plot(\n    figsize=(12, 5),\n    title='HHI: Industry Concentration Over Time',\n    ylabel='Herfindahl Index',\n    xlabel='Month'\n)\nplt.legend(bbox_to_anchor=(1.0, 1.0))\nplt.show()\n\n# Top-5 Share plot\ntop5_share[selected_industries].plot(\n    figsize=(12, 5),\n    title='Top-5 Market Cap Share in Each Industry Over Time',\n    ylabel='Top-5 Share',\n    xlabel='Month'\n)\nplt.legend(bbox_to_anchor=(1.0, 1.0))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe sharp increase in Top-5 market capitalization share since the post-2009 period highlights a structural shift toward greater industry concentration—particularly within the transportation, utilities, retail, manufacturing, and services sectors. This trend indicates that a small number of dominant firms increasingly account for a disproportionate share of total industry market value.\nWhile average firm size already suggested this pattern, the Top-5 share offers a direct and quantifiable measure. Notably, the Services sector has experienced a persistent rise in concentration since 2011, likely driven by digital transformation, platform-based business models, and network effects. The Manufacturing sector, meanwhile, remained relatively stable until 2019 before undergoing a rapid increase in dominance, possibly due to technology-driven scale economies.\nThese developments coincide with macroeconomic shifts following the 2008 financial crisis, including accommodative policies like quantitative easing (QE), which may have reinforced the “winner-takes-most” dynamics. Importantly, this rising concentration may partially explain observed deviations from CAPM predictions, as industry-level returns become increasingly shaped by a few large-cap firms with idiosyncratic risk-return profiles.\n\n\n5.3 Evolution of Industry Market Cap Shares (1999–2023)\n\n\nCode\n#@title df: Drop industry 'Missing' and Re-classify industry 'Public' to 'Services'\n\n# Copy original\ndf = crsp_monthly.copy()\n\n# Drop Missing\ndf = df[df['industry'] != 'Missing']\n\n# Reclassify Public → Services\ndf.loc[df['industry'] == 'Public', 'industry'] = 'Services'\n\n# Merge with factor data and beta\ndf = (df\n  .merge(beta, how=\"inner\", on=[\"permno\", \"month\"])\n  .merge(beta_lag, how=\"inner\", on=[\"permno\", \"month\"])\n  .merge(factors_ff3_monthly, how=\"inner\", on=[\"month\"])\n)\n\n\n\n\nCode\n#@title Market Cap Share of industry portfolios\npfo_share = df.pivot_table(index='month', columns='industry', values='mktcap', aggfunc='sum')\n\n# Normalize pfo_share to sum to 1 for each row\npfo_share[:] = pfo_share.div(pfo_share.sum(axis=1), axis=0)\n\nsorted_columns = pfo_share.mean().sort_values(ascending=False).index\npfo_share[sorted_columns].plot(\n    kind='line',\n    xlabel='month',\n    ylabel='mktcap',\n    title='Market Cap Share of industry portfolios'\n)\nplt.legend(bbox_to_anchor=(1.0, 1.0)) # legend outside\nplt.show()\n\n\n\n\n\n\n\n\n\nThe market portfolio’s composition—measured by the value-weighted market capitalization share of each industry—has experienced notable structural changes since 1999. While the majority of industries have remained relatively small in terms of aggregate market weight, three sectors—Manufacturing, Services, and Finance—have consistently dominated.\nIn particular, the Manufacturing sector’s dominance has gradually declined from nearly 50% in 1999 to below 40% in 2023. Conversely, the Services sector, especially after absorbing “Public” firms, has expanded significantly, rising from under 20% to over 30% in the same period. The Finance sector saw a sharp decline following the 2008 financial crisis and has since stabilized at a lower level.\nThese compositional shifts reflect evolving patterns in industrial dominance and have direct implications for the systematic risk profile of the aggregate market portfolio. As sectoral weights change, so too does the market beta composition underlying the CAPM framework.\n\n\n5.4 Time-Varying Systematic Risk by Industry\n\n\nCode\n#@title Time-varying industry Market Betas\n\n# ===============================================\n# 1. Market Cap-weighted Industry Beta (Value-Weighted Beta)\n# ===============================================\n# CAPM의 factor loading인 beta는 산업 내 대형 기업일수록 시장과의 공분산에 더 큰 영향을 미치므로,\n# 산업별 단순 평균 beta는 산업의 실제 systematic risk를 과소/과대평가할 수 있습니다.\n# 따라서 각 기업의 시가총액으로 가중평균한 value-weighted beta를 계산합니다.\n\n# Step 1: Beta weighted by market cap\ndf['beta_weighted'] = df['beta_monthly'] * df['mktcap']\n\n# Step 2: Group by month and industry to compute weighted beta\npfo_beta_weighted = (\n    df.groupby(['month', 'industry'])[['beta_weighted', 'mktcap']]\n      .sum()\n      .assign(beta_vw=lambda x: x['beta_weighted'] / x['mktcap'])\n      .reset_index()\n      .pivot(index='month', columns='industry', values='beta_vw')\n)\n\n# ===============================================\n# 2. Time-Series Plot of Value-Weighted Industry Betas\n# ===============================================\nsorted_columns = pfo_beta_weighted.mean().sort_values(ascending=False).index\n\npfo_beta_weighted[sorted_columns].plot(\n    kind='line',\n    figsize=(12, 6),\n    xlabel='Month',\n    ylabel='Value-weighted Beta',\n    title='Time-varying Value-weighted Industry Beta'\n)\nplt.legend(bbox_to_anchor=(1.0, 1.0))\nplt.show()\n\n# ===============================================\n# 3. Boxplot of Value-Weighted Industry Betas\n# ===============================================\n# Melt for seaborn\npfo_beta_weighted_melted = pd.melt(\n    pfo_beta_weighted.reset_index(),\n    id_vars=['month'],\n    value_vars=pfo_beta_weighted.columns\n)\npfo_beta_weighted_melted.columns = ['month', 'industry', 'beta']\n\n# Sort industries by average beta\nmean_beta_vw = pfo_beta_weighted_melted.groupby('industry')['beta'].mean().sort_values(ascending=False)\n\n# Boxplot\nplt.figure(figsize=(10, 6))\nsns.boxplot(\n    y='industry',\n    x='beta',\n    data=pfo_beta_weighted_melted,\n    order=mean_beta_vw.index,\n    orient='h'\n)\nplt.title('Boxplot of Value-weighted Beta for Each Industry (Sorted by Mean)')\nplt.show()\n\n# ===============================================\n# 4. Scatter Plot: Mean Beta vs. Mean Market Cap Share\n# ===============================================\n# Mean industry beta (value-weighted)\nbeta_mean = pfo_beta_weighted.mean()\n\n# Mean market cap share (already normalized)\nmktcap_share_mean = pfo_share.mean()\n\n# Scatter Plot\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=beta_mean, y=mktcap_share_mean)\nfor industry in beta_mean.index:\n    plt.text(beta_mean[industry], mktcap_share_mean[industry], industry, fontsize=9)\nplt.xlabel('Mean Industry Beta (Value-weighted)')\nplt.ylabel('Mean Market Cap Share')\nplt.title('Mean Industry Beta vs. Mean Market Cap Share')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of value-weighted industry betas uncovers several important dynamics in the evolution of systematic risk exposures across sectors.\nFirst, the time-series plot shows a random-walk-like behavior in beta trajectories, suggesting that industry-level risk exposure is far from stable and must be modeled as time-varying. The boxplot reinforces this heterogeneity:\n\nIndustries such as Manufacturing and Retail exhibit narrow beta distributions, indicating consistent risk exposure among firms in these sectors.\n\nIn contrast, Mining and Construction show wide dispersion, pointing to greater intra-industry variability in systematic risk.\n\nA comparison of beta levels also reveals structural asymmetries:\n\nRetail, Utilities, and Agriculture maintain beta values consistently below one, aligning with their roles as defensive sectors.\n\nConversely, the Services sector displays beta values above one, along with a rising market cap share—suggesting it has become a key driver of market returns.\n\nThis imbalance implies that a value-weighted market portfolio—heavily exposed to high-beta sectors—offers limited hedging potential, especially in downturns.\nThe scatter plot of mean beta vs. mean market cap share further illustrates this, with Manufacturing standing out as a structural outlier: it holds an average beta near 1 but dominates in market share. These findings support a more strategic allocation to low-beta sectors, particularly in anticipation of macroeconomic risks. This aligns with recent investment behavior by Buffett, who has increased exposure to retail-sector firms like Ulta Beauty, likely as a hedge against cyclical downturns.\n\n\n5.5 Empirical Testing of CAPM Using Fama-MacBeth Regressions\n\n\nCode\n#@title 10 Value-Weighted industry pfos\n\ndef weighted_avg(x, weights):\n    \"\"\"Calculates the weighted average of a series.\"\"\"\n    return np.average(x, weights=weights)\n\n# Apply weighted_avg function to pivot_table\npfo_vw_ret_excess = df.pivot_table(\n    index='month',\n    columns='industry',\n    values='ret_excess',\n    aggfunc=lambda x: weighted_avg(x, df.loc[x.index, 'mktcap'])\n)\n\npfo_vw_beta_lag = df.pivot_table(\n    index='month',\n    columns='industry',\n    values='beta_lag',\n    aggfunc=lambda x: weighted_avg(x, df.loc[x.index, 'mktcap'])\n)\n\nmean_vw_beta_lag = pfo_vw_beta_lag.mean().rename('mean_beta_lag')\nmean_vw_ret_excess = pfo_vw_ret_excess.mean().rename('mean_ret_excess')\n\nmkt_excess = factors_ff3_monthly['mkt_excess'].mean()\nrf = factors_ff3_monthly['rf'].mean()\n\n\n\n\nCode\n#@title Cross-sectional regressions for each month\n\n# Fama-MacBeth (1973) two-pass procedure \n\nrisk_premiums = (df\n  .groupby(\"month\")[['ret_excess', 'beta_lag']]\n  .apply(lambda x: smf.ols(formula=\"ret_excess ~ beta_lag\", data=x).fit().params)\n  .reset_index()\n)\n\n# Time-series Aggregation (i.e. average)\n# average across the time-series dimension to get the mean risk premium for each characteristic\n# calculate t-test statistics for each regressor,\n# critical values of 1.96 (at 5% significance) or 2.576 (at 1% significance) for two-tailed significance tests\n\nmean_premiums = (risk_premiums\n  .melt(id_vars=\"month\", var_name=\"factor\", value_name=\"estimate\")\n  .groupby(\"factor\")[\"estimate\"]\n  .apply(lambda x: pd.Series({\n      \"mean_premium\": 100*x.mean(),\n      \"t_statistic\": x.mean()/x.std()*np.sqrt(len(x))\n    })\n  )\n  .reset_index()\n  .pivot(index=\"factor\", columns=\"level_1\", values=\"estimate\")\n  .reset_index()\n)\n\n# reporting standard errors of risk premiums, after adjusting for autocorrelation (Newey and West (1987) standard errors)\n\nmean_premiums_newey_west = (risk_premiums\n  .melt(id_vars=\"month\", var_name=\"factor\", value_name=\"estimate\")\n  .groupby(\"factor\")\n  .apply(lambda x: (\n      x[\"estimate\"].mean()/\n        smf.ols(\"estimate ~ 1\", x)\n        .fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 6}).bse\n    ), include_groups=False\n  )\n  .reset_index()\n  .rename(columns={\"Intercept\": \"t_statistic_newey_west\"})\n)\n\nfm_reg = (mean_premiums\n  .merge(mean_premiums_newey_west, on=\"factor\")\n  .round(3)\n)\nfm_reg['mean_premium'] = fm_reg['mean_premium']*12\n\nprint('Annual Risk Premium of Market Beta')\nfm_reg\n\n\nAnnual Risk Premium of Market Beta\n\n\n\n\n\n\n\n\n\nfactor\nmean_premium\nt_statistic\nt_statistic_newey_west\n\n\n\n\n0\nIntercept\n9.432\n3.891\n3.046\n\n\n1\nbeta_lag\n1.860\n0.766\n0.725\n\n\n\n\n\n\n\nWe employ the Fama-MacBeth (1973) two-pass regression method to estimate the annualized market risk premium under the single-factor CAPM framework. The first pass consists of estimating rolling betas for each firm, which are then aggregated into value-weighted industry betas. The second pass involves running monthly cross-sectional regressions of industry excess returns on lagged betas from 1999 to 2023. To account for possible serial correlation, we report both naïve t-statistics and Newey-West (1987) adjusted statistics.\nThe results show a striking pattern:\n\nThe estimated intercept (alpha) averages 9.43% annually, and this deviation from the theoretical risk-free rate is statistically significant (t = 3.89; NW-adjusted t = 3.05).\n\nThe estimated beta risk premium, on the other hand, is only 1.86% annually, with no statistical significance (t = 0.77; NW-adjusted t = 0.73).\n\nThis finding leads to two critical implications:\n\nThe CAPM fails to explain a substantial portion of the cross-sectional variation in industry returns.\nThere is likely a mispricing component or omitted factor structure that the single-factor model cannot capture.\n\nFrom a modeling perspective, the coexistence of a strong alpha and weak beta suggests that estimation errors are compounding: both the time-varying nature of betas and the instability of risk premia contribute to the overall model misspecification. These results are consistent with the view that industry-specific risk profiles may involve multiple dimensions of risk, and that static CAPM assumptions are empirically untenable over long horizons.\n\n\n5.6 Security Market Line and Conditional Alpha\n\n\nCode\n#@title CAPM SML prediction plot\n\nimport matplotlib.ticker as mtick\n\n# Combine beta and return\npfo_sml = pd.concat([mean_vw_beta_lag, mean_vw_ret_excess], axis=1)\npfo_sml = pfo_sml.reset_index().rename(columns={'index': 'industry'})\n\n# CAPM Regression Line (fitted to 10 points)\nmodel = smf.ols('mean_ret_excess ~ mean_beta_lag', data=pfo_sml).fit()\nintercept_capm_fit = model.params['Intercept']\n\n# SML: CAPM predicted line (Rf intercept)\nintercept_capm_theory = rf\n\n# SML: Fama-MacBeth implied line (intercept from fm_reg table)\nintercept_fm = fm_reg.loc[fm_reg['factor'] == 'Intercept', 'mean_premium'].values[0] / 100 / 12  # monthly rate\n\n# Start plot\nplt.figure(figsize=(8, 6))\n\n# Scatter plot of 10 industries\nfor _, row in pfo_sml.iterrows():\n    plt.scatter(row['mean_beta_lag'], row['mean_ret_excess'], color='black')\n    plt.annotate(row['industry'], (row['mean_beta_lag'] + 0.01, row['mean_ret_excess']), fontsize=9)\n\n# Draw SMLs\n# Theoretical CAPM SML (Rf, slope = E[Rm - Rf])\nplt.axline((0, intercept_capm_theory), slope=mkt_excess, linestyle='dashed', color='black', label='CAPM (Rf Intercept)')\n\n# Regression fit line (OLS over 10 industry points)\nplt.axline((0, intercept_capm_fit), slope=mkt_excess, linestyle='dashed', color='red', label='OLS Fit on 10 Points')\n\n# Fama-MacBeth implied line (Intercept from FM regression)\nplt.axline((0, intercept_fm), slope=mkt_excess, linestyle='dashed', color='blue', label='Fama-MacBeth Intercept')\n\n# Format\nplt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\nplt.xlabel('Mean Beta (Lagged)')\nplt.ylabel('Mean Excess Return (Monthly)')\nplt.title('Unconditional Security Market Line (Industry-Level CAPM)')\nplt.legend(loc='lower right')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n#@title 개별 산업의 mispricing 정도를 파악\n\n# 1. Calculate conditional alpha\npfo_sml['capm_pred'] = pfo_sml['mean_beta_lag'] * mkt_excess\npfo_sml['alpha'] = pfo_sml['mean_ret_excess'] - pfo_sml['capm_pred']\n\n# 2. Sort industries by alpha\npfo_sml_sorted = pfo_sml.sort_values(by='alpha', ascending=False)\n\n# 3. Barplot of alpha\nimport seaborn as sns\nplt.figure(figsize=(10, 6))\nsns.barplot(data=pfo_sml_sorted, x='alpha', y='industry', hue='industry', palette='coolwarm', dodge=False)\nplt.axvline(0, color='black', linestyle='--')\nplt.gca().xaxis.set_major_formatter(mtick.PercentFormatter(1.0))\nplt.xlabel('Conditional Alpha (Monthly)')\nplt.title('Industry-level Conditional Alpha under CAPM')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nWe visualize the unconditional Security Market Line (SML) implied by the single-factor CAPM across 10 value-weighted industry portfolios. Each point on the plot corresponds to an industry, placed according to its average market beta (horizontal axis) and realized average excess return (vertical axis) over the sample period.\nThree lines are shown for comparison:\n\nThe black dashed line represents the theoretical CAPM SML, where the intercept equals the average risk-free rate and the slope equals the average market risk premium.\nThe red dashed line is a simple OLS fit across the 10 industry points, which minimizes cross-sectional error.\nThe blue dashed line uses the Fama-MacBeth estimated intercept, incorporating pricing errors in the CAPM framework.\n\nMost industry points lie between the CAPM-predicted line and the Fama-MacBeth-adjusted line. This suggests that while the risk-return relationship remains approximately linear, the CAPM fails to account for substantial pricing errors, as reflected in large intercept terms.\nTo quantify these deviations more precisely, we calculate conditional alphas for each industry. These alphas represent the difference between realized and CAPM-predicted returns, conditional on the industry’s average beta.\n\nPositive alpha implies the industry earned more than predicted by its systematic risk exposure.\nNegative alpha suggests overvaluation relative to CAPM expectations.\n\nThe results reveal persistent mispricing across several sectors, reinforcing earlier conclusions about the model’s empirical inadequacy. The CAPM may still serve as a baseline pricing model, but the presence of large unexplained returns calls for either a multi-factor extension or a fundamental rethinking of the linear risk-return paradigm.",
    "crumbs": [
      "Apps",
      "금융",
      "Industry CAPM"
    ]
  },
  {
    "objectID": "capm_industry.html#conclusions",
    "href": "capm_industry.html#conclusions",
    "title": "Industry CAPM",
    "section": "6 Conclusions",
    "text": "6 Conclusions\nThis short study evaluates the empirical validity of the single-factor CAPM using value-weighted industry portfolios over a 25-year period (1999–2023). Several key findings emerge:\n\nIndustry-specific market betas exhibit substantial time variation, contradicting the CAPM’s assumption of stable factor loadings. This instability weakens the model’s explanatory power over long horizons and complicates its use in asset pricing and cost-of-capital estimation.\nThe market risk premium, when estimated empirically, shows large uncertainty and wide confidence bounds, limiting its practical usefulness in capital budgeting and valuation decisions.\nFama-MacBeth regressions reveal economically large and statistically significant intercepts (alphas), while the estimated risk premium on beta is both small and statistically insignificant. This suggests that the single-factor CAPM omits important pricing components or fails to capture cross-sectional return dynamics.\nIndustry-level CAPM predictions show structural deviations from theoretical SML predictions. Finance and Transportation exhibit relatively low pricing errors, potentially due to regulatory distortions (e.g., “Too Big to Fail”) or reduced market responsiveness.\nLastly, the increasing dominance of a few large-cap firms—particularly in Services and Manufacturing—implies that market-wide returns are increasingly shaped by concentrated industry dynamics. This structural concentration further limits the diversification benefits assumed under standard portfolio theory.\n\nOverall, while CAPM remains a foundational framework in asset pricing, this analysis highlights its limitations in capturing the complexities of modern equity markets—particularly when applied at the industry level over long horizons.",
    "crumbs": [
      "Apps",
      "금융",
      "Industry CAPM"
    ]
  },
  {
    "objectID": "capm_industry.html#footnotes",
    "href": "capm_industry.html#footnotes",
    "title": "Industry CAPM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn this study, firms originally categorized under the “Public” industry in the CRSP database are reassigned to the “Services” industry group. This decision is based on a review of the largest firms within the Public category—such as Tesla, Zoom, Airbnb, PayPal, and Coinbase—which predominantly operate in service-driven, software-based, or platform-oriented business models. Although a few firms like Tesla or Kraft Heinz engage in manufacturing, the overall structure and revenue sources of the Public group are better aligned with the characteristics of modern service industries. This reassignment enhances interpretability in CAPM-based industry comparisons while maintaining consistency in economic logic.↩︎",
    "crumbs": [
      "Apps",
      "금융",
      "Industry CAPM"
    ]
  },
  {
    "objectID": "correlation_crypto.html",
    "href": "correlation_crypto.html",
    "title": "Correlation in Cryptos",
    "section": "",
    "text": "Abstract: 2025년 3월 현재, 시가총액이 크거나 투자자들에게 인기가 많은 주요 암호화폐(popular cryptocurrencies)를 선정하여 지난 1년간의 상관관계를 분석하였다. 대부분의 암호화폐 투자자들은 이러한 주요 암호화폐에 집중적으로 투자하는 경향이 있다. 한편, 암호화폐 자산에 대한 투자자의 평균 투자 기간은 단기(short-term)로, 일반적으로 1개월에서 3개월 사이에 해당한다. 이에 따라 본 연구에서는 데이터의 관측 빈도(observation frequency)를 일간(daily) 단위로 설정하고, 30일, 60일, 90일의 롤링 윈도우(rolling window)를 적용하여 주요 암호화폐 수익률의 선형 상관계수(Pearson’s coefficient)를 분석하였다. 이러한 분석은 변동성 헤징(volatility hedging)을 고려한 분산 투자(diversified investment) 전략 수립에 도움이 될 수 있다. 예를 들어, 일정한 투자 금액(예: 1억 원)을 주요 암호화폐 자산군 내에서 어떻게 배분할지 결정하는 데 있어, 상관계수 분석 결과가 투자 비중 조정에 유용한 정보를 제공할 것으로 기대된다.",
    "crumbs": [
      "Apps",
      "금융",
      "Correlation in Cryptos"
    ]
  },
  {
    "objectID": "correlation_crypto.html#서론",
    "href": "correlation_crypto.html#서론",
    "title": "Correlation in Cryptos",
    "section": "1 서론",
    "text": "1 서론\n비트코인(BTC)의 가격 및 수익률은, 단기적으로 다음과 같은 관계를 보여왔다.\n\nNDX (나스닥 100 지수)와 강한 양의 상관관계 (Nasdaq 2024, 2023),\nDXY (미국 달러 지수)와 강한 음의 상관관계 (Coindesk 2023; Coinglass 2023). 만약 비트코인 가격이 달러 가격과 장기적으로도 반대 방향으로 움직인다면, 이는 비트코인이 인플레이션 헤지 자산으로 간주될 수도 있을 가능성을 나타낸다 (Dyhrberg 2021).\n금 가격 (GOLD), 국내 실질 총생산량 (GDP) 과의 상관관계는 불명확하거나 간접적인 것으로 알려져 있음 (Cointelegraph 2023; Cryptoslate 2022).\n\n역사적 사례\n\n2020년 COVID-19 위기 이후 BTC와 NDX의 상관관계가 강화됨 (Nasdaq 2020),\n2022년 5월 연방준비제도(Fed)의 금리 인상 발표 당시, BTC와 NDX 모두 하락.\n2023년 비트코인 빙하기 기간 동안 BTC와 NDX의 상관관계 변화 분석 필요.\n2024년 3월 비트코인 ETF가 출시하여 기관 투자자 참여가 증가와 함께께 BTC과 NDX의 coupling이 심해짐.\n\n\n1.1 주요 암호화폐 목록 및 카테고리\n\n\n\n\n\n\n\n\n암호화폐 (Cryptocurrency)\n심볼 (Ticker)\n카테고리 (Category)\n\n\n\n\n비트코인 (Bitcoin)\nBTC/USD\nLayer 1\n\n\n이더리움 (Ethereum)\nETH/USD\nLayer 1, Smart Contract\n\n\n테더 (Tether)\nUSDT/USD\nStablecoin\n\n\n리플 (XRP)\nXRP/USD\nPayment Network\n\n\n솔라나 (Solana)\nSOL/USD\nLayer 1\n\n\n체인링크 (Chainlink)\nLINK/USD\nOracle\n\n\n온도 (Ondo)\nONDO/USD\nReal-World Asset (RWA)\n\n\n카르다노 (Cardano)\nADA/USD\nLayer 1\n\n\n트론 (Tron)\nTRX/USD\nLayer 1\n\n\n도지코인 (Dogecoin)\nDOGE/USD\nMeme Coin\n\n\n\n\n\n1.2 암호화폐 관련 정보 제공 매체 리뷰\n\n시세 데이터 (Price Data): 실시간 및 과거 가격 변동, 거래량(volume) 등\n\nCoinMarketCap\nCoinGecko\n\n온체인 데이터 (On-Chain Data): 거래량, 지갑 주소 변화, 네트워크 활성도 등\n\nGlassnode\nIntoTheBlock\n\n시장 분석 (Market Analysis): 전문가 및 AI 기반 분석 리포트\n\nMessari\nCryptoQuant\n\n뉴스 및 이벤트 (News & Events): 프로젝트 업데이트, 규제 변화 등\n\nCoinDesk\nThe Block\n\n소셜 미디어 분석 (Social Media Analysis): 트위터(X), 레딧(Reddit) 등에서의 커뮤니티 반응\n\nLunarCrush\nSantiment",
    "crumbs": [
      "Apps",
      "금융",
      "Correlation in Cryptos"
    ]
  },
  {
    "objectID": "correlation_crypto.html#데이터-분석",
    "href": "correlation_crypto.html#데이터-분석",
    "title": "Correlation in Cryptos",
    "section": "2 데이터 분석",
    "text": "2 데이터 분석\n\n2.1 데이터\n\n데이터 소스: CCXT\n데이터 기간: 2024년 3월 1일 - 2025년 2월 28일\n데이터 빈도 (Data Frequency): 일간(Daily)\n분석 대상 암호화폐:\n\nBTC/USD, ETH/USD, USDT/USD, XRP/USD, SOL/USD, LINK/USD, ONDO/USD, ADA/USD, TRX/USD, DOGE/USD\n\n롤링 윈도우 크기 (Rolling Window Size): 30일, 60일, 90일\n\n\n\n2.2 분석 방법\n\n암호화폐의 일간 수익률(daily return)을 계산.\n각 롤링 윈도우 크기(30, 60, 90일)에 대해 롤링 상관 행렬(rolling correlation matrix)을 계산.\n평균 상관계수(mean of rolling correlation matrix)를 도출하여 암호화폐 간의 관계를 분석.\n\n\n\nCode\n# 분석 결과 (Results)\n\n# 여러 거래소에서 지원하는 거래쌍을 확인\n\nimport ccxt\nimport pandas as pd\n\n# 주요 암호화폐 목록\nTICKER_COIN = ['BTC/USDT', 'ETH/USDT', 'USDT/USD', 'XRP/USDT', 'SOL/USDT', 'LINK/USDT', 'ONDO/USDT', 'ADA/USDT', 'TRX/USDT', 'DOGE/USDT']\n\n# 지원하는 거래소 목록\nexchanges = ['binance', 'kraken', 'bitfinex', 'poloniex']\n\n# 각 거래소에서 지원하는 거래쌍 확인\nfor exchange_id in exchanges:\n    exchange = getattr(ccxt, exchange_id)()\n    markets = exchange.load_markets()\n    supported_pairs = [pair for pair in TICKER_COIN if pair in markets]\n    print(f\"{exchange_id} supports: {supported_pairs}\")\n\n# 주요 암호화폐 목록\nbinance_tickers = ['BTC/USDT', 'ETH/USDT', 'XRP/USDT', 'SOL/USDT', 'LINK/USDT', 'ADA/USDT', 'TRX/USDT', 'DOGE/USDT']\nkraken_tickers = ['USDT/USD']\npoloniex_tickers = ['ONDO/USDT']\n\n# 데이터 기간 설정\nSTART_DATE = '2024-03-01'\nEND_DATE = '2025-02-28'\n\n# 거래소 설정\nbinance = ccxt.binance()\nkraken = ccxt.kraken()\npoloniex = ccxt.poloniex()\n\n# 데이터 불러오기 함수\ndef fetch_crypto_data(exchange, tickers, start, end):\n    data = {}\n    start_timestamp = exchange.parse8601(f'{start}T00:00:00Z')\n    end_timestamp = exchange.parse8601(f'{end}T00:00:00Z')\n    for ticker in tickers:\n        try:\n            ohlcv = exchange.fetch_ohlcv(ticker, '1d', since=start_timestamp, limit=1000)\n            df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n            df.set_index('timestamp', inplace=True)\n            data[ticker] = df['close']\n        except Exception as e:\n            print(f\"Error fetching {ticker} from {exchange.id}: {e}\")\n    return pd.DataFrame(data)\n\n# 데이터 불러오기\nbinance_data = fetch_crypto_data(binance, binance_tickers, START_DATE, END_DATE)\nkraken_data = fetch_crypto_data(kraken, kraken_tickers, START_DATE, END_DATE)\npoloniex_data = fetch_crypto_data(poloniex, poloniex_tickers, START_DATE, END_DATE)\n\n# 모든 데이터를 하나의 DataFrame으로 병합\ncrypto_prices = pd.concat([binance_data, kraken_data, poloniex_data], axis=1)\n\n# 1) 일간 수익률 계산\ndef compute_returns(price_data: pd.DataFrame) -&gt; pd.DataFrame:\n    return price_data.pct_change().dropna(how='all')\n\ncrypto_returns = compute_returns(crypto_prices)\n\n# 2) 롤링 상관계수 계산\ndef rolling_correlation(returns: pd.DataFrame, window: int) -&gt; pd.DataFrame:\n    \"\"\"\n    returns: (date x tickers) DataFrame\n    window:  rolling window size (days)\n    \n    returns.rolling(window).corr() 결과는\n      - MultiIndex (date, ticker1)\n      - columns = ticker2\n    형태를 가집니다.\n    \"\"\"\n    corr_rolling = returns.rolling(window).corr()\n    return corr_rolling\n\n# 3) 날짜별 상관행렬을 모아서 평균 상관행렬을 산출\ndef average_correlation_matrix(returns: pd.DataFrame, window: int) -&gt; pd.DataFrame:\n    \"\"\"\n    - returns.rolling(window).corr() 결과를 사용\n    - 각 날짜별 (티커 x 티커) 상관행렬을 합산 후, 날짜 개수로 나누어 평균\n    \"\"\"\n    corr_rolling = rolling_correlation(returns, window)\n    \n    # MultiIndex에서 날짜(level=0) 목록을 추출\n    unique_dates = corr_rolling.index.get_level_values(0).unique()\n    tickers = returns.columns\n    \n    # 상관행렬 누적 합을 위한 (티커 x 티커) 형태의 빈 DataFrame\n    sum_matrix = pd.DataFrame(0.0, index=tickers, columns=tickers)\n    count = 0\n    \n    for date in unique_dates:\n        # (ticker1 x ticker2) 형태를 얻기 위해 xs(date, level=0)\n        date_corr = corr_rolling.xs(date, level=0)\n        # date_corr.index = ticker1, date_corr.columns = ticker2\n        \n        # 혹시 일부 티커에 대한 데이터가 누락되었을 경우를 대비하여 reindex\n        date_corr = date_corr.reindex(index=tickers, columns=tickers)\n        \n        # 날짜별 상관행렬(N x N)을 모두 누적\n        if date_corr.notna().all().all():\n            sum_matrix += date_corr.fillna(0.0)\n            count += 1\n    \n    # 평균 계산 (count가 0이 되지 않는다고 가정)\n    mean_matrix = sum_matrix / count\n    \n    return mean_matrix\n\n# 4) 롤링 상관계수 평균 계산\nrolling_corr_results = {}\nfor window in [30, 60, 90]:\n    mean_corr_matrix = average_correlation_matrix(crypto_returns, window)\n    rolling_corr_results[window] = mean_corr_matrix\n\n\nbinance supports: ['BTC/USDT', 'ETH/USDT', 'XRP/USDT', 'SOL/USDT', 'LINK/USDT', 'ONDO/USDT', 'ADA/USDT', 'TRX/USDT', 'DOGE/USDT']\nkraken supports: ['BTC/USDT', 'ETH/USDT', 'USDT/USD', 'XRP/USDT', 'SOL/USDT', 'LINK/USDT', 'ADA/USDT', 'DOGE/USDT']\nbitfinex supports: ['BTC/USDT', 'ETH/USDT', 'USDT/USD', 'XRP/USDT', 'SOL/USDT', 'LINK/USDT', 'ADA/USDT', 'TRX/USDT', 'DOGE/USDT']\npoloniex supports: ['BTC/USDT', 'ETH/USDT', 'XRP/USDT', 'SOL/USDT', 'LINK/USDT', 'ONDO/USDT', 'ADA/USDT', 'TRX/USDT', 'DOGE/USDT']\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# 모든 행과 열이 출력되도록 설정\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.expand_frame_repr', False)\n\n# 결과 출력 및 시각화\nfor window, result in rolling_corr_results.items():\n    # 상관 행렬을 DataFrame으로 변환\n    result_df = result.dropna(how='all')\n    print(f\"\\n[Window = {window} days] Mean Correlation Matrix\\n\", result_df)\n    \n    # 히트맵 시각화\n    plt.figure(figsize=(10, 8))\n    \n    # 대각선 요소를 마스킹\n    mask = np.triu(np.ones(result_df.shape, dtype=bool))\n    \n    sns.heatmap(result_df, annot=True, cmap='coolwarm', center=0, mask=mask)\n    plt.title(f'Mean Rolling Correlation Matrix (Window Size: {window} days)')\n    plt.show()\n\n\n\n[Window = 30 days] Mean Correlation Matrix\n            BTC/USDT  ETH/USDT  XRP/USDT  SOL/USDT  LINK/USDT  ADA/USDT  TRX/USDT  DOGE/USDT  USDT/USD  ONDO/USDT\nBTC/USDT   1.000000  0.812751  0.607459  0.759442   0.710901  0.726252  0.373219   0.824787  0.486519   0.395272\nETH/USDT   0.812751  1.000000  0.597512  0.724565   0.757776  0.738003  0.380331   0.762864  0.333017   0.419611\nXRP/USDT   0.607459  0.597512  1.000000  0.592538   0.606949  0.682793  0.312756   0.622800  0.226918   0.284394\nSOL/USDT   0.759442  0.724565  0.592538  1.000000   0.699289  0.708103  0.320065   0.717061  0.307900   0.344905\nLINK/USDT  0.710901  0.757776  0.606949  0.699289   1.000000  0.773862  0.317306   0.695526  0.285704   0.457395\nADA/USDT   0.726252  0.738003  0.682793  0.708103   0.773862  1.000000  0.404491   0.743431  0.302803   0.368594\nTRX/USDT   0.373219  0.380331  0.312756  0.320065   0.317306  0.404491  1.000000   0.360882  0.170416   0.137955\nDOGE/USDT  0.824787  0.762864  0.622800  0.717061   0.695526  0.743431  0.360882   1.000000  0.355017   0.393610\nUSDT/USD   0.486519  0.333017  0.226918  0.307900   0.285704  0.302803  0.170416   0.355017  1.000000   0.274014\nONDO/USDT  0.395272  0.419611  0.284394  0.344905   0.457395  0.368594  0.137955   0.393610  0.274014   1.000000\n\n\n\n\n\n\n\n\n\n\n[Window = 60 days] Mean Correlation Matrix\n            BTC/USDT  ETH/USDT  XRP/USDT  SOL/USDT  LINK/USDT  ADA/USDT  TRX/USDT  DOGE/USDT  USDT/USD  ONDO/USDT\nBTC/USDT   1.000000  0.806902  0.559687  0.755827   0.691801  0.698022  0.323507   0.825653  0.473639   0.369164\nETH/USDT   0.806902  1.000000  0.540319  0.707804   0.733414  0.699286  0.322430   0.746043  0.328703   0.393115\nXRP/USDT   0.559687  0.540319  1.000000  0.550307   0.578009  0.663658  0.276250   0.578877  0.183762   0.242281\nSOL/USDT   0.755827  0.707804  0.550307  1.000000   0.678059  0.685017  0.284950   0.710304  0.286279   0.311963\nLINK/USDT  0.691801  0.733414  0.578009  0.678059   1.000000  0.745630  0.274904   0.672512  0.264784   0.425054\nADA/USDT   0.698022  0.699286  0.663658  0.685017   0.745630  1.000000  0.366857   0.713890  0.268321   0.313164\nTRX/USDT   0.323507  0.322430  0.276250  0.284950   0.274904  0.366857  1.000000   0.315596  0.164262   0.104249\nDOGE/USDT  0.825653  0.746043  0.578877  0.710304   0.672512  0.713890  0.315596   1.000000  0.346573   0.360646\nUSDT/USD   0.473639  0.328703  0.183762  0.286279   0.264784  0.268321  0.164262   0.346573  1.000000   0.259714\nONDO/USDT  0.369164  0.393115  0.242281  0.311963   0.425054  0.313164  0.104249   0.360646  0.259714   1.000000\n\n\n\n\n\n\n\n\n\n\n[Window = 90 days] Mean Correlation Matrix\n            BTC/USDT  ETH/USDT  XRP/USDT  SOL/USDT  LINK/USDT  ADA/USDT  TRX/USDT  DOGE/USDT  USDT/USD  ONDO/USDT\nBTC/USDT   1.000000  0.802569  0.521451  0.751743   0.674933  0.685430  0.292551   0.820818  0.456609   0.354759\nETH/USDT   0.802569  1.000000  0.497249  0.698130   0.721569  0.679298  0.277568   0.730402  0.317331   0.380166\nXRP/USDT   0.521451  0.497249  1.000000  0.505134   0.547214  0.644037  0.254648   0.540650  0.150647   0.217965\nSOL/USDT   0.751743  0.698130  0.505134  1.000000   0.655497  0.665090  0.268793   0.700802  0.262762   0.291730\nLINK/USDT  0.674933  0.721569  0.547214  0.655497   1.000000  0.727091  0.248582   0.648950  0.240845   0.404284\nADA/USDT   0.685430  0.679298  0.644037  0.665090   0.727091  1.000000  0.335776   0.696838  0.245097   0.292516\nTRX/USDT   0.292551  0.277568  0.254648  0.268793   0.248582  0.335776  1.000000   0.279397  0.152573   0.093492\nDOGE/USDT  0.820818  0.730402  0.540650  0.700802   0.648950  0.696838  0.279397   1.000000  0.334601   0.338825\nUSDT/USD   0.456609  0.317331  0.150647  0.262762   0.240845  0.245097  0.152573   0.334601  1.000000   0.252754\nONDO/USDT  0.354759  0.380166  0.217965  0.291730   0.404284  0.292516  0.093492   0.338825  0.252754   1.000000",
    "crumbs": [
      "Apps",
      "금융",
      "Correlation in Cryptos"
    ]
  },
  {
    "objectID": "correlation_crypto.html#토론-discussions",
    "href": "correlation_crypto.html#토론-discussions",
    "title": "Correlation in Cryptos",
    "section": "3 토론 (Discussions)",
    "text": "3 토론 (Discussions)\n\n\n\n\n\n\nImportant\n\n\n\n변수들의 관찰 주기 (단기? 장기?)에 따라 또는 관찰 시기 (10년전? 지금?)에 따라 변수들 간의 선형관계는 유지되지 않을 수 있습니다. 2025년 현재 비트코인 (BTC) 가격은 시장 심리, 규제 변화, 기술적 요인 등에 크게 영향을 받고 있습니다.\n\n\n\n상관계수가 낮은 암호화폐 자산 조합을 식별하고, 헤지 투자 전략을 논의.\n특정 암호화폐 간의 높은 상관관계가 나타나는 이유 및 그에 따른 리스크 분석.\n\n2024년 3월부터 2025년 2월까지 암호화폐 시장에 큰 영향을 미친 주요 변화 시기와 원인:\n\n비트코인 반감기 (2024년 4월 20일): 비트코인 채굴 보상이 6.25 BTC에서 3.125 BTC로 절반으로 감소. 이는 비트코인의 공급 감소로 이어져 가격에 상승 압력을 가함.\n비트코인 ETF 자금 유입 증가 (2024년 10월): 비트코인 ETF로의 지속적인 자금 유입이 관찰됨. 10월까지 ETF 투자자들이 총 345,200 BTC(200억 달러 이상의 가치)를 매입.\n트럼프의 대통령 당선 (2024년 11월): 도널드 트럼프가 “암호화폐 대통령”이 되겠다는 공약을 내세우며 당선됨. 이는 암호화폐 시장에 대한 긍정적인 기대감을 불러일으킴.\nEU의 암호화폐 시장 규제(MiCA) 전면 시행 (2024년 12월 30일): 유럽연합에서 암호화폐 시장 규제(MiCA)가 전면 시행됨. 이로 인해 EU 전역에서 암호화폐 서비스 제공업체들에 대한 통일된 규제 프레임워크가 적용되기 시작.\n트럼프의 암호화폐 정책 발표 (2025년 1월): 트럼프 대통령이 취임 후 미국을 “암호화폐의 수도”로 만들겠다는 계획을 발표함. 여기에는 비트코인 전략적 비축 등의 아이디어가 포함됨.\n\nstars and bins에서 위의 변화 시기가 bins 역할을 한다는 가정하여, 기간 stars을 다음과 같이 나누어 상관관계를 conditional 해 본다.\n\n20224년 3월 1일 (관측기간 시작일) - 2024년 4월 20일\n2024년 4월 21일 - 2024년 9월 30일\n2024년 10월 1일 - 2024년 11월 5일\n2024년 11월 6일 - 2024년 12월 31일\n2025년 1월 1일 - 2025년 2월 28일 (관측기간 종료일)\n\n변화를 \\(Z\\)로 표기했다면, covariance decomposition formula에 의해,\n(추후 계속)",
    "crumbs": [
      "Apps",
      "금융",
      "Correlation in Cryptos"
    ]
  },
  {
    "objectID": "correlation_crypto.html#부록-conditioning-theorems-in-probability-theory",
    "href": "correlation_crypto.html#부록-conditioning-theorems-in-probability-theory",
    "title": "Correlation in Cryptos",
    "section": "4 부록: Conditioning Theorems in Probability Theory",
    "text": "4 부록: Conditioning Theorems in Probability Theory\n\n4.1 Adam’s Law: Smoothing Property of Conditional Expectation\nAlso known as the Law of Total Expectation or Law of Iterated Expectations.\nIf \\(X\\) is a random variable whose expectation \\(E(X)\\) is defined and \\(Z\\) is any random variable defined on the same probability space, then: \\[E(X) = E(E(X|Z)).\\]\nA conditional expectation can be viewed as a Radon–Nikodym derivative, making the tower property a direct consequence of the chain rule for conditional expectations.\nA special discrete case: If \\(\\{Z_{i}\\}\\) is a finite or countable partition of the sample space, then: \\[E(X) = \\sum_{i} E(X \\mid Z_i)P(Z_i).\\]\n\n\n4.2 Eve’s Law: Variance Decomposition Formula\nKnown as the Conditional Variance Formula or the Law of Iterated Variances.\nIf \\(X\\) and \\(Y\\) are random variables defined on the same probability space, and if \\(Y\\) has finite variance, then:\n\\[\\operatorname{Var}(Y)=E[\\operatorname{Var}(Y\\mid X)]+\\operatorname{Var}(E[Y\\mid X]).\\]\nThis is a special case of the covariance decomposition formula.\nApplications\n\n분산 = “(조건부 분산)의 평균” + “(조건부 평균)의 분산”\nAnalysis of Variance (ANOVA): Variability in \\(Y\\) splits into an “unexplained” within-group variance and an “explained” between-group variance. The F-test examines if the explained variance is significantly large, indicating a meaningful effect of \\(X\\) on \\(Y\\).\nLinear Regression Models: The proportion of explained variance is measured as \\(R^2\\). For simple linear regression (single predictor), \\(R^2\\) equals the squared Pearson correlation coefficient between \\(X\\) and \\(Y\\).\nMachine Learning and Bayesian Inference: In many Bayesian and ensemble methods, one decomposes prediction uncertainty via the law of total variance. For a Bayesian neural network with random parameters \\(\\theta\\): \\(\\operatorname {Var} (Y)=\\operatorname {E} {\\bigl [}\\operatorname {Var} (Y\\mid \\theta ){\\bigr ]}+\\operatorname {Var} {\\bigl (}\\operatorname {E} [Y\\mid \\theta ]{\\bigr )}\\) often referred to as “aleatoric” (within-model) vs. “epistemic” (between-model) uncertainty\nInformation Theory: For jointly Gaussian \\((X,Y)\\), the fraction \\(\\operatorname {Var} (\\operatorname {E} [Y\\mid X])/\\operatorname {Var} (Y)\\) relates directly to the mutual information \\(I(Y;X)\\). In non-Gaussian settings, a high explained-variance ratio still indicates significant information about Y contained in X\n\nExample 1 (Exam Scores): Suppose students’ exam scores vary between two classrooms. The variance of all scores (\\(Y\\)) can be decomposed into the variance within classrooms (unexplained) and the variance between classroom averages (explained), reflecting differences in teaching quality or resources.\nExample 2 (Mixture of Two Gaussians): Consider \\(Y\\) as a mixture of two normal distributions, where the mixing distribution is Bernoulli with parameter \\(p\\). Suppose:\n\\[Y \\mid (X=0) \\sim N(\\mu_0, \\sigma_0^2), \\quad Y \\mid (X=1) \\sim N(\\mu_1, \\sigma_1^2).\\]\nThen the law of total variance gives:\n\\[\\operatorname{Var}(Y) = p\\sigma_1^2 + (1-p)\\sigma_0^2 + p(1-p)(\\mu_1 - \\mu_0)^2.\\]\n\n\n4.3 Covariance Decomposition Formula\nKnown as the Law of Total Covariance or Conditional Covariance Formula.\nIf \\(X\\), \\(Y\\), and \\(Z\\) are random variables defined on the same probability space, with finite covariance between \\(X\\) and \\(Y\\), then:\n\\[\\operatorname{cov}(X,Y)=E[\\operatorname{cov}(X,Y\\mid Z)]+\\operatorname{cov}(E[X\\mid Z],E[Y\\mid Z]).\\]\nThis relationship is a particular instance of the general Law of Total Cumulance and is crucial for analyzing dependencies among variables conditioned on a third variable or groupings.\n\n\n4.4 Bias-Variance Decomposition of MSE\nKey: The Bias-Variance Decomposition emphasizes the trade-off between making \\(\\hat{Y}\\) reliably close to its own expected value (low variance) and aligning that expected value with the true target \\(\\mathbb{E}[Y]\\) (low bias).\nIn many estimation or prediction settings, we have a random outcome \\(Y\\) and an estimator (or model prediction) \\(\\hat{Y}\\). The Mean Squared Error (MSE) of \\(\\hat{Y}\\) is:\n\\[\n\\mathrm{MSE}(\\hat{Y})\n= \\mathbb{E}\\bigl[(Y - \\hat{Y})^2\\bigr].\n\\]\nIf we decompose \\(\\hat{Y}\\) around its expected value, we can split MSE into a bias term and a variance term (plus any irreducible noise in certain contexts). Formally, assume\n\\[\n\\hat{Y} = f(X) + \\text{estimation noise},\n\\quad\nY = f(X) + \\varepsilon,\n\\]\nwhere \\(\\varepsilon\\) is an irreducible error term with mean zero (e.g., observational or inherent noise). Then:\n\\[\n\\begin{aligned}\n\\mathrm{MSE}(\\hat{Y})\n&= \\mathbb{E}\\bigl[(Y - \\hat{Y})^2\\bigr]\\\\\n&= \\underbrace{\\mathbb{E}\\Bigl[\\bigl(\\hat{Y} - \\mathbb{E}[\\hat{Y}]\\bigr)^2\\Bigr]}_{\\text{Variance}(\\hat{Y})}\n\\;+\\;\n\\underbrace{\\Bigl(\\mathbb{E}[\\hat{Y}] \\;-\\; \\mathbb{E}[Y]\\Bigr)^2}_{\\text{Bias}(\\hat{Y})^2}\n\\;+\\;\n\\underbrace{\\mathbb{E}\\bigl[\\varepsilon^2\\bigr]}_{\\text{Irreducible noise}},\n\\end{aligned}\n\\]\nwhere:\n\nVariance: \\(\\text{Var}(\\hat{Y})\\) represents how much \\(\\hat{Y}\\) fluctuates around its own mean.\nBias: \\(\\text{Bias}(\\hat{Y})^2\\) measures how far \\(\\hat{Y}\\) (on average) deviates from the true mean \\(\\mathbb{E}[Y]\\).\nIrreducible noise: \\(\\mathbb{E}[\\varepsilon^2]\\) is the part of the error that cannot be reduced by any estimator.\n\nIn a strictly theoretical sense (when \\(\\varepsilon\\) is embedded in \\(Y\\)), one often writes the Bias-Variance decomposition as:\n\\[\n\\mathrm{MSE}(\\hat{Y})\n= \\underbrace{\\mathrm{Var}(\\hat{Y})}_{\\text{Variance term}}\n\\;+\\;\n\\underbrace{\\mathrm{Bias}(\\hat{Y})^2}_{\\text{Bias term}}.\n\\]\nHere, if there is additional irreducible noise, it appears as a separate constant term. This decomposition closely aligns with the Law of Total Variance (Eve’s Law) in the sense that the total mean squared difference can be split into a “spread around the estimator’s mean” plus the “squared difference of that mean from the true value,” mirroring how variance itself decomposes into conditional components.",
    "crumbs": [
      "Apps",
      "금융",
      "Correlation in Cryptos"
    ]
  },
  {
    "objectID": "d_1_critique.html",
    "href": "d_1_critique.html",
    "title": "주류경제학 비판",
    "section": "",
    "text": "The term elite is often used loosely, but here we adopt a precise and structural definition. Elites are not merely individuals with high income or substantial assets. Rather, they are those who:\n\nExercise disproportionate influence over the institutional architecture of capital flow and resource allocation;\n\nActively or passively maintain policies and mechanisms that reproduce this asymmetry across time and context.\n\nIn this sense, elites function less as price-takers within a competitive system and more as price-makers—those who shape, define, and often insulate themselves from the very mechanisms that regulate others.\nStandard economic models typically begin with the assumption of competitive markets, where no individual agent can significantly influence prices or systemic outcomes. But in practice, elites operate in a structurally non-competitive environment—one governed not by equal access or market-clearing forces, but by institutional design, asymmetrical privileges, and regulatory capture. They shape boundary conditions, institutional norms, informational flows, and legal architectures—determining which forms of capital, whether economic, symbolic, or algorithmic, receive reward or recognition.\n\nWhat mainstream economics treats as an assumption—perfect competition—is, in practice, a privilege strategically denied to the many and tactically avoided by the few who benefit most from its rhetoric.\n\nThis produces a critical asymmetry: competition becomes a binding constraint for the majority, but a flexible and selectively applied instrument for the elite. They do not merely follow the rules—they define them. They do not merely navigate the system—they construct and preserve it in ways that ensure long-term advantage.\nMainstream economics often describes inequality in impersonal terms—as a byproduct of technological progress, market dynamics, or individual preferences. While such forces undoubtedly play a role, this perspective often underplays the degree to which inequality is a designed outcome: structured through policy choices, institutional incentives, and ideologically informed narratives. The locus of responsibility lies not in “the system” as an abstraction, but in those who benefit from and sustain its asymmetries.\nThis brings us to a deeper concern—not only the existence of exploitation, but the gradual erosion of ethical sensitivity to it. What was once considered morally questionable may, over time, come to be seen as natural, inevitable, or even virtuous. This quiet normalization of exploitative asymmetry echoes the dynamics of moral hazard, though in a structurally expanded and more diffuse form.\nIf we distinguish between systematic and idiosyncratic risk, the elite can be seen as minimizing their personal exposure—maximizing economic rents—while amplifying systemic risks that are collectively absorbed. Insulation from loss does not eliminate risk; it simply redistributes it—often downward.\nIn classical models, moral hazard arises when agents take excessive risks because they do not bear the full consequences—typically due to asymmetric information or externalized insurance. The risk in question refers to the downside exposure that would ordinarily discipline a decision-making agent, but is shifted to another party, often through informational opacity or legal insulation.\nIn elite-driven inequality, the asymmetry lies not only in information, but in systemic unaccountability—a diffuse condition in which responsibility is diluted through complexity, obscured through bureaucracy, or excused through ideology. Elites benefit from structural advantages—preferential access to capital, institutional insulation, and regulatory leniency—while often remaining largely immune to the collective costs those privileges generate.\n\nThey bear only the virtual risk of systemic failure, while the real cost is distributed socially.\n\nThis dynamic unfolds through at least two distinct mechanisms:\n\nBy commission: when elites actively shape systems—through lobbying, legal structuring, or financial engineering—that secure private gains while transferring downside risks to others. This may take the form of offshore tax regimes, deregulated capital markets, or algorithmic decision-making tools that concentrate risk without corresponding accountability.\nBy omission: when they fail to act upon foreseeable harms embedded in the structures they benefit from. This includes willful neglect of environmental degradation, labor precarity, or declining public infrastructure. In legal terms, this is closer to reckless disregard—a failure to fulfill the duties implicit in one’s structural position.\n\nBoth mechanisms are ethically equivalent: they represent a deliberate or negligent transfer of responsibility away from those most capable of managing it, toward those least equipped to bear its consequences.\nTake, for instance, the erosion of corporate tax bases in the Global South. Multinational corporations may legally minimize tax burdens via transfer pricing or base erosion schemes. While this behavior is technically compliant with international law, its consequences—underfunded healthcare, stalled education systems, and fragile public institutions—are absorbed by populations with no legal recourse or political leverage. These effects are visible, measurable, and morally weighty—yet routinely dismissed as irrelevant externalities. When structural imbalance generates private gain at the cost of public harm, institutional behavior may internalize moral detachment as a norm rather than an exception.\nThe deeper problem, then, lies not just in the structure, but in the failure to recognize its moral weight. What begins as technical rationality evolves into ideological complacency. What begins as privilege hardens into entitlement.\nWhat emerges is not merely a policy failure or economic misallocation, but a deeper failure of moral imagination. The classical term for this is hubris: the overconfidence that blinds one to contingency, the refusal to see one’s own position as historically or morally constructed, and the erosion of empathy toward those structurally disadvantaged.\n\nHubris is not merely arrogance. It is the blindness to one’s contingency—and the unwillingness to bear the moral weight of that contingency.\n\nIn Greek tragedy, hubris often leads to nemesis—a reckoning that follows unchecked arrogance. In the context of political economy, the parallel is clear: ignoring the ethical implications of systemic privilege can result in social breakdown, erosion of democratic institutions, or environmental collapse. The way forward, however, does not lie in blame or punishment, but in restoring a sense of moral awareness—the ability to recognize how systems of power, advantage, and responsibility are connected and carry ethical significance.",
    "crumbs": [
      "Apps",
      "돈의 동역학",
      "주류경제학 비판"
    ]
  },
  {
    "objectID": "d_1_critique.html#ethical-blindness-of-the-elites",
    "href": "d_1_critique.html#ethical-blindness-of-the-elites",
    "title": "주류경제학 비판",
    "section": "",
    "text": "The term elite is often used loosely, but here we adopt a precise and structural definition. Elites are not merely individuals with high income or substantial assets. Rather, they are those who:\n\nExercise disproportionate influence over the institutional architecture of capital flow and resource allocation;\n\nActively or passively maintain policies and mechanisms that reproduce this asymmetry across time and context.\n\nIn this sense, elites function less as price-takers within a competitive system and more as price-makers—those who shape, define, and often insulate themselves from the very mechanisms that regulate others.\nStandard economic models typically begin with the assumption of competitive markets, where no individual agent can significantly influence prices or systemic outcomes. But in practice, elites operate in a structurally non-competitive environment—one governed not by equal access or market-clearing forces, but by institutional design, asymmetrical privileges, and regulatory capture. They shape boundary conditions, institutional norms, informational flows, and legal architectures—determining which forms of capital, whether economic, symbolic, or algorithmic, receive reward or recognition.\n\nWhat mainstream economics treats as an assumption—perfect competition—is, in practice, a privilege strategically denied to the many and tactically avoided by the few who benefit most from its rhetoric.\n\nThis produces a critical asymmetry: competition becomes a binding constraint for the majority, but a flexible and selectively applied instrument for the elite. They do not merely follow the rules—they define them. They do not merely navigate the system—they construct and preserve it in ways that ensure long-term advantage.\nMainstream economics often describes inequality in impersonal terms—as a byproduct of technological progress, market dynamics, or individual preferences. While such forces undoubtedly play a role, this perspective often underplays the degree to which inequality is a designed outcome: structured through policy choices, institutional incentives, and ideologically informed narratives. The locus of responsibility lies not in “the system” as an abstraction, but in those who benefit from and sustain its asymmetries.\nThis brings us to a deeper concern—not only the existence of exploitation, but the gradual erosion of ethical sensitivity to it. What was once considered morally questionable may, over time, come to be seen as natural, inevitable, or even virtuous. This quiet normalization of exploitative asymmetry echoes the dynamics of moral hazard, though in a structurally expanded and more diffuse form.\nIf we distinguish between systematic and idiosyncratic risk, the elite can be seen as minimizing their personal exposure—maximizing economic rents—while amplifying systemic risks that are collectively absorbed. Insulation from loss does not eliminate risk; it simply redistributes it—often downward.\nIn classical models, moral hazard arises when agents take excessive risks because they do not bear the full consequences—typically due to asymmetric information or externalized insurance. The risk in question refers to the downside exposure that would ordinarily discipline a decision-making agent, but is shifted to another party, often through informational opacity or legal insulation.\nIn elite-driven inequality, the asymmetry lies not only in information, but in systemic unaccountability—a diffuse condition in which responsibility is diluted through complexity, obscured through bureaucracy, or excused through ideology. Elites benefit from structural advantages—preferential access to capital, institutional insulation, and regulatory leniency—while often remaining largely immune to the collective costs those privileges generate.\n\nThey bear only the virtual risk of systemic failure, while the real cost is distributed socially.\n\nThis dynamic unfolds through at least two distinct mechanisms:\n\nBy commission: when elites actively shape systems—through lobbying, legal structuring, or financial engineering—that secure private gains while transferring downside risks to others. This may take the form of offshore tax regimes, deregulated capital markets, or algorithmic decision-making tools that concentrate risk without corresponding accountability.\nBy omission: when they fail to act upon foreseeable harms embedded in the structures they benefit from. This includes willful neglect of environmental degradation, labor precarity, or declining public infrastructure. In legal terms, this is closer to reckless disregard—a failure to fulfill the duties implicit in one’s structural position.\n\nBoth mechanisms are ethically equivalent: they represent a deliberate or negligent transfer of responsibility away from those most capable of managing it, toward those least equipped to bear its consequences.\nTake, for instance, the erosion of corporate tax bases in the Global South. Multinational corporations may legally minimize tax burdens via transfer pricing or base erosion schemes. While this behavior is technically compliant with international law, its consequences—underfunded healthcare, stalled education systems, and fragile public institutions—are absorbed by populations with no legal recourse or political leverage. These effects are visible, measurable, and morally weighty—yet routinely dismissed as irrelevant externalities. When structural imbalance generates private gain at the cost of public harm, institutional behavior may internalize moral detachment as a norm rather than an exception.\nThe deeper problem, then, lies not just in the structure, but in the failure to recognize its moral weight. What begins as technical rationality evolves into ideological complacency. What begins as privilege hardens into entitlement.\nWhat emerges is not merely a policy failure or economic misallocation, but a deeper failure of moral imagination. The classical term for this is hubris: the overconfidence that blinds one to contingency, the refusal to see one’s own position as historically or morally constructed, and the erosion of empathy toward those structurally disadvantaged.\n\nHubris is not merely arrogance. It is the blindness to one’s contingency—and the unwillingness to bear the moral weight of that contingency.\n\nIn Greek tragedy, hubris often leads to nemesis—a reckoning that follows unchecked arrogance. In the context of political economy, the parallel is clear: ignoring the ethical implications of systemic privilege can result in social breakdown, erosion of democratic institutions, or environmental collapse. The way forward, however, does not lie in blame or punishment, but in restoring a sense of moral awareness—the ability to recognize how systems of power, advantage, and responsibility are connected and carry ethical significance.",
    "crumbs": [
      "Apps",
      "돈의 동역학",
      "주류경제학 비판"
    ]
  },
  {
    "objectID": "d_1_critique.html#the-origin-of-inequality",
    "href": "d_1_critique.html#the-origin-of-inequality",
    "title": "주류경제학 비판",
    "section": "2 The Origin of Inequality",
    "text": "2 The Origin of Inequality\nOne of the most consistent stylized facts of the contemporary global economy is the concentration of wealth and income in the hands of a vanishingly small elite. Empirical data across countries and decades show persistent Pareto-distributed income and wealth distributions, with increasingly thinner tails. A small minority holds a majority of productive physical capital, controls disproportionate flows of capital, and sustains this position across generations.\nYet paradoxically, as this phenomenon becomes more extreme, academic and policy discourse tends increasingly toward justification rather than structural critique. Even when proposals for addressing inequality are offered, they often remain in the realm of general principles or abstract ideals—emphasizing efficiency, incentives, or inclusive growth—without confronting the specific institutional arrangements that sustain concentration. The rhetorical move is subtle but effective: inequality is framed less as an anomaly to be corrected than as an inevitable byproduct of meritocratic allocation.\nAt the core of this rationalization lies a conflation of normative and descriptive propositions. The moral claim that “those who contribute more should receive more”—a principle of normative ordinality rooted in intuitive fairness—is transformed into a descriptive claim that those who have more must have contributed more. This slippage commits what philosophers call the naturalistic fallacy, and more specifically, a category error: mistaking a moral imperative for a statistical inference.\n\nTo infer from what ought to be, what is—and worse, to claim what is must be because it ought to be—is not logic, but moral sleight of hand.\n\nThe assertion that extreme economic outcomes reflect extreme differences in ability, effort, or contribution is empirically unsubstantiated. Research in biology, cognitive science, and behavioral psychology consistently finds that distributions of human ability and effort—however operationalized—cluster symmetrically around the median, with far less variance than that of observed wealth or income distributions. Outliers exist, but they are neither frequent enough nor extreme enough to account for the economic inequalities we observe.\nTo put it plainly: most people are more similar in capacity than different. While ability and effort may differ across individuals, those differences tend to fall within a narrow band. Wealth and capital, on the other hand, are distributed along a far more skewed axis—one that does not mirror the structure of human potential.\n\nAbility and effort are approximately normally distributed around the median. Wealth is not. This is not a mathematical anomaly, but a structural and social fact.\n\nThe primary origin of inequality, then, is not variance in individual input, but asymmetry in structural mechanisms of distribution and accumulation. Among the most studied are:\n\nWinner-takes-all dynamics, where marginal initial differences generate exponentially skewed outcomes.\nPreferential attachment, where capital attracts capital by virtue of its existing mass.\nAbsorbing elite states, in which transitions out of the top wealth decile become statistically negligible.\n\nThese mechanisms can be captured through quasi-closed Markov chain models, calibrated on real-world capital flow data across individuals or firms. Even assuming comparable effort and ability across agents, these systems naturally generate persistent inequality through differential transition probabilities alone.\n\nThe system rewards contribution only partially—and selectively. Among many who contribute, only a few are elevated; and among those few, rewards often exceed any plausible measure of contribution. This is not merely inequality in outcome, but inequity in reward assignment.",
    "crumbs": [
      "Apps",
      "돈의 동역학",
      "주류경제학 비판"
    ]
  },
  {
    "objectID": "d_1_critique.html#formalism-as-obfuscation-the-limits-of-mathematical-economics",
    "href": "d_1_critique.html#formalism-as-obfuscation-the-limits-of-mathematical-economics",
    "title": "주류경제학 비판",
    "section": "3 Formalism as Obfuscation: The Limits of Mathematical Economics",
    "text": "3 Formalism as Obfuscation: The Limits of Mathematical Economics\nMainstream economic models frequently fail to account for the fundamental origins of inequality, not because the mathematics is insufficient, but because it is strategically simplified. The field has embraced a style of formalism that emphasizes ex post internal consistency while sacrificing ex ante explanatory relevance.\nMainstream models rely heavily on:\n\nClosed systems with no structural innovation;\nSymmetric interaction matrices, often assumed to be positive-definite;\nLinearized dynamics near local equilibria, excluding global nonlinearity, bifurcation, or path-dependence;\nEquilibrium-seeking behavior, despite the empirical prevalence of disequilibrium.\n\nMathematically, many such models reduce to the most simple form of Lotka–Volterra systems, which possess well-known properties:\n\nConvergence to fixed points;\nAbsence of innovation;\nElimination of structural asymmetry by design.\n\nWhen these dynamics are applied to complex social phenomena, they do not reveal hidden truths—they reaffirm hidden assumptions. The supposed “rigor” of the model becomes a veil behind which ideological commitments masquerade as neutral analysis.\n\nThis is not analytical clarity. It is the theater of rigor—a ritual performance in which precision displaces relevance, and formalism conceals structural omission.\n\nUltimately, this is not a failure of economics per se, but of economic epistemology. It is the result of a field that has:\n\nBorrowed the symbols of mathematics without absorbing its disciplinary ethos;\nApplied the tools of formal logic without asking what they conceal;\nSubstituted consistency for truth, and plausibility for justice.\n\n\nMathematics, when used properly, is a language of structure. However, in mainstream economics, it is too often a rhetorical tool of authority rather than an instrument of insight.",
    "crumbs": [
      "Apps",
      "돈의 동역학",
      "주류경제학 비판"
    ]
  },
  {
    "objectID": "d_3_model.html",
    "href": "d_3_model.html",
    "title": "AI시대 동역학 모형",
    "section": "",
    "text": "From Closed to Open\nIn closed systems, labor and capital exist in mutual interaction. Labor produces output, capital accumulates via labor input, and the feedback between them sustains the economy’s dynamical structure. Such systems can cycle, stabilize, or collapse—but they do so within a conservative phase space.\nHowever, automation introduces a structural mutation. It does not simply accelerate capital accumulation; it changes the topology of the system itself. Capital becomes self-replicating—decoupled from labor. The result is a qualitative shift: the system becomes open, non-conservative, and potentially labor-exclusionary.\nWe formalize this mutation using a modified Lotka–Volterra interaction system between labor \\(L(t)\\) and capital \\(C(t)\\):\n\\[\n\\frac{dL}{dt} = a_{1,1} L - a_{1,2} L C,\n\\]\n\\[\n\\frac{dC}{dt} = a_{2,1} L C + a_{2,2} C - a_{2,3} C - a_{2,4} C^2.\n\\]",
    "crumbs": [
      "Apps",
      "돈의 동역학",
      "AI시대 동역학 모형"
    ]
  },
  {
    "objectID": "d_3_model.html#parameters",
    "href": "d_3_model.html#parameters",
    "title": "AI시대 동역학 모형",
    "section": "1 Parameters",
    "text": "1 Parameters\nEach parameter governs a fundamental mechanism:\n\n\\(a_{1,1}\\): autonomous growth of labor—birth, training, or onboarding.\n\\(a_{1,2}\\): capital-induced labor dissipation—outsourcing, automation, or deskilling.\n\\(a_{2,1}\\): labor-driven capital accumulation—human-centered production.\n\\(a_{2,2}\\): capital self-replication—automation, platform scaling, AI compounding.\n\\(a_{2,3}\\): depreciation—maintenance, obsolescence, or asset decay.\n\\(a_{2,4}\\): nonlinear dissipation within capital—interpreted not merely as diminishing returns, but as elite–elite rivalry, strategic cannibalism, and systemic congestion at the top.\n\nThe first equation describes labor’s fate: a positive growth term (\\(a_{1,1} L\\)) is offset by an absorption term (\\(-a_{1,2} L C\\)), which accelerates as capital increases. This structure encodes labor’s vulnerability to capital dominance: if capital grows faster than labor, labor becomes increasingly exposed to structural replacement.\nThe second equation governs capital: it can grow through labor (\\(a_{2,1} LC\\)), but also through internal momentum (\\(a_{2,2} C\\)), so long as depreciation (\\(a_{2,3} C\\)) and dissipation (\\(a_{2,4} C^2\\)) do not overwhelm it.\nThe nonlinearity introduced by \\(-a_{2,4} C^2\\) is essential. Unlike traditional diminishing returns, which reduce marginal productivity, this term reflects conflict internal to the elite class. In financialized economies where capital is abundant and labor is scarce or irrelevant, competition is not against workers—it is against other capital holders. Here, \\(C^2\\) signifies structural overcrowding at the top: redundant infrastructure, speculative bubbles, or defensive investments to block new entrants. It is not friction from scarcity; it is friction from saturation.",
    "crumbs": [
      "Apps",
      "돈의 동역학",
      "AI시대 동역학 모형"
    ]
  },
  {
    "objectID": "d_3_model.html#asymptotic-scenarios",
    "href": "d_3_model.html#asymptotic-scenarios",
    "title": "AI시대 동역학 모형",
    "section": "2 Asymptotic Scenarios",
    "text": "2 Asymptotic Scenarios\nThis system admits multiple asymptotic regimes depending on the relationship between parameter values:\n1. Classical Mutualism (Closed System)\nWhen \\(a_{2,2} = 0\\) and \\(a_{1,1}, a_{2,1}, a_{1,2}, a_{2,3} &gt; 0\\), the system behaves like a classical Lotka–Volterra cycle:\n\n\\(L\\) grows when capital is low, declines when capital is high.\n\\(C\\) grows when labor is abundant, decays otherwise.\nOrbits emerge in the phase space, bounded and conservative.\n\nInterpretation: Capital and labor are locked in mutual dependence. Neither can survive alone.\n2. Transitional Instability (Automation Activation)\nWhen \\(a_{2,2} &gt; 0\\) but still small relative to \\(a_{2,1} L\\) in most regions, the system begins to destabilize:\n\n\\(C\\) receives a small automation boost.\nIf labor weakens slightly, \\(C\\) continues to grow—less dependent on \\(L\\).\nFeedback begins to tilt asymmetrically toward capital dominance.\n\nInterpretation: The system is no longer symmetric. Capital begins to escape the orbit of labor.\n3. Structural Mutation (Irreversibility Activated)\nOnce \\(a_{2,2} C \\gg a_{2,1} LC\\), capital’s growth is no longer a function of labor.\nConditions for mutation:\n\n\\(a_{2,2} &gt; a_{2,3}\\): capital can grow faster than it decays,\n\\(a_{1,2} C &gt; a_{1,1}\\): labor is absorbed faster than it reproduces,\n\\(a_{2,4} C\\) is not too large: elite–elite rivalry remains subcritical.\n\nThen:\n\n\\(L(t) \\to 0\\),\n\n\\(C(t) \\to C^* &gt; 0\\) (if friction), or \\(\\to \\infty\\) (if friction is weak),\n\n\\((L = 0, C = C^*)\\) becomes an absorbing state—a terminal attractor.\n\nInterpretation: Labor is not outcompeted—it is made irrelevant. Capital sustains itself and eventually excludes labor from its dynamic logic.\n4. Elite Saturation Collapse\nIf \\(a_{2,4}\\) becomes too large (e.g., due to capital hoarding, overconcentration, or elite cannibalism):\n\nEven autonomous capital growth stalls,\n\\(C(t)\\) declines after reaching a peak,\nSystem may stabilize at a low-labor, low-capital regime—or collapse entirely.\n\nInterpretation: Overaccumulation leads to implosion. Not because of labor resistance, but from excess at the top.",
    "crumbs": [
      "Apps",
      "돈의 동역학",
      "AI시대 동역학 모형"
    ]
  },
  {
    "objectID": "d_3_model.html#structural-irreversibility-and-phase-space-mutation",
    "href": "d_3_model.html#structural-irreversibility-and-phase-space-mutation",
    "title": "AI시대 동역학 모형",
    "section": "3 Structural Irreversibility and Phase Space Mutation",
    "text": "3 Structural Irreversibility and Phase Space Mutation\nThe transition from a labor–capital mutual system to one governed by autopoietic capital dynamics is not gradual—it is governed by a critical threshold beyond which the system undergoes a topological mutation. Prior to this threshold, the system behaves conservatively: labor and capital remain coupled through mutual feedback, and trajectories in the phase space exhibit cyclical or bounded behavior. However, once capital exceeds a structurally determined critical level, the term \\(a_{2,2}C\\) overtakes labor-mediated growth (\\(a_{2,1}LC\\)), shifting the dynamical core from interaction to self-replication. At this point, the system’s structure changes discontinuously: closed orbits vanish, invariant sets collapse toward the capital axis, and the line \\(L=0\\) becomes an absorbing boundary. Labor, once displaced, cannot re-enter the system—a condition of topological irreversibility. The phase space itself no longer supports recovery pathways. This transformation is not a new equilibrium within the same geometry; it is a qualitative reconfiguration of the geometry itself.\nThis structural interpretation of automation reveals it as a mechanism of decoupling, not simply one of acceleration. It dismantles the co-evolutionary logic of labor and capital, replacing it with an extractive trajectory that systematically erodes labor’s role. As labor vanishes, the site of conflict shifts inward: among capital holders themselves. The nonlinear dissipation term \\(a_{2,4}C^2\\) captures this emergent elite–elite competition, where positional advantage and resource saturation determine outcomes. The core asymmetry of the system thus moves from inter-class struggle to intra-elite positional rivalry—a transformation concealed in aggregate growth but revealed in structural dynamics.\nWith this theoretical architecture established, we now turn to the empirical question: does current macroeconomic data exhibit signals of this irreversible transition? In the following section, we estimate the parameters of the system using historical data and identify which regime—closed, transitional, or exclusionary—the contemporary economy occupies within the modeled phase space.",
    "crumbs": [
      "Apps",
      "돈의 동역학",
      "AI시대 동역학 모형"
    ]
  },
  {
    "objectID": "d_3_model.html#estimating-parameters-with-level-data",
    "href": "d_3_model.html#estimating-parameters-with-level-data",
    "title": "AI시대 동역학 모형",
    "section": "4 Estimating Parameters with Level Data",
    "text": "4 Estimating Parameters with Level Data\nThe empirical calibration of structural dynamical models often begins with time series data expressed in absolute levels. In the context of labor–capital systems, this typically involves tracking population, capital stock, investment flows, and income components—each representing one component of the theoretical system:\n\\[\n\\begin{aligned}\n\\frac{dL}{dt} &= a_{1,1} L - a_{1,2} L C, \\\\\n\\frac{dC}{dt} &= a_{2,1} L C + a_{2,2} C - a_{2,3} C - a_{2,4} C^2.\n\\end{aligned}\n\\]\nEach parameter in this system has a corresponding economic interpretation, often approximated using national account statistics or industry-level indicators:\n\n\n\n\n\n\n\n\nParameter\nInterpretation\nEmpirical Proxy\n\n\n\n\n\\(a_{1,1}\\)\nEndogenous labor growth\nPopulation and participation rates (BLS, UN)\n\n\n\\(a_{1,2}\\)\nCapital-induced labor absorption\nWage–productivity divergence, labor share trend\n\n\n\\(a_{2,1}\\)\nLabor-driven capital formation\nProfit per worker, reinvestment ratios\n\n\n\\(a_{2,2}\\)\nAutomation-induced capital growth\nRobot density, IT capital share, patent counts\n\n\n\\(a_{2,3}\\)\nCapital depreciation\nConsumption of fixed capital (BEA, Eurostat)\n\n\n\\(a_{2,4}\\)\nFrictions to capital expansion\nDiminishing returns, saturation metrics\n\n\n\nWhile conceptually straightforward, level-based estimation encounters several technical and interpretive challenges. Among these:\n\nHigh sensitivity to measurement errors in levels, especially when working with small changes (e.g., net investment).\nHeterogeneity in units and scaling, which complicates system-level identification without extensive normalization.\nImplicit steady-state assumptions, particularly when using calibration methods that fix terminal values or target observed ratios (e.g., \\(C^*/Y^*\\) or \\(K/L\\) ratios).\nDependence on aggregation conventions, such as capital stock estimation or depreciation schedules, which vary significantly across countries and sources.\n\nA variety of empirical strategies have been employed in this context:\n\nClosed-form identification: Parameters like \\(a_{2,3}\\) (capital depreciation) can often be directly observed from national accounts.\nTime-series regression: Parameters such as \\(a_{2,2}\\) (automation growth) or \\(a_{1,2}\\) (labor extraction rate) can be estimated by regressing capital growth or labor share decline on automation-related indicators.\nSteady-state calibration: Assuming long-run convergence, one can calibrate parameters to match terminal or average observed values (e.g., setting \\(a_{2,4}\\) to ensure bounded \\(C\\) given observed capital-output ratios).\nBayesian updating: Priors from the macroeconomic literature (e.g., Solow-type growth parameters) are updated using observed data.\nSystem-level structural estimation: Full-model techniques such as GMM or maximum likelihood estimation (MLE) are applied to estimate coupled ODEs from observed \\((L_t, C_t)\\) time series.\n\nDespite its prevalence, the level-based framework is limited in its ability to track distributional dynamics or relative concentration patterns, particularly in systems undergoing structural mutation. In such cases, absolute levels may grow exponentially while the internal composition of the system changes qualitatively.\nTo address these limitations, the next section introduces a share-based estimation strategy that reinterprets elite capital concentration data as a proxy for system-level capital dominance, enabling estimation of nonlinear dynamics via normalized replicator-like forms. This approach aligns more naturally with inequality-focused modeling, where relative shares and structural saturation matter more than total quantities.",
    "crumbs": [
      "Apps",
      "돈의 동역학",
      "AI시대 동역학 모형"
    ]
  },
  {
    "objectID": "d_3_model.html#estimating-parameters-with-share-data",
    "href": "d_3_model.html#estimating-parameters-with-share-data",
    "title": "AI시대 동역학 모형",
    "section": "5 Estimating Parameters with Share Data",
    "text": "5 Estimating Parameters with Share Data\nTraditional macroeconomic estimation strategies are typically rooted in level data—aggregate capital stock, total output, labor force levels—used to calibrate dynamic models of production, accumulation, and distribution. While effective in certain contexts, this approach suffers from scaling inconsistencies, comparability issues across time, and sensitivity to nominal shocks. Moreover, level data fails to reveal the internal stratification of economic systems—particularly the distribution of capital across heterogeneous agents.\nTo address this, we introduce a share-based approach. Rather than modeling the absolute size of capital or labor, we model their relative shares:\n\nCapital share: \\(c(t) = \\frac{C(t)}{C(t) + L(t)}\\)\nLabor share: \\(\\ell(t) = 1 - c(t)\\)\n\nThis reframing allows us to interpret economic dynamics in terms of structural positioning and relative dominance, rather than absolute growth. Importantly, this representation is invariant to total system size, rendering it more robust for long-term analysis and cross-group comparison.\n\n5.1 U.S. Wealth Share Data (1989–2024, Quarterly)\nWe utilize quarterly data from the Federal Reserve’s Distributional Financial Accounts (via FRED) to construct empirical capital share trajectories for three elite percentile groups:\n\nTop 0.1% Net Wealth Share (WFRBSTP1300)\nTop 1% Net Wealth Share (WFRBST01134)\nTop 10% Net Wealth Share, computed as the sum of top 1% and 90–99% (WFRBSN09161)\n\nThese series serve as empirical proxies for the capital share \\(c(t)\\) held by each group. Corresponding labor shares are then computed as \\(\\ell(t) = 1 - c(t)\\).\nThis share-based framing bypasses issues with raw levels (GDP deflators, capital depreciation schedules, labor hours) and instead centers the analysis on relative control over accumulated wealth—a key structural feature in capitalist systems.\n\n\n5.2 Share-Based Dynamics\nThe structural labor–capital model is given in level terms as:\n\\[\n\\begin{aligned}\n\\frac{dL}{dt} &= a_{11} L - a_{12} L C, \\\\\n\\frac{dC}{dt} &= a_{21} L C + a_{22} C - a_{23} C - a_{24} C^2.\n\\end{aligned}\n\\]\nDefine total system size \\(S(t) = L(t) + C(t)\\) and the capital share: \\[\nc(t) = \\frac{C(t)}{S(t)}.\n\\]\nBy differentiating this definition with respect to time: \\[\n\\frac{dc}{dt} = \\frac{\\dot{C} S - C \\dot{S}}{S^2}.\n\\]\nSubstituting the dynamics: - \\(\\dot{C} = a_{21} L C + a_{22} C - a_{23} C - a_{24} C^2\\) - \\(\\dot{L} = a_{11} L - a_{12} L C\\) - So \\(\\dot{S} = \\dot{L} + \\dot{C}\\)\nNow let \\(C = c S\\), \\(L = (1 - c) S\\) to express everything in terms of \\(c(t)\\) and \\(S(t)\\).\nAssuming \\(a_{21} \\approx 0\\) (negligible labor-driven capital growth) and quasi-stationary \\(S(t)\\) over short windows, the expression simplifies to:\n\\[\n\\frac{dc}{dt} \\approx c (a_{22} - a_{24} c).\n\\]\nThis replicator-type equation captures the nonlinear interplay between:\n\nSelf-replicating capital (via \\(a_{22}\\)), and\nIntra-elite saturation effects (via \\(a_{24}\\)).\n\nEmpirically, this continuous form becomes: \\[\n\\frac{\\Delta c_t}{c_t} \\approx a_{22}(t) - a_{24}(t) c_t.\n\\]\nBy running rolling-window OLS regressions (e.g., 20 quarters), we estimate \\(a_{22}(t)\\) and \\(a_{24}(t)\\) as time-varying structural coefficients, capturing technological change, financial regime shifts, and institutional transitions.\nTo estimate the remaining parameters, we turn to the full coupled system. Normalizing the labor-capital system in share terms:\n\nFor labor: \\[\n\\frac{\\Delta \\ell_t}{\\ell_t} = a_{11}(t) - a_{12}(t) c_t + \\varepsilon_t.\n\\]\nFor capital (expanded): \\[\n\\frac{\\Delta c_t}{c_t} = a_{21}(t) \\ell_t + a_{22}(t) - a_{23}(t) - a_{24}(t) c_t + \\varepsilon_t.\n\\]\n\nGiven previously estimated \\(a_{22}(t)\\) and \\(a_{24}(t)\\), and setting \\(a_{23}(t) = 0.025\\) (fixed quarterly depreciation), we identify:\n\n\\(a_{11}(t), a_{12}(t)\\): from labor-share regression,\n\\(a_{21}(t)\\): by back-solving from capital equation residuals.\n\n\n\n5.3 Empirical Findings\n\n\\(a_{22}(t)\\) rises sharply during known automation epochs: dot-com boom, smartphone era, and post-2020 AI expansion.\n\\(a_{24}(t)\\) is consistently positive and increasing with percentile, indicating growing intra-elite competition.\n\\(a_{21}(t) \\approx 0\\) across all groups, sometimes on the order of \\(10^{-7}\\) or lower.\n\n\nThis implies that capital is self-reproducing, with minimal dependence on labor—a hallmark of late-stage automation economies.\n\nHowever, setting \\(a_{21} = 0\\) creates degenerate dynamics: over time, both capital and labor vanish. Surprisingly, even capital collapses without minimal labor input.\nBy setting \\(a_{21} = 0.0007\\) (small but positive), simulations match observed empirical tail behavior in \\(c(t)\\)—showing that even negligible labor-capital coupling is structurally vital for long-run viability.\nThese findings underscore the fragility of modern accumulation systems. The capital-labor system is not sustained by productivity, but by network-dependent feedback loops. Small structural parameters—such as \\(a_{21}\\)—govern the existence of long-run attractors.\nPolicy interventions must thus go beyond redistribution. They must alter structural parameters: - Lower \\(a_{12}\\) to reduce labor extraction, - Cap \\(a_{24}\\) to prevent elite overaccumulation, - Increase \\(a_{21}\\) via labor-linked investment, - Shift \\(a_{22}\\) toward inclusive automation.\nThis model reframes inequality not as an outcome of imbalance, but as a phase-space feature—a geometrical consequence of structural dynamics. In summary, share-based empirical modeling reveals structural dependencies invisible in level-data systems. The future of capital depends not on output maximization, but on the topology of feedback and the fragility of replication.",
    "crumbs": [
      "Apps",
      "돈의 동역학",
      "AI시대 동역학 모형"
    ]
  },
  {
    "objectID": "d_3_model.html#plots",
    "href": "d_3_model.html#plots",
    "title": "AI시대 동역학 모형",
    "section": "6 Plots",
    "text": "6 Plots\n\n6.1 US Wealth Share Data\n\n\nCode\nimport pandas_datareader.data as web\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.integrate import solve_ivp\nfrom scipy.stats import trim_mean, mode\n\n# Define the time range\nstart_date = \"1989-07-01\"\nend_date = \"2024-10-01\"\n\n# Download data directly for each group\ndata = {}\ndata['Top 0.1%'] = web.DataReader('WFRBSTP1300', 'fred', start_date, end_date)\ndata['Top 1%'] = web.DataReader('WFRBST01134', 'fred', start_date, end_date)\ndata['Top 90-99%'] = web.DataReader('WFRBSN09161', 'fred', start_date, end_date)\n\n# Combine Top 1% and 90-99% to compute Top 10%\ntop_10 = pd.DataFrame(index=data['Top 1%'].index)\ntop_10['Top 10%'] = data['Top 1%']['WFRBST01134'] + data['Top 90-99%']['WFRBSN09161']\n\n# Combine all into a single DataFrame for analysis\ndf = pd.DataFrame(index=data['Top 0.1%'].index)\ndf['Top 0.1%'] = data['Top 0.1%']['WFRBSTP1300']\ndf['Top 1%'] = data['Top 1%']['WFRBST01134']\ndf['Top 10%'] = top_10['Top 10%']\n\ndf = df.dropna()\n\nprint(\"Sample Start: \", start_date)\nprint(\"Sample End: \", end_date)\ndf.head()\n\n\nSample Start:  1989-07-01\nSample End:  2024-10-01\n\n\n\n\nTable 1: Quarterly net wealth share data (Top 0.1%, 1%, and 10%) are retrieved directly from the FRED database and used to compute normalized capital share trajectories over the period 1989Q3 to 2024Q4.\n\n\n\n\n\n\n\n\n\n\nTop 0.1%\nTop 1%\nTop 10%\n\n\nDATE\n\n\n\n\n\n\n\n1989-07-01\n8.6\n22.9\n60.9\n\n\n1989-10-01\n8.7\n23.0\n60.9\n\n\n1990-01-01\n8.6\n22.8\n60.6\n\n\n1990-04-01\n8.7\n22.9\n60.6\n\n\n1990-07-01\n8.5\n22.5\n60.0\n\n\n\n\n\n\n\n\n\n\n\n\n6.2 Time-Varying Parameter Estimates (All Groups)\n\n\nCode\n# Time-varying parameter estimation for each wealth share group, using a rolling window regression on the nonlinear replicator-transformed system. Parameters $a_{2,2}(t)$ and $a_{2,4}(t)$ are inferred from capital share growth, while $a_{1,1}(t)$ and $a_{1,2}(t)$ are extracted from labor share contraction. Residual dynamics yield estimates of $a_{2,1}(t)$ and implied $a_{2,3}(t)$.\n\n# Calculate c(t) and l(t) for each group\nct_01 = df['Top 0.1%']\nct_1 = df['Top 1%']\nct_10 = df['Top 10%']\n\nlt_01 = 1 - ct_01\nlt_1 = 1 - ct_1\nlt_10 = 1 - ct_10\n\n# Define a function to compute rolling parameter estimates\ndef estimate_parameters(c_t, window=20, depreciation=0.025):\n    c_t = c_t.dropna()\n    l_t = 1 - c_t\n    dc = c_t.diff()\n    dl = l_t.diff()\n    c_t_lag = c_t.shift(1)\n    l_t_lag = l_t.shift(1)\n    \n    rel_dc = (dc / c_t_lag).dropna()\n    rel_dl = (dl / l_t_lag).dropna()\n\n    df_rolling = pd.DataFrame({\n        'c': c_t_lag,\n        'l': l_t_lag,\n        'dc_rel': rel_dc,\n        'dl_rel': rel_dl\n    }).dropna()\n\n    results = []\n\n    for i in range(window, len(df_rolling)):\n        window_df = df_rolling.iloc[i - window:i]\n        X_c = np.vstack([np.ones(window), -window_df['c']]).T\n        y_c = window_df['dc_rel']\n        a22, a24 = np.linalg.lstsq(X_c, y_c, rcond=None)[0]\n\n        X_l = np.vstack([np.ones(window), -window_df['c']]).T\n        y_l = window_df['dl_rel']\n        a11, a12 = np.linalg.lstsq(X_l, y_l, rcond=None)[0]\n\n        a21_minus_a23 = y_c - (a22 - a24 * window_df['c'])\n        X_a21 = window_df['l']\n        a21 = np.mean(a21_minus_a23 / X_a21)\n        a23 = a21 - depreciation  # using assumed depreciation\n\n        results.append({\n            'date': df_rolling.index[i],\n            'a11': a11,\n            'a12': a12,\n            'a21': a21,\n            'a22': a22,\n            'a23': a23,\n            'a24': a24\n        })\n\n    return pd.DataFrame(results).set_index('date')\n\n# Estimate for all three groups\nres_df_01 = estimate_parameters(ct_01, window=20)\nres_df_1 = estimate_parameters(ct_1, window=20)\nres_df_10 = estimate_parameters(ct_10, window=20)\n\n\n\n\nCode\n# 추정 대상 그룹\ngroup_labels = ['Top 0.1%', 'Top 1%', 'Top 10%']\nresults = [res_df_01, res_df_1, res_df_10]\n\n# 파라미터 목록\nparams = ['a11', 'a12', 'a21', 'a22', 'a23', 'a24']\n\n# Plotting\nfig, axes = plt.subplots(3, 2, figsize=(16, 12), sharex=True)\naxes = axes.flatten()\n\nfor i, param in enumerate(params):\n    for group_df, label in zip(results, group_labels):\n        if param in group_df.columns:\n            axes[i].plot(group_df.index, group_df[param], label=label)\n    axes[i].set_title(f\"Parameter: {param}\")\n    axes[i].legend()\n    axes[i].grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Estimated time-varying parameters for each wealth percentile group, based on rolling window regressions. Each panel corresponds to one structural parameter of the labor–capital dynamic system. Differences across percentile groups reflect heterogeneity in structural regimes, such as automation dominance (\\(a_{2,2}\\)) or labor extraction intensity (\\(a_{1,2}\\)).\n\n\n\n\n\n\n\n6.3 Distribution of Estimated Parameters (Histogram)\n\n\nCode\nimport matplotlib.pyplot as plt\n\n# 추정된 모든 파라미터 리스트\nparameters = ['a11', 'a12', 'a21', 'a22', 'a23', 'a24']\ngroup_dfs = [res_df_01, res_df_1, res_df_10]\ngroup_labels = ['Top 0.1%', 'Top 1%', 'Top 10%']\ncolors = ['tab:blue', 'tab:orange', 'tab:green']\n\nfig, axes = plt.subplots(3, 2, figsize=(14, 10))\naxes = axes.flatten()\n\nfor i, param in enumerate(parameters):\n    ax = axes[i]\n    for df, label, color in zip(group_dfs, group_labels, colors):\n        ax.hist(df[param].dropna(), bins=50, alpha=0.6, label=label, color=color)\n    ax.set_title(f\"Parameter: {param}\")\n    ax.set_xlabel(\"Value\")\n    ax.set_ylabel(\"Frequency\")\n    ax.legend()\n    ax.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Empirical distribution of estimated parameters across Top 0.1%, Top 1%, and Top 10% wealth share groups. Each subplot overlays the histogram of a single parameter’s rolling estimates for all three groups. The visual comparison reveals systematic differences in structural dynamics across percentile tiers.",
    "crumbs": [
      "Apps",
      "돈의 동역학",
      "AI시대 동역학 모형"
    ]
  },
  {
    "objectID": "d_3_model.html#parameter-summary-table-group-wise-statistics-by-parameter",
    "href": "d_3_model.html#parameter-summary-table-group-wise-statistics-by-parameter",
    "title": "AI시대 동역학 모형",
    "section": "7 Parameter Summary Table: Group-Wise Statistics by Parameter",
    "text": "7 Parameter Summary Table: Group-Wise Statistics by Parameter\n\n\nCode\ndepreciation = 0.025\n\nprint(\"depreciation a23 (quarterly): \", depreciation)\n\nfrom scipy.stats import trim_mean, mode\n\n# Define a function to compute descriptive statistics for each parameter\ndef summarize_parameters(df, name):\n    summary = pd.DataFrame(index=df.columns)\n    summary[\"median\"] = df.median()\n    summary[\"trimmed_mean\"] = df.apply(lambda x: trim_mean(x.dropna(), 0.1))\n    summary[\"mode\"] = df.mode().iloc[0]\n    summary[\"group\"] = name\n    return summary\n\nsummary_01 = summarize_parameters(res_df_01, \"Top 0.1%\")\nsummary_1 = summarize_parameters(res_df_1, \"Top 1%\")\nsummary_10 = summarize_parameters(res_df_10, \"Top 10%\")\n\n# Combine summaries\nparam_summary = pd.concat([summary_01, summary_1, summary_10])\nparam_summary = param_summary.reset_index().rename(columns={\"index\": \"parameter\"})\n\n# Overwrite a23 with constant 0.25 (quarterly) for all groups\nparam_summary.loc[param_summary[\"parameter\"] == \"a23\", [\"median\", \"trimmed_mean\", \"mode\"]] = depreciation\n\n# Extract median values as parameter sets\ndef extract_params(df, group_name, depreciation=0.025):\n    row = df[df[\"group\"] == group_name].set_index(\"parameter\")\n    return {\n        \"a11\": row.loc[\"a11\", \"median\"],\n        \"a12\": row.loc[\"a12\", \"median\"],\n        \"a21\": row.loc[\"a21\", \"median\"],\n        \"a22\": row.loc[\"a22\", \"median\"],\n        \"a23\": depreciation,  # override\n        \"a24\": row.loc[\"a24\", \"median\"]\n    }\n\nparams_top_01 = extract_params(param_summary, \"Top 0.1%\")\nparams_top_1 = extract_params(param_summary, \"Top 1%\")\nparams_top_10 = extract_params(param_summary, \"Top 10%\")\n\n# Define representative median parameters (with calibrated a23 = 0.025)\n\n# 통계량별로 long-form에서 wide-form으로 pivot\npivot_all = param_summary.pivot_table(\n    index=\"parameter\",\n    columns=\"group\",\n    values=[\"median\", \"trimmed_mean\", \"mode\"]\n)\n\n# 열 순서를 보기 좋게 정렬\nordered_columns = [\n    (\"median\", \"Top 0.1%\"), (\"median\", \"Top 1%\"), (\"median\", \"Top 10%\"),\n    (\"trimmed_mean\", \"Top 0.1%\"), (\"trimmed_mean\", \"Top 1%\"), (\"trimmed_mean\", \"Top 10%\"),\n    (\"mode\", \"Top 0.1%\"), (\"mode\", \"Top 1%\"), (\"mode\", \"Top 10%\"),\n]\npivot_all = pivot_all[ordered_columns]\n\n# 소수점 자리수 정리\npivot_all = pivot_all.round(5)\npivot_all\n\n\ndepreciation a23 (quarterly):  0.025\n\n\n\n\nTable 2: Parameter-wise summary statistics across elite groups (Top 0.1%, Top 1%, Top 10%), including median, trimmed mean, and mode for each parameter.\n\n\n\n\n\n\n\n\n\n\nmedian\ntrimmed_mean\nmode\n\n\ngroup\nTop 0.1%\nTop 1%\nTop 10%\nTop 0.1%\nTop 1%\nTop 10%\nTop 0.1%\nTop 1%\nTop 10%\n\n\nparameter\n\n\n\n\n\n\n\n\n\n\n\n\n\na11\n0.12041\n0.13333\n0.10653\n0.11903\n0.18327\n0.11065\n-0.17888\n-0.07761\n-0.14894\n\n\na12\n0.01050\n0.00460\n0.00160\n0.00995\n0.00626\n0.00167\n-0.01510\n-0.00356\n-0.00211\n\n\na21\n-0.00000\n-0.00000\n0.00000\n-0.00000\n-0.00000\n-0.00000\n-0.00001\n-0.00000\n-0.00000\n\n\na22\n0.10940\n0.12865\n0.10479\n0.10796\n0.17678\n0.10894\n-0.16071\n-0.07477\n-0.14669\n\n\na23\n0.02500\n0.02500\n0.02500\n0.02500\n0.02500\n0.02500\n0.02500\n0.02500\n0.02500\n\n\na24\n0.00950\n0.00443\n0.00158\n0.00902\n0.00604\n0.00164\n-0.01356\n-0.00343\n-0.00208",
    "crumbs": [
      "Apps",
      "돈의 동역학",
      "AI시대 동역학 모형"
    ]
  },
  {
    "objectID": "d_3_model.html#simulated-system-dynamics-with-calibrated-labor-driven-capital-growth-a_21-0.0007",
    "href": "d_3_model.html#simulated-system-dynamics-with-calibrated-labor-driven-capital-growth-a_21-0.0007",
    "title": "AI시대 동역학 모형",
    "section": "8 Simulated System Dynamics with Calibrated Labor-Driven Capital Growth (\\(a_{21} = 0.0007\\))",
    "text": "8 Simulated System Dynamics with Calibrated Labor-Driven Capital Growth (\\(a_{21} = 0.0007\\))\n\n\nCode\nfrom scipy.integrate import solve_ivp\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define the system of ODEs with fixed a21 = 0.0007\ndef simulate_system(params, L0=10, C0=3, t_span=(0, 140), t_steps=2000):\n    def system(t, y):\n        L, C = y\n        a11, a12, a21, a22, a23, a24 = (\n            params[\"a11\"], params[\"a12\"], 0.0007,  # fixed!\n            params[\"a22\"], params[\"a23\"], params[\"a24\"]\n        )\n        dLdt = a11 * L - a12 * L * C\n        dCdt = a21 * L * C + a22 * C - a23 * C - a24 * C**2\n        return [dLdt, dCdt]\n\n    t_eval = np.linspace(t_span[0], t_span[1], t_steps)\n    sol = solve_ivp(system, t_span, [L0, C0], t_eval=t_eval)\n    return sol.t, sol.y[0], sol.y[1]\n\n# Simulations for each group\nt, L_01, C_01 = simulate_system(params_top_01)\n_, L_1, C_1 = simulate_system(params_top_1)\n_, L_10, C_10 = simulate_system(params_top_10)\n\n# Normalize to capital share c(t)\nc_01 = C_01 / (L_01 + C_01)\nc_1 = C_1 / (L_1 + C_1)\nc_10 = C_10 / (L_10 + C_10)\n\n# Plot: Capital share over time\nplt.figure(figsize=(12, 5))\nplt.plot(t, c_01, label=\"Top 0.1%\", linewidth=2)\nplt.plot(t, c_1, label=\"Top 1%\", linewidth=2)\nplt.plot(t, c_10, label=\"Top 10%\", linewidth=2)\nplt.title(\"Simulated Capital Share Dynamics by Group ($a_{21} = 0.0007$)\")\nplt.xlabel(\"Time (Quarters)\")\nplt.ylabel(\"Capital Share $c(t)$\")\nplt.ylim(0, 1.05)\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Simulated time-domain capital share dynamics \\(c(t)\\) for each percentile group using calibrated \\(a_{21} = 0.0007\\) and median parameter values.\n\n\n\n\n\n\n\nCode\n# \"Simulated phase trajectories and normalized vector fields in $(L, C)$ space for each percentile group, using calibrated $a_{21} = 0.0007$.\"\n\n# Define simulation function\ndef simulate_system(params, L0=10, C0=3, t_span=(0, 140), t_steps=2000):\n    def system(t, y):\n        L, C = y\n        dLdt = params[\"a11\"] * L - params[\"a12\"] * L * C\n        dCdt = 0.0007 * L * C + params[\"a22\"] * C - params[\"a23\"] * C - params[\"a24\"] * C**2\n        return [dLdt, dCdt]\n    t_eval = np.linspace(t_span[0], t_span[1], t_steps)\n    sol = solve_ivp(system, t_span, [L0, C0], t_eval=t_eval)\n    return sol.t, sol.y[0], sol.y[1]\n\n# Grid for vector field\nL_vals = np.linspace(0, 150, 150)\nC_vals = np.linspace(0, 150, 150)\nL_grid, C_grid = np.meshgrid(L_vals, C_vals)\n\n# Plotting\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\ngroup_params = {\n    \"Top 0.1%\": params_top_01,\n    \"Top 1%\": params_top_1,\n    \"Top 10%\": params_top_10\n}\n\nfor ax, (label, params) in zip(axes, group_params.items()):\n    # Vector field\n    a11, a12 = params[\"a11\"], params[\"a12\"]\n    a22, a23, a24 = params[\"a22\"], params[\"a23\"], params[\"a24\"]\n    a21 = 0.0007  # fixed\n\n    dL = a11 * L_grid - a12 * L_grid * C_grid\n    dC = a21 * L_grid * C_grid + a22 * C_grid - a23 * C_grid - a24 * C_grid**2\n    mag = np.sqrt(dL**2 + dC**2)\n    dL_norm = dL / (mag + 1e-8)\n    dC_norm = dC / (mag + 1e-8)\n\n    ax.quiver(L_grid, C_grid, dL_norm, dC_norm, angles='xy', color='gray', alpha=0.5)\n\n    # Trajectory\n    t, L_sim, C_sim = simulate_system(params)\n    ax.plot(L_sim, C_sim, 'r-', linewidth=2, label='Trajectory')\n\n    ax.set_title(f\"Phase Space: {label}\")\n    ax.set_xlabel(\"Labor (L)\")\n    ax.set_ylabel(\"Capital (C)\")\n    ax.set_xlim(0, 150)\n    ax.set_ylim(0, 150)\n    ax.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: Level-Based Phase Space with Vector Field and Simulated Trajectories",
    "crumbs": [
      "Apps",
      "돈의 동역학",
      "AI시대 동역학 모형"
    ]
  },
  {
    "objectID": "d_overview.html",
    "href": "d_overview.html",
    "title": "AI시대 자본과 노동",
    "section": "",
    "text": "Labor-Capital Dynamics in the Age of Automation",
    "crumbs": [
      "Apps",
      "돈의 동역학",
      "AI시대 자본과 노동"
    ]
  },
  {
    "objectID": "d_overview.html#highlights",
    "href": "d_overview.html#highlights",
    "title": "AI시대 자본과 노동",
    "section": "1 Highlights",
    "text": "1 Highlights\n\nIntroduces a nonlinear dynamical model where capital can persist or collapse depending on minimal structural links to labor, calibrated on U.S. top-percentile wealth share data (1989–2024).\nShows that automation-led capital growth, without sufficient labor coupling, leads to systemic extinction—even for elite capital holders—revealing a degenerate equilibrium regime.\nReinterprets economic equilibrium not as a sign of fairness or efficiency, but as a geometric endpoint of exclusion, calling for ethical intervention based on structural curvature.\n\nThis study develops a nonlinear dynamical framework to examine the structural decoupling of capital from labor under automation, declining reinvestment, and saturation among elite holders. Building on the ecological foundations of Lotka–Volterra systems and their reinterpretation in Goodwin-type class dynamics, we estimate time-varying parameters using U.S. top-percentile wealth share data from 1989 to 2024. The empirical evidence reveals a gradual erosion of labor’s functional relevance—not only across the general population, but also within the capital-dominant strata themselves. When labor’s contribution to capital formation is fully severed, the system undergoes a long-run collapse, including of capital itself. Yet even minimal positive linkage is sufficient to sustain realistic long-run distributions. These findings challenge conventional notions of equilibrium, merit, and redistribution by demonstrating that economic persistence is governed less by fairness than by structural geometry. We conclude by proposing a normative framework for economic design, emphasizing curvature-based intervention—where stability, justice, and inclusion are functions of the system’s underlying phase-space topology.",
    "crumbs": [
      "Apps",
      "돈의 동역학",
      "AI시대 자본과 노동"
    ]
  },
  {
    "objectID": "d_overview.html#contents",
    "href": "d_overview.html#contents",
    "title": "AI시대 자본과 노동",
    "section": "2 Contents",
    "text": "2 Contents\n\nCritique on Mainstream Economics\nElementary Mathematics for System Dynamics\nStructural Mutation Model via Automation\nEthics in the Geometry of Capital–Labor",
    "crumbs": [
      "Apps",
      "돈의 동역학",
      "AI시대 자본과 노동"
    ]
  },
  {
    "objectID": "d_overview.html#literature-review",
    "href": "d_overview.html#literature-review",
    "title": "AI시대 자본과 노동",
    "section": "3 Literature Review",
    "text": "3 Literature Review\nThe conceptual bridge between ecological modeling and economic dynamics spans a rich and varied intellectual history, reflecting the enduring quest to understand distributional conflicts and endogenous cycles in capitalist economies. The philosophical roots of this inquiry trace back to Marx (1867), who conceptualized capital as a historically contingent social relation grounded in the extraction of surplus labor. Our approach, however, diverges from Marx’s value-theoretic lens, prioritizing nonlinear dynamics, system trajectories, and bifurcation behaviors. We view the labor-capital relation as an evolving system, with equilibrium properties—and potential pathologies—emerging endogenously from its interactive structure.\nThis economic analogy finds its mathematical antecedents in early biological systems research. The foundational contributions of (Lotka 1925) and (Volterra 1926) established the Lotka-Volterra equations, which formalized population dynamics with a predator-prey metaphor. These models demonstrated considerable explanatory power, extending beyond ecology to inform macroeconomic theory. Building on this legacy, Rosenzweig and MacArthur (1963) clarified the conditions fostering stable equilibria in such systems, a property later leveraged in economic models to capture endogenous oscillations. These ecological insights directly inspired (Goodwin 1967), whose canonical growth-cycle model adapted Lotka-Volterra equations to depict the cyclical interplay between labor and capital classes. In Goodwin’s framework, wage-employment dynamics trace closed orbits: labor bargaining power strengthens during economic expansion, weakens as profits decline, and ultimately precipitates contraction, reverting to low employment and suppressed wages. This approach has since anchored heterodox macroeconomics.\nEfforts to integrate technological change into these dynamic systems, however, have often been constrained and stylized. Shah and Desai (1981) introduced labor-augmenting productivity growth as an exogenous shock, preserving the core dependency of capital on labor. Progressing this line of inquiry, Ploeg (1987) incorporated inflationary dynamics, illuminating how monetary policy and interest rates shape the amplitude and persistence of labor-capital cycles. Further advancing the analysis of economic periodicity, Sportelli (1995) explored how shifts in capital intensity influence cyclical behavior, offering a nuanced perspective on structural dynamics. This focus on system stability was enriched by (Hofbauer and Sigmund 1998), whose replicator dynamics framework—rooted in evolutionary game theory—provided rigorous stability criteria derived from the system’s Jacobian near equilibrium. These ideas found economic resonance in (Flaschel 2009), which examined the interplay of capital intensity and wage bargaining in shaping cyclical patterns. Subsequently, Grasselli and Lima (2012) extended the Goodwin model by integrating Keynesian demand channels, monetary effects, and fiscal policy, maintaining the endogenous nature of business cycles while addressing the intricate institutional realities of modern economies. In a similar vein, Tavani and Zamparelli (2015) endogenized technological innovation within the Goodwin framework, demonstrating how distributional conflicts and profit dynamics steer the path of technological adoption. Nevertheless, these studies consistently assume that capital accumulation hinges on labor input.\nIn contrast, mainstream approaches to automation and technological displacement, often grounded in general equilibrium theory, have evolved largely independently of ecological modeling traditions. Piketty (2014) reinvigorated discourse on structural inequality, underscoring the long-term dominance of capital returns over economic growth (r &gt; g). While his empirical analysis of wealth concentration and redistributive implications is profound, it leaves unresolved how such patterns arise from micro- or meso-level interactions. Addressing automation’s socioeconomic impacts, Seth Benzell and Sachs (2015) employed overlapping generations (OLG) and DSGE models to simulate scenarios where automation drives rising inequality and wage stagnation, particularly absent robust redistributive policies. This focus on labor market dynamics was extended by (Acemoglu and Restrepo 2018) and (Andrew Berg and Zanna 2018), who adopted task-based and structural macroeconomic approaches, respectively, to highlight automation’s dual effects: displacing labor in existing tasks while spurring productivity gains in newly automated sectors. Acemoglu and Restrepo (2020) further refined these insights, yet their neoclassical reliance on optimization and equilibrium concepts abstracts from the nonlinear interactions central to dynamic systems.\nOur study introduces a pivotal innovation by adapting dynamic system analysis to accommodate time-varying parameters, drawing inspiration from advances in applied mathematics and systems ecology. This perspective aligns with forward-looking concerns raised by Susskind (2020), which cautions that technological automation may render human labor economically obsolete. Yet, such analyses remain largely qualitative or comparative-static, lacking the dynamic modeling tools needed to explore transition paths or threshold behaviors. We depart from this lineage by positing that technological change can fundamentally reshape the system’s structural topology. Specifically, we introduce an automation-driven growth coefficient that enables capital to accumulate autonomously, decoupled from labor. This yields a degenerate equilibrium where labor vanishes without precipitating capital collapse—a phase-space configuration unaddressed by prior models.\nIn summary, our work synthesizes ecological modeling, dynamic systems theory, and labor economics to forge a novel framework for understanding the interplay of technological change and capital-labor relations. From the philosophical insights of (Marx 1867) to the ecological foundations of (Lotka 1925) and the automation concerns of (Susskind 2020), we trace a century-long evolution of thought. By modeling automation as a channel for capital self-replication, we propose equilibrium outcomes where capital persists without labor, challenging the assumptions of both neoclassical and Marxian traditions. This invites a profound rethinking of what constitutes a structurally just, dynamically viable, and ethically acceptable economic system.",
    "crumbs": [
      "Apps",
      "돈의 동역학",
      "AI시대 자본과 노동"
    ]
  },
  {
    "objectID": "finance_existence.html",
    "href": "finance_existence.html",
    "title": "Rethinking Asset Existence",
    "section": "",
    "text": "Financial Assets as Risk-Reward Instruments\n\nIn classical asset pricing theory, financial assets are presumed to exist to reward investors for bearing risk. Through mechanisms such as arbitrage and diversification, idiosyncratic risk is mitigated, and equilibrium returns are determined by each asset’s exposure to systematic risk. This framework justifies the existence of any asset through the promise of a fair, risk-adjusted return. Asset prices thus reflect not market power or concentration, but the compensated cost of risk.\nThis view embeds a philosophical logic of fairness: that market participants are compensated proportionally for risk undertaken, and that capital flows dynamically toward efficiency. Such a model assumes fluid reallocations, reversibility of state transitions, and the dominance of fundamentals over flow mechanics.",
    "crumbs": [
      "Apps",
      "금융",
      "Rethinking Asset Existence"
    ]
  },
  {
    "objectID": "gilded_age.html",
    "href": "gilded_age.html",
    "title": "New Gilded Age",
    "section": "",
    "text": "The term ‘Gilded Age’ was originally coined by Mark Twain in his novel The Gilded Age: A Tale of Today (Twain and Warner 1873), describing an era characterized by rapid economic expansion, extreme wealth concentration, and political corruption. A similar dynamic is emerging today, where financial and technological elites dominate economic output while wealth inequality reaches historic highs (Piketty 2014). The late 19th century saw industrial monopolies like Standard Oil and U.S. Steel controlling markets; today, tech giants such as Amazon, Apple, and Google exhibit similar dominance (Zucman 2019).\nSimultaneously, the Federal Reserve’s response to financial instability, particularly through excessive monetary expansion, contrasts with past policy mistakes that led to severe economic contractions due to monetary shrinkage (Bernanke 2000). If current economic trends persist—marked by the increasing concentration of wealth, hyperinflation risks, and geopolitical tensions—then the U.S. may be heading toward another crisis akin to the 1929 stock market collapse (Kindleberger 1978).",
    "crumbs": [
      "Apps",
      "경제",
      "New Gilded Age"
    ]
  },
  {
    "objectID": "gilded_age.html#extreme-wealth-concentration-and-economic-disparities",
    "href": "gilded_age.html#extreme-wealth-concentration-and-economic-disparities",
    "title": "New Gilded Age",
    "section": "2.1 Extreme Wealth Concentration and Economic Disparities",
    "text": "2.1 Extreme Wealth Concentration and Economic Disparities\nIn the late 19th century, “Robber Barons” controlled vast industrial empires while working-class Americans suffered under exploitative labor conditions (Irwin 2017). Today, the economic landscape reflects a similar dynamic: the top 1% of Americans hold over 30% of total U.S. wealth, and financial markets remain dominated by a handful of institutional investors and corporations (Saez and Zucman 2020). If historical trends hold, wealth concentration at this level often precedes financial and political crises.",
    "crumbs": [
      "Apps",
      "경제",
      "New Gilded Age"
    ]
  },
  {
    "objectID": "gilded_age.html#financial-market-distortions-due-to-federal-reserve-policies",
    "href": "gilded_age.html#financial-market-distortions-due-to-federal-reserve-policies",
    "title": "New Gilded Age",
    "section": "2.2 Financial Market Distortions Due to Federal Reserve Policies",
    "text": "2.2 Financial Market Distortions Due to Federal Reserve Policies\nHistorically, the Federal Reserve’s failure to manage monetary policy effectively has exacerbated financial downturns. During the Great Depression, the Fed allowed the money supply to contract, worsening deflation (Friedman and Schwartz 1993). Conversely, in the 2008 financial crisis, the Fed implemented massive QE programs to avoid liquidity shortages (Gopinath and Gourinchas 2020). If the Fed continues expanding the money supply unchecked while maintaining low interest rates, it could trigger runaway inflation or asset bubbles (Reinhart and Rogoff 2010).",
    "crumbs": [
      "Apps",
      "경제",
      "New Gilded Age"
    ]
  },
  {
    "objectID": "gilded_age.html#protectionist-policies-and-global-trade-disruptions",
    "href": "gilded_age.html#protectionist-policies-and-global-trade-disruptions",
    "title": "New Gilded Age",
    "section": "3.1 Protectionist Policies and Global Trade Disruptions",
    "text": "3.1 Protectionist Policies and Global Trade Disruptions\nIn response to financial instability, the U.S. may turn to protectionist measures similar to those seen in the early 20th century, such as the Smoot-Hawley Tariff Act (Irwin 2017). If the U.S. imposes broad tariffs on allies like Canada, Mexico, and the EU (excluding the UK), retaliatory tariffs could significantly reduce global trade, accelerating economic fragmentation (Acker 2020).",
    "crumbs": [
      "Apps",
      "경제",
      "New Gilded Age"
    ]
  },
  {
    "objectID": "gilded_age.html#capital-controls",
    "href": "gilded_age.html#capital-controls",
    "title": "New Gilded Age",
    "section": "3.2 Capital Controls",
    "text": "3.2 Capital Controls\nTo prevent capital flight, the U.S. government might implement capital controls, restricting the movement of funds outside the country (Dornbusch 1996). Such policies could initially stabilize domestic financial markets by preventing liquidity outflows, but they would ultimately deter foreign investment and reduce the credibility of the U.S. dollar (Prasad 2021). If capital controls are implemented alongside protectionist trade policies, the global financial system could realign, reducing reliance on the dollar (Eichengreen 2019).",
    "crumbs": [
      "Apps",
      "경제",
      "New Gilded Age"
    ]
  },
  {
    "objectID": "gilded_age.html#internal-conflict",
    "href": "gilded_age.html#internal-conflict",
    "title": "New Gilded Age",
    "section": "5.1 Internal Conflict",
    "text": "5.1 Internal Conflict\nWith increasing partisan division, the U.S. could experience state-led resistance against federal economic policies. Democratic-led states might oppose Republican federal mandates, leading to legal disputes over taxation, social policies, and trade regulations (Levitsky and Ziblatt 2018). In extreme cases, states like California could advocate for economic or political autonomy, mirroring secessionist movements of the 19th century, while Texas, despite its strong Republican leanings, might push for greater state sovereignty in response to federal overreach or shifting national policies (Acker 2020).",
    "crumbs": [
      "Apps",
      "경제",
      "New Gilded Age"
    ]
  },
  {
    "objectID": "gilded_age.html#u.s.-dollar-as-the-global-reserve-currency",
    "href": "gilded_age.html#u.s.-dollar-as-the-global-reserve-currency",
    "title": "New Gilded Age",
    "section": "5.2 U.S. Dollar as the Global Reserve Currency",
    "text": "5.2 U.S. Dollar as the Global Reserve Currency\nThe decline of the British pound post-World War II illustrates how global reserve currencies can lose dominance due to internal and external economic shifts (Eichengreen 2019). If U.S. political instability continues, central banks worldwide may accelerate diversification away from dollar holdings, increasing reliance on alternative financial networks such as BRICS payment systems, Bitcoin, and other emerging digital currencies. (Prasad 2021).",
    "crumbs": [
      "Apps",
      "경제",
      "New Gilded Age"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "gitsam 연구",
    "section": "",
    "text": "gitsam은 단기 성적 향상이 아닌, 구체적 사고력과 장기적 책임 의식을 갖춘 미래 리더 양성을 목표로 합니다. 교육은 “세상을 해석하고 변화시키는 힘이 되어야 한다”는 철학이 그 중심에 있습니다. 현대 사회의 공적 담론에서는 “모든 수사는 법과 절차에 따라 공정하게 진행됩니다”와 같은 항진명제적 발언이 반복되며, 구체적인 정보 제공은 회피됩니다. 이는 책임을 전가하고 정보 비대칭을 유지하려는 전략으로, 시민들에게 불투명한 사고를 강요하는 구조입니다. gitsam은 이러한 모호함을 극복하고자, “학생 스스로 문제를 구체적으로 정의하고 창의적으로 해석하며 책임감있게 해결할 수 있는 사고력 중심의 교육”을 지향합니다. 대수학, 미적분학, 확률통계, 고전역학, 리더십, 경제학 등 다양한 분야의 핵심 개념을 유기적으로 결합하며, 다양한 컴퓨터 도구들을 활용한 시각화 실습을 통해 수학과 과학 개념을 체화하게 합니다. 주가, 소득, 날씨 등 실제 데이터를 기반으로 문제를 수학적으로 모형화하고 해석하는 과정을 통해, 학생들은 단순한 풀이를 넘어 문제의 구조를 스스로 파악하고 다룰 수 있는 능력을 기르게 됩니다.\ngitsam이 강조하는 두 축:\n\nMath is an effective language\n\nFormulate and solve your problem\nInterpret and test your solution\n\nComputer is an efficient tool\n\nCompute and visualize with Python in Colab\nPublish with Quarto in local environment\n\n\n\n\nSee Table 1.\n\n\nSee Definition 1.\n\n\nSee Theorem 1.",
    "crumbs": [
      "Apps",
      "gitsam 연구"
    ]
  },
  {
    "objectID": "index.html#education-교육",
    "href": "index.html#education-교육",
    "title": "gitsam 연구",
    "section": "",
    "text": "gitsam은 단기 성적 향상이 아닌, 구체적 사고력과 장기적 책임 의식을 갖춘 미래 리더 양성을 목표로 합니다. 교육은 “세상을 해석하고 변화시키는 힘이 되어야 한다”는 철학이 그 중심에 있습니다. 현대 사회의 공적 담론에서는 “모든 수사는 법과 절차에 따라 공정하게 진행됩니다”와 같은 항진명제적 발언이 반복되며, 구체적인 정보 제공은 회피됩니다. 이는 책임을 전가하고 정보 비대칭을 유지하려는 전략으로, 시민들에게 불투명한 사고를 강요하는 구조입니다. gitsam은 이러한 모호함을 극복하고자, “학생 스스로 문제를 구체적으로 정의하고 창의적으로 해석하며 책임감있게 해결할 수 있는 사고력 중심의 교육”을 지향합니다. 대수학, 미적분학, 확률통계, 고전역학, 리더십, 경제학 등 다양한 분야의 핵심 개념을 유기적으로 결합하며, 다양한 컴퓨터 도구들을 활용한 시각화 실습을 통해 수학과 과학 개념을 체화하게 합니다. 주가, 소득, 날씨 등 실제 데이터를 기반으로 문제를 수학적으로 모형화하고 해석하는 과정을 통해, 학생들은 단순한 풀이를 넘어 문제의 구조를 스스로 파악하고 다룰 수 있는 능력을 기르게 됩니다.\ngitsam이 강조하는 두 축:\n\nMath is an effective language\n\nFormulate and solve your problem\nInterpret and test your solution\n\nComputer is an efficient tool\n\nCompute and visualize with Python in Colab\nPublish with Quarto in local environment\n\n\n\n\nSee Table 1.\n\n\nSee Definition 1.\n\n\nSee Theorem 1.",
    "crumbs": [
      "Apps",
      "gitsam 연구"
    ]
  },
  {
    "objectID": "index.html#writings-쓰기",
    "href": "index.html#writings-쓰기",
    "title": "gitsam 연구",
    "section": "Writings (쓰기)",
    "text": "Writings (쓰기)\n\n\n\n\nDefinition 1 (오일러 항등식) \\[\ne^{\\pi i} = -1\n\\]\n\n\n\n\nTheorem 1 (피타고라스 정리) For a right triangle in the Euclidean space, \\[\na^2+b^2=c^2\n\\]\n\n\n\n\n\n\nTable 1: My Caption\n\n\n\n\n\nDefault\nLeft\nRight\nCenter\n\n\n\n\n12\n12\n12\n12\n\n\n123\n123\n123\n123\n\n\n1\n1\n1\n1",
    "crumbs": [
      "Apps",
      "gitsam 연구"
    ]
  },
  {
    "objectID": "index.html#images-그리기",
    "href": "index.html#images-그리기",
    "title": "gitsam 연구",
    "section": "Images (그리기)",
    "text": "Images (그리기)\nSee My Streamlit Cloud\n\n\n\nimage-web 대한민국 인구분포 변화\n\n\n\n\n\n\n\nflowchart LR\n  A[Hard edge] --&gt; B(Round edge)\n  B --&gt; C{Decision}\n  C --&gt; D[Result one]\n  C --&gt; E[Result two]\n\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n\n\nrun\n\nrun\n\n\n\nintr\n\nintr\n\n\n\nrun--intr\n\n\n\n\nkernel\n\nkernel\n\n\n\nrun--kernel\n\n\n\n\nrunbl\n\nrunbl\n\n\n\nintr--runbl\n\n\n\n\nrunbl--run\n\n\n\n\nzombie\n\nzombie\n\n\n\nkernel--zombie\n\n\n\n\nsleep\n\nsleep\n\n\n\nkernel--sleep\n\n\n\n\nrunmem\n\nrunmem\n\n\n\nkernel--runmem\n\n\n\n\nsleep--runmem\n\n\n\n\nswap\n\nswap\n\n\n\nsleep--swap\n\n\n\n\nrunswap\n\nrunswap\n\n\n\nswap--runswap\n\n\n\n\nrunswap--runmem\n\n\n\n\nnew\n\nnew\n\n\n\nrunswap--new\n\n\n\n\nnew--runmem",
    "crumbs": [
      "Apps",
      "gitsam 연구"
    ]
  },
  {
    "objectID": "labor_decoupling.html",
    "href": "labor_decoupling.html",
    "title": "노동: 기여 vs. 분배",
    "section": "",
    "text": "Neoclassical Growth Model(신고전적 성장 모형)은 경제성장의 핵심 지표로 총생산량(Gross Domestic Product, GDP)을 중시한다. 이 모형에서는 생산의 핵심 요소를 다음과 같이 세 가지로 구분한다 (Solow 1956).\n\n총 노동량(Labor Input, \\(L\\))\n\n총 자본량(Capital Input, \\(K\\))\n\n노동증강기술(Labor-Augmenting Technology, \\(A\\))\n\n그러나 자본량(\\(K\\))은 직접적으로 측정하기 어려우며, 일반적으로 Perpetual Inventory Method(영속적 재고법)과 같은 방식을 활용하여 추정한다 (Hall and Jones 1999). 이에 더하여, 노동증강기술(\\(A\\))은 더욱 측정이 어려운 정성적 요소이기 때문에, 많은 연구에서는 Constant Returns to Scale(규모에 대한 수익 불변)을 가정한 Cobb-Douglas 생산함수를 이용하고, 추정 과정에서 발생하는 오차항(Error Term)을 기술진보(\\(A\\))로 대체하기도 한다 (Barro 1991). 이러한 방식이 현실을 얼마나 정확하게 반영하는지는 논란의 여지가 크다.\n모형(Model)은 본질적으로 이론적 가설이며, 경제성장에 대한 다양한 견해를 반영할 수 있는 도구이기 때문에, 각자의 가설과 주장을 펼치는 것은 학문적으로 허용될 수 있다. 그러나 문제의 본질은 해당 모형이 현실을 얼마나 정확하게 설명하고 예측할 수 있는지에 있다. 현실을 반영하지 못하고, 지속적으로 예측에 실패하는 경제성장 모형은 실질적으로 활용 가치가 떨어질 수밖에 없다 (Stiglitz 2017).\n경제성장과 관련하여 실용적인 측면에서 가장 중요한 변수는 총 노동량(\\(L\\))이라고 볼 수 있다. 이는 비교적 객관적인 측정이 가능하며, 경제의 실질적 변화를 분석하는 데 유용하다. 이러한 맥락에서 노동 생산성(Productivity)과 노동 분배성(Distributivity)라는 두 가지 핵심 개념을 살펴볼 필요가 있다.",
    "crumbs": [
      "Apps",
      "경제",
      "노동: 기여 vs. 분배"
    ]
  },
  {
    "objectID": "labor_decoupling.html#서론",
    "href": "labor_decoupling.html#서론",
    "title": "노동: 기여 vs. 분배",
    "section": "",
    "text": "Neoclassical Growth Model(신고전적 성장 모형)은 경제성장의 핵심 지표로 총생산량(Gross Domestic Product, GDP)을 중시한다. 이 모형에서는 생산의 핵심 요소를 다음과 같이 세 가지로 구분한다 (Solow 1956).\n\n총 노동량(Labor Input, \\(L\\))\n\n총 자본량(Capital Input, \\(K\\))\n\n노동증강기술(Labor-Augmenting Technology, \\(A\\))\n\n그러나 자본량(\\(K\\))은 직접적으로 측정하기 어려우며, 일반적으로 Perpetual Inventory Method(영속적 재고법)과 같은 방식을 활용하여 추정한다 (Hall and Jones 1999). 이에 더하여, 노동증강기술(\\(A\\))은 더욱 측정이 어려운 정성적 요소이기 때문에, 많은 연구에서는 Constant Returns to Scale(규모에 대한 수익 불변)을 가정한 Cobb-Douglas 생산함수를 이용하고, 추정 과정에서 발생하는 오차항(Error Term)을 기술진보(\\(A\\))로 대체하기도 한다 (Barro 1991). 이러한 방식이 현실을 얼마나 정확하게 반영하는지는 논란의 여지가 크다.\n모형(Model)은 본질적으로 이론적 가설이며, 경제성장에 대한 다양한 견해를 반영할 수 있는 도구이기 때문에, 각자의 가설과 주장을 펼치는 것은 학문적으로 허용될 수 있다. 그러나 문제의 본질은 해당 모형이 현실을 얼마나 정확하게 설명하고 예측할 수 있는지에 있다. 현실을 반영하지 못하고, 지속적으로 예측에 실패하는 경제성장 모형은 실질적으로 활용 가치가 떨어질 수밖에 없다 (Stiglitz 2017).\n경제성장과 관련하여 실용적인 측면에서 가장 중요한 변수는 총 노동량(\\(L\\))이라고 볼 수 있다. 이는 비교적 객관적인 측정이 가능하며, 경제의 실질적 변화를 분석하는 데 유용하다. 이러한 맥락에서 노동 생산성(Productivity)과 노동 분배성(Distributivity)라는 두 가지 핵심 개념을 살펴볼 필요가 있다.",
    "crumbs": [
      "Apps",
      "경제",
      "노동: 기여 vs. 분배"
    ]
  },
  {
    "objectID": "labor_decoupling.html#본론",
    "href": "labor_decoupling.html#본론",
    "title": "노동: 기여 vs. 분배",
    "section": "2 본론",
    "text": "2 본론\n\n2.1 1. 노동생산성(Labor Productivity)의 변화\n노동생산성은 단위 노동량당 산출량을 의미하며, 다음과 같이 정의할 수 있다 (Acemoglu et al. 2014).\n\\[\n\\text{노동생산성} = \\frac{Y_t}{L_t}\n\\]\n여기서 \\(Y_t\\)는 총생산량(GDP)의 대표적 대리 변수(proxy)이며, \\(L_t\\)는 총 노동량을 나타낸다. 각각의 변수는 다음과 같이 측정하였다.\n\n총생산량 Proxy (\\(Y_t\\)): Real Gross Domestic Product per Capita\n\n장기적으로 볼 때, 총생산량은 갈수록 증가하는 추세를 보였다.\n\n총노동량 Proxy (\\(L_t\\)): Hours Worked by Full-Time and Part-Time Employees\n\n총 노동시간\n이 역시 갈수록 증가하는 경향을 보였다.\n\n\n이로부터 계산한 노동생산성(\\(Y_t/L_t\\)) 역시 시간에 따라 증가하는 함수임을 확인할 수 있다. 즉, 경제가 성장함에 따라 노동자 1인당 산출하는 생산량은 꾸준히 증가해 왔다.\n\n\n2.2 2. 노동분배성(Labor Distributivity)의 변화\n노동생산성이 증가하면, 일반적으로 노동자에게 돌아가는 보상 역시 증가해야 한다는 것이 경제적 정의(economic fairness)와 균형적 성장(balanced growth)의 핵심 원칙이다 (Piketty 2014). 그렇다면 노동 분배성(Labor Distributivity) 역시 시간이 갈수록 증가하였을까?\n이를 확인하기 위해, 노동자의 시간당 실질 중위임금 (Real Median hourly Wage) 을 대리 변수로 활용하였다.\n\n노동에 분배된 총량 Proxy (\\(X_t\\)): Employed Full-Time: Median Usual Weekly Real Earnings\n\n장기적으로 증가하는 경향을 보였으나, 변동성이 존재하였다.\n\n노동분배성 Proxy (\\(\\frac{X_t}{L_t}\\))\n\nReal Median hourly Wage = median real wage per hour\n노동생산성이 증가하는 것과는 대조적으로, 노동자에게 분배된 시간당 소득의 중위값 (\\(X_t/L_t\\))은 오히려 갈수록 감소하는 경향을 보였다.\n\n\n이를 시각적으로 명확히 비교하기 위해, 노동분배성을 \\(\\frac{X_t}{L_t} \\times 100\\)으로 스케일링하여 그래프로 나타냈다.\nFRED Graph (1980년 이후 노동생산성과 노동분배성 비교)\n그래프를 살펴보면, 1980년대 이후 노동생산성과 노동분배성 사이의 격차가 점점 더 커지고 있음을 확인할 수 있다. 이는 무엇을 의미하는가? 노동생산성이 증가함에도 불구하고, 노동자에게 돌아가는 보상의 증가 속도가 이에 미치지 못하고 있다는 점을 시사한다. 다시 말해, “노동생산성과 노동자 보상의 분리 현상(decoupling)”이 지속적으로 심화되고 있음을 보여준다 (Stansbury and Summers 2020). 이러한 현상은 장기적으로 경제적 불평등(economic inequality)을 악화시키는 주요 원인 중 하나로 작용할 수 있다 (Stiglitz 2012).\n\n\n2.3 3. 노동분배성 감소의 원인\n노동분배성(Labor distributivity)과 총노동소득분배율 (Labor share)의 감소에 대한 경제학적 분석은 다양한 요인을 고려해야 하지만, 주요한 원인으로 다음 두 가지가 지적된다.\n\nAI 기술 진보(Technological Progress)\n\nOECD의 분석에 따르면, 인공지능(AI)과 같은 첨단 기술의 발전은 특정 고숙련 노동자(high-skilled workers)에게는 유리하게 작용하지만, 그렇지 않은 노동자들에게는 불리한 영향을 미칠 수 있다 (OECD 2018). 이는 노동시장 내 임금 불평등(wage inequality)을 심화시키는 요인으로 작용한다.\n\n선진국 주도형 세계화(Globalization)\n\n생산 공정의 해외 이전(offshoring)과 국제 무역의 확대는 저임금 노동력을 활용한 생산 방식을 증가시켜, 선진국 내 노동자의 소득 증가율을 둔화시키는 결과를 초래할 수 있다 (Autor, Dorn, and Hanson 2013).\n\n\n\n\n2.4 4. 노동참여율(Labor Force Participation Rate)의 변화\n또한, 노동소득 분배율 감소와 노동시장 변화를 분석하기 위해, 노동참여율(Labor Force Participation Rate)을 함께 고려할 필요가 있다. 이를 위해 Current Population Survey (CPS)에서 조사한 Labor Force Participation Rate를 분석하였다.\n\nCOVID-19 팬데믹이 발생한 2020년에는 노동참여율이 일시적으로 급락했으나, 이후 빠르게 정상 수준으로 회복되었다 (Coibion, Gorodnichenko, and Weber 2020).\n\n장기적으로는 완만한 하락세를 보이고 있으며, 이는 인구 고령화(demographic aging) 등의 요인과도 관련이 있을 것으로 추정된다 (Krueger 2017).",
    "crumbs": [
      "Apps",
      "경제",
      "노동: 기여 vs. 분배"
    ]
  },
  {
    "objectID": "labor_decoupling.html#결론",
    "href": "labor_decoupling.html#결론",
    "title": "노동: 기여 vs. 분배",
    "section": "3 결론",
    "text": "3 결론\n생산에 대한 노동의 기여와 노동자 보상의 분리 현상(decoupling)은 실증적 데이터에 의해 명확히 확인된다. 노동생산성이 지속적으로 증가함에도 불구하고, 노동자에게 분배되는 소득 상대적으로 감소해 왔다 (Karabarbounis and Neiman 2014). 단위 노동량당 노동자에게 분배되는 소득도 감소해 왔다. 경제 전문가들은 이러한 현상의 주요 원인으로 AI 기술 진보와 선진국 주도형 세계화를 지목하고 있으며, 이는 장기적으로 노동시장 내 불평등 심화 및 경제적 불안정성을 초래할 가능성이 크다 (Milanovic 2016). 향후 연구에서는 노동분배성 (Labor distributivity) 회복을 위한 제도적 개선책을 추가적으로 검토할 필요가 있다.",
    "crumbs": [
      "Apps",
      "경제",
      "노동: 기여 vs. 분배"
    ]
  },
  {
    "objectID": "market_incomplete.html",
    "href": "market_incomplete.html",
    "title": "Incomplete Markets",
    "section": "",
    "text": "Neoclassical economics emphasizes that rational individuals base decisions primarily on long-term trends rather than short-term fluctuations. In this view, rational agents with forward-looking expectations base decisions on an infinite-horizon optimization, meaning they will not invest in assets or projects that have a declining long-term trajectory or negative net present value. Market prices, in turn, are thought to reflect these rational expectations about future fundamentals. This philosophy is exemplified in models of Milton Friedman and his successors, where individuals smooth consumption over time and focus on permanent income, and in modern macroeconomic models that assume agents plan for the long run. The neoclassical mindset links naturally with Friedman’s monetarism, which asserts that steady, rules-based growth of the money supply yields better outcomes than reactive fine-tuning. Both frameworks assume that people anticipate the future well, so erratic policy shifts mainly lead to price changes rather than lasting gains in output or employment. Representative models include Friedman’s Permanent Income Hypothesis, Bewley’s incomplete-market model, Aiyagari’s general equilibrium model with heterogeneous agents, and the speculative asset-pricing model by Harrison & Kreps (Friedman 1957; Bewley 1986; Aiyagari 1994; Harrison and Kreps 1978).\nFriedman’s monetarism closely aligns with neoclassical thinking, promoting predictable, rules-based monetary policy to ensure market stability and long-term neutrality of money. In contrast, Keynesian and New Keynesian frameworks highlight the importance of short-term interventions due to market frictions such as price stickiness. New Keynesians agree that people are forward-looking, but they highlight that wages and prices can be “sticky” (slow to adjust), which prevents markets from clearing quickly. As a result, even rational agents may be temporarily unable to reach optimal outcomes, and monetary policy can have powerful real effects in the short term. For example, if prices cannot adjust immediately, an unexpected cut in interest rates may boost real spending and output rather than just raising prices. New Keynesian models therefore rationalize active stabilization policy – they contend that without it, the economy can suffer prolonged recessions or unemployment that a well-timed policy intervention could ameliorate. This stands in contrast to the monetarist/neoclassical view that markets self-correct relatively quickly. Despite these differences, both schools provide valuable insight: neoclassical and monetarist models offer clarity about long-run tendencies and private sector behavior under rational expectations, while Keynesian models stress the importance of short-run dynamics and coordination failures. The analysis in this paper bridges these perspectives by using theoretical models to examine how market imperfections – such as incomplete markets and heterogeneous expectations – modify the standard predictions. Although the models are abstract, each provides important long-term policy guidance. Understanding their lessons can help policymakers design monetary strategies that foster stability without creating new problems in the process.",
    "crumbs": [
      "Apps",
      "경제",
      "Incomplete Markets"
    ]
  },
  {
    "objectID": "market_incomplete.html#introduction",
    "href": "market_incomplete.html#introduction",
    "title": "Incomplete Markets",
    "section": "",
    "text": "Neoclassical economics emphasizes that rational individuals base decisions primarily on long-term trends rather than short-term fluctuations. In this view, rational agents with forward-looking expectations base decisions on an infinite-horizon optimization, meaning they will not invest in assets or projects that have a declining long-term trajectory or negative net present value. Market prices, in turn, are thought to reflect these rational expectations about future fundamentals. This philosophy is exemplified in models of Milton Friedman and his successors, where individuals smooth consumption over time and focus on permanent income, and in modern macroeconomic models that assume agents plan for the long run. The neoclassical mindset links naturally with Friedman’s monetarism, which asserts that steady, rules-based growth of the money supply yields better outcomes than reactive fine-tuning. Both frameworks assume that people anticipate the future well, so erratic policy shifts mainly lead to price changes rather than lasting gains in output or employment. Representative models include Friedman’s Permanent Income Hypothesis, Bewley’s incomplete-market model, Aiyagari’s general equilibrium model with heterogeneous agents, and the speculative asset-pricing model by Harrison & Kreps (Friedman 1957; Bewley 1986; Aiyagari 1994; Harrison and Kreps 1978).\nFriedman’s monetarism closely aligns with neoclassical thinking, promoting predictable, rules-based monetary policy to ensure market stability and long-term neutrality of money. In contrast, Keynesian and New Keynesian frameworks highlight the importance of short-term interventions due to market frictions such as price stickiness. New Keynesians agree that people are forward-looking, but they highlight that wages and prices can be “sticky” (slow to adjust), which prevents markets from clearing quickly. As a result, even rational agents may be temporarily unable to reach optimal outcomes, and monetary policy can have powerful real effects in the short term. For example, if prices cannot adjust immediately, an unexpected cut in interest rates may boost real spending and output rather than just raising prices. New Keynesian models therefore rationalize active stabilization policy – they contend that without it, the economy can suffer prolonged recessions or unemployment that a well-timed policy intervention could ameliorate. This stands in contrast to the monetarist/neoclassical view that markets self-correct relatively quickly. Despite these differences, both schools provide valuable insight: neoclassical and monetarist models offer clarity about long-run tendencies and private sector behavior under rational expectations, while Keynesian models stress the importance of short-run dynamics and coordination failures. The analysis in this paper bridges these perspectives by using theoretical models to examine how market imperfections – such as incomplete markets and heterogeneous expectations – modify the standard predictions. Although the models are abstract, each provides important long-term policy guidance. Understanding their lessons can help policymakers design monetary strategies that foster stability without creating new problems in the process.",
    "crumbs": [
      "Apps",
      "경제",
      "Incomplete Markets"
    ]
  },
  {
    "objectID": "market_incomplete.html#core-assumptions-of-neoclassical-economics-and-monetarism",
    "href": "market_incomplete.html#core-assumptions-of-neoclassical-economics-and-monetarism",
    "title": "Incomplete Markets",
    "section": "2 Core Assumptions of Neoclassical Economics and Monetarism",
    "text": "2 Core Assumptions of Neoclassical Economics and Monetarism\nThree core assumptions underpin the neoclassical and monetarist perspective in macroeconomics:\n\nRational expectations and infinite-horizon optimization: Individuals and firms are assumed to form expectations about the future in a rational way (using all available information) and to optimize their decisions over an infinite time horizon. In practice, this means economic agents base current consumption, saving, and investment on the expected present value of long-term outcomes, rather than overreacting to short-lived changes. They anticipate the future effects of policies, so systematic policy actions are largely already “priced in” to decisions. This assumption was emphasized by the new classical economists who followed Friedman, such as Robert Lucas, in arguing that only unexpected policy moves affect real behavior in the short run. Under rational expectations, people won’t persistently spend windfalls or chase assets whose fundamentals don’t justify their price, since they foresee the eventual reversion to fundamental value.\nMarket clearing in the long run (flexible prices): Neoclassical models typically assume that prices of goods, services, and factors adjust to equilibrate supply and demand, at least in the long run. While short-term frictions can occur, the long-run default is an economy at full employment with resources fully utilized. Any deviations (recessions or booms) are seen as temporary, provided policy does not introduce long-term distortions. This view contrasts with Keynesian models where wages or prices might remain out of equilibrium for an extended period. The neoclassical stance is that given enough time, economic forces will push the economy back to its potential output with stable growth. Monetarists, too, believed that “markets naturally move toward a stable center” in the absence of big shocks. Thus, they argue against aggressive intervention that attempts to exploit short-run trade-offs (like pumping up output at the cost of higher inflation), because eventually prices adjust and only inflation remains.\nNeutrality of money regarding real economic outcomes in the long run:(Lucas Jr 1972; Friedman 1968). A cornerstone of monetarism and neoclassical thought is that changes in the money supply only have transient effects on real variables (output, employment) and no effect in the long run. In the long run, an exogenous increase in the money stock is reflected in higher nominal prices and wages, but real consumption, investment, and output return to their original path. In other words, money is “neutral” with respect to real economic activity once prices have fully adjusted. Most economists agree that this long-run neutrality holds approximately true in practice – doubling the money supply eventually doubles the price level – and monetarists place great importance on it. This assumption underlies the monetarist recommendation to avoid monetary surprises: any attempt to permanently boost employment by printing money will just create inflation once people’s expectations catch up. Rational agents, thinking in an infinite-horizon framework, will not be tricked for long; they come to expect higher inflation, negating any output gains. Monetary policy, therefore, is seen primarily as a tool for controlling inflation and nominal variables, not as a way to engineer long-term higher growth.\n\nThese core assumptions shape the policy mindset in the neoclassical/monetarist framework. If agents are highly forward-looking and markets tend to clear, discretionary stabilization policy has limited power – it might only cause short-term blips or even destabilize expectations. Instead, maintaining credible, consistent policy (such as a steady money growth rule or inflation target) is viewed as the optimal approach for long-run welfare. In the next sections, we examine how relaxing some of these assumptions – by introducing incomplete markets, borrowing constraints, or heterogeneous beliefs – changes the conclusions and policy implications.",
    "crumbs": [
      "Apps",
      "경제",
      "Incomplete Markets"
    ]
  },
  {
    "objectID": "market_incomplete.html#complete-vs.-incomplete-markets",
    "href": "market_incomplete.html#complete-vs.-incomplete-markets",
    "title": "Incomplete Markets",
    "section": "3 Complete vs. Incomplete Markets",
    "text": "3 Complete vs. Incomplete Markets\nComplete markets allow full insurance against risks and unlimited borrowing, resulting in smooth consumption aligned with permanent income. Incomplete markets, however, feature borrowing constraints and uninsurable shocks, creating heterogeneity and precautionary savings, leading to wealth disparities and more volatile consumption patterns (Aiyagari 1994; Bewley 1986). Thus, one fundamental way real economies depart from the idealized benchmark is that markets are incomplete. In a complete market environment, individuals can fully insure against uncertainties and can borrow or save freely at a given interest rate. In that ideal case, people smooth their consumption over time nearly perfectly. Consumption ends up being much less volatile than income, because during bad times individuals can borrow or draw on savings, and during good times they can save extra income for the future. In fact, theory predicts that with complete markets, consumption at any point reflects an individual’s permanent income (the expected long-term average income), not the transitory ups and downs of current income. A direct implication is that temporary policy measures (like one-time stimulus checks or short-lived tax cuts) would have only a small effect on consumption—because rational consumers know such windfalls are transitory, they prefer to save a large portion of them, aiming to maintain a stable consumption path. In a complete market, households effectively pool risks and smooth out idiosyncratic shocks; as a result, their spending is steady and mainly influenced by changes in expected lifetime resources rather than short-term liquidity fluctuations.\nBy contrast, in incomplete market settings, individuals do not have access to perfect insurance or unlimited borrowing. They face idiosyncratic income shocks (e.g., job loss, illness) that they must largely bear on their own. Additionally, they may encounter liquidity constraints or borrowing limits that prevent them from smoothing consumption fully. The models developed by Truman Bewley, S. Rao Aiyagari, and others formalize this situation. In these models, all agents are ex ante identical (they have the same preferences and potential income distribution), but they become ex post heterogeneous because each experiences different income shocks over time and they cannot completely insure against these shocks (Bewley 1986; Aiyagari 1994). Households thus engage in precautionary saving—they tend to save more when they have income, building a buffer of assets to self-insure against future bad draws. Consumption is no longer completely smooth; when a negative shock hits a liquidity-constrained household, it may have to cut consumption sharply because it cannot borrow easily. Conversely, a positive shock to a hand-to-mouth household leads to a spike in consumption if they were previously constrained.\nIncomplete markets therefore produce higher marginal propensities to consume out of transitory income changes—in other words, constrained people would spend a larger fraction of any temporary income windfall than they would under complete markets (Aiyagari 1994). This is consistent with empirical data showing many households, especially those with low wealth, quickly spend stimulus payments or bonuses, as they have unmet needs or debts to pay.\nAnother key difference is that incomplete markets generate a non-trivial distribution of wealth. Since each individual’s asset accumulation depends on their history of shocks and their precautionary saving motive, over time the economy develops inequality in wealth and consumption. Some agents will build up sizable precautionary balances (if they experience good income luck or have frugal preferences), while others might remain near the borrowing constraint with minimal savings. The wealth distribution in such models is typically highly skewed, capturing the fact that a small fraction of people may hold a large share of total assets – a feature very much in line with real-world data. By contrast, in a representative agent or complete markets model, distributional issues are either absent or of no consequence, since everyone effectively pools risks together. Incomplete markets thus bring distributional considerations to the forefront of macroeconomic analysis.\nFor policymakers, these differences mean that monetary and fiscal policy can have uneven effects across the population and can influence aggregate demand through channels that are muted in complete-market models. For example, an interest rate cut in an incomplete market setting might stimulate borrowing and spending for some agents, but for others it mainly reduces their interest income (if they are savers), potentially widening inequality. Likewise, a government stimulus targeted at liquidity-constrained households could yield a relatively large boost to consumption (due to their high propensity to consume out of additional income), whereas the same payment to a wealthier, fully insured household might just be saved. In summary, incomplete markets make the macroeconomy less “frictionless” and more sensitive to distribution and credit conditions. We next examine specific models that incorporate these features, to draw out their policy insights.\n\n3.1 Friedman Consumption Smoothing Model (Complete Markets)\nMilton Friedman’s model of consumption – known as the Permanent Income Hypothesis (PIH) – suggests consumption depends primarily on permanent rather than temporary income changes. Hence, monetary policy must focus on long-term credibility rather than short-run stimulus (Friedman 1957; Hall 1978). This is a cornerstone of the neoclassical view on consumption behavior. Friedman proposed that an individual’s consumption at any given time is determined not by current income alone, but by their permanent income, which is the expected long-term average income. Temporary fluctuations in income, according to this theory, have only a small effect on consumption because people use saving and borrowing to smooth out those fluctuations. In other words, households act like long-term planners: if they receive an unexpectedly high income this year, they will not dramatically raise their spending, understanding that the extra income may not last. Instead, they will save most of it (or pay down debt), spreading the benefit over future years. Conversely, if income dips briefly, they can draw on past savings or borrow to maintain their usual consumption level, expecting to repay when income recovers. This behavior leads to relatively stable consumption paths, as illustrated by Friedman’s famous observation that consumption is much smoother than the often volatile income streams that individuals experience year to year.\nFriedman’s model assumes that credit markets function well (people can borrow against future income) and that consumers are forward-looking and rational. Under these conditions, monetary and fiscal policy have limited ability to alter consumption unless they affect expected long-term income. For example, a one-time tax rebate or a temporarily lower interest rate might not stimulate much extra spending – consumers recognize that this is a short-term change. Indeed, a key takeaway of the permanent income theory is that policies which only increase current income without raising expected future income will mostly lead to higher saving rather than higher spending. Friedman contrasted this with the Keynesian view in which consumers have a high marginal propensity to consume out of current income (perhaps because they are myopic or liquidity-constrained). He argued that the Keynesian assumption was flawed in ignoring forward-looking behavior. Empirical puzzles of the mid-20th century (such as why consumption didn’t rise one-for-one with income gains from, say, war-time fiscal expansions) could be explained by PIH: people understood those income gains were temporary and saved much of them.\nIn policy terms, the Friedman consumption model supports a rather conservative use of demand management. A central bank that rapidly expands money or lowers interest rates might not trigger a large consumption boom unless people believe those actions will persist and raise their permanent income or wealth. Similarly, a government stimulus check will be partly saved if households treat it as a transitory windfall. An important implication is that discretionary policy “surprises” are not a reliable way to boost aggregate demand – rational agents will react mainly to the expected persistent components of policy. Monetarists like Friedman instead advocated rule-based policies (such as steadily growing the money supply at a fixed rate) to provide a stable environment for consumers and investors to plan. If policy is erratic, it could even be counterproductive: for instance, trying to exploit a short-run trade-off by pushing unemployment lower than its natural rate would just raise inflation expectations, with little lasting benefit to output (this is essentially Friedman’s adaptive expectations version of the Phillips Curve argument). In summary, Friedman’s complete-market consumption model underscores the importance of expectations and permanent income. It suggests that monetary policy should focus on the long-term nominal stability (controlling inflation) and avoid frequent discretionary shifts, because people will see through those shifts and adjust their saving behavior accordingly. It also implies that fiscal stabilization (e.g. stimulus payments) will be most effective when aimed at households likely to be liquidity-constrained, a point that becomes clearer once we consider incomplete market models.\n\n\n3.2 Consumption-Investment Trade-off under Liquidity Constraints\nLiquidity constraints disrupt optimal consumption-investment trade-offs. Constrained agents cannot invest sufficiently during downturns, weakening monetary policy’s effectiveness, particularly in stimulating investment or consumption among constrained households. For example, consider a household facing liquidity constraints and uncertain future income. An interest rate cut reduces the returns on their precautionary savings, forcing the household to reduce the buffer stock meant to protect against income fluctuations. Consequently, this household may have limited resources available for productive investments such as education or small business expansion. Younger or non-saver households facing liquidity constraints prioritize current consumption over future consumption due to diminishing marginal returns of future utility. Thus, when interest rates decrease, these households are more likely to immediately spend additional available funds rather than accumulate precautionary savings or invest in long-term productive assets. In contrast, a wealthier, unconstrained household may use lower interest rates to cheaply finance additional investment opportunities, potentially increasing their wealth relative to constrained households.\nA central theme in a standard intertemporal choice problem is the trade-off between consuming today and investing for tomorrow. In a frictionless world, consumers equate the marginal benefit of spending an extra dollar today with the marginal benefit of saving that dollar (investing it to spend later). This optimality condition (often called the Euler equation in macroeconomics) ensures that resources are allocated to their most valued use over time. However, in reality many households and firms face liquidity constraints or borrowing limits that prevent them from freely making this trade-off. Such constraints are a key imperfection that alters the impact of monetary policy and other shocks.\nWhen agents are liquidity-constrained, they cannot borrow as much as they would like against future income. This means in bad times they might want to maintain consumption or invest in opportunities (human capital, business expansion, etc.), but they simply lack the funds or credit access to do so. Consequently, current consumption may fall below the level that would be chosen under complete markets, and valuable investments might be foregone. For instance, a skilled worker who becomes unemployed may cut back sharply on consumption – not because their lifetime income prospects are shattered, but because in that moment they don’t have liquid assets or credit to smooth over the gap. Likewise, a small business might pass up a profitable investment because banks refuse credit due to the firm’s lack of collateral. These scenarios lead to a suboptimal allocation of resources over time, amplifying short-run fluctuations and causing longer-run consequences (lost growth from underinvestment, etc.).\nFrom a policy perspective, liquidity constraints mean that monetary policy may have an asymmetric effect. If the central bank raises interest rates, it generally cools off borrowing and spending – both unconstrained and constrained agents will cut back (the former by choice, the latter perhaps by necessity as credit becomes more expensive or scarce). But if the central bank lowers interest rates to stimulate the economy, those who are constrained might still be unable to borrow (banks may not lend to them even at low rates, if their balance sheet is weak or job uncertain) and thus cannot increase consumption or investment. In other words, there is a segment of the population for whom monetary easing doesn’t translate into more spending because they were not borrowing in the first place (they were at their borrowing limit). Instead, the stimulus might mainly induce already well-capitalized agents to borrow or invest more – which can have distributional effects.\nOn the other hand, consider fiscal policy: a transfer (like a stimulus check or unemployment benefit extension) given to a liquidity-constrained household is likely to be spent in large part, precisely because that household’s consumption was suppressed by lack of funds. Empirical evidence and incomplete-market models both find that households with little liquid wealth have high marginal propensities to consume (MPCs) out of such transfers. This contrasts with the near-zero MPC out of a transitory income increase for a fully smoothed consumer in Friedman’s framework. Therefore, liquidity constraints reconcile why Keynesian-style demand stimulus can work in practice (many people do spend most of an extra dollar if they were cash-strapped), even though Friedman’s theory might suggest they shouldn’t. Modern heterogeneous agent models incorporate this insight by showing that when a large fraction of consumers are hand-to-mouth or buffer-stock savers, aggregate consumption is sensitive to the distribution of income and cash-on-hand.\nFor investment, liquidity constraints imply that not all investment opportunities are realized, especially among smaller firms or entrepreneurs, if external finance is costly or unavailable. In a recession, even if the central bank slashes interest rates, banks may be risk-averse and tighten lending standards, so only the safest borrowers benefit from low rates. This can lead to a situation often described as “pushing on a string,” where monetary policy loses traction in stimulating additional private investment or consumption because the bottleneck is in credit access, not the cost of credit per se.\nIn summary, the consumption-investment trade-off under liquidity constraints highlights that market imperfections can dampen or distort the transmission of monetary policy. A perfectly rational, unconstrained agent might respond to lower interest rates by optimally borrowing and spending more (since the opportunity cost of funds is lower). But a constrained agent does nothing (they can’t borrow anyway), and an unconstrained wealthy agent might already be satiated in consumption and only shift their portfolio. These dynamics mean that in downturns, monetary policy might need support from fiscal measures that target constrained agents to be fully effective. It also means that policymakers should be aware of credit conditions and possibly use regulatory tools to ensure that rate cuts get passed through to borrowers. The general principle is that in the presence of liquidity constraints, short-run fluctuations can have long-run costs (foregone investment, lower human capital accumulation) and policies should aim to alleviate these constraints during bad times.\n\n\n3.3 Bewley Model: Precautionary Savings in an Incomplete Market\n\nAssumes heterogeneous agents face idiosyncratic income shocks and borrowing limits.\nExogenous interest rates.\nGenerates wealth inequality through precautionary savings.\nHighlights the importance of social safety nets and targeted fiscal policies for macroeconomic stability (Bewley 1986).\n\nThe Bewley model (named after economist Truman Bewley) is a foundational framework for analyzing incomplete markets with heterogeneous agents. In Bewley’s setup, we consider a large number of infinitely-lived consumers who face idiosyncratic income shocks in each period. These shocks are uninsurable – there is no complete set of insurance markets for them – and consumers can only trade a single risk-free asset (such as a bond or money) to self-insure. Moreover, consumers face a borrowing limit (they cannot have debt beyond a certain level, often this ad hoc level is set to zero for simplicity). Despite all consumers having the same preferences and income process ex ante, the randomness of shocks makes them heterogeneous ex post in terms of their asset holdings and current income. This type of model is often called a heterogeneous agent incomplete-markets model, or simply a Bewley model, after the seminal work in (Bewley 1986). It has become a workhorse for understanding consumption, saving, and wealth distribution under uncertainty.\nIn the Bewley model, each consumer solves a consumption-saving problem: how much to consume today versus save as a buffer for future uncertainty. A typical finding is the emergence of a precautionary saving motive – people save not just for lifecycle reasons (retirement, etc.) but also to buffer against income risk. Those who experience good shocks build up assets, while those hit by bad shocks draw down assets or if they have none, they hit the borrowing constraint and their consumption drops. Over time, the model reaches an equilibrium where the cross-sectional distribution of wealth is stationary (in a statistical sense): some fraction of the population has high wealth, some has low wealth, with persistent inequality generated purely from idiosyncratic risk and saving behavior. This equilibrium typically features a fat-tailed distribution, meaning there are some very high-wealth individuals (who had a run of good shocks or especially strong saving discipline) and a significant mass of low-wealth individuals who might be frequently at the edge of the borrowing constraint. Quantitatively, such models can generate wealth concentration that qualitatively resembles that observed in real economies (though matching the extreme concentration in actual data often requires adding other elements like heterogeneity in earnings ability or rates of return).\nOne key aspect of the Bewley model is that the interest rate is treated as exogenous (or determined outside the model, say by a central bank or a global capital market). In other words, Bewley’s original formulation is a partial equilibrium analysis: it looks at an individual’s optimal saving given an interest rate, but does not necessarily determine that interest rate from within the model. This is akin to studying a small open economy where people can save or borrow at a fixed world interest rate, or a situation with a perfectly elastic supply of funds. Under this fixed interest rate, not everyone can dissave indefinitely because of the borrowing constraint, so in aggregate there will typically be positive net saving (since precautionary motives induce people to hold assets). If the interest rate is high relative to people’s time preference and risk, the low-wealth agents will borrow up to the limit and the high-wealth will save a lot, and an equilibrium wealth distribution forms. If the interest rate is too high, precautionary saving might not be enough to sustain it (people try to borrow too much); if it’s too low, people accumulate assets and the economy might reach a point where the lowest wealth is at the borrowing limit and highest is still saving – typically there is some interest rate that balances asset demand and the “excess” of precautionary saving.\nWhile the technical details can be involved, the intuition gleaned from the Bewley model is powerful for policy. It shows how incomplete markets alone (without any price rigidity or aggregate shocks) can lead to under-consumption by some and the accumulation of large buffers by others. This has implications for long-run growth and inequality. If many people are constrained and cannot invest in their education or businesses, the economy might underperform its potential. It also implies that policies like social insurance (unemployment insurance, social security, etc.) can affect aggregate outcomes: for example, providing more generous unemployment benefits might reduce the need for precautionary saving, which could actually stimulate consumption among lower-wealth households and reduce inequality. On the flip side, too generous a safety net could reduce the incentive to save at all. Bewley-type models have been used to examine optimal policy in this context, such as what level of unemployment insurance optimally trades off providing insurance versus maintaining incentives.\nAn important extension of the Bewley model is to use it for wealth distribution insights. The model clarifies that even if everyone has identical earning potential, incomplete markets will generate inequality simply due to luck and precautionary behavior. This suggests that some observed inequality is not due to differences in skill or hard work, but due to insufficient insurance against life’s risks. Policymakers concerned with excessive inequality might draw on this insight to justify progressive taxation or public insurance programs that effectively do what missing markets would have – help smooth incomes and consumption across states of the world. Indeed, one policy implication highlighted in such models is that improving access to credit for credit-worthy but constrained households, or providing more public insurance, could make the overall economy better off by allowing more efficient consumption and investment choices (though there are always trade-offs and moral hazard issues to consider).\nIn summary, the Bewley model provides a micro-founded explanation for why some people end up liquidity-constrained and how that influences their behavior. For monetary policy, it warns that aggregate demand may be more sensitive to the distribution of wealth and income than traditional models would suggest – if a recession hits the lower-wealth population hard, their consumption will contract strongly (since they can’t borrow), potentially deepening the downturn. Purely focusing on interest rates as a lever might be insufficient; fiscal redistributive tools or direct transfers could be more potent in such scenarios. The model’s relevance has grown as economists recognize the limitations of the representative-agent paradigm and seek to incorporate heterogeneous agent effects into macroeconomic policy analysis.\n\n\n3.4 Aiyagari Model: General Equilibrium with Incomplete Markets\n\nIncorporates endogenous determination of interest rates through production equilibrium.\nDemonstrates “excess capital accumulation” due to precautionary motives.\nAdvocates capital income taxation for improved welfare and highlights the distributional consequences of monetary policy changes (Aiyagari 1994).\n\nS. Rao Aiyagari’s model builds directly on the Bewley framework but adds an important layer: a production economy that yields a general equilibrium determination of prices (interest rate and possibly wages). In the Aiyagari (1994) model, we still have infinitely-lived agents with idiosyncratic income shocks and borrowing constraints (precisely the Bewley setup on the household side), but now those households supply savings to, and borrow from, a productive sector with capital. In essence, Aiyagari embeds the precautionary savings behavior into a full macroeconomic model with capital accumulation. The result is a self-contained macroeconomic equilibrium where the interest rate is endogenously determined by the supply and demand for capital, rather than being fixed externally. Households’ collective saving (driven by precautionary motives) feeds into the capital stock, and firms’ demand for capital (based on productivity and diminishing returns) determines the equilibrium interest rate that clears the capital market.\nOne of Aiyagari’s key findings is that in an economy with uninsurable income risk, the equilibrium interest rate will generally be lower than it would be in a comparable complete-markets economy. Intuitively, because households value holding assets as a buffer (beyond what they would in a no-risk scenario), they tend to save more, which pushes down the return to capital. In other words, there is excess aggregate saving due to precautionary motives, leading to a larger capital stock and lower interest rate than the classical model without income risk would predict. This is sometimes referred to as the “Aiyagari excess capital result.” It implies that the laissez-faire outcome might not be socially optimal – there could be “too much” capital from a certain perspective, because individuals don’t internalize that by saving so much for themselves, they depress the return for everyone. One practical implication Aiyagari pointed out is that a government could improve welfare by taxing capital income and redistributing it (or using it to fund social insurance) in such an economy. By doing so, it reduces the need for individuals to self-insure via excessive capital accumulation, potentially moving the economy closer to the golden-rule level of capital (where consumption is maximized). This was a striking result since in a standard frictionless model, capital taxation is often detrimental in the long run – but here, moderate capital taxation can correct an inefficiency arising from incomplete markets.\nThe Aiyagari model also provides insight into the interplay between inequality and aggregate production. Unlike the Bewley model, which was partial equilibrium, here the distribution of wealth affects aggregate supply (through capital accumulation). If the wealth is concentrated in fewer hands, the aggregate consumption could be lower (since wealthy individuals have lower MPCs, they might save a lot of their income), and the aggregate capital might be higher (since those with excess wealth invest it). This has led to extensive research on the quantitative impact of redistributive policies on growth and output. For instance, if you redistribute wealth from the rich (low MPC) to the poor (high MPC), you might raise current consumption but reduce saving and thus future capital – whether that is good or bad for long-run output depends on parameters, but in some cases it can actually increase output if the economy was above the golden rule level of capital to start with (Marcet, Obiols-Homs, and Weil 2007).\nAnother aspect is the feedback of interest rates on inequality. In Aiyagari’s equilibrium, the interest rate settles at a level where households are indifferent between saving and not saving (on the margin). If interest rates are very low, borrowing is cheap, but also the reward for saving is low, which could discourage some saving. However, typically in these models many households still save because of risk aversion and precaution. The low interest rate also means that those who are borrowing-constrained are not paying a huge interest burden (assuming they can borrow at that rate), but many cannot borrow much anyway due to the constraint. Overall, compared to a representative-agent model, the Aiyagari model predicts different responses to monetary policy. For example, if the central bank lowers the interest rate (below the equilibrium that would prevail from just technology and time preference), it transfers resources from savers to borrowers. In an economy with inequality, this has non-neutral effects: borrowers (often poorer agents) gain relief and might consume more, while savers (wealthier agents) earn less on their assets and might consume less (or seek riskier investments). The distributional effects of monetary policy come into play. Recent research in heterogeneous agent New Keynesian (HANK) models builds on this by adding nominal rigidities, but even in the basic Aiyagari model, one can see that monetary policy is not just about one representative agent’s intertemporal choice – it will create winners and losers due to heterogeneity in assets and consumption propensities.\nFor policymakers, Aiyagari’s work underscores a few points: (1) Monetary neutrality may not hold cleanly in the short run even if prices are flexible, because redistributions caused by interest rate changes can affect aggregate demand; (2) there may be a role for permanent fiscal policy (like capital taxation or debt issuance) to influence the long-run capital stock and interest rate in a way that improves welfare, countering the incomplete-market externality; (3) evaluating monetary policy requires understanding the underlying wealth distribution – for instance, a low interest rate environment will tend to benefit borrowers and younger households (via cheaper credit, higher asset values) while hurting those who rely on interest income (like pensioners or wealthier rentiers). If mismanaged, prolonged ultra-low rates can contribute to asset price inflation (as savers seek returns in real estate or stocks), thereby widening wealth inequality if only the already-wealthy hold those appreciating assets. Indeed, some attribute the rise in asset valuations and wealth concentration in recent decades partly to very low global real interest rates and ample liquidity, consistent with the mechanisms in Aiyagari-type models.\nIn conclusion, the Aiyagari model enriches our understanding by marrying heterogeneity with production. It reminds us that macroeconomic policy cannot be divorced from distributional considerations. The long-term natural rate of interest, the effectiveness of fiscal redistribution, and the impact of monetary policy all look different once we acknowledge that not everyone is alike in the economy. By capturing how liquidity constraints and precautionary savings influence aggregate capital, this model provides guidance on questions like whether and how to tax wealth, and how aggressive monetary policy should be in, say, pushing interest rates to very low levels. Policymakers drawing on these insights might strive for a balance: ensuring there is enough aggregate saving for investment and growth, but not so much that it reflects unmet social insurance needs or creates financial imbalances.\n\n\n3.5 Harrison & Kreps (1978) Model: Asset Pricing with Heterogeneous Beliefs\n\nHeterogeneous investor beliefs combined with short-sale constraints can cause speculative bubbles, elevating asset prices above fundamental values.\nSuggests improving market completeness and transparency to curb speculation and volatility (Harrison and Kreps 1978).\n\nThe Harrison and Kreps (1978) model introduces a different kind of market imperfection into macro-finance: heterogeneous beliefs among investors, combined with constraints on short selling. Unlike the previous models (which focused on borrowing constraints and income risk), this model lives in the world of asset trading and speculation. Harrison and Kreps asked what happens in an asset market when investors have differing opinions about an asset’s value and they are not allowed to short sell the asset freely. Their answer was groundbreaking: even if all investors are rational (in that they update beliefs consistently with their own information), the mere diversity of opinion can lead to asset prices exceeding the valuation of even the most optimistic individual investor.\nHere’s the intuition: suppose some investors (“optimists”) believe a stock or house will be very valuable in the future, while others (“pessimists”) believe it will not. If short selling is constrained (pessimists cannot easily borrow the asset to sell it short), the market price will be determined largely by the optimists’ willingness to pay. Now add the element of speculation – investors may buy an asset not just for its fundamental value (like dividends or rent) but also for the option to resell it in the future. Harrison & Kreps showed that when beliefs differ, an investor might pay more than their own estimate of the asset’s fundamental value because they anticipate that someone even more optimistic might buy it at a higher price later. In effect, a resale option is priced in. This leads to what we might call a speculative premium on the asset price. The price can rise above the level that any single investor would pay if they had to hold the asset forever. In their words, the right to resell makes investors willing to pay more than the asset’s “hold-to-maturity” value. The inability of pessimists to short sell means nothing counteracts this upward pressure – the pessimists simply sit out of the market rather than actively pushing the price down by shorting. Thus, the market price reflects an over-optimistic valuation, driven by the most bullish views and the prospect of flipping the asset.\nThis mechanism helps explain phenomena like asset price bubbles or situations where market prices seem to detach from fundamental values. Real estate is a commonly cited example: investors might buy houses at high prices not only because they expect rising rents or income (fundamentals), but because they think they can later sell the house to someone else at an even higher price (speculation). If enough people believe housing prices will keep rising, and skeptics can’t effectively short the housing market, the result is a self-reinforcing price boom. The Harrison-Kreps model formalized how even fully rational agents with rational expectations (each given their own belief) can end up trading at prices that embed a speculative component. It doesn’t require irrational exuberance; it only requires disagreement and some friction (short-sale constraints) that prevents full arbitrage. In their equilibrium, everyone understands the price is above their own fundamental valuation, but they also know someone else might be willing to pay even more, so it can still be rational to buy now and plan to sell later – a clear parallel to the greater fool theory, but derived in a rigorous way.\nPolicy implications from the Harrison & Kreps model revolve around financial market regulation and information disclosure. One implication is that short-selling constraints can fuel overpricing. If regulators make short selling too restrictive (perhaps in an attempt to curb volatility or prevent speculative attacks), they might inadvertently remove a balancing force that keeps prices close to fundamentals. The model would suggest that allowing more short selling (with proper oversight to avoid abuse) could actually lead to more informative, less one-sided pricing. Another implication is the value of transparency and common information. In the model, beliefs are heterogeneous and “dogmatic” – each trader sticks to their prior and interprets signals in their own way. If public information can help align beliefs (or at least inform the pessimists and optimists of each other’s views), it might reduce the degree of disagreement. However, complete agreement is unrealistic; differences in models, data interpretation, or risk appetite will always create some dispersion of opinion.\nFrom a monetary policy perspective, one might not immediately see a connection, since H&K is about asset pricing in a frictional financial market. But there are subtle links. Central banks today pay close attention to asset markets – housing, equities, etc. – because large deviations of asset prices from fundamentals can pose risks to financial stability and the broader economy. For instance, if low interest rates contribute to a speculative housing boom (by making borrowing cheap and encouraging optimistic beliefs about ongoing price growth), a subsequent crash could harm banks and consumers, leading to a recession. The H&K model suggests that a booming asset market is not necessarily a sign of solid fundamentals; it could be a sign of constrained pessimism and resale-driven pricing. Policymakers, therefore, should be cautious in interpreting asset price signals. It also provides an argument for macroprudential policies: tools that directly address asset market excess (for example, tighter loan-to-value ratios in mortgage lending during a housing boom, or stricter margin requirements in stock trading). These can be seen as ways to mitigate the speculative dynamics – essentially pricking bubbles before they grow too large. By making it harder to purely speculate (through leverage restrictions) or by encouraging more two-sided markets (perhaps by permitting certain derivatives or short positions), regulators might reduce the likelihood of severe mispricings.\nIn summary, the Harrison & Kreps model adds another layer to our understanding: market outcomes can be inefficient not just because of real-side frictions (like incomplete insurance) but also because of financial-side frictions (like trading constraints and belief dispersion). It is a reminder that even with rational actors, markets may need regulatory oversight to ensure they reflect true economic value. For a policymaker, being aware of this mechanism is important. It cautions against assuming that all investors have the same expectations (they don’t), and it illustrates why asset price booms can develop even without obvious irrationality. Recognizing a speculative bubble early is notoriously difficult, but understanding models like this helps officials appreciate the warning signs (e.g., when asset prices only make sense under very optimistic scenarios and buyers cite the ability to resell as justification). It also supports measures to improve market completeness – such as permitting more sophisticated financial instruments – because a more complete market (ability to hedge, to short, etc.) ironically may prevent the wild swings that incomplete markets allow.",
    "crumbs": [
      "Apps",
      "경제",
      "Incomplete Markets"
    ]
  },
  {
    "objectID": "market_incomplete.html#policy-implications-of-models",
    "href": "market_incomplete.html#policy-implications-of-models",
    "title": "Incomplete Markets",
    "section": "4 Policy Implications of Models",
    "text": "4 Policy Implications of Models\n\nMonetarism emphasizes stable, predictable policy frameworks.\nBewley and Aiyagari models highlight social insurance and targeted redistribution.\nHarrison & Kreps stress regulation of speculative financial markets.\n\nEach theoretical framework provides distinct policy insights for designing monetary and economic strategies. Key implications can be summarized clearly:\n\nFriedman/Monetarist (Complete Markets)\nMonetarist models emphasize predictable and stable monetary policies. Temporary stimulus measures are ineffective since consumers respond mainly to permanent income changes. Monetary policy should thus follow clear, credible rules (e.g., a fixed money growth rate or inflation targeting), focusing on long-term price stability. Fiscal prudence is advised, as large deficits can be counterproductive due to anticipated future taxation (Ricardian equivalence). Ultimately, monetary policy should avoid attempts to permanently influence real variables like unemployment or income distribution directly, which are better addressed through structural and fiscal reforms.\nBewley (Incomplete Markets)\nThe Bewley model underscores the importance of social insurance and targeted redistribution. Because individuals face uninsurable risks and liquidity constraints, they maintain precautionary savings, limiting investment and consumption in downturns. Thus, policies providing social safety nets (unemployment insurance, healthcare, targeted stimulus payments) help stabilize aggregate demand and prevent severe economic downturns. Furthermore, targeted redistribution or improved credit access can mitigate inequality-driven demand shortfalls, addressing systemic underconsumption and secular stagnation risks. Effective policy involves balancing adequate insurance to stabilize demand without overly dampening incentives to work or save.\nAiyagari (Incomplete Markets & General Equilibrium)\nExtending Bewley’s framework, Aiyagari emphasizes that precautionary saving leads to excessive capital accumulation and lower equilibrium interest rates. A crucial policy implication is that moderate capital income taxation, combined with redistribution, can improve overall welfare. Monetary policy, particularly interest-rate adjustments, significantly affects wealth distribution—low rates benefit borrowers (often younger or lower-wealth groups) at the expense of savers. Policymakers should therefore coordinate monetary and fiscal policies to achieve optimal capital allocation, moderate inequality, and promote balanced, sustainable economic growth.\nHarrison & Kreps (Speculative Markets)\nThe Harrison & Kreps model highlights the necessity of financial market regulation and transparency to address speculative bubbles driven by heterogeneous beliefs and short-sale constraints. Policy interventions should facilitate market completeness (e.g., by allowing regulated short selling or derivatives trading) and enforce transparency through robust disclosure standards. During speculative booms, targeted macroprudential tools (loan-to-value ratios, capital buffers) and proactive central-bank communication can mitigate bubbles without overly aggressive monetary tightening, thus integrating financial stability concerns effectively into monetary policy.\n\nIn summary, a robust monetary policy approach integrates these perspectives, recognizing the importance of stable monetary frameworks, adequate social insurance, fiscal coordination, and prudent financial regulation. Successful policy practice involves using complementary tools (monetary, fiscal, regulatory) and adapting dynamically based on the interplay of inflation stability, wealth distribution, and financial stability. Historical experiences, such as the 2008 crisis and the COVID-19 response, illustrate the effectiveness of combining these insights into comprehensive, multidimensional policy strategies.",
    "crumbs": [
      "Apps",
      "경제",
      "Incomplete Markets"
    ]
  },
  {
    "objectID": "market_incomplete.html#bridging-the-gap-between-theory-and-reality",
    "href": "market_incomplete.html#bridging-the-gap-between-theory-and-reality",
    "title": "Incomplete Markets",
    "section": "5 Bridging the Gap Between Theory and Reality",
    "text": "5 Bridging the Gap Between Theory and Reality\nModels should guide, not dictate, policy. No single model fully captures the complexity of real economies; thus, policymakers must integrate insights across multiple frameworks—complete markets, incomplete markets, and speculative dynamics—to design effective policies.\nReal economies simultaneously involve heterogeneous agents, liquidity constraints, price rigidities, and speculative market behavior. Friedman’s monetarism emphasizes stable rules and credible expectations; Bewley and Aiyagari highlight liquidity constraints and precautionary savings; Harrison & Kreps emphasize speculation driven by belief heterogeneity. Modern macroeconomic approaches increasingly integrate these elements (e.g., Heterogeneous Agent New Keynesian models).\nHowever, real-world policymaking faces challenges not fully accounted for by theory:\n\nExpectations and Behavioral Factors: Individuals often deviate from rational expectations, resulting in behavioral biases and herd behaviors. Effective policy should therefore actively manage expectations through credible communication (forward guidance).\nPolitical Economy and Credibility: Optimal theoretical policies may not align with political realities. Policymakers thus rely on robust frameworks—such as inflation targeting and automatic stabilizers—that remain effective even under delayed or politically compromised responses.\nFinancial and Global Contexts: Traditional models underestimated financial sector impacts on policy effectiveness. Policymakers must explicitly account for banking sector stability and international capital flows in policy design.\n\nIn practice, policymakers should stress-test policies using multiple models, employ targeted empirical approaches informed by microdata, and regularly update strategies based on observed outcomes. For instance, contrary to initial views, the monetary, fiscal, and financial-market policies implemented during the COVID-19 pandemic—particularly in the U.S.—highlight a failure of effective coordination, as evidenced by subsequent extreme wealth inequality and persistent inflation. Many experts criticize the lack of coherent policy integration, arguing it exacerbated economic distortions and reduced overall policy effectiveness.\nIn summary, bridging theory and reality requires flexible and empirically informed policy strategies, recognizing theoretical insights and practical limitations while ensuring effective coordination to mitigate unintended adverse outcomes.",
    "crumbs": [
      "Apps",
      "경제",
      "Incomplete Markets"
    ]
  },
  {
    "objectID": "market_incomplete.html#empirical-review-evaluating-policy-recommendations-in-practice",
    "href": "market_incomplete.html#empirical-review-evaluating-policy-recommendations-in-practice",
    "title": "Incomplete Markets",
    "section": "6 Empirical Review: Evaluating Policy Recommendations in Practice",
    "text": "6 Empirical Review: Evaluating Policy Recommendations in Practice\nHistorical experience provides valuable tests for theoretical predictions, highlighting both successes and failures in policy implementation:\n\nGreat Depression: Friedman and Schwartz demonstrated that monetary contraction deepened the Depression. The period validated the importance of maintaining stable aggregate demand and money supply, while illustrating risks of debt-deflation spirals. It emphasized roles for both monetary stabilization and fiscal intervention (e.g., New Deal, WWII spending), leading to institutional innovations like deposit insurance and lender-of-last-resort roles for central banks.\nPostwar Keynesian-Monetarist Debate & Stagflation: The stagflation of the 1970s validated Friedman’s critique of simplistic Keynesian demand management and the natural rate hypothesis. Volcker’s aggressive monetary tightening demonstrated that inflation control requires credible commitment and acceptance of short-term economic pain. This period underscored the difficulty of targeting monetary aggregates directly, resulting in central banks shifting towards inflation targeting regimes with more flexible interest rate rules.\nGreat Moderation and 2008 Financial Crisis: The New Keynesian synthesis (price-stickiness, monetary rules, and representative agents) initially seemed successful during the Great Moderation, but failed to predict growing inequality and financial vulnerabilities that triggered the 2008 crisis. The crisis validated incomplete-market frameworks (Bewley/Aiyagari) and speculative asset-price models (Harrison-Kreps), highlighting the critical need to include financial frictions and heterogeneity in macroeconomic analysis. Quantitative easing stabilized financial markets, but disproportionately benefited wealthier asset holders, intensifying inequality.\nCOVID-19 Pandemic and Inflation Spike: The aggressive fiscal and monetary responses during COVID-19 prevented immediate economic collapse but led to persistent inflation and increased wealth inequality due to poor coordination and mis-targeted stimulus. In the U.S., particularly, stimulus measures lacked coherence, causing asset bubbles and skewed benefits to the wealthy. Current inflation challenges highlight monetarist concerns about excessive stimulus and underline the importance of coordinated fiscal-monetary policy frameworks.\n\nKey empirical lessons:\n\nCredible monetary policy frameworks that stabilize inflation are critical for long-run economic stability.\nMarket imperfections (liquidity constraints, incomplete markets, financial fragility) significantly affect policy effectiveness and must be explicitly considered.\nDistributional issues matter: Monetary policy can inadvertently exacerbate inequality, stressing the need for complementary fiscal redistributive policies.\nExpectations and clear communication are essential for policy effectiveness, reinforcing the rational expectations perspective on policy transparency.\n\nIn sum, successful policy in practice requires combining disciplined monetary frameworks, proactive management of market imperfections and inequalities, and flexibility to adapt based on evolving empirical evidence and economic conditions.",
    "crumbs": [
      "Apps",
      "경제",
      "Incomplete Markets"
    ]
  },
  {
    "objectID": "market_incomplete.html#conclusion",
    "href": "market_incomplete.html#conclusion",
    "title": "Incomplete Markets",
    "section": "7 Conclusion",
    "text": "7 Conclusion\nEffective monetary policy requires a balance between long-run credibility and stability (as emphasized by neoclassical and monetarist models) and short-run interventions addressing market imperfections and inequality. Stable monetary frameworks—such as credible inflation targets and rule-based policies—anchor expectations, reduce uncertainty, and support sustainable economic growth. However, real economies exhibit significant market frictions, requiring complementary fiscal and regulatory interventions.\nIncomplete-market models (Bewley, Aiyagari) highlight the necessity of policies that alleviate liquidity constraints and promote broader economic participation. Financial-market models (Harrison & Kreps) stress the importance of regulatory vigilance to prevent speculative excesses. Recent empirical experiences—particularly the uncoordinated COVID-19 policy response in the U.S., which contributed to extreme wealth inequality and persistent inflation—underscore the risks of fragmented policy approaches.\nThe long-term orientation of neoclassical models also reminds us that economic growth and efficiency are paramount for raising living standards broadly. Sustainable reductions in wealth inequality are best achieved through inclusive growth—ensuring more people can participate in economic advancement via skill-building, access to capital, and productive investments. Policies that enhance productivity—education, infrastructure, and innovation—are crucial complements to monetary policy. While these are typically fiscal or structural measures, monetary policy plays a role by maintaining a low-inflation, stable macroeconomic backdrop and preventing the misallocation of capital into unproductive speculative ventures.\nTo mitigate wealth inequality without sacrificing efficiency, policymakers should consider measures such as:\n\nProgressive taxation and wealth taxes to fund public goods and transfers efficiently (as Aiyagari’s framework suggests, moderate capital taxation can be welfare-enhancing).\n\nPublic investment in education, healthcare, and economic opportunity to foster broad-based growth and human capital development, reducing inequality over time.\n\nEncouraging broad-based asset ownership through mechanisms like employee stock ownership plans or tax incentives for retirement savings, ensuring more households benefit from asset appreciation.\n\nMaintaining low and stable inflation to protect real incomes, particularly for lower-income households who lack financial hedges against rising prices.\n\nUltimately, the most effective policy approach integrates monetary stability, fiscal coordination, and regulatory prudence to foster inclusive, sustainable economic prosperity while ensuring that short-term interventions do not undermine long-term growth and efficiency.",
    "crumbs": [
      "Apps",
      "경제",
      "Incomplete Markets"
    ]
  },
  {
    "objectID": "market_incomplete.html#appendix",
    "href": "market_incomplete.html#appendix",
    "title": "Incomplete Markets",
    "section": "8 Appendix",
    "text": "8 Appendix\nThe appendix provides technical details for the theoretical models referenced. Each section includes the formal derivation of key equations, calibration and simulation exercises, and case analyses illustrating the implications of each model.\n\n8.1 Friedman’s Permanent Income Model\n\n8.1.1 1. Theoretical Formulation\nFriedman’s Permanent Income Hypothesis (PIH) posits that an individual’s consumption at time \\(t\\) is based on expected lifetime income rather than current income. The standard model assumes perfect capital markets, quadratic utility, and rational expectations.\nLet the consumer’s lifetime utility function be: \\[\nU = \\sum_{t=0}^{\\infty} \\beta^t u(c_t),\n\\] where \\(\\beta\\) is the subjective discount factor and \\(u(c_t)\\) is quadratic utility: \\[\nu(c_t) = -\\frac{1}{2} (c_t - c^*)^2.\\]\nThe budget constraint is: \\[\nA_{t+1} = (1+r)(A_t + Y_t - C_t),\n\\] where \\(A_t\\) is assets, \\(r\\) is the interest rate, \\(Y_t\\) is income, and \\(C_t\\) is consumption. Given rational expectations, Hall (1978) derived that optimal consumption follows a martingale process: \\[\nE_t[c_{t+1}] = c_t.\n\\] This implies that changes in consumption are unpredictable: \\[\nE_t[c_{t+1} - c_t] = 0.\n\\] Thus, only permanent changes in income affect consumption significantly, while transitory changes are mostly saved.\n\n\n8.1.2 2. Calibration and Simulation\nWe calibrate the model using standard parameter values: - Discount factor: \\(\\beta = 0.96\\) - Interest rate: \\(r = 0.04\\) - Income process: \\(Y_t = Y_p + Y_t^T\\), where \\(Y_p\\) is permanent income and \\(Y_t^T\\) is a transitory shock. - Variance of transitory income shock: \\(\\sigma_T^2 = 0.02\\) - Variance of permanent income shock: \\(\\sigma_P^2 = 0.01\\)\nSimulation results illustrate consumption smoothing. Given a one-time positive income shock of \\(\\Delta Y_T\\), consumption increases only modestly: \\[\n\\Delta C_t \\approx \\frac{\\sigma_T^2}{\\sigma_P^2 + \\sigma_T^2} \\Delta Y_T.\n\\] This highlights that temporary income changes have minimal effects on consumption compared to permanent income changes.\n\n\n8.1.3 3. Case Analysis: Impact of Temporary vs. Permanent Income Changes\nTo empirically validate the model, we compare U.S. consumption responses to temporary stimulus payments vs. permanent tax cuts:\n\n2001 U.S. Tax Rebate (temporary): Studies found that only 20-40% of the rebate was spent immediately, consistent with PIH predictions.\n1980s Reagan Tax Cuts (permanent): Consumption increased significantly, aligning with the model’s implication that permanent income shifts drive behavior.\n\nThis supports the policy conclusion that temporary monetary stimulus has limited consumption effects, reinforcing the monetarist argument for stable, rules-based policy frameworks over discretionary interventions.\n\n\n\n8.2 Bewley Model\n\n8.2.1 1. Theoretical Formulation\nThe Bewley model describes a heterogeneous agent economy with idiosyncratic income shocks and liquidity constraints. Agents optimize consumption and savings in response to uncertain income paths, leading to precautionary savings behavior.\nThe agent maximizes expected lifetime utility: \\[\nU = \\sum_{t=0}^{\\infty} \\beta^t u(c_t),\n\\] subject to the budget constraint: \\[\nA_{t+1} = (1+r)A_t + Y_t - C_t,\n\\] and a borrowing constraint: \\[\nA_t \\geq 0.\n\\] The income process follows a stochastic evolution: \\[\nY_t = Y_p + \\varepsilon_t,\n\\] where \\(Y_p\\) is the persistent component and \\(\\varepsilon_t\\) is a transitory shock.\nThe Euler equation governs optimal consumption: \\[\nE_t \\left[ u'(c_t) \\right] = \\beta (1+r) E_t \\left[ u'(c_{t+1}) \\right].\n\\] Binding borrowing constraints lead to higher marginal propensities to consume (MPC), distinguishing this model from the complete-market benchmark.\n\n\n8.2.2 2. Calibration and Simulation\nWe calibrate the model using empirically relevant parameters:\n\nRisk aversion: \\(\\sigma = 2\\)\nDiscount factor: \\(\\beta = 0.96\\)\nInterest rate: \\(r = 0.04\\)\nIncome process variance: \\(\\sigma_Y^2 = 0.02\\)\n\nSimulating the model shows the emergence of a stationary wealth distribution. Agents with lower wealth levels exhibit high MPCs, while wealthier agents accumulate savings to self-insure against income shocks.\n\n\n8.2.3 3. Case Analysis: Wealth Distribution and Policy Implications\nEmpirical evidence shows that wealth inequality observed in real economies is well captured by Bewley models. For example:\n\nU.S. Wealth Distribution: The model reproduces the fact that the top 10% of wealth holders own a disproportionately large share of total assets.\nImpact of Credit Market Access: Relaxing borrowing constraints leads to higher consumption smoothing, reducing excess precautionary savings.\nEffects of Monetary Policy: Interest rate cuts primarily benefit liquidity-constrained households, increasing consumption, while wealthier agents respond weakly due to already accumulated assets.\n\nThis model underscores the importance of credit access and social insurance policies to counteract excessive precautionary savings and stabilize aggregate demand.\n\n\n\n8.3 Aiyagari Model\n\n8.3.1 1. Theoretical Formulation\nBuilding on the Bewley model, Aiyagari introduces production and general equilibrium. Households supply capital and labor, and firms use a Cobb-Douglas production function: \\[\nY = K^\\alpha L^{1-\\alpha}.\n\\] The interest rate \\(r\\) and wage \\(w\\) adjust to ensure market clearing: \\[\nK = \\sum A_i, \\quad L = \\sum l_i.\n\\] The stationary distribution of wealth results in an endogenous equilibrium interest rate lower than in the complete markets model due to precautionary savings.\n\n\n8.3.2 2. Calibration and Simulation\n\nCapital share: \\(\\alpha = 0.36\\)\nDiscount factor: \\(\\beta = 0.96\\)\nDepreciation: \\(\\delta = 0.08\\)\nRisk aversion: \\(\\sigma = 2\\)\n\nSimulations confirm excess capital accumulation, implying that moderate capital taxation can improve welfare by reallocating excess savings towards consumption.\n\n\n8.3.3 3. Case Analysis: Redistribution and Policy Implications\n\nEffect of Redistribution: Moderate taxation on capital income reallocates resources and increases aggregate welfare.\nImpact on Interest Rates: The equilibrium interest rate is lower than in the complete markets model, reflecting the precautionary savings motive.\n\nThis model highlights the distributional consequences of monetary and fiscal policy and suggests that targeted redistribution can enhance efficiency without major losses in output.\n\n\n\n8.4 Harrison & Kreps Model\n\n8.4.1 1. Theoretical Formulation\nThe Harrison & Kreps (1978) model explores how heterogeneous beliefs among investors can lead to speculative bubbles in asset pricing. The key insight is that optimistic investors dominate the pricing mechanism when short-selling constraints exist, leading to equilibrium prices that can exceed fundamental values.\nConsider a two-period model where investors have differing subjective probabilities about a risky asset’s payoff in period \\(t=2\\). The risky asset pays \\(X_H\\) with probability \\(p_H\\) (optimists’ belief) and \\(X_L\\) with probability \\(1 - p_H\\). Similarly, pessimists believe the probabilities are \\(p_L\\) and \\(1 - p_L\\), where \\(p_H &gt; p_L\\).\nThe price of the asset in period \\(t=1\\) is determined by the highest bidder, given short-sale constraints: \\[\nP_1 = \\max\\{ P_H, P_L \\},\n\\] where \\(P_H\\) and \\(P_L\\) are the optimists’ and pessimists’ valuation of the asset, respectively: \\[\nP_H = \\frac{p_H X_H + (1 - p_H) X_L}{1 + r}, \\quad P_L = \\frac{p_L X_H + (1 - p_L) X_L}{1 + r}.\n\\] If short selling is not allowed, the price is set by the optimists, even if pessimists believe it to be overvalued.\nIn period \\(t=2\\), the true payoff \\(X\\) is realized, and prices adjust accordingly: \\[\nP_2 = X.\n\\] If the optimists’ belief was overly optimistic, a crash occurs, showing that speculative bubbles arise due to heterogeneous expectations rather than fundamental mispricing alone.\n\n\n8.4.2 2. Calibration and Simulation\nTo simulate this model, we set:\n\nRisk-free rate: \\(r = 0.03\\)\nPayoffs: \\(X_H = 120\\), \\(X_L = 80\\)\nBeliefs: \\(p_H = 0.8\\), \\(p_L = 0.5\\)\n\nIf short selling is restricted, the period-1 price reflects the optimists’ valuation: \\[\nP_1 = \\frac{0.8(120) + 0.2(80)}{1.03} = 97.09.\n\\] If short selling is allowed, the pessimists’ valuation influences pricing: \\[\nP_1 = \\frac{0.5(120) + 0.5(80)}{1.03} = 97.09.\n\\]\nSimulated Results:\n\nIf \\(p_H\\) is too optimistic, asset prices rise above fundamentals, and when reality sets in at \\(t=2\\), prices drop, mimicking bubble dynamics.\nIf short-selling is unrestricted, prices better reflect fundamentals, reducing volatility.\n\n\n\n8.4.3 3. Case Analysis: Speculative Bubbles and Policy Implications\nEmpirical applications of the Harrison & Kreps model highlight key cases of speculative booms and busts:\n\nDot-com Bubble (1999–2000): Investor optimism, combined with limited mechanisms for short-selling, led to massive overvaluation.\nHousing Market Crash (2008): Housing assets were driven by speculative demand and optimistic credit assessments; when reality hit, prices crashed.\n\nPolicy takeaways:\n\nMarket Transparency: Improving information flow reduces belief dispersion, dampening bubbles.\nShort-Selling Mechanisms: Allowing short positions prevents excessive speculative premiums.\nMacroprudential Policies: Loan-to-value and capital requirements mitigate credit-fueled bubbles.\n\nThis model underscores how heterogeneous beliefs and market constraints drive speculative price deviations, necessitating a balanced approach to financial regulation.",
    "crumbs": [
      "Apps",
      "경제",
      "Incomplete Markets"
    ]
  },
  {
    "objectID": "pareto_index.html",
    "href": "pareto_index.html",
    "title": "Modeling Wealth Distribution",
    "section": "",
    "text": "The Pareto distribution has long been used to model wealth or income distributions, starting from the seminal work of Vilfredo Pareto in the late 19th century (Pareto 1964). One of the most intuitive ways to visualize the inequality within such a distribution is through the Lorenz curve, introduced by Lorenz in 1905 (Lorenz 1905). The Lorenz curve describes the cumulative proportion of total wealth (or resources) held by the bottom fraction of the population. Closely related to the Lorenz curve is the Gini coefficient, which provides a scalar measure of inequality (Gini 1912).\nThis document first derives the Lorenz curve for a Pareto distribution, then demonstrates how to estimate the Pareto index (shape parameter) from Lorenz curve observations. We additionally relate the Pareto distribution to Constant Relative Risk Aversion (CRRA) utility function, highlighting the same shared structure in a differential equation that governs both.",
    "crumbs": [
      "Apps",
      "경제",
      "Modeling Wealth Distribution"
    ]
  },
  {
    "objectID": "pareto_index.html#introduction",
    "href": "pareto_index.html#introduction",
    "title": "Modeling Wealth Distribution",
    "section": "",
    "text": "The Pareto distribution has long been used to model wealth or income distributions, starting from the seminal work of Vilfredo Pareto in the late 19th century (Pareto 1964). One of the most intuitive ways to visualize the inequality within such a distribution is through the Lorenz curve, introduced by Lorenz in 1905 (Lorenz 1905). The Lorenz curve describes the cumulative proportion of total wealth (or resources) held by the bottom fraction of the population. Closely related to the Lorenz curve is the Gini coefficient, which provides a scalar measure of inequality (Gini 1912).\nThis document first derives the Lorenz curve for a Pareto distribution, then demonstrates how to estimate the Pareto index (shape parameter) from Lorenz curve observations. We additionally relate the Pareto distribution to Constant Relative Risk Aversion (CRRA) utility function, highlighting the same shared structure in a differential equation that governs both.",
    "crumbs": [
      "Apps",
      "경제",
      "Modeling Wealth Distribution"
    ]
  },
  {
    "objectID": "pareto_index.html#deriving-the-lorenz-curve-of-a-pareto-distribution",
    "href": "pareto_index.html#deriving-the-lorenz-curve-of-a-pareto-distribution",
    "title": "Modeling Wealth Distribution",
    "section": "2 Deriving the Lorenz Curve of a Pareto Distribution",
    "text": "2 Deriving the Lorenz Curve of a Pareto Distribution\n\n2.1 Definition of the Lorenz Curve\nFor a continuous distribution of wealth (X), the Lorenz curve \\(L(p)\\) is defined as the fraction of total wealth owned by the bottom \\(p\\) fraction of the population:\n\\[\nL(p) \\;=\\; \\frac{\\int_{x_m}^{x(p)} x\\,f(x)\\,dx}{\\int_{x_m}^{\\infty} x\\,f(x)\\,dx},\n\\]\nwhere\n\n\\(p = F\\bigl(x(p)\\bigr)\\) is the cumulative proportion of individuals with wealth below \\(x(p)\\),\nThe numerator represents the cumulative wealth of the bottom \\(p\\) fraction,\nThe denominator represents the total wealth in the system, given by the expected value of \\(X\\) over its support.\n\n\n\n2.2 Pareto Distribution\nA Pareto distribution with shape parameter \\(\\alpha&gt;0\\) and scale parameter \\(x_m&gt;0\\) is defined by the PDF\n\\[\nf(x) \\;=\\; \\alpha\\,\\frac{x_m^\\alpha}{x^{\\alpha+1}},\n\\quad x \\ge x_m,\n\\]\nand the corresponding CDF\n\\[\nF(x) \\;=\\; 1 \\;-\\;\\Bigl(\\tfrac{x_m}{x}\\Bigr)^\\alpha,\n\\quad x \\ge x_m.\n\\]\nEquivalently, for \\(0 &lt; p &lt; 1\\), the quantile \\(x(p)\\) satisfying \\(F\\bigl(x(p)\\bigr)=p\\) is\n\\[\np \\;=\\; 1 \\;-\\;\\Bigl(\\tfrac{x_m}{x(p)}\\Bigr)^\\alpha\n\\;\\;\\Longleftrightarrow\\;\\;\nx(p) \\;=\\; \\frac{x_m}{\\bigl(1 - p\\bigr)^{1/\\alpha}}.\n\\]\n\n\n2.3 Total Wealth\nLet the total wealth be \\(W_{\\text{total}} = E[X]\\), the expected value of \\(X\\). Substituting the PDF of the Pareto distribution into the definition of expectation,\n\\[\nE[X]\n\\;=\\; \\int_{x_m}^{\\infty} x\\,f(x)\\,dx\n\\;=\\; \\int_{x_m}^{\\infty} x \\,\\alpha\\,\\frac{x_m^\\alpha}{x^{\\alpha+1}}\\,dx\n\\;=\\; \\alpha\\,x_m^\\alpha \\int_{x_m}^{\\infty} x^{-\\alpha}\\,dx.\n\\]\nFor \\(\\alpha&gt;1\\), the improper integral converges and we obtain\n\\[\nW_{\\text{total}}\n\\;=\\; E[X]\n\\;=\\; \\frac{\\alpha\\,x_m}{\\alpha - 1}.\n\\]\n\n\n2.4 Cumulative Wealth for the Bottom \\(p\\) Fraction\nThe cumulative wealth held by the bottom \\(p\\) fraction is\n\\[\nW(p)\n\\;=\\;\\int_{x_m}^{x(p)} x\\,f(x)\\,dx\n\\;=\\;\\alpha\\,x_m^\\alpha \\int_{x_m}^{x(p)} x^{-\\alpha}\\,dx\n\\;=\\;\\alpha\\,x_m^\\alpha\n\\Bigl[\\frac{x^{-\\alpha+1}}{-\\alpha+1}\\Bigr]_{x_m}^{x(p)}.\n\\]\nSimplifying,\n\\[\nW(p)\n\\;=\\; \\frac{\\alpha\\,x_m}{\\alpha - 1}\n\\;\\Bigl(1 - \\bigl(\\tfrac{x_m}{x(p)}\\bigr)^{\\alpha - 1}\\Bigr).\n\\]\n\n\n2.5 Lorenz Curve for the Pareto Distribution\nBy definition,\n\\[\nL(p)\n\\;=\\; \\frac{W(p)}{W_{\\text{total}}}\n\\;=\\; \\frac{\\frac{\\alpha\\,x_m}{\\alpha - 1}\\,\\Bigl(1 - \\bigl(\\tfrac{x_m}{x(p)}\\bigr)^{\\alpha - 1}\\Bigr)}\n            {\\frac{\\alpha\\,x_m}{\\alpha - 1}}\n\\;=\\; 1 - \\bigl(1 - p\\bigr)^{\\frac{\\alpha - 1}{\\alpha}}.\n\\]\nHence, for a Pareto distribution, the Lorenz curve is\n\\[\nL(p) \\;=\\; 1 \\;-\\; \\bigl(1 - p\\bigr)^{\\tfrac{\\alpha - 1}{\\alpha}}.\n\\]\n\nIf \\(\\alpha \\gg 1\\), the distribution is more equal, and \\(L(p)\\) is closer to the 45-degree line of perfect equality.\n\nIf \\(\\alpha\\) is only slightly larger than 1, the distribution is more unequal, with significant concentration of wealth in the upper tail.",
    "crumbs": [
      "Apps",
      "경제",
      "Modeling Wealth Distribution"
    ]
  },
  {
    "objectID": "pareto_index.html#estimating-the-pareto-index-from-the-lorenz-curve",
    "href": "pareto_index.html#estimating-the-pareto-index-from-the-lorenz-curve",
    "title": "Modeling Wealth Distribution",
    "section": "3 Estimating the Pareto Index from the Lorenz Curve",
    "text": "3 Estimating the Pareto Index from the Lorenz Curve\nSuppose empirical data or external studies indicate specific points \\((p, L(p))\\) on the Lorenz curve. We can use\n\\[\nL(p) \\;=\\; 1 - (1 - p)^{\\frac{\\alpha - 1}{\\alpha}}\n\\]\nto solve numerically for \\(\\alpha\\). Commonly cited examples:\nSeveral Empirical Illustrations\n\nPareto 80:20 Rule: \\(p=0.80\\), \\(L(p)=0.20\\)\n\\(\\Rightarrow\\) \\(\\alpha \\approx \\frac{\\ln(4)}{\\ln(5)} \\approx 1.16\\).\nPareto 90:10 Rule: \\(p=0.90\\), \\(L(p)=0.10\\)\n\\(\\Rightarrow\\) \\(\\alpha \\approx 1.05\\).\nU.S. Stock Market: According to a report by Axios (2024), the top 10% own about 93% of total equity wealth, implying \\(p=0.90\\) and \\(L(p)=0.07\\). Solving yields \\(\\alpha \\approx 1.03\\).\n\nCredit Suisse Global Wealth Report: In 2013, it was reported that the top 1% control about 50% of global wealth, which implies \\(p=0.99\\) and \\(L(p)=0.50\\). Solving gives \\(\\alpha \\approx 1.18\\). Additionally, the top 10% were said to own about 85% of global wealth (\\(p=0.90\\), \\(L(p)=0.15\\)), giving \\(\\alpha \\approx 1.08\\). Comparing such estimates across years (e.g., 2013 vs. 2020) can reveal the time dynamics of the global wealth distribution (Credit Suisse 2013, 2020).",
    "crumbs": [
      "Apps",
      "경제",
      "Modeling Wealth Distribution"
    ]
  },
  {
    "objectID": "pareto_index.html#the-gini-coefficient",
    "href": "pareto_index.html#the-gini-coefficient",
    "title": "Modeling Wealth Distribution",
    "section": "4 The Gini Coefficient",
    "text": "4 The Gini Coefficient\nThe Gini coefficient is a measure of wealth or income inequality that is closely related to the Lorenz curve. The Gini coefficient is defined as the ratio of the area between the Lorenz curve and the 45-degree equality line to the total area under the 45-degree line. Mathematically, the Gini coefficient ( G ) is given by:\n\\[\nG = 1 - 2 \\int_0^1 L(p) \\, dp.\n\\]\nSubstituting the Lorenz curve for a Pareto distribution:\n\\[\nL(p) = 1 - (1 - p)^{\\frac{\\alpha - 1}{\\alpha}},\n\\]\nwe obtain:\n\\[\nG = 1 - 2 \\int_0^1 \\big[1 - (1 - p)^{\\frac{\\alpha - 1}{\\alpha}}\\big] \\, dp = \\frac{1}{2\\alpha - 1}.\n\\]\nThus, for a Pareto distribution, the Gini coefficient is:\n\\[\nG = \\frac{1}{2\\alpha - 1}, \\quad \\text{for } \\alpha &gt; \\frac{1}{2}.\n\\]\nSome features of the Gini coefficient:\n\nAs \\(\\alpha \\to 1^+\\), the Gini coefficient approaches 1, indicating extreme inequality (a few individuals hold nearly all the wealth).\nAs \\(\\alpha \\to \\infty\\), the Gini coefficient approaches 0, indicating perfect equality.\nFor typical empirical values of \\(\\alpha\\) in wealth distributions (e.g., 1.1 to 1.8), the Gini coefficient ranges from 0.83 to 0.38, reflecting significant inequality\nRelative measure: \\(G\\) compares the distribution to perfect equality, but does not capture absolute differences.\n\nNon‐additivity: One cannot simply average the Gini coefficients of subpopulations to obtain an overall Gini coefficient.\n\nSensitivity: The Gini coefficient is sensitive to changes in the middle of the distribution, but less so at the tails.",
    "crumbs": [
      "Apps",
      "경제",
      "Modeling Wealth Distribution"
    ]
  },
  {
    "objectID": "pareto_index.html#pareto-distribution-and-crra-utility",
    "href": "pareto_index.html#pareto-distribution-and-crra-utility",
    "title": "Modeling Wealth Distribution",
    "section": "5 Pareto Distribution and CRRA Utility",
    "text": "5 Pareto Distribution and CRRA Utility\nIn microeconomic theory, a widely used utility specification is the Constant Relative Risk Aversion (CRRA) form (Arrow et al. 1974; Pratt 1978). The CRRA utility function \\(u(x)\\) satisfies\n\\[\n-\\frac{x\\,u''(x)}{u'(x)} \\;=\\; \\gamma,\n\\]\nwhere \\(\\gamma&gt;0\\) is the coefficient of relative risk aversion. Solving for \\(u(x)\\) under boundary conditions such as \\(u(1)=0\\) yields:\n\nIf \\(\\gamma=1\\), \\(u(x)=\\ln x\\). (using the L’hopital’s Rule)\nIf \\(\\gamma\\neq 1\\), \\(u(x)=\\frac{x^{\\,1-\\gamma}-1}{\\,1-\\gamma\\,}\\).\n\nLarger \\(\\gamma\\) indicates higher risk aversion, while \\(\\gamma=0\\) corresponds to risk neutrality (\\(u(x)=x\\)).\nA Pareto PDF can also be derived from a differential equation with similar form. If \\(f(x)\\) is the PDF of a Pareto random variable on \\(x\\ge x_m&gt;0\\), one can write:\n\\[\n-\\frac{x\\,f'(x)}{\\,f(x)\\!}\\;=\\;(1+\\alpha)\\,x_m^{\\alpha},\n\\]\nwhich likewise has a “power‐law” solution structure. Thus, Pareto distributions and CRRA utilities each emerge from a linear differential equation of analogous form, underscoring a conceptual parallel in how “power‐type” functional solutions can appear in both economic choice models (through marginal utility) and in heavy‐tailed probability distributions.\nFurthermore, in mainstream economic theory, marginal utility \\(u'(x)\\) is assumed to be strictly positive, and \\(u''(x)\\) typically negative (diminishing marginal utility). In probability theory, any valid PDF \\(f(x)\\) must be positive, and for heavy‐tailed distributions like Pareto, \\(f(x)\\) decreases for large \\(x\\). These parallels lead to a one‐to‐one analogy between certain types of declining utilities and distributions whose density functions also decline in \\(x\\).\n\nRemark: There is a well‐known relationship via logarithmic transforms: if \\(X\\) is Pareto(\\(x_m,\\alpha\\)), then \\(Y=\\ln(X/x_m)\\) is exponentially distributed with rate \\(\\alpha\\). This exponential distribution also arises from a first‐order linear differential equation, reinforcing these structural similarities.",
    "crumbs": [
      "Apps",
      "경제",
      "Modeling Wealth Distribution"
    ]
  },
  {
    "objectID": "pareto_index.html#conclusion",
    "href": "pareto_index.html#conclusion",
    "title": "Modeling Wealth Distribution",
    "section": "6 Conclusion",
    "text": "6 Conclusion\nBecause the Pareto distribution has only two parameters (\\(x_m\\) and \\(\\alpha\\)), even minimal distributional data—such as “the bottom 80% own 20% of total wealth”—enables one to solve directly for the Pareto index \\(\\alpha\\). This simplicity makes the Pareto distribution a convenient model or approximation for global wealth distribution, although in practice the estimated \\(\\alpha\\) can vary greatly depending on the dataset, sampling, and specific segment of the population observed.\nIn wealth and income distribution analysis, pairing empirical Lorenz curves with Pareto modeling remains a powerful—if simplified—approach to gauging inequality. For both theoretical and practical reasons, it continues to be integral in economic research, policy discussions, and broader studies of social welfare. Meanwhile, connections to CRRA utility function illustrate that core economic principles and certain types of heavy‐tailed probabilistic behavior can share similar mathematical underpinnings.",
    "crumbs": [
      "Apps",
      "경제",
      "Modeling Wealth Distribution"
    ]
  },
  {
    "objectID": "regressions.html",
    "href": "regressions.html",
    "title": "From Linear to Curved Structures",
    "section": "",
    "text": "Inner Product, Projection, and the Role of Positive Definite Kernels",
    "crumbs": [
      "Apps",
      "경제",
      "From Linear to Curved Structures"
    ]
  },
  {
    "objectID": "regressions.html#ols-as-orthogonal-projection-in-euclidean-space",
    "href": "regressions.html#ols-as-orthogonal-projection-in-euclidean-space",
    "title": "From Linear to Curved Structures",
    "section": "1 OLS as Orthogonal Projection in Euclidean Space",
    "text": "1 OLS as Orthogonal Projection in Euclidean Space\nOLS solves the following problem:\n\\[\n\\hat{\\beta}_{OLS} = \\arg\\min_\\beta \\| y - X\\beta \\|_2^2 = (X^\\top X)^{-1} X^\\top y\n\\]\n\nThis corresponds to projecting \\(y\\) orthogonally onto the column space of \\(X\\).\nThe residual \\(\\varepsilon = y - X\\hat{\\beta}\\) satisfies:\n\n\\[\nX^\\top \\varepsilon = 0\n\\]\n\n1.1 Inner Product and Geometry\n\nThe \\(L^2\\) norm used in OLS is induced by the standard Euclidean inner product:\n\n\\[\n\\langle u, v \\rangle = u^\\top v\n\\]\n\nThe distance function becomes:\n\n\\[\n\\| u \\|_2 = \\sqrt{u^\\top u}\n\\]\n\nThe set of parameter values yielding equal error defines a level set (isocurve):\n\n\\[\n\\{ \\beta \\mid \\| y - X\\beta \\|_2^2 = c \\} \\Rightarrow \\text{spheres in parameter space}\n\\]\n\nThis reflects Pythagorean geometry — the isocurves are circles (in 2D), spheres (in 3D), or hyperspheres in higher dimensions when \\(X^\\top X = I\\); otherwise, elliptical.\nIn this sense, OLS performs Euclidean projection onto a linear subspace using the standard inner product.",
    "crumbs": [
      "Apps",
      "경제",
      "From Linear to Curved Structures"
    ]
  },
  {
    "objectID": "regressions.html#generalized-least-squares-gls",
    "href": "regressions.html#generalized-least-squares-gls",
    "title": "From Linear to Curved Structures",
    "section": "2 Generalized Least Squares (GLS)",
    "text": "2 Generalized Least Squares (GLS)\nGLS extends OLS by accounting for non-spherical error covariance structure:\n\\[\n\\hat{\\beta}_{GLS} = \\arg\\min_\\beta (y - X\\beta)^\\top \\Sigma^{-1} (y - X\\beta)\n\\]\n\nHere, \\(\\Sigma\\) is the covariance matrix of the errors.\nThis defines a new inner product over the residual space:\n\n\\[\n\\langle u, v \\rangle_\\Sigma = u^\\top \\Sigma^{-1} v\n\\]\n\nThe level sets:\n\n\\[\n\\{ \\beta \\mid (y - X\\beta)^\\top \\Sigma^{-1} (y - X\\beta) = c \\} \\Rightarrow \\text{ellipsoids in parameter space}\n\\]\n\nThis reflects how different directions in parameter space are penalized differently depending on the variance and correlation structure of the errors.\n\n\nThe ellipsoidal level sets visualized earlier correspond precisely to this GLS structure.",
    "crumbs": [
      "Apps",
      "경제",
      "From Linear to Curved Structures"
    ]
  },
  {
    "objectID": "regressions.html#gmm-as-orthogonality-of-moment-conditions-not-projection",
    "href": "regressions.html#gmm-as-orthogonality-of-moment-conditions-not-projection",
    "title": "From Linear to Curved Structures",
    "section": "3 GMM as Orthogonality of Moment Conditions (Not Projection)",
    "text": "3 GMM as Orthogonality of Moment Conditions (Not Projection)\nGMM generalizes the estimation framework by relying on moment conditions, not residuals:\n\\[\n\\hat{\\theta}_{GMM} = \\arg\\min_\\theta \\left[ \\bar{g}_n(\\theta)^\\top W \\bar{g}_n(\\theta) \\right]\n\\]\nwhere:\n\n\\(\\bar{g}_n(\\theta) = \\frac{1}{n} \\sum_{i=1}^n g(Z_i, \\theta)\\) is the sample mean of moment functions\n\\(W\\) is a positive definite weighting matrix, usually the inverse of the estimated variance of \\(\\bar{g}_n(\\theta)\\)\n\n\n3.1 Clarifying the Inner Product\nAlthough GMM also defines a quadratic form like GLS, it operates in a fundamentally different space:\n\nIn GLS: \\(u, v\\) are residual vectors\nIn GMM: \\(u, v\\) are moment function evaluations\n\nThus, while one can write:\n\\[\n\\langle g_i(\\theta), g_j(\\theta) \\rangle_W = g_i(\\theta)^\\top W g_j(\\theta)\n\\]\nit is important to emphasize:\n\nGMM is not a projection-based method. It minimizes the violation of moment orthogonality conditions in expectation.\n\n\nThe isocurves:\n\n\\[\n\\{ \\theta \\mid \\bar{g}_n(\\theta)^\\top W \\bar{g}_n(\\theta) = c \\}\n\\]\nmay also appear elliptical — but they are not projections, and should not be conflated with GLS geometrically.\n\n\n3.2 Local Approximation and Connection to GLS\nIn some cases, GMM can be viewed as a local GLS approximation. If moment conditions are close to linear in \\(\\theta\\), and \\(g(Z_i, \\theta) \\approx A(Z_i)(\\theta - \\theta_0)\\), then the GMM objective becomes:\n\\[\n(\\theta - \\theta_0)^\\top A^\\top W A (\\theta - \\theta_0)\n\\]\nwhich resembles GLS — but only locally and approximately.\n\nTherefore, the geometric intuition of ellipsoids applies more precisely to GLS. For GMM, it’s a useful visual aid — but structurally distinct.",
    "crumbs": [
      "Apps",
      "경제",
      "From Linear to Curved Structures"
    ]
  },
  {
    "objectID": "regressions.html#einstein-field-equations-and-gmm-structural-analogy",
    "href": "regressions.html#einstein-field-equations-and-gmm-structural-analogy",
    "title": "From Linear to Curved Structures",
    "section": "4 Einstein Field Equations and GMM: Structural Analogy",
    "text": "4 Einstein Field Equations and GMM: Structural Analogy\nEinstein’s Field Equations (EFE) in general relativity:\n\\[\nG_{\\mu\\nu} = R_{\\mu\\nu} - \\frac{1}{2} R g_{\\mu\\nu} = 8\\pi T_{\\mu\\nu}\n\\]\nwhere:\n\n\\(T_{\\mu\\nu}\\): Stress-energy tensor, representing energy and matter (matter-energy distribution)\n\\(G_{\\mu\\nu}\\): Einstein tensor, encoding the curvature of spacetime (curvature)\n\n\\(R_{\\mu\\nu}\\): Ricci curvature tensor\n\\(R\\): Scalar curvature\n\\(g_{\\mu\\nu}\\): Metric tensor, defining the inner product in spacetime and governing geodesics\n\n\n\n4.1 Analogy to Estimation Frameworks\n\n\n\n\n\n\n\n\n\n\nFeature\nOLS\nGLS\nGMM\nEFE (Physics)\n\n\n\n\nSpace\nEuclidean\nCovariance-weighted\nMoment function space\nCurved spacetime\n\n\nInner product\n\\(I\\)\n\\(\\Sigma^{-1}\\)\n\\(W\\) (moment-weighted)\n\\(g_{\\mu\\nu}\\) (metric)\n\n\nOptimization\nProjection\nWeighted projection\nMoment orthogonality\nEnergy-curvature balance\n\n\nLevel sets\nCircles\nEllipses\nEllipsoid-like\nLightcones / geodesics\n\n\n\n\n\n4.2 Lightcones and Degenerate Conics\nIn EFE, the metric tensor \\(g_{\\mu\\nu}\\) defines how distances and angles are measured — it is the analogue of a positive definite kernel in GMM. The field equations determine how the geometry (curvature) of spacetime reacts to matter and energy. In this sense, spacetime is optimized or shaped in response to external inputs, just as GMM shapes its estimation space based on the kernel \\(W\\) and moment functions.\nThe level sets in general relativity are often visualized as lightcones — the surface separating causal influence from spacelike separation. Geometrically, a lightcone can be interpreted as the degenerate case of a conic section, where the quadric form:\n\\[\nQ(x) = x^\\top g_{\\mu\\nu} x = 0\n\\]\nresults in a pair of intersecting lines: this represents all null (light-like) directions emanating from a point. These are the boundary cases between time-like and space-like intervals, analogous to the way ellipsoids in GMM collapse into degenerate forms under singular kernel matrices.\nThus, in both GMM and EFE, the shape and degeneracy of level sets encode deep information about the underlying structure — whether it is a statistical model or the geometry of spacetime.",
    "crumbs": [
      "Apps",
      "경제",
      "From Linear to Curved Structures"
    ]
  },
  {
    "objectID": "regressions.html#summary",
    "href": "regressions.html#summary",
    "title": "From Linear to Curved Structures",
    "section": "5 Summary",
    "text": "5 Summary\n\nOLS performs Euclidean projection in a space endowed with the standard inner product.\nGLS generalizes this using a positive definite kernel over the residual space, yielding ellipsoidal level sets.\nGMM further generalizes the concept by applying positive definite weighting over moment function space, not residuals.\nGeometric analogies (spheres and ellipsoids) are structurally valid for OLS and GLS, and visually helpful — but must be used with care in GMM.\nAll three frameworks share a unifying theme: geometry induced by positive definite structure.\n\n\n“OLS and GLS estimate by projection. GMM estimates by aligning empirical moments. Einstein’s theory bends space to match mass — estimation theory bends geometry to match data.”",
    "crumbs": [
      "Apps",
      "경제",
      "From Linear to Curved Structures"
    ]
  },
  {
    "objectID": "regressions.html#visuals",
    "href": "regressions.html#visuals",
    "title": "From Linear to Curved Structures",
    "section": "6 Visuals",
    "text": "6 Visuals\n\n6.1 OLS vs. GLS: Different Projections Under the Same Linear Model\nThis section visualizes how OLS and GLS (with heteroskedastic weighting) yield different estimators even under the same linear model structure. The key distinction lies in how each method defines distance and importance via its respective inner product.\n\n6.1.1 Simulation Setup\n\nData-generating process: \\[\ny = \\beta_0 + \\beta_1 x + \\varepsilon\n\\] where the noise term \\(\\varepsilon\\) is heteroskedastic — its variance increases with \\(x\\).\nThat is, the reliability of observations decreases toward the right end of the domain.\nOLS estimation:\n\nMinimizes the unweighted squared residuals: \\[\n\\min_\\beta \\| y - X\\beta \\|_2^2\n\\]\nAssumes all observations are equally informative.\nThe projection balances the residuals across the entire domain, including high-noise regions.\n\nGLS estimation:\n\nMinimizes a precision-weighted residual norm: \\[\n\\min_\\beta (y - X\\beta)^\\top W (y - X\\beta)\n\\] where \\(W\\) is a positive definite diagonal matrix with weights inversely proportional to the noise variance.\nObservations with lower noise (on the left) are more heavily weighted.\nThe resulting projection is skewed toward the low-variance region, yielding a steeper estimated slope.\n\n\n\n\n6.1.2 Interpretation\nEven though both methods fit a linear model, they differ fundamentally in the geometry of projection: - OLS projects onto the column space of \\(X\\) using the standard Euclidean inner product. - GLS projects in a space where the residuals are measured using a Mahalanobis-type norm, effectively reshaping the loss geometry.\nThis difference becomes visually striking when we compare their fitted lines on heteroskedastic data — OLS follows the average trend, while GLS emphasizes the cleaner data and suppresses the noisy part.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulate data with heteroskedastic noise (to favor GMM adjustment)\nnp.random.seed(0)\nn = 100\nx = np.linspace(0, 10, n)\nX = np.vstack([np.ones(n), x]).T\n\n# True model\nbeta_true = np.array([1, 2])\n# Heteroskedastic noise: variance increases with x\nnoise_std = 0.5 + 1.5 * (x / x.max())  # ranges from 0.5 to 2.0\ny = X @ beta_true + np.random.normal(0, noise_std)\n\n# OLS estimation\nbeta_ols = np.linalg.inv(X.T @ X) @ X.T @ y\ny_hat_ols = X @ beta_ols\n\n# GMM weighting: inverse of variance (precision weighting)\nW = np.diag(1 / noise_std**2)\n\n# GMM estimation (optimal weighting under heteroskedasticity)\nXTWX = X.T @ W @ X\nXTWy = X.T @ W @ y\nbeta_gmm = np.linalg.inv(XTWX) @ XTWy\ny_hat_gmm = X @ beta_gmm\n\n# Plot with aspect ratio 1:1\nplt.figure(figsize=(8, 8))\nplt.scatter(x, y, color='lightgray', label='Observed data')\nplt.plot(x, y_hat_ols, label='OLS projection', color='blue', linewidth=2)\nplt.plot(x, y_hat_gmm, label='GMM projection (precision-weighted)', color='red', linestyle='--', linewidth=2)\nplt.title(\"OLS vs GMM Projection with Heteroskedastic Data\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.axis('equal')  # Set equal aspect ratio\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n6.2 Different Level set of objective function (quadratic form)\n\n\n\n\n\n\n\n\n\nMethod\nObjective Function\nGeometry of Norm\nShape of Level Set\n\n\n\n\nOLS (Unnormalized)\n\\((y - X\\beta)^\\top (y - X\\beta)\\)\nEuclidean norm\nEllipsoidal (if \\(X^\\top X \\neq I\\))\n\n\nOLS (Normalized)\n\\((\\beta - \\hat{\\beta})^\\top (X^\\top X)^{-1} (\\beta - \\hat{\\beta})\\)\nMahalanobis norm\nSpherical\n\n\nGLS\n\\((y - X\\beta)^\\top W (y - X\\beta)\\)\nMahalanobis norm on residuals\nEllipsoidal\n\n\n\nNote:\n\nAlthough the GLS objective has the same mathematical form as GMM, its geometry is defined over the residual space, not over the space of moment conditions.\n\nThe weighting matrix \\(W\\) in GLS is typically \\(\\Sigma^{-1}\\), the inverse of the error covariance matrix.\n\n\n\nCode\n# ===============================================\n# GLS vs OLS: Visualization of Quadratic Forms\n#\n# This code compares the level sets of:\n# (1) OLS objective (normalized via X'X)\n# (2) GLS objective with heteroskedastic weighting W\n#\n# Although GLS and GMM share a similar quadratic structure,\n# this code reflects GLS estimation:\n#   J(β) = (y - Xβ)' W (y - Xβ)\n# where W is a user-defined positive definite matrix (e.g., precision).\n#\n# The level sets visualize how the estimation geometry is altered\n# under different inner products: Euclidean vs. Mahalanobis.\n# ===============================================\n\n# Z-score normalization of x to improve XtX condition\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nx_normalized = scaler.fit_transform(x.reshape(-1, 1)).flatten()\nX_normalized = np.vstack([np.ones(n), x_normalized]).T\n\n# Recalculate OLS and GMM using normalized X\nbeta_ols_norm = np.linalg.inv(X_normalized.T @ X_normalized) @ X_normalized.T @ y\ny_hat_ols_norm = X_normalized @ beta_ols_norm\n\n# GMM estimation with same W\nXTWX_norm = X_normalized.T @ W @ X_normalized\nXTWy_norm = X_normalized.T @ W @ y\nbeta_gmm_norm = np.linalg.inv(XTWX_norm) @ XTWy_norm\ny_hat_gmm_norm = X_normalized @ beta_gmm_norm\n\n# New grid around beta_ols_norm\nb0_vals = np.linspace(beta_ols_norm[0] - 1, beta_ols_norm[0] + 1, 100)\nb1_vals = np.linspace(beta_ols_norm[1] - 1, beta_ols_norm[1] + 1, 100)\nB0, B1 = np.meshgrid(b0_vals, b1_vals)\nB_flat = np.vstack([B0.ravel(), B1.ravel()])\n\n# Normalized OLS objective (Mahalanobis)\nXtX_inv_norm = np.linalg.inv(X_normalized.T @ X_normalized)\ndelta_norm = B_flat - beta_ols_norm[:, None]\nJ_ols_normalized = np.einsum('ji,jk,ki-&gt;i', delta_norm, XtX_inv_norm, delta_norm).reshape(B0.shape)\n\n# GLS objective with normalized X\nJ_gmm_norm = []\nfor i in range(B_flat.shape[1]):\n    r = y - X_normalized @ B_flat[:, i]\n    obj = r.T @ W @ r\n    J_gmm_norm.append(obj)\nJ_gmm_norm = np.array(J_gmm_norm).reshape(B0.shape)\n\n# Plotting\nfig, axs = plt.subplots(1, 2, figsize=(14, 6))\n\n# Normalized OLS (should be spherical)\ncs1 = axs[0].contour(B0, B1, J_ols_normalized, levels=20, cmap='Blues')\naxs[0].plot(beta_ols_norm[0], beta_ols_norm[1], 'bo', label='OLS solution')\naxs[0].set_title(\"OLS (Normalized X): Spherical Level Sets\")\naxs[0].set_xlabel(r\"$\\beta_0$\")\naxs[0].set_ylabel(r\"$\\beta_1$\")\naxs[0].axis('equal')\naxs[0].legend()\naxs[0].grid(True)\n\n# GLS with normalized X\ncs2 = axs[1].contour(B0, B1, J_gmm_norm, levels=20, cmap='Reds')\naxs[1].plot(beta_gmm_norm[0], beta_gmm_norm[1], 'ro', label='GMM solution')\naxs[1].set_title(\"GMM (Normalized X): Ellipsoidal Level Sets\")\naxs[1].set_xlabel(r\"$\\beta_0$\")\naxs[1].set_ylabel(r\"$\\beta_1$\")\naxs[1].axis('equal')\naxs[1].legend()\naxs[1].grid(True)\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Apps",
      "경제",
      "From Linear to Curved Structures"
    ]
  },
  {
    "objectID": "structure_tfp.html",
    "href": "structure_tfp.html",
    "title": "Time-varying TFP",
    "section": "",
    "text": "The growth rate of Total Factor Productivity (TFP) is not a fixed constant; rather, it evolves over time and may be subject to long-term decline. While mainstream macroeconomic models frequently assume a constant TFP growth rate, this assumption is increasingly being questioned. Several structural factors—diminishing returns to R&D, demographic constraints, and sectoral shifts towards lower-productivity industries—suggest that aggregate TFP growth may continue to slow. However, TFP growth is not uniform across sectors. Certain industries, such as AI, biotechnology, and advanced energy technologies, exhibit significantly higher productivity gains, leading to growing sectoral disparities in innovation.\nFrom an econometric perspective, a decline in TFP growth could alter its stationarity properties, weakening its stochastic trend and making it more likely to transition from an \\(I(1)\\) (non-stationary) stochastic process to an \\(I(0)\\) (stationary) stochastic process. If structural breaks exist in TFP growth, traditional time-series models based on constant-trend cointegration assumptions may fail. Thus, accurately modeling TFP trends requires dynamic econometric techniques that account for time-varying structural changes.",
    "crumbs": [
      "Apps",
      "경제",
      "Time-varying TFP"
    ]
  },
  {
    "objectID": "structure_tfp.html#introduction",
    "href": "structure_tfp.html#introduction",
    "title": "Time-varying TFP",
    "section": "",
    "text": "The growth rate of Total Factor Productivity (TFP) is not a fixed constant; rather, it evolves over time and may be subject to long-term decline. While mainstream macroeconomic models frequently assume a constant TFP growth rate, this assumption is increasingly being questioned. Several structural factors—diminishing returns to R&D, demographic constraints, and sectoral shifts towards lower-productivity industries—suggest that aggregate TFP growth may continue to slow. However, TFP growth is not uniform across sectors. Certain industries, such as AI, biotechnology, and advanced energy technologies, exhibit significantly higher productivity gains, leading to growing sectoral disparities in innovation.\nFrom an econometric perspective, a decline in TFP growth could alter its stationarity properties, weakening its stochastic trend and making it more likely to transition from an \\(I(1)\\) (non-stationary) stochastic process to an \\(I(0)\\) (stationary) stochastic process. If structural breaks exist in TFP growth, traditional time-series models based on constant-trend cointegration assumptions may fail. Thus, accurately modeling TFP trends requires dynamic econometric techniques that account for time-varying structural changes.",
    "crumbs": [
      "Apps",
      "경제",
      "Time-varying TFP"
    ]
  },
  {
    "objectID": "structure_tfp.html#main",
    "href": "structure_tfp.html#main",
    "title": "Time-varying TFP",
    "section": "2 Main",
    "text": "2 Main\n\n2.1 Understanding TFP in Macroeconomic Models\nTFP represents technological progress that enhances economic output beyond capital and labor inputs. In the Solow-Swan growth model, TFP follows an exponential trend, expressed as:\n\\[\nA_t = A_0 e^{gt} \\epsilon_t\n\\]\nwhere \\(g\\) is the growth rate and \\(\\epsilon_t\\) captures short-term fluctuations. This implies that TFP is not strictly stationary; rather, its log form follows a unit root process with a deterministic trend:\n\\[\n\\log A_t = g t + u_t, \\quad u_t \\sim I(0)\n\\]\nThus, mainstream macroeconomic models do not assume TFP is stationary in levels, but often assume its deviation from trend is stationary.\n\n\n2.2 Empirical Evidence on TFP Stationarity\nEmpirical studies show:\n\nLong-Run TFP behaves as a non-stationary process (\\(I(1)\\)), meaning that it exhibits persistent growth and shocks that do not revert over time.\nShort-Run TFP Shocks (deviations from trend) are often stationary (\\(I(0)\\)) process.\n\nIf GDP and capital stock are also \\(I(1)\\), Solow’s production function suggests they should be cointegrated:\n\\[\n\\log Y_t - \\alpha \\log K_t - (1-\\alpha) \\log L_t - g t = u_t, \\quad u_t \\sim I(0)\n\\]\nThus, GDP, capital, and labor may be cointegrated due to the common trend driven by TFP.\n\n\n2.3 Is TFP Growth Declining in the Long Run?\nMany studies suggest that TFP growth rates have declined since the mid-20th century.\nU.S. TFP Growth Trends (Annual Growth Rates):\n\n1950s–1970s: ~2.5% per year\n\n1980s–1990s: ~1.5% per year\n\n2000s–2020s: ~0.5%-1.0% per year\n\nThe productivity slowdown hypothesis suggests that long-term economic growth potential is becoming weaker than before.\n\n\n2.4 Why Might TFP Growth Decline Over Time?\nSeveral key factors contribute to the observed decline in TFP growth:\n\nThe Low-Hanging Fruit Hypothesis (Gordon, 2016)\n\nPast technological revolutions (electricity, automobiles, antibiotics) were one-time events that provided massive boosts.\n\nRecent innovations (social media, fintech, AI) may not have the same economy-wide productivity effects.\n\nDeclining R&D Efficiency (Bloom et al., 2020)\n\n“Ideas are getting harder to find.”\n\nR&D investment has increased, but the number of researchers needed to sustain TFP growth has risen exponentially.\n\nDemographic Constraints and Human Capital Decay\n\nAging populations in advanced economies reduce labor force dynamism and innovation.\n\nEducation quality improvements may have plateaued, reducing skilled labor supply.\n\nSectoral Shift to Low-Productivity Industries\n\nAdvanced economies have shifted from manufacturing to services (e.g., healthcare, education, public sector).\n\nServices typically have lower TFP growth, as they rely heavily on human labor and are difficult to automate.\n\nIncreased Regulation and Market Distortions\n\nGrowing regulations, tax policies, and political uncertainty increase inefficiencies, reducing overall productivity.\n\n\n\n\n2.5 Could TFP Growth Accelerate Again?\nWhile aggregate TFP growth may appear to be declining, there are huge disparities across sectors. Just as wealth concentration is increasing, TFP growth could also be highly concentrated within specific industries:\n\nAI and Automation\n\nAI-driven automation could drastically improve productivity, particularly in certain white-collar jobs.\n\nHowever, productivity gains may be highly concentrated in tech-driven industries, rather than the economy as a whole.\n\nSectoral Disparities in TFP Growth\n\nWithin-market concentration: A small number of firms (e.g., winners in the winner-takes-all structure) are experiencing high TFP growth, while others stagnate.\n\nCross-market concentration: Some sectors (AI, biotech) may experience massive productivity boosts, while others (traditional services, public sector) may stagnate.\n\nEnergy Revolution and New Technological Frontiers\n\nAdvances in nuclear fusion, renewables, and space-based energy production could dramatically shift TFP growth.\n\nHowever, large-scale adoption is slow due to political, regulatory, and financial constraints.",
    "crumbs": [
      "Apps",
      "경제",
      "Time-varying TFP"
    ]
  },
  {
    "objectID": "structure_tfp.html#conclusion",
    "href": "structure_tfp.html#conclusion",
    "title": "Time-varying TFP",
    "section": "3 Conclusion",
    "text": "3 Conclusion\n\n3.1 Implications for Time Series Analysis\nIf the long-run TFP growth rate is declining, its stochastic trend may weaken, making it more likely to transition from a unit root (\\(I(1)\\)) process to a stationary (\\(I(0)\\)) process over time. This has important implications for empirical modeling and forecasting, particularly in cases where researchers assume a constant growth trend in TFP-driven models. To properly account for these shifts, researchers should:\n\nUse structural break tests (e.g., Bai-Perron) to detect shifts in TFP growth trends and assess whether different growth regimes exist.\n\nApply rolling unit root tests to examine whether TFP has moved from an \\(I(1)\\) process toward an \\(I(0)\\) process over time.\n\nIncorporate time-varying cointegration approaches to avoid biased estimations that assume stable relationships in economic growth models.\n\n\n\n3.2 Remarks\nTFP growth is not uniform across industries; while aggregate TFP growth may be declining, specific sectors are experiencing rapid technological advancements. Just as wealth and market concentration have increased, TFP gains are increasingly concentrated within high-tech and innovation-driven industries. The result is a growing disparity between leading-edge industries (AI, biotech, energy) and traditional sectors (public services, healthcare, education), which exhibit slower productivity improvements.\nAs sectoral divergence in productivity widens, traditional macroeconomic models that assume homogeneous TFP growth may require revision. Future research should focus on developing heterogeneous growth models that account for cross-sectoral differences, while also employing more dynamic econometric techniques to capture the evolving nature of TFP trends.",
    "crumbs": [
      "Apps",
      "경제",
      "Time-varying TFP"
    ]
  },
  {
    "objectID": "structure_tfp.html#integration-order",
    "href": "structure_tfp.html#integration-order",
    "title": "Time-varying TFP",
    "section": "5 Integration Order",
    "text": "5 Integration Order\nThe integration order of a time series determines how many times it must be differenced to become stationary. A series is:\n\n\\(I(0)\\) (stationary) process if it has a constant mean, variance, and autocovariance.\n\\(I(1)\\) (unit root) process if it is non-stationary but becomes stationary after first differencing.\n\\(I(d)\\) process if it requires \\(d\\) differences to become stationary.\n\nEconomic time series such as GDP, money supply, and asset prices often exhibit \\(I(1)\\) behavior, meaning they contain stochastic trends and require differencing to achieve stationarity.",
    "crumbs": [
      "Apps",
      "경제",
      "Time-varying TFP"
    ]
  },
  {
    "objectID": "structure_tfp.html#joint-covariance-stationarity-vs.-cointegration",
    "href": "structure_tfp.html#joint-covariance-stationarity-vs.-cointegration",
    "title": "Time-varying TFP",
    "section": "6 Joint Covariance Stationarity vs. Cointegration",
    "text": "6 Joint Covariance Stationarity vs. Cointegration\nJoint covariance stationarity applies when each time-series maintains a constant mean, variance, and autocovariance over time. Cointegration, on the other hand, describes cases where two or more non-stationary \\(I(1)\\) time-series share a long-term equilibrium, forming a stationary linear combination.\n\n6.1 Joint Covariance Stationary Series (Weak-Sense Stationarity)\nA set of time series \\(X_t\\) and \\(Y_t\\) are jointly covariance stationary if they satisfy:\n\nConstant Mean: \\(E[X_t] = \\mu_X\\), \\(E[Y_t] = \\mu_Y\\) for all \\(t\\).\nConstant Variance: \\(Var(X_t)\\) and \\(Var(Y_t)\\) do not change over time.\nAutocovariance Depends Only on Lag: \\(Cov(X_t, X_{t-h})\\) and \\(Cov(Y_t, Y_{t-h})\\) depend only on the lag \\(h\\), not on \\(t\\).\n\nIf two time series are both weakly stationary, then any linear combination of them is also stationary.\n\n\n6.2 Cointegrated Series\nA set of time series \\(X_t\\) and \\(Y_t\\) are cointegrated if:\n\nEach series is \\(I(1)\\) (non-stationary) process.\nA linear combination exists that is \\(I(0)\\) (stationary) process:\n\\[\n\\beta_1 X_t + \\beta_2 Y_t = u_t\n\\]\n\nwhere \\(u_t\\) is \\(I(0)\\) (stationary) process.\nThus, even though individual variables are non-stationary, their linear combination is stationary, implying a long-run equilibrium relationship.\n\n\n6.3 The Relationship Between Covariance Stationarity and Cointegration\n\n\n\n\n\n\n\n\nProperty\nJoint Covariance Stationary Series\nCointegrated Series\n\n\n\n\nStationarity\nEach series is stationary (I(0))\nEach series is non-stationary (I(1)), but a linear combination is stationary\n\n\nUnit Root \\(I(d)\\)\nI(0) for each series\nI(1) for each series, but a specific linear combination is I(0)\n\n\nMean & Variance Stability\nMean & variance are constant over time\nIndividual series do not have stable mean & variance, but the combination does\n\n\nLong-run Relationship\nNo long-term relationship constraint\nA long-run equilibrium relationship exists\n\n\n\n\n6.3.1 Cointegrated Series Can Be Transformed into Covariance Stationary Series\nIf \\(X_t\\) and \\(Y_t\\) are cointegrated, their first differences \\(\\Delta X_t\\), \\(\\Delta Y_t\\) (or the residual \\(u_t\\)) are stationary.\n\nThe error correction term \\(u_t\\) is stationary \\(I(0)\\) process, meaning it satisfies the covariance stationarity conditions.\n\n\n\n6.3.2 Joint Covariance Stationary Series Are Not Cointegrated\nIf \\(X_t\\) and \\(Y_t\\) are both already \\(I(0)\\) (stationary), then any linear combination of them is also stationary.\n\nThey cannot be cointegrated because cointegration only applies to non-stationary (\\(I(1)\\)) series.\nIf all series are already covariance stationary, testing for cointegration is unnecessary.",
    "crumbs": [
      "Apps",
      "경제",
      "Time-varying TFP"
    ]
  },
  {
    "objectID": "structure_tfp.html#implications-for-empirical-analysis",
    "href": "structure_tfp.html#implications-for-empirical-analysis",
    "title": "Time-varying TFP",
    "section": "7 Implications for Empirical Analysis",
    "text": "7 Implications for Empirical Analysis\n\nBefore testing for cointegration, check for stationarity. If all series are \\(I(0)\\), cointegration does not apply.\nIf series are cointegrated, their residuals (error correction term) should be covariance stationary.\nMany macroeconomic variables (e.g., GDP & consumption, money supply & inflation) are cointegrated rather than purely covariance stationary.",
    "crumbs": [
      "Apps",
      "경제",
      "Time-varying TFP"
    ]
  },
  {
    "objectID": "gilded_age.html#introduction",
    "href": "gilded_age.html#introduction",
    "title": "New Gilded Age",
    "section": "",
    "text": "The term ‘Gilded Age’ was originally coined by Mark Twain in his novel The Gilded Age: A Tale of Today (Twain and Warner 1873), describing an era characterized by rapid economic expansion, extreme wealth concentration, and political corruption. A similar dynamic is emerging today, where financial and technological elites dominate economic output while wealth inequality reaches historic highs (Piketty 2014). The late 19th century saw industrial monopolies like Standard Oil and U.S. Steel controlling markets; today, tech giants such as Amazon, Apple, and Google exhibit similar dominance (Zucman 2019).\nSimultaneously, the Federal Reserve’s response to financial instability, particularly through excessive monetary expansion, contrasts with past policy mistakes that led to severe economic contractions due to monetary shrinkage (Bernanke 2000). If current economic trends persist—marked by the increasing concentration of wealth, hyperinflation risks, and geopolitical tensions—then the U.S. may be heading toward another crisis akin to the 1929 stock market collapse (Kindleberger 1978).",
    "crumbs": [
      "Apps",
      "경제",
      "New Gilded Age"
    ]
  },
  {
    "objectID": "gilded_age.html#wealth-concentration-and-market-distortions",
    "href": "gilded_age.html#wealth-concentration-and-market-distortions",
    "title": "New Gilded Age",
    "section": "2 Wealth Concentration and Market Distortions",
    "text": "2 Wealth Concentration and Market Distortions\n\n2.1 Extreme Wealth Concentration and Economic Disparities\nIn the late 19th century, “Robber Barons” controlled vast industrial empires while working-class Americans suffered under exploitative labor conditions (Irwin 2017). Today, the economic landscape reflects a similar dynamic: the top 1% of Americans hold over 30% of total U.S. wealth, and financial markets remain dominated by a handful of institutional investors and corporations (Saez and Zucman 2020). If historical trends hold, wealth concentration at this level often precedes financial and political crises.\n\n\n2.2 Financial Market Distortions Due to Federal Reserve Policies\nHistorically, the Federal Reserve’s failure to manage monetary policy effectively has exacerbated financial downturns. During the Great Depression, the Fed allowed the money supply to contract, worsening deflation (Friedman and Schwartz 1993). Conversely, in the 2008 financial crisis, the Fed implemented massive QE programs to avoid liquidity shortages (Gopinath and Gourinchas 2020). If the Fed continues expanding the money supply unchecked while maintaining low interest rates, it could trigger runaway inflation or asset bubbles (Reinhart and Rogoff 2010).",
    "crumbs": [
      "Apps",
      "경제",
      "New Gilded Age"
    ]
  },
  {
    "objectID": "gilded_age.html#economic-protectionism-and-capital-controls",
    "href": "gilded_age.html#economic-protectionism-and-capital-controls",
    "title": "New Gilded Age",
    "section": "3 Economic Protectionism and Capital Controls",
    "text": "3 Economic Protectionism and Capital Controls\n\n3.1 Protectionist Policies and Global Trade Disruptions\nIn response to financial instability, the U.S. may turn to protectionist measures similar to those seen in the early 20th century, such as the Smoot-Hawley Tariff Act (Irwin 2017). If the U.S. imposes broad tariffs on allies like Canada, Mexico, and the EU (excluding the UK), retaliatory tariffs could significantly reduce global trade, accelerating economic fragmentation (Acker 2020).\n\n\n3.2 Capital Controls\nTo prevent capital flight, the U.S. government might implement capital controls, restricting the movement of funds outside the country (Dornbusch 1996). Such policies could initially stabilize domestic financial markets by preventing liquidity outflows, but they would ultimately deter foreign investment and reduce the credibility of the U.S. dollar (Prasad 2021). If capital controls are implemented alongside protectionist trade policies, the global financial system could realign, reducing reliance on the dollar (Eichengreen 2019).",
    "crumbs": [
      "Apps",
      "경제",
      "New Gilded Age"
    ]
  },
  {
    "objectID": "gilded_age.html#military-interventions",
    "href": "gilded_age.html#military-interventions",
    "title": "New Gilded Age",
    "section": "4 Military Interventions",
    "text": "4 Military Interventions\nIf economic instability threatens U.S. financial dominance, military interventions could become a tool for securing resource control, as seen in the Iraq War (Stiglitz 2008). If the U.S. uses force to maintain oil-based dollar hegemony, it risks broader geopolitical conflict, increasing economic uncertainty and reducing investor confidence (Gopinath and Gourinchas 2020).",
    "crumbs": [
      "Apps",
      "경제",
      "New Gilded Age"
    ]
  },
  {
    "objectID": "gilded_age.html#political-instability-and-the-future-of-the-u.s.-dollar",
    "href": "gilded_age.html#political-instability-and-the-future-of-the-u.s.-dollar",
    "title": "New Gilded Age",
    "section": "5 Political Instability and the Future of the U.S. Dollar",
    "text": "5 Political Instability and the Future of the U.S. Dollar\n\n5.1 Internal Conflict\nWith increasing partisan division, the U.S. could experience state-led resistance against federal economic policies. Democratic-led states might oppose Republican federal mandates, leading to legal disputes over taxation, social policies, and trade regulations (Levitsky and Ziblatt 2018). In extreme cases, states like California could advocate for economic or political autonomy, mirroring secessionist movements of the 19th century, while Texas, despite its strong Republican leanings, might push for greater state sovereignty in response to federal overreach or shifting national policies (Acker 2020).\n\n\n5.2 U.S. Dollar as the Global Reserve Currency\nThe decline of the British pound post-World War II illustrates how global reserve currencies can lose dominance due to internal and external economic shifts (Eichengreen 2019). If U.S. political instability continues, central banks worldwide may accelerate diversification away from dollar holdings, increasing reliance on alternative financial networks such as BRICS payment systems, Bitcoin, and other emerging digital currencies. (Prasad 2021).",
    "crumbs": [
      "Apps",
      "경제",
      "New Gilded Age"
    ]
  },
  {
    "objectID": "gilded_age.html#conclusion",
    "href": "gilded_age.html#conclusion",
    "title": "New Gilded Age",
    "section": "6 Conclusion",
    "text": "6 Conclusion\nThe parallels between the late 19th century Gilded Age and the modern U.S. economy suggest that wealth concentration, financial market distortions, and political instability could lead to a systemic crisis. The combination of aggressive Federal Reserve monetary expansion, protectionist policies, and potential military interventions may further weaken U.S. financial dominance. If these trends continue unchecked, the global financial system could transition away from U.S. dollar hegemony, and political polarization within the U.S. could escalate into unprecedented state-level disputes. Beyond further research, fostering greater dialogue, mutual understanding, and cooperative governance will be essential to mitigating escalation and ensuring long-term economic and political stability",
    "crumbs": [
      "Apps",
      "경제",
      "New Gilded Age"
    ]
  },
  {
    "objectID": "market_conditions.html#perfectly-competitive-market-pcm-and-its-implications-for-financial-markets",
    "href": "market_conditions.html#perfectly-competitive-market-pcm-and-its-implications-for-financial-markets",
    "title": "Free Market Conditions",
    "section": "Perfectly Competitive Market (PCM) and Its Implications for Financial Markets",
    "text": "Perfectly Competitive Market (PCM) and Its Implications for Financial Markets\nPCM represents an idealized economic structure where prices are determined by market forces under strict competition. The defining characteristics include:\n\nMany buyers and sellers, ensuring a high degree of competition.\n\nPrice-taking behavior, where no single participant influences market prices.\n\nHomogeneous goods, meaning that products are perfect substitutes.\n\nInformation symmetry, ensuring that all participants have equal access to relevant data.\n\nNo friction on entry and exit, allowing participants to freely enter or exit without cost.\n\nZero long-run economic profits, as competition eliminates excess returns in equilibrium.\n\nThe welfare theorem states that if all traders have convex preferences in PCM, then the equilibrium allocation of limited resources is Pareto efficient. However, PCM does not explicitly incorporate risk, distinguishing it from financial models such as CAPM. Although PCM shares price-taking behavior and information symmetry with CAPM, it does not inherently account for risk-return trade-offs, which are central to financial market equilibrium.\n\nThe Capital Asset Pricing Model (CAPM) and Systematic Risk Pricing\nCAPM is a single-factor asset pricing model that determines equilibrium asset prices based on systematic risk exposure. It extends the competitive market framework to financial markets under the following assumptions:\n\nMean-variance optimization, where investors maximize expected utility based on risk-return trade-offs.\n\nRisk characterization through two moments, assuming that asset returns are fully described by their mean and variance.\n\nHomogeneous assets, meaning financial assets can be interpreted as contingent claims (i.e., claims on future payoffs that can be normalized using a risk-free currency), ensuring that the law of one price holds.\n\nInformation symmetry, allowing all investors to have identical expectations about risk and returns.\n\nFrictionless markets, with no transaction costs, taxes, or constraints on borrowing and lending.\n\nMarket portfolio as the tangency portfolio, implying that all investors hold a combination of the risk-free asset and the market portfolio.\n\nWhile CAPM shares PCM’s assumption of competitive equilibrium, it explicitly incorporates systematic risk pricing, which is absent in traditional PCM models.",
    "crumbs": [
      "Apps",
      "경제",
      "Free Market Conditions"
    ]
  },
  {
    "objectID": "market_conditions.html#no-arbitrage-na-and-the-role-of-market-structures",
    "href": "market_conditions.html#no-arbitrage-na-and-the-role-of-market-structures",
    "title": "Free Market Conditions",
    "section": "No-Arbitrage (NA) and the Role of Market Structures",
    "text": "No-Arbitrage (NA) and the Role of Market Structures\nThe NA condition is fundamental to financial market sustainability. Unlike PCM and CAPM, which describe equilibrium-based price formation, NA ensures that risk-free arbitrage does not persist indefinitely.\nPersistent arbitrage leads to:\n\nCapital concentration, where wealth accumulates disproportionately among arbitrageurs, reducing market competitiveness.\n\nInefficient risk-sharing, as distortions in capital allocation prevent the effective pricing of idiosyncratic risk.\n\nMarket failure, where financial markets lose their role as efficient allocators of capital.\n\nNA is not a direct consequence of PCM or CAPM but rather a structural requirement for a well-functioning financial system. The absence of arbitrage opportunities is necessary for risk-adjusted returns to reflect systematic risk rather than mispricing.",
    "crumbs": [
      "Apps",
      "경제",
      "Free Market Conditions"
    ]
  },
  {
    "objectID": "market_conditions.html#the-efficient-market-hypothesis-emh-as-an-informational-condition-for-na",
    "href": "market_conditions.html#the-efficient-market-hypothesis-emh-as-an-informational-condition-for-na",
    "title": "Free Market Conditions",
    "section": "The Efficient Market Hypothesis (EMH) as an Informational Condition for NA",
    "text": "The Efficient Market Hypothesis (EMH) as an Informational Condition for NA\nEMH provides an informational framework under which NA can hold. NA requires that market participants act on available information to eliminate arbitrage. This is feasible if:\n\nInvestors are rational, meaning they respond optimally to profit opportunities.\n\nInformation is symmetric, ensuring that arbitrage opportunities do not persist due to asymmetric knowledge.\n\nHowever, empirical evidence suggests that NA does not always hold due to:\n\nBehavioral biases, where sentiment-driven trading creates arbitrage opportunities.\n\nMarket microstructure effects, where asymmetric information and liquidity constraints delay arbitrage elimination.\n\nWhile EMH supports NA in principle, it is not sufficient for ensuring arbitrage-free markets in the presence of bounded rationality and institutional frictions.",
    "crumbs": [
      "Apps",
      "경제",
      "Free Market Conditions"
    ]
  },
  {
    "objectID": "market_conditions.html#empirical-implications-of-arbitrage-persistence",
    "href": "market_conditions.html#empirical-implications-of-arbitrage-persistence",
    "title": "Free Market Conditions",
    "section": "Empirical Implications of Arbitrage Persistence",
    "text": "Empirical Implications of Arbitrage Persistence\nEmpirical deviations from NA and EMH do not necessarily invalidate these theories but highlight structural inefficiencies in financial markets. Persistent arbitrage opportunities suggest:\n\nInvestor irrationality, supporting behavioral finance explanations.\n\nInformation asymmetry, consistent with market microstructure theories.\n\nRegulatory distortions, such as too-big-to-fail policies, short-selling bans, and liquidity constraints.\n\nIf arbitrage persists over long time horizons, this may signal failures in market structure that undermine competition and risk allocation. The consequences include:\n\nExcessive capital concentration, reinforcing financial monopolies.\n\nDeterioration in risk-sharing mechanisms, reducing market efficiency.\n\nNA is not just an empirical observation but a necessary structural condition for financial market stability.",
    "crumbs": [
      "Apps",
      "경제",
      "Free Market Conditions"
    ]
  },
  {
    "objectID": "market_conditions.html#the-role-of-institutional-frameworks",
    "href": "market_conditions.html#the-role-of-institutional-frameworks",
    "title": "Free Market Conditions",
    "section": "The Role of Institutional Frameworks",
    "text": "The Role of Institutional Frameworks\nThe interplay between NA, EMH, and CAPM depends on regulatory and institutional safeguards. While theoretical models assume that arbitrage disappears naturally, real-world markets require institutional mechanisms to enforce NA and maintain EMH. Key mechanisms include:\n\nFinancial disclosure requirements, ensuring that relevant information is accessible to all participants.\n\nMarket integrity measures, such as circuit breakers and trade halts, preventing extreme mispricings.\n\nRegulations on leverage and short-selling, reducing excessive arbitrage concentration.\n\nLiquidity provisions and market-making incentives, ensuring that mispricings do not persist due to temporary liquidity shortages.\n\nWithout these structural conditions, arbitrage can persist, distorting the intended function of financial markets. The ability of CAPM to predict asset prices, the validity of EMH, and the enforcement of NA all depend on institutional frameworks that mitigate market failures.",
    "crumbs": [
      "Apps",
      "경제",
      "Free Market Conditions"
    ]
  },
  {
    "objectID": "market_conditions.html#conceptual-summary",
    "href": "market_conditions.html#conceptual-summary",
    "title": "Free Market Conditions",
    "section": "Conceptual Summary",
    "text": "Conceptual Summary\n\n\n\n\n\n\n\n\n\n\nFeature\nPCM\nCAPM\nNA\nEMH\n\n\n\n\nMarket Type\nCompetitive goods market\nCompetitive capital markets\nStructural equilibrium condition\nInformational efficiency condition\n\n\nPrice Mechanism\nSupply and demand equilibrium\nRisk-return equilibrium (beta-based)\nElimination of risk-free arbitrage\nPrices fully reflect information\n\n\nInformation Symmetry\nYes\nYes\nNot required, but supports NA\nNecessary for strong-form EMH\n\n\nMarket Frictions\nNo friction on entry & exit\nUnlimited borrowing/lending assumption\nPrevents arbitrage persistence\nCan be affected by liquidity constraints\n\n\n\nNA, EMH, and CAPM all rely on market structures to function effectively. While PCM provides the foundation for competitive equilibrium, it lacks a risk framework, which CAPM introduces. NA serves as a constraint that ensures risk-free arbitrage does not disrupt financial markets, while EMH posits that prices reflect available information, a condition necessary but not sufficient for NA. These theories, though distinct, are interconnected through institutional safeguards and regulatory mechanisms that sustain financial market efficiency.",
    "crumbs": [
      "Apps",
      "경제",
      "Free Market Conditions"
    ]
  },
  {
    "objectID": "structure_tfp.html#appendix",
    "href": "structure_tfp.html#appendix",
    "title": "Time-varying TFP",
    "section": "4 Appendix",
    "text": "4 Appendix\nCovariance Stationarity, Integration Order, and Cointegration\nCovariance stationarity and cointegration describe different statistical properties of time series data. Covariance stationarity applies to series where mean, variance, and autocovariance do not change over time, while cointegration describes cases where non-stationary \\(I(1)\\) time-series maintain a stable long-run equilibrium. This study explores how these concepts relate and how cointegration allows for non-stationary series to form stationary linear combinations.\n\n4.1 Integration Order\nThe integration order of a time series determines how many times it must be differenced to become stationary. A series is:\n\n\\(I(0)\\) (stationary) process if it has a constant mean, variance, and autocovariance.\n\\(I(1)\\) (unit root) process if it is non-stationary but becomes stationary after first differencing.\n\\(I(d)\\) process if it requires \\(d\\) differences to become stationary.\n\nEconomic time series such as GDP, money supply, and asset prices often exhibit \\(I(1)\\) behavior, meaning they contain stochastic trends and require differencing to achieve stationarity.\n\n\n4.2 Joint Covariance Stationarity vs. Cointegration\nJoint covariance stationarity applies when each time-series maintains a constant mean, variance, and autocovariance over time. Cointegration, on the other hand, describes cases where two or more non-stationary \\(I(1)\\) time-series share a long-term equilibrium, forming a stationary linear combination.\n\n4.2.1 Joint Covariance Stationary Series (Weak-Sense Stationarity)\nA set of time series \\(X_t\\) and \\(Y_t\\) are jointly covariance stationary if they satisfy:\n\nConstant Mean: \\(E[X_t] = \\mu_X\\), \\(E[Y_t] = \\mu_Y\\) for all \\(t\\).\nConstant Variance: \\(Var(X_t)\\) and \\(Var(Y_t)\\) do not change over time.\nAutocovariance Depends Only on Lag: \\(Cov(X_t, X_{t-h})\\) and \\(Cov(Y_t, Y_{t-h})\\) depend only on the lag \\(h\\), not on \\(t\\).\n\nIf two time series are both weakly stationary, then any linear combination of them is also stationary.\n\n\n4.2.2 Cointegrated Series\nA set of time series \\(X_t\\) and \\(Y_t\\) are cointegrated if:\n\nEach series is \\(I(1)\\) (non-stationary) process.\nA linear combination exists that is \\(I(0)\\) (stationary) process:\n\\[\n\\beta_1 X_t + \\beta_2 Y_t = u_t\n\\]\n\nwhere \\(u_t\\) is \\(I(0)\\) (stationary) process.\nThus, even though individual variables are non-stationary, their linear combination is stationary, implying a long-run equilibrium relationship.\n\n\n4.2.3 The Relationship Between Covariance Stationarity and Cointegration\n\n\n\n\n\n\n\n\nProperty\nJoint Covariance Stationary Series\nCointegrated Series\n\n\n\n\nStationarity\nEach series is stationary (I(0))\nEach series is non-stationary (I(1)), but a linear combination is stationary\n\n\nUnit Root \\(I(d)\\)\nI(0) for each series\nI(1) for each series, but a specific linear combination is I(0)\n\n\nMean & Variance Stability\nMean & variance are constant over time\nIndividual series do not have stable mean & variance, but the combination does\n\n\nLong-run Relationship\nNo long-term relationship constraint\nA long-run equilibrium relationship exists\n\n\n\n\n\n4.2.4 Cointegrated Series Can Be Transformed into Covariance Stationary Series\nIf \\(X_t\\) and \\(Y_t\\) are cointegrated, their first differences \\(\\Delta X_t\\), \\(\\Delta Y_t\\) (or the residual \\(u_t\\)) are stationary.\n\nThe error correction term \\(u_t\\) is stationary \\(I(0)\\) process, meaning it satisfies the covariance stationarity conditions.\n\n\n\n4.2.5 Joint Covariance Stationary Series Are Not Cointegrated\nIf \\(X_t\\) and \\(Y_t\\) are both already \\(I(0)\\) (stationary), then any linear combination of them is also stationary.\n\nThey cannot be cointegrated because cointegration only applies to non-stationary (\\(I(1)\\)) series.\nIf all series are already covariance stationary, testing for cointegration is unnecessary.\n\n\n\n\n4.3 Implications for Empirical Analysis\n\nBefore testing for cointegration, check for stationarity. If all series are \\(I(0)\\), cointegration does not apply.\nIf series are cointegrated, their residuals (error correction term) should be covariance stationary.\nMany macroeconomic variables (e.g., GDP & consumption, money supply & inflation) are cointegrated rather than purely covariance stationary.",
    "crumbs": [
      "Apps",
      "경제",
      "Time-varying TFP"
    ]
  },
  {
    "objectID": "finance_existence.html#conventional-view",
    "href": "finance_existence.html#conventional-view",
    "title": "Rethinking Asset Existence",
    "section": "",
    "text": "Financial Assets as Risk-Reward Instruments\n\nIn classical asset pricing theory, financial assets are presumed to exist to reward investors for bearing risk. Through mechanisms such as arbitrage and diversification, idiosyncratic risk is mitigated, and equilibrium returns are determined by each asset’s exposure to systematic risk. This framework justifies the existence of any asset through the promise of a fair, risk-adjusted return. Asset prices thus reflect not market power or concentration, but the compensated cost of risk.\nThis view embeds a philosophical logic of fairness: that market participants are compensated proportionally for risk undertaken, and that capital flows dynamically toward efficiency. Such a model assumes fluid reallocations, reversibility of state transitions, and the dominance of fundamentals over flow mechanics.",
    "crumbs": [
      "Apps",
      "금융",
      "Rethinking Asset Existence"
    ]
  },
  {
    "objectID": "finance_existence.html#tbtf-view",
    "href": "finance_existence.html#tbtf-view",
    "title": "Rethinking Asset Existence",
    "section": "2 TBTF View",
    "text": "2 TBTF View\n\nFinancial Assets as Power-Concentrating Instruments\n\nIn contrast, the TBTF framework rejects this equilibrium-based justification. It posits that some financial assets (especially in the public secondary market) persist not because of their risk-return characteristics, but because they are capital sinks—entities with entrenched market dominance, self-reinforcing narratives, and structural insulation from volatility.\nThese assets do not compensate for uncertainty—they absorb uncertainty by offering investors a sense of safety rooted in scale, liquidity, and systemic centrality. TBTF assets act as narrative anchors in uncertain markets: not because they offer greater upside, but because they symbolize continuity, legitimacy, and collective default.\nThis redefines the rationale for capital allocation: from (short-run) return-maximization to (long-run) structure-maximization. Investors increasingly allocate not to “the best opportunities” in the short-term, but to “the safest dominant incumbents” in the long-term. The performance of TBTF strategies arises not from myopic risk pricing, but from farsighted market structure engineering—index inclusion, ETF flows, regulatory inertia, and the psychology of too-big-to-fail.",
    "crumbs": [
      "Apps",
      "금융",
      "Rethinking Asset Existence"
    ]
  },
  {
    "objectID": "finance_existence.html#philosophical-and-mathematical-justification",
    "href": "finance_existence.html#philosophical-and-mathematical-justification",
    "title": "Rethinking Asset Existence",
    "section": "3 Philosophical and Mathematical Justification",
    "text": "3 Philosophical and Mathematical Justification\nThis philosophical inversion has deep implications:\n\nExistential Justification: Financial assets exist not to reward risk-takers but to retain power. Their performance is a function of initial endowment, not market responsiveness.\nStatistical Structure: The return-generating process resembles a non-ergodic, non-reversible Markov chain, where top and exit states become quasi-absorbing.\nMathematical Inertia: TBTF assets exhibit relative second-moment stationarity, even when first moments shift. This aligns with the capital inertia hypothesis: relative structure is stable, absolute returns are residual.\nNarrative Economics: Echoing Shiller and Gennaioli–Shleifer, capital allocation is driven less by fundamentals than by narratives of dominance, size, and safety.",
    "crumbs": [
      "Apps",
      "금융",
      "Rethinking Asset Existence"
    ]
  },
  {
    "objectID": "finance_existence.html#implication-for-market-theory",
    "href": "finance_existence.html#implication-for-market-theory",
    "title": "Rethinking Asset Existence",
    "section": "4 Implication for Market Theory",
    "text": "4 Implication for Market Theory\nThe TBTF perspective challenges the canonical assumptions of:\n\nArbitrage efficiency: capital does not equalize returns across assets\nRisk-reward equilibrium: structural power trumps systematic beta\nDynamic reallocation: capital flows become locked, not optimized\nAllocative neutrality: flow-based reinforcements create concentration spirals\n\nIn this view, financial markets do not price risk—they price dominance. The empirical success of TBTF strategies is not an anomaly; it is a manifestation of long-term structural inertia, capital path dependence, and the self-fulfilling logic of institutional safety.",
    "crumbs": [
      "Apps",
      "금융",
      "Rethinking Asset Existence"
    ]
  },
  {
    "objectID": "capm.html#introduction",
    "href": "capm.html#introduction",
    "title": "Revisiting the CAPM",
    "section": "",
    "text": "The Capital Asset Pricing Model (CAPM), developed by Sharpe (1964), Lintner (1965), and Mossin (1966), remains a cornerstone of modern finance, linking expected returns to risk. It classifies risk into two categories: systematic risk, which stems from market-wide factors and cannot be diversified away, and unsystematic risk, which is asset-specific and can be mitigated through diversification. The CAPM posits that only systematic risk, measured by beta, justifies a return premium, as investors can eliminate unsystematic risk by holding a diversified portfolio, ideally approximating the market portfolio. This principle aligns with broader linear factor models like the Arbitrage Pricing Theory (APT) (Ross 1976), which extends the CAPM by incorporating multiple systematic risk factors while similarly dismissing diversifiable risk under no-arbitrage conditions.\nHowever, the CAPM’s empirical validity has been contested. Banz (1981) documented the size effect, where small-cap stocks outperform CAPM predictions, while Basu (1977) identified the value effect, showing excess returns for stocks with high earnings-to-price ratios. These anomalies spurred the development of multifactor models, such as the Fama-French three-factor model (Fama and French 1993), which augment beta with size and value factors. Beyond these, market concentration has emerged as a critical lens for understanding asset pricing deviations. Hou and Robinson (2006) found that firms in concentrated industries earn higher returns, attributing this to economic rents from market power. Edmans (2009) linked ownership concentration to superior performance, while Choi et al. (2017) showed that institutional investors with concentrated portfolios outperform diversified ones. Neuhann and Sockin (2024) explored how financial market concentration distorts capital allocation, and Bustamante and Donangelo (2017) tied product market concentration to industry returns.\nFrom a theoretical perspective, Magill and Quinzii (1996) argued that incomplete markets—lacking sufficient contingent claims—prevent full risk hedging, challenging CAPM assumptions. Cochrane (1996) emphasized the role of investment-based pricing, while Campbell (1992) critiqued volatility as an incomplete risk measure. Socioeconomic analyses, such as Saez and Zucman (2016), further connect market concentration to wealth inequality, highlighting broader implications. This rich body of literature suggests that market structure and concentration significantly complicate the CAPM’s risk-return framework, necessitating a deeper examination.",
    "crumbs": [
      "Apps",
      "금융",
      "Revisiting the CAPM"
    ]
  },
  {
    "objectID": "capm.html#main",
    "href": "capm.html#main",
    "title": "Revisiting the CAPM",
    "section": "2 Main",
    "text": "2 Main\n\n2.1 Empirical and Mathematical Foundations of Diversification\nDiversification’s risk-reducing power is well-established empirically and mathematically. Elton and Gruber (1977) analyzed 3,290 securities, demonstrating that a portfolio of just four stocks markedly reduces variance compared to a single stock, underscoring diversification’s practical utility.\nMathematically, consider an equally weighted portfolio of \\(n\\) securities. The portfolio variance \\(\\sigma_p^2\\) is given by:\n\\[\n\\sigma_p^2 = \\frac{1}{n} \\bar{\\sigma}^2 + \\frac{n-1}{n} \\bar{\\rho} \\bar{\\sigma}^2\n\\]\nwhere:\n\n\\(\\bar{\\sigma}^2\\) = average variance of individual securities\n\n\\(\\bar{\\rho}\\) = average correlation between securities\n\nAs \\(n\\) increases, \\(\\sigma_p^2\\) converges to \\(\\bar{\\rho} \\bar{\\sigma}^2\\), indicating that covariance, not individual variance, dominates portfolio risk. When \\(\\bar{\\rho} &lt; 1\\), diversification lowers volatility, resembling how higher-order terms in Taylor polynomials diminish to smooth a function or how fractal geometry simplifies irregularities with scale.\n\n\n2.2 A Critique of Risk as Volatility\nThe CAPM equates risk with volatility, but this assumption is narrow. Long-term investors may prioritize structural risks—e.g., economic shifts or sector obsolescence—over short-term price swings. A volatile growth stock might be less “risky” to them than a stable but declining asset. Campbell (1992) supports this critique, arguing that volatility oversimplifies the multifaceted nature of risk, a view echoed by behavioral finance perspectives (Shiller 2003).\n\n\n2.3 Diversification and the Risk-Return Trade-Off\nThe CAPM ties diversification to the risk-return trade-off, suggesting investors can eliminate unsystematic risk while earning returns proportional to systematic risk exposure. In a stochastic setting, diverse agents (e.g., farmers, energy producers) share idiosyncratic risks, enhancing welfare (Cochrane 2009). Yet, this assumes a broad, competitive market. When the market portfolio—say, the S&P 500—is dominated by a few highly correlated stocks (e.g., tech giants), diversification falters. High \\(\\bar{\\rho}\\) reduces variance’s sensitivity to \\(n\\), undermining the CAPM’s benefits (Grullon, Larkin, and Michaely 2019).\n\n\n2.4 Market Concentration and the Upper Frontier\nIn the standard arbitrage-free asset pricing framework, the upper mean-variance frontier assets correlate perfectly (negatively) with the stochastic discount factor (SDF). In concentrated markets, dominant firms with economic moats (Bustamante and Donangelo 2017) act as principal components, compressing the payoff space. For example, if the Herfindahl-Hirschman Index (HHI) measures this dominance or concentration, then a high HHI signals reliance on few assets, limiting diversification. Investors may then seek arbitrage in these stocks, amplifying concentration (Valta 2012).\n\n\n2.5 Implications for Investors and Market Stability\nIn concentrated markets, diversification yields to capturing rents from dominant firms. These firms use primary-market capital to reinforce moats, distributing profits rather than fostering competition (Hou and Robinson 2006). Secondary-market trading becomes zero-sum, redistributing wealth without value creation. Early investors in concentrated stocks gain disproportionately, widening inequality (Saez and Zucman 2016) and challenging the CAPM’s traditional data-driven risk-return logic.\n\n\n2.6 Broader Socioeconomic Consequences\nConcentration reduces contingent claim diversity, impairing hedging capacity (Magill and Quinzii 1996). A shock to a dominant sector triggers systemic ripples, increasing instability. This homogeneity shifts markets from managing uncertainty to rewarding market power, exacerbating wealth gaps and contradicting the CAPM’s egalitarian risk-sharing ideal.",
    "crumbs": [
      "Apps",
      "금융",
      "Revisiting the CAPM"
    ]
  },
  {
    "objectID": "capm.html#conclusion",
    "href": "capm.html#conclusion",
    "title": "Revisiting the CAPM",
    "section": "3 Conclusion",
    "text": "3 Conclusion\nThe CAPM’s diversification-driven risk-return framework faces significant challenges from market concentration. As diversification weakens, investors prioritize rents over risk reduction, simplifying markets into systems dominated by a few firms. This shift threatens stability, equity, and hedging capacity, urging a rethinking of the CAPM and policies to enhance market diversity.",
    "crumbs": [
      "Apps",
      "금융",
      "Revisiting the CAPM"
    ]
  }
]