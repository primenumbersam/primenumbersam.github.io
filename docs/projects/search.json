[
  {
    "objectID": "structure_tfp.html",
    "href": "structure_tfp.html",
    "title": "Time-varying TFP",
    "section": "",
    "text": "The growth rate of Total Factor Productivity (TFP) is not a fixed constant; rather, it evolves over time and may be subject to long-term decline. While mainstream macroeconomic models frequently assume a constant TFP growth rate, this assumption is increasingly being questioned. Several structural factors—diminishing returns to R&D, demographic constraints, and sectoral shifts towards lower-productivity industries—suggest that aggregate TFP growth may continue to slow. However, TFP growth is not uniform across sectors. Certain industries, such as AI, biotechnology, and advanced energy technologies, exhibit significantly higher productivity gains, leading to growing sectoral disparities in innovation.\nFrom an econometric perspective, a decline in TFP growth could alter its stationarity properties, weakening its stochastic trend and making it more likely to transition from an \\(I(1)\\) (non-stationary) stochastic process to an \\(I(0)\\) (stationary) stochastic process. If structural breaks exist in TFP growth, traditional time-series models based on constant-trend cointegration assumptions may fail. Thus, accurately modeling TFP trends requires dynamic econometric techniques that account for time-varying structural changes.\n\n\n\n\n\nTFP represents technological progress that enhances economic output beyond capital and labor inputs. In the Solow-Swan growth model, TFP follows an exponential trend, expressed as:\n\\[\nA_t = A_0 e^{gt} \\epsilon_t\n\\]\nwhere \\(g\\) is the growth rate and \\(\\epsilon_t\\) captures short-term fluctuations. This implies that TFP is not strictly stationary; rather, its log form follows a unit root process with a deterministic trend:\n\\[\n\\log A_t = g t + u_t, \\quad u_t \\sim I(0)\n\\]\nThus, mainstream macroeconomic models do not assume TFP is stationary in levels, but often assume its deviation from trend is stationary.\n\n\n\nEmpirical studies show:\n\nLong-Run TFP behaves as a non-stationary process (\\(I(1)\\)), meaning that it exhibits persistent growth and shocks that do not revert over time.\nShort-Run TFP Shocks (deviations from trend) are often stationary (\\(I(0)\\)) process.\n\nIf GDP and capital stock are also \\(I(1)\\), Solow’s production function suggests they should be cointegrated:\n\\[\n\\log Y_t - \\alpha \\log K_t - (1-\\alpha) \\log L_t - g t = u_t, \\quad u_t \\sim I(0)\n\\]\nThus, GDP, capital, and labor may be cointegrated due to the common trend driven by TFP.\n\n\n\nMany studies suggest that TFP growth rates have declined since the mid-20th century.\nU.S. TFP Growth Trends (Annual Growth Rates):\n\n1950s–1970s: ~2.5% per year\n\n1980s–1990s: ~1.5% per year\n\n2000s–2020s: ~0.5%-1.0% per year\n\nThe productivity slowdown hypothesis suggests that long-term economic growth potential is becoming weaker than before.\n\n\n\nSeveral key factors contribute to the observed decline in TFP growth:\n\nThe Low-Hanging Fruit Hypothesis (Gordon, 2016)\n\nPast technological revolutions (electricity, automobiles, antibiotics) were one-time events that provided massive boosts.\n\nRecent innovations (social media, fintech, AI) may not have the same economy-wide productivity effects.\n\nDeclining R&D Efficiency (Bloom et al., 2020)\n\n“Ideas are getting harder to find.”\n\nR&D investment has increased, but the number of researchers needed to sustain TFP growth has risen exponentially.\n\nDemographic Constraints and Human Capital Decay\n\nAging populations in advanced economies reduce labor force dynamism and innovation.\n\nEducation quality improvements may have plateaued, reducing skilled labor supply.\n\nSectoral Shift to Low-Productivity Industries\n\nAdvanced economies have shifted from manufacturing to services (e.g., healthcare, education, public sector).\n\nServices typically have lower TFP growth, as they rely heavily on human labor and are difficult to automate.\n\nIncreased Regulation and Market Distortions\n\nGrowing regulations, tax policies, and political uncertainty increase inefficiencies, reducing overall productivity.\n\n\n\n\n\nWhile aggregate TFP growth may appear to be declining, there are huge disparities across sectors. Just as wealth concentration is increasing, TFP growth could also be highly concentrated within specific industries:\n\nAI and Automation\n\nAI-driven automation could drastically improve productivity, particularly in certain white-collar jobs.\n\nHowever, productivity gains may be highly concentrated in tech-driven industries, rather than the economy as a whole.\n\nSectoral Disparities in TFP Growth\n\nWithin-market concentration: A small number of firms (e.g., winners in the winner-takes-all structure) are experiencing high TFP growth, while others stagnate.\n\nCross-market concentration: Some sectors (AI, biotech) may experience massive productivity boosts, while others (traditional services, public sector) may stagnate.\n\nEnergy Revolution and New Technological Frontiers\n\nAdvances in nuclear fusion, renewables, and space-based energy production could dramatically shift TFP growth.\n\nHowever, large-scale adoption is slow due to political, regulatory, and financial constraints.\n\n\n\n\n\n\n\n\nIf the long-run TFP growth rate is declining, its stochastic trend may weaken, making it more likely to transition from a unit root (\\(I(1)\\)) process to a stationary (\\(I(0)\\)) process over time. This has important implications for empirical modeling and forecasting, particularly in cases where researchers assume a constant growth trend in TFP-driven models. To properly account for these shifts, researchers should:\n\nUse structural break tests (e.g., Bai-Perron) to detect shifts in TFP growth trends and assess whether different growth regimes exist.\n\nApply rolling unit root tests to examine whether TFP has moved from an \\(I(1)\\) process toward an \\(I(0)\\) process over time.\n\nIncorporate time-varying cointegration approaches to avoid biased estimations that assume stable relationships in economic growth models.\n\n\n\n\nTFP growth is not uniform across industries; while aggregate TFP growth may be declining, specific sectors are experiencing rapid technological advancements. Just as wealth and market concentration have increased, TFP gains are increasingly concentrated within high-tech and innovation-driven industries. The result is a growing disparity between leading-edge industries (AI, biotech, energy) and traditional sectors (public services, healthcare, education), which exhibit slower productivity improvements.\nAs sectoral divergence in productivity widens, traditional macroeconomic models that assume homogeneous TFP growth may require revision. Future research should focus on developing heterogeneous growth models that account for cross-sectoral differences, while also employing more dynamic econometric techniques to capture the evolving nature of TFP trends.",
    "crumbs": [
      "교육",
      "설명? 예측?",
      "Time-varying TFP"
    ]
  },
  {
    "objectID": "structure_tfp.html#introduction",
    "href": "structure_tfp.html#introduction",
    "title": "Time-varying TFP",
    "section": "",
    "text": "The growth rate of Total Factor Productivity (TFP) is not a fixed constant; rather, it evolves over time and may be subject to long-term decline. While mainstream macroeconomic models frequently assume a constant TFP growth rate, this assumption is increasingly being questioned. Several structural factors—diminishing returns to R&D, demographic constraints, and sectoral shifts towards lower-productivity industries—suggest that aggregate TFP growth may continue to slow. However, TFP growth is not uniform across sectors. Certain industries, such as AI, biotechnology, and advanced energy technologies, exhibit significantly higher productivity gains, leading to growing sectoral disparities in innovation.\nFrom an econometric perspective, a decline in TFP growth could alter its stationarity properties, weakening its stochastic trend and making it more likely to transition from an \\(I(1)\\) (non-stationary) stochastic process to an \\(I(0)\\) (stationary) stochastic process. If structural breaks exist in TFP growth, traditional time-series models based on constant-trend cointegration assumptions may fail. Thus, accurately modeling TFP trends requires dynamic econometric techniques that account for time-varying structural changes.",
    "crumbs": [
      "교육",
      "설명? 예측?",
      "Time-varying TFP"
    ]
  },
  {
    "objectID": "structure_tfp.html#main",
    "href": "structure_tfp.html#main",
    "title": "Time-varying TFP",
    "section": "",
    "text": "TFP represents technological progress that enhances economic output beyond capital and labor inputs. In the Solow-Swan growth model, TFP follows an exponential trend, expressed as:\n\\[\nA_t = A_0 e^{gt} \\epsilon_t\n\\]\nwhere \\(g\\) is the growth rate and \\(\\epsilon_t\\) captures short-term fluctuations. This implies that TFP is not strictly stationary; rather, its log form follows a unit root process with a deterministic trend:\n\\[\n\\log A_t = g t + u_t, \\quad u_t \\sim I(0)\n\\]\nThus, mainstream macroeconomic models do not assume TFP is stationary in levels, but often assume its deviation from trend is stationary.\n\n\n\nEmpirical studies show:\n\nLong-Run TFP behaves as a non-stationary process (\\(I(1)\\)), meaning that it exhibits persistent growth and shocks that do not revert over time.\nShort-Run TFP Shocks (deviations from trend) are often stationary (\\(I(0)\\)) process.\n\nIf GDP and capital stock are also \\(I(1)\\), Solow’s production function suggests they should be cointegrated:\n\\[\n\\log Y_t - \\alpha \\log K_t - (1-\\alpha) \\log L_t - g t = u_t, \\quad u_t \\sim I(0)\n\\]\nThus, GDP, capital, and labor may be cointegrated due to the common trend driven by TFP.\n\n\n\nMany studies suggest that TFP growth rates have declined since the mid-20th century.\nU.S. TFP Growth Trends (Annual Growth Rates):\n\n1950s–1970s: ~2.5% per year\n\n1980s–1990s: ~1.5% per year\n\n2000s–2020s: ~0.5%-1.0% per year\n\nThe productivity slowdown hypothesis suggests that long-term economic growth potential is becoming weaker than before.\n\n\n\nSeveral key factors contribute to the observed decline in TFP growth:\n\nThe Low-Hanging Fruit Hypothesis (Gordon, 2016)\n\nPast technological revolutions (electricity, automobiles, antibiotics) were one-time events that provided massive boosts.\n\nRecent innovations (social media, fintech, AI) may not have the same economy-wide productivity effects.\n\nDeclining R&D Efficiency (Bloom et al., 2020)\n\n“Ideas are getting harder to find.”\n\nR&D investment has increased, but the number of researchers needed to sustain TFP growth has risen exponentially.\n\nDemographic Constraints and Human Capital Decay\n\nAging populations in advanced economies reduce labor force dynamism and innovation.\n\nEducation quality improvements may have plateaued, reducing skilled labor supply.\n\nSectoral Shift to Low-Productivity Industries\n\nAdvanced economies have shifted from manufacturing to services (e.g., healthcare, education, public sector).\n\nServices typically have lower TFP growth, as they rely heavily on human labor and are difficult to automate.\n\nIncreased Regulation and Market Distortions\n\nGrowing regulations, tax policies, and political uncertainty increase inefficiencies, reducing overall productivity.\n\n\n\n\n\nWhile aggregate TFP growth may appear to be declining, there are huge disparities across sectors. Just as wealth concentration is increasing, TFP growth could also be highly concentrated within specific industries:\n\nAI and Automation\n\nAI-driven automation could drastically improve productivity, particularly in certain white-collar jobs.\n\nHowever, productivity gains may be highly concentrated in tech-driven industries, rather than the economy as a whole.\n\nSectoral Disparities in TFP Growth\n\nWithin-market concentration: A small number of firms (e.g., winners in the winner-takes-all structure) are experiencing high TFP growth, while others stagnate.\n\nCross-market concentration: Some sectors (AI, biotech) may experience massive productivity boosts, while others (traditional services, public sector) may stagnate.\n\nEnergy Revolution and New Technological Frontiers\n\nAdvances in nuclear fusion, renewables, and space-based energy production could dramatically shift TFP growth.\n\nHowever, large-scale adoption is slow due to political, regulatory, and financial constraints.",
    "crumbs": [
      "교육",
      "설명? 예측?",
      "Time-varying TFP"
    ]
  },
  {
    "objectID": "structure_tfp.html#conclusion",
    "href": "structure_tfp.html#conclusion",
    "title": "Time-varying TFP",
    "section": "",
    "text": "If the long-run TFP growth rate is declining, its stochastic trend may weaken, making it more likely to transition from a unit root (\\(I(1)\\)) process to a stationary (\\(I(0)\\)) process over time. This has important implications for empirical modeling and forecasting, particularly in cases where researchers assume a constant growth trend in TFP-driven models. To properly account for these shifts, researchers should:\n\nUse structural break tests (e.g., Bai-Perron) to detect shifts in TFP growth trends and assess whether different growth regimes exist.\n\nApply rolling unit root tests to examine whether TFP has moved from an \\(I(1)\\) process toward an \\(I(0)\\) process over time.\n\nIncorporate time-varying cointegration approaches to avoid biased estimations that assume stable relationships in economic growth models.\n\n\n\n\nTFP growth is not uniform across industries; while aggregate TFP growth may be declining, specific sectors are experiencing rapid technological advancements. Just as wealth and market concentration have increased, TFP gains are increasingly concentrated within high-tech and innovation-driven industries. The result is a growing disparity between leading-edge industries (AI, biotech, energy) and traditional sectors (public services, healthcare, education), which exhibit slower productivity improvements.\nAs sectoral divergence in productivity widens, traditional macroeconomic models that assume homogeneous TFP growth may require revision. Future research should focus on developing heterogeneous growth models that account for cross-sectoral differences, while also employing more dynamic econometric techniques to capture the evolving nature of TFP trends.",
    "crumbs": [
      "교육",
      "설명? 예측?",
      "Time-varying TFP"
    ]
  },
  {
    "objectID": "structure_tfp.html#integration-order",
    "href": "structure_tfp.html#integration-order",
    "title": "Time-varying TFP",
    "section": "Integration Order",
    "text": "Integration Order\nThe integration order of a time series determines how many times it must be differenced to become stationary. A series is:\n\n\\(I(0)\\) (stationary) process if it has a constant mean, variance, and autocovariance.\n\\(I(1)\\) (unit root) process if it is non-stationary but becomes stationary after first differencing.\n\\(I(d)\\) process if it requires \\(d\\) differences to become stationary.\n\nEconomic time series such as GDP, money supply, and asset prices often exhibit \\(I(1)\\) behavior, meaning they contain stochastic trends and require differencing to achieve stationarity.",
    "crumbs": [
      "교육",
      "설명? 예측?",
      "Time-varying TFP"
    ]
  },
  {
    "objectID": "structure_tfp.html#joint-covariance-stationarity-vs.-cointegration",
    "href": "structure_tfp.html#joint-covariance-stationarity-vs.-cointegration",
    "title": "Time-varying TFP",
    "section": "Joint Covariance Stationarity vs. Cointegration",
    "text": "Joint Covariance Stationarity vs. Cointegration\nJoint covariance stationarity applies when each time-series maintains a constant mean, variance, and autocovariance over time. Cointegration, on the other hand, describes cases where two or more non-stationary \\(I(1)\\) time-series share a long-term equilibrium, forming a stationary linear combination.\n\nJoint Covariance Stationary Series (Weak-Sense Stationarity)\nA set of time series \\(X_t\\) and \\(Y_t\\) are jointly covariance stationary if they satisfy:\n\nConstant Mean: \\(E[X_t] = \\mu_X\\), \\(E[Y_t] = \\mu_Y\\) for all \\(t\\).\nConstant Variance: \\(Var(X_t)\\) and \\(Var(Y_t)\\) do not change over time.\nAutocovariance Depends Only on Lag: \\(Cov(X_t, X_{t-h})\\) and \\(Cov(Y_t, Y_{t-h})\\) depend only on the lag \\(h\\), not on \\(t\\).\n\nIf two time series are both weakly stationary, then any linear combination of them is also stationary.\n\n\nCointegrated Series\nA set of time series \\(X_t\\) and \\(Y_t\\) are cointegrated if:\n\nEach series is \\(I(1)\\) (non-stationary) process.\nA linear combination exists that is \\(I(0)\\) (stationary) process:\n\\[\n\\beta_1 X_t + \\beta_2 Y_t = u_t\n\\]\n\nwhere \\(u_t\\) is \\(I(0)\\) (stationary) process.\nThus, even though individual variables are non-stationary, their linear combination is stationary, implying a long-run equilibrium relationship.\n\n\nThe Relationship Between Covariance Stationarity and Cointegration\n\n\n\n\n\n\n\n\nProperty\nJoint Covariance Stationary Series\nCointegrated Series\n\n\n\n\nStationarity\nEach series is stationary (I(0))\nEach series is non-stationary (I(1)), but a linear combination is stationary\n\n\nUnit Root \\(I(d)\\)\nI(0) for each series\nI(1) for each series, but a specific linear combination is I(0)\n\n\nMean & Variance Stability\nMean & variance are constant over time\nIndividual series do not have stable mean & variance, but the combination does\n\n\nLong-run Relationship\nNo long-term relationship constraint\nA long-run equilibrium relationship exists\n\n\n\n\nCointegrated Series Can Be Transformed into Covariance Stationary Series\nIf \\(X_t\\) and \\(Y_t\\) are cointegrated, their first differences \\(\\Delta X_t\\), \\(\\Delta Y_t\\) (or the residual \\(u_t\\)) are stationary.\n\nThe error correction term \\(u_t\\) is stationary \\(I(0)\\) process, meaning it satisfies the covariance stationarity conditions.\n\n\n\nJoint Covariance Stationary Series Are Not Cointegrated\nIf \\(X_t\\) and \\(Y_t\\) are both already \\(I(0)\\) (stationary), then any linear combination of them is also stationary.\n\nThey cannot be cointegrated because cointegration only applies to non-stationary (\\(I(1)\\)) series.\nIf all series are already covariance stationary, testing for cointegration is unnecessary.",
    "crumbs": [
      "교육",
      "설명? 예측?",
      "Time-varying TFP"
    ]
  },
  {
    "objectID": "structure_tfp.html#implications-for-empirical-analysis",
    "href": "structure_tfp.html#implications-for-empirical-analysis",
    "title": "Time-varying TFP",
    "section": "Implications for Empirical Analysis",
    "text": "Implications for Empirical Analysis\n\nBefore testing for cointegration, check for stationarity. If all series are \\(I(0)\\), cointegration does not apply.\nIf series are cointegrated, their residuals (error correction term) should be covariance stationary.\nMany macroeconomic variables (e.g., GDP & consumption, money supply & inflation) are cointegrated rather than purely covariance stationary.",
    "crumbs": [
      "교육",
      "설명? 예측?",
      "Time-varying TFP"
    ]
  },
  {
    "objectID": "pricing_equal.html",
    "href": "pricing_equal.html",
    "title": "Market Price of Education",
    "section": "",
    "text": "By applying the EMM-based asset pricing approach, this study quantifies the impact of structural inequalities on the valuation of ostensibly equal opportunities. The results emphasize the importance of considering underlying economic disparities when evaluating the fairness and effectiveness of qualification standards in society.",
    "crumbs": [
      "교육",
      "분열",
      "Market Price of Education"
    ]
  },
  {
    "objectID": "pricing_equal.html#conclusion",
    "href": "pricing_equal.html#conclusion",
    "title": "Market Price of Education",
    "section": "",
    "text": "By applying the EMM-based asset pricing approach, this study quantifies the impact of structural inequalities on the valuation of ostensibly equal opportunities. The results emphasize the importance of considering underlying economic disparities when evaluating the fairness and effectiveness of qualification standards in society.",
    "crumbs": [
      "교육",
      "분열",
      "Market Price of Education"
    ]
  },
  {
    "objectID": "pricing_equal.html#main",
    "href": "pricing_equal.html#main",
    "title": "Market Price of Education",
    "section": "Main",
    "text": "Main\n\nModel Scenario\nIn short, we consider a society divided into two distinct classes:\n\nProletariat (P): Individuals with access to general opportunities.\nCapitalist (C): Individuals with access to both general and exclusive special opportunities.\n\nInternally, both classes operate within perfectly competitive markets that adhere to the no-arbitrage principle. However, due to capital constraints, members of the P class cannot access the special opportunities available to the C class, rendering these opportunities unattainable. Consequently, while the absolute value of special opportunities is equal to or greater than that of general opportunities, the P class cannot exploit potential arbitrage opportunities due to these constraints. Both classes have offspring who inherit the economic outcomes of their parents’ investments. To claim these inherited assets, all offspring must meet a minimum qualification standard (e.g., educational credentials), analogous to the strike price (\\(K\\)) in option pricing. Until this qualification is met, the offspring hold a call option on their parents’ assets.\nIn detail, consider two distinct social classes: the proletariat (P), representing working-class individuals, and the capitalist class (C), representing wealthy individuals. These two classes face fundamentally different opportunity sets due to deep-seated structural inequalities. Such societal barriers, from both democratic and utilitarian viewpoints, constitute a significant social inefficiency.\nTo analyze the structural origins of this inefficiency, we apply the concept of Limits to Arbitrage Theory, which suggests markets are not always fully efficient. This inefficiency arises because of various constraints, such as behavioral biases, risk constraints, and notably, capital constraints. Capital constraints imply that significant capital is required to engage in arbitrage, effectively isolating classes economically. We assume, therefore, that the proletariat and capitalist classes are economically segregated due to these capital constraints.\nWithin each class, economic opportunities exist in a perfectly competitive market, satisfying the no-arbitrage condition internally. Members of the proletariat (P) freely select from a common set of general opportunities available to them. Conversely, members of the capitalist class (C) not only share these general opportunities but also exclusively access additional “special opportunities.” From the viewpoint of proletariat class members, these special opportunities represent unattainable benefits (“grapes beyond reach”), carrying higher or equal absolute value compared to general opportunities.\nTheoretically, if members of the capitalist class could short-sell general opportunities and simultaneously long-position special opportunities, they could realize arbitrage profits. However, due to aforementioned capital constraints (“limits to arbitrage”), such arbitrage trading is practically impossible within this model.\nBoth classes have offspring who inherit the economic results (absolute asset values) of their parents’ investment choices. To claim these inherited assets, children from both classes must meet identical minimum qualification standards (e.g., university diplomas, basic educational credentials). Although the qualification standard is identical for both, outcomes differ significantly due to inherited assets. For instance, a business administration graduate from the capitalist class inherits and manages substantial capital (businesses), while an identically qualified individual from the proletariat class works as an employee, earning wages in capitalist-owned enterprises.\nThe minimum qualification standard can be understood as a fixed barrier or strike price (K) of a call option. Until the qualification requirement is fulfilled, the children effectively hold call options on their parents’ assets. This model aims to quantify the absolute value of these call options for each class, reflecting the inherited economic outcomes accessible to the offspring.\n\n\nEMM-based Call Option Valuation Theory\nAssume a simple 1 period setting with only two possible states (e.g. good or bad). Under the Equivalent Martingale Measure (EMM), the fair market value of a call option at time 0, denoted as \\(\\hat{f}_0\\), must satisfy:\n\\[\\hat{f_0}\\cdot R = E^q[f_1]\\] where \\(f_1=max(S_0 \\cdot U-K,0)\\)\n\\[S_0\\cdot R=E^q[S_1]\\] where \\(S_0\\) = the current price of risky underlying asset\nFrom these two equations, we have the EMM or the state price density for good state \\(q\\) as: \\[\nq = \\frac{R - D}{U - D}\n\\]\nThus, the present fair value of the call option is given by:\n\\[\n\\hat{f}_0 = \\frac{1}{R}\\left[ \\frac{R - D}{U - D}(S_0 \\cdot U - K) + \\frac{U - R}{U - D} \\cdot 0 \\right] = \\frac{(R - D)(U - 1)}{R(U - D)}\n\\]\nassuming that\n\nInitial Asset Price (normalized): \\(S_0=1\\)\n\nStrike Price (qualification threshold): \\(K=1\\)\n\nMaturity: 18 years (quarterly steps = 72 periods)\n\nThis formula indicates a clear proportional relationship for estimating current fair values representing inherited qualification-based claims for each class over a 18-year maturity period. While this valuation could be approximated using continuous distributions (e.g., Black-Scholes under symmetric assumptions \\(U \\cdot D=1\\)), our discrete binomial model allows straightforward interpretation without loss of economic intuition.\n\n\nEmpirical Analysis\nWe employ distinct underlying assets for each class. Using the historical dataset (Q1 1982–Q4 2019, 152 quarterly observations), we estimated the parameters and their associated fair values of call options for each class.\nFor Proletariat (P) Class Children:\n\nRisky Asset: US Median usual weekly Real earnings (LES1252881600Q)\n\nRisk-Free Asset: US Real GDP per capita (A939RX0Q048SBEA)\n\nParameters:\n\n\\(U_p\\):= 75th percentile growth of wage (risky asset)\n\n\\(D_p\\):= 25th percentile growth of wage\n\n\\(R_p\\):= Median growth rate of Real GDP per capita\n\n\nFor Capitalist (C) Class Children:\n\nRisky Asset: S&P 500 equity (SPX)\n\nRisk-Free Asset: US 10-Year Treasury Bond\nParameters:\n\n\\(U_c\\):= 75th percentile quarterly growth of the equity index\n\n\\(D_c\\):= 25th percentile quarterly growth of the equity index\n\n\\(R_c\\):= Median of quarterly US 10-Year Treasury Bond Yield (DGS10)\n\n\nEmpirical Results:\n\nFor Proletariat (P) class children, \\(\\hat{P}_0=?\\)\n\nFor Capitalist (C) class children, \\(\\hat{C}_0=?\\)\n\n\n\nCode\nimport yfinance as yf\nimport pandas_datareader.data as web\nimport pandas as pd\nimport numpy as np\n\n# 데이터 기간 설정\nstart_date = '1982-01-01'\nend_date = '2019-12-31'\n\n# S&P 500 데이터 가져오기\nsp500 = yf.download(\"^GSPC\", start=start_date, end=end_date, interval=\"1d\")\nsp500_q = sp500['Close'].resample('QE').last()  # 분기별 종가 데이터\n\n# FRED 데이터 가져오기\nus_median_weekly_earnings = web.DataReader('LES1252881600Q', 'fred', start_date, end_date)\nus_real_gdp_per_capita = web.DataReader('A939RX0Q048SBEA', 'fred', start_date, end_date)\nus_10yr_treasury_yield = web.DataReader('DGS10', 'fred', start_date, end_date)\n\n# 인덱스를 맞추기 위해 분기별로 재샘플링\nus_median_weekly_earnings = us_median_weekly_earnings.resample('QE').last()\nus_real_gdp_per_capita = us_real_gdp_per_capita.resample('QE').last()\nus_10yr_treasury_yield = us_10yr_treasury_yield.resample('QE').last()\n\n# 데이터프레임으로 변환\ndata = pd.DataFrame({\n    'SP500': sp500_q.squeeze(),\n    'Median_Weekly_Earnings': us_median_weekly_earnings['LES1252881600Q'].squeeze(),\n    'Real_GDP_per_Capita': us_real_gdp_per_capita['A939RX0Q048SBEA'].squeeze(),\n    '10yr_Treasury_Yield': us_10yr_treasury_yield['DGS10'].squeeze()\n}, index=sp500_q.index)\n\n# 결측값 제거\ndata.dropna(inplace=True)\n\n# 수익률 계산\ndata['SP500_Return'] = data['SP500'].pct_change()\ndata['Earnings_Growth'] = data['Median_Weekly_Earnings'].pct_change()\ndata['GDP_Growth'] = data['Real_GDP_per_Capita'].pct_change()\n\n# 통계치 계산\nU_p = data['Earnings_Growth'].quantile(0.75)+1\nD_p = data['Earnings_Growth'].quantile(0.25)+1\nR_p = data['GDP_Growth'].median()+1\n\nU_c = data['SP500_Return'].quantile(0.75)+1\nD_c = data['SP500_Return'].quantile(0.25)+1\nR_c = data['10yr_Treasury_Yield'].median()\nR_c = R_c / 100 +1\n\nprint(f\"Proletariat Class Children Parameters:\")\nprint(f\"U_p: {U_p:.2f}\")\nprint(f\"D_p: {D_p:.2f}\")\nprint(f\"R_p: {R_p:.2f}\")\n\nprint(f\"\\nCapitalist Class Children Parameters:\")\nprint(f\"U_c: {U_c:.2f}\")\nprint(f\"D_c: {D_c:.2f}\")\nprint(f\"R_c: {R_c:.2f}\")\n\n\n# Binomial Option Pricing Model\ndef binomial_option_pricing(K, S_0, T, U, D, R, dt):\n    \"\"\"\n    K: Strike price\n    S_0: Initial stock price\n    T: Time to maturity (in years)\n    U: Up factor\n    D: Down factor\n    R: Risk-free rate\n    dt: Number of steps for each year\n    \"\"\"\n    n = T*dt # Number of steps in the binomial tree\n    q = (R - D) / (U - D)\n\n    # Initialize option values at maturity\n    option_values = np.zeros((n + 1, 1))\n    for i in range(n + 1):\n        ST = S_0 * (U ** i) * (D ** (n - i))\n        option_values[i] = max(0, ST - K)\n\n    # Backward recursion for option values\n    for j in range(n - 1, -1, -1):\n        for i in range(j + 1):\n            option_values[i] = (q * option_values[i + 1] + (1 - q) * option_values[i]) / R\n\n    return option_values[0, 0]\n\n\n# Parameters\nK = 1  # Strike price\nS_0 = 1  # Initial stock price\nT = 18  # Time to maturity (18 years)\ndt = 4 # Number of steps for each year\n\n# Calculate option prices\noption_price_proletariat = binomial_option_pricing(K, S_0, T, U_p, D_p, R_p, dt)\noption_price_capitalist = binomial_option_pricing(K, S_0, T, U_c, D_c, R_c, dt)\n\nprint(f\"\\nFair price of Call Option, held by Proletariat Class Children:\\n {option_price_proletariat:.2f}\")\nprint(f\"Fair price of Call Option, held by Capitalist Class Children:\\n {option_price_capitalist:.2f}\")\n\n\nYF.download() has changed argument auto_adjust default to True\n\n\n[*********************100%***********************]  1 of 1 completed\n\n\nProletariat Class Children Parameters:\nU_p: 1.01\nD_p: 1.00\nR_p: 1.01\n\nCapitalist Class Children Parameters:\nU_c: 1.07\nD_c: 0.99\nR_c: 1.05\n\nFair price of Call Option, held by Proletariat Class Children:\n 0.30\nFair price of Call Option, held by Capitalist Class Children:\n 0.97\n\n\n\n\nDiscussion\nThis model clarifies the stark inequality underlying “ostensibly equal” qualification standards. Although formally identical, the call options’ absolute valuations significantly diverge, reflecting distinct economic inheritances accessible to each class. This disparity highlights structural inefficiencies and deep-rooted inequalities, persisting despite nominally identical qualification standards.\nUltimately, this analysis underscores how asset-based class differentiation profoundly impacts the perceived and realized absolute value of educational and economic opportunities, illuminating critical implications for economic policy, educational equity, and social justice frameworks.",
    "crumbs": [
      "교육",
      "분열",
      "Market Price of Education"
    ]
  },
  {
    "objectID": "pricing_equal.html#introduction",
    "href": "pricing_equal.html#introduction",
    "title": "Market Price of Education",
    "section": "Introduction",
    "text": "Introduction\nIn societies characterized by pronounced economic stratification, opportunities presented as equal often yield disparate outcomes across different social strata. This disparity arises from inherent structural inefficiencies that restrict access to certain opportunities based on class. The Limits to Arbitrage Theory posits that market inefficiencies can persist due to various constraints, including capital limitations, preventing rational traders from correcting mispricings (Shleifer and Vishny 1997). This paper explores how such constraints contribute to the unequal valuation of opportunities between the proletariat (P) and capitalist (C) classes.",
    "crumbs": [
      "교육",
      "분열",
      "Market Price of Education"
    ]
  },
  {
    "objectID": "pricing_equal.html#literature-review",
    "href": "pricing_equal.html#literature-review",
    "title": "Market Price of Education",
    "section": "Literature Review",
    "text": "Literature Review\nThe theoretical foundation relies primarily on the limits to arbitrage theory, initially articulated by Shleifer and Vishny (Shleifer and Vishny 1997). Their seminal work shows how market inefficiencies persist due to practical constraints, particularly capital constraints, restricting the ability of arbitrageurs to exploit and correct mispricings. These constraints arise from significant capital requirements that effectively segregate participants into distinct economic spheres, as emphasized by subsequent studies on financially constrained arbitrageurs (Gromb and Vayanos 2002; Xiong 2001).\nGeanakoplos’ research introduces the leverage cycle, which explains how fluctuations in leverage and capital availability perpetuate systemic inequality and financial instability (Geanakoplos 2010). Complementary studies, such as that by Gromb and Vayanos, also demonstrate the welfare implications of constrained arbitrageurs operating under capital limitations, further exacerbating persistent inequality (Gromb and Vayanos 2002). Moreover, Barberis and Thaler’s comprehensive survey on behavioral finance indicates how cognitive and behavioral constraints exacerbate market inefficiencies, reinforcing the structural barriers that differentiate economic outcomes across social strata (Barberis and Thaler 2003).\nExtending beyond purely financial contexts, sociological and economic research provides additional perspectives on structural inequalities and opportunity valuation. Bourdieu’s concept of social reproduction underscores how cultural capital perpetuates socioeconomic inequalities across generations (Bourdieu 1973). Recent empirical evidence by Stansbury (Stansbury 2024) and findings by the Social Mobility Commission (Social Mobility Commission 2023) further demonstrate how economic capital inherited through generations shapes differential outcomes, even when individuals ostensibly possess identical qualifications.\nChetty et al. provide compelling evidence linking parental economic conditions to children’s educational outcomes and future earnings, strongly supporting the relevance of inherited economic positions in determining opportunity valuations (Chetty et al. 2014). Similarly, Piketty’s influential book highlights the crucial role inherited wealth plays in perpetuating structural economic disparities, emphasizing the critical nature of capital inheritance in shaping individuals’ economic trajectories and their access to opportunities (Piketty 2014).\nPolicy implications regarding these structural inequalities and efforts to enhance social mobility have been explored extensively by institutions such as the OECD. Their analyses suggest policy frameworks that might alleviate the persistent inequalities discussed herein (OECD 2018). Reeves’ concept of the “glass floor” further illustrates how affluent socioeconomic backgrounds systematically maintain class advantages despite equal or even lesser merit-based qualifications (Reeves 2017).\nCollectively, these studies underline a coherent narrative: ostensibly equal opportunities often conceal significant disparities rooted in inherited structural inequalities, persistent capital constraints, and behavioral limitations to arbitrage. Our model complements this literature by quantitatively evaluating how these structural factors systematically influence the absolute value of identical qualification standards across distinct socioeconomic groups.",
    "crumbs": [
      "교육",
      "분열",
      "Market Price of Education"
    ]
  },
  {
    "objectID": "market_incomplete.html",
    "href": "market_incomplete.html",
    "title": "Incomplete Markets",
    "section": "",
    "text": "Neoclassical economics emphasizes that rational individuals base decisions primarily on long-term trends rather than short-term fluctuations. In this view, rational agents with forward-looking expectations base decisions on an infinite-horizon optimization, meaning they will not invest in assets or projects that have a declining long-term trajectory or negative net present value. Market prices, in turn, are thought to reflect these rational expectations about future fundamentals. This philosophy is exemplified in models of Milton Friedman and his successors, where individuals smooth consumption over time and focus on permanent income, and in modern macroeconomic models that assume agents plan for the long run. The neoclassical mindset links naturally with Friedman’s monetarism, which asserts that steady, rules-based growth of the money supply yields better outcomes than reactive fine-tuning. Both frameworks assume that people anticipate the future well, so erratic policy shifts mainly lead to price changes rather than lasting gains in output or employment. Representative models include Friedman’s Permanent Income Hypothesis, Bewley’s incomplete-market model, Aiyagari’s general equilibrium model with heterogeneous agents, and the speculative asset-pricing model by Harrison & Kreps (Friedman 1957; Bewley 1986; Aiyagari 1994; Harrison and Kreps 1978).\nFriedman’s monetarism closely aligns with neoclassical thinking, promoting predictable, rules-based monetary policy to ensure market stability and long-term neutrality of money. In contrast, Keynesian and New Keynesian frameworks highlight the importance of short-term interventions due to market frictions such as price stickiness. New Keynesians agree that people are forward-looking, but they highlight that wages and prices can be “sticky” (slow to adjust), which prevents markets from clearing quickly. As a result, even rational agents may be temporarily unable to reach optimal outcomes, and monetary policy can have powerful real effects in the short term. For example, if prices cannot adjust immediately, an unexpected cut in interest rates may boost real spending and output rather than just raising prices. New Keynesian models therefore rationalize active stabilization policy – they contend that without it, the economy can suffer prolonged recessions or unemployment that a well-timed policy intervention could ameliorate. This stands in contrast to the monetarist/neoclassical view that markets self-correct relatively quickly. Despite these differences, both schools provide valuable insight: neoclassical and monetarist models offer clarity about long-run tendencies and private sector behavior under rational expectations, while Keynesian models stress the importance of short-run dynamics and coordination failures. The analysis in this paper bridges these perspectives by using theoretical models to examine how market imperfections – such as incomplete markets and heterogeneous expectations – modify the standard predictions. Although the models are abstract, each provides important long-term policy guidance. Understanding their lessons can help policymakers design monetary strategies that foster stability without creating new problems in the process.",
    "crumbs": [
      "교육",
      "분열",
      "Incomplete Markets"
    ]
  },
  {
    "objectID": "market_incomplete.html#introduction",
    "href": "market_incomplete.html#introduction",
    "title": "Incomplete Markets",
    "section": "",
    "text": "Neoclassical economics emphasizes that rational individuals base decisions primarily on long-term trends rather than short-term fluctuations. In this view, rational agents with forward-looking expectations base decisions on an infinite-horizon optimization, meaning they will not invest in assets or projects that have a declining long-term trajectory or negative net present value. Market prices, in turn, are thought to reflect these rational expectations about future fundamentals. This philosophy is exemplified in models of Milton Friedman and his successors, where individuals smooth consumption over time and focus on permanent income, and in modern macroeconomic models that assume agents plan for the long run. The neoclassical mindset links naturally with Friedman’s monetarism, which asserts that steady, rules-based growth of the money supply yields better outcomes than reactive fine-tuning. Both frameworks assume that people anticipate the future well, so erratic policy shifts mainly lead to price changes rather than lasting gains in output or employment. Representative models include Friedman’s Permanent Income Hypothesis, Bewley’s incomplete-market model, Aiyagari’s general equilibrium model with heterogeneous agents, and the speculative asset-pricing model by Harrison & Kreps (Friedman 1957; Bewley 1986; Aiyagari 1994; Harrison and Kreps 1978).\nFriedman’s monetarism closely aligns with neoclassical thinking, promoting predictable, rules-based monetary policy to ensure market stability and long-term neutrality of money. In contrast, Keynesian and New Keynesian frameworks highlight the importance of short-term interventions due to market frictions such as price stickiness. New Keynesians agree that people are forward-looking, but they highlight that wages and prices can be “sticky” (slow to adjust), which prevents markets from clearing quickly. As a result, even rational agents may be temporarily unable to reach optimal outcomes, and monetary policy can have powerful real effects in the short term. For example, if prices cannot adjust immediately, an unexpected cut in interest rates may boost real spending and output rather than just raising prices. New Keynesian models therefore rationalize active stabilization policy – they contend that without it, the economy can suffer prolonged recessions or unemployment that a well-timed policy intervention could ameliorate. This stands in contrast to the monetarist/neoclassical view that markets self-correct relatively quickly. Despite these differences, both schools provide valuable insight: neoclassical and monetarist models offer clarity about long-run tendencies and private sector behavior under rational expectations, while Keynesian models stress the importance of short-run dynamics and coordination failures. The analysis in this paper bridges these perspectives by using theoretical models to examine how market imperfections – such as incomplete markets and heterogeneous expectations – modify the standard predictions. Although the models are abstract, each provides important long-term policy guidance. Understanding their lessons can help policymakers design monetary strategies that foster stability without creating new problems in the process.",
    "crumbs": [
      "교육",
      "분열",
      "Incomplete Markets"
    ]
  },
  {
    "objectID": "market_incomplete.html#core-assumptions-of-neoclassical-economics-and-monetarism",
    "href": "market_incomplete.html#core-assumptions-of-neoclassical-economics-and-monetarism",
    "title": "Incomplete Markets",
    "section": "Core Assumptions of Neoclassical Economics and Monetarism",
    "text": "Core Assumptions of Neoclassical Economics and Monetarism\nThree core assumptions underpin the neoclassical and monetarist perspective in macroeconomics:\n\nRational expectations and infinite-horizon optimization: Individuals and firms are assumed to form expectations about the future in a rational way (using all available information) and to optimize their decisions over an infinite time horizon. In practice, this means economic agents base current consumption, saving, and investment on the expected present value of long-term outcomes, rather than overreacting to short-lived changes. They anticipate the future effects of policies, so systematic policy actions are largely already “priced in” to decisions. This assumption was emphasized by the new classical economists who followed Friedman, such as Robert Lucas, in arguing that only unexpected policy moves affect real behavior in the short run. Under rational expectations, people won’t persistently spend windfalls or chase assets whose fundamentals don’t justify their price, since they foresee the eventual reversion to fundamental value.\nMarket clearing in the long run (flexible prices): Neoclassical models typically assume that prices of goods, services, and factors adjust to equilibrate supply and demand, at least in the long run. While short-term frictions can occur, the long-run default is an economy at full employment with resources fully utilized. Any deviations (recessions or booms) are seen as temporary, provided policy does not introduce long-term distortions. This view contrasts with Keynesian models where wages or prices might remain out of equilibrium for an extended period. The neoclassical stance is that given enough time, economic forces will push the economy back to its potential output with stable growth. Monetarists, too, believed that “markets naturally move toward a stable center” in the absence of big shocks. Thus, they argue against aggressive intervention that attempts to exploit short-run trade-offs (like pumping up output at the cost of higher inflation), because eventually prices adjust and only inflation remains.\nNeutrality of money regarding real economic outcomes in the long run:(Lucas Jr 1972; Friedman 1968). A cornerstone of monetarism and neoclassical thought is that changes in the money supply only have transient effects on real variables (output, employment) and no effect in the long run. In the long run, an exogenous increase in the money stock is reflected in higher nominal prices and wages, but real consumption, investment, and output return to their original path. In other words, money is “neutral” with respect to real economic activity once prices have fully adjusted. Most economists agree that this long-run neutrality holds approximately true in practice – doubling the money supply eventually doubles the price level – and monetarists place great importance on it. This assumption underlies the monetarist recommendation to avoid monetary surprises: any attempt to permanently boost employment by printing money will just create inflation once people’s expectations catch up. Rational agents, thinking in an infinite-horizon framework, will not be tricked for long; they come to expect higher inflation, negating any output gains. Monetary policy, therefore, is seen primarily as a tool for controlling inflation and nominal variables, not as a way to engineer long-term higher growth.\n\nThese core assumptions shape the policy mindset in the neoclassical/monetarist framework. If agents are highly forward-looking and markets tend to clear, discretionary stabilization policy has limited power – it might only cause short-term blips or even destabilize expectations. Instead, maintaining credible, consistent policy (such as a steady money growth rule or inflation target) is viewed as the optimal approach for long-run welfare. In the next sections, we examine how relaxing some of these assumptions – by introducing incomplete markets, borrowing constraints, or heterogeneous beliefs – changes the conclusions and policy implications.",
    "crumbs": [
      "교육",
      "분열",
      "Incomplete Markets"
    ]
  },
  {
    "objectID": "market_incomplete.html#complete-vs.-incomplete-markets",
    "href": "market_incomplete.html#complete-vs.-incomplete-markets",
    "title": "Incomplete Markets",
    "section": "Complete vs. Incomplete Markets",
    "text": "Complete vs. Incomplete Markets\nComplete markets allow full insurance against risks and unlimited borrowing, resulting in smooth consumption aligned with permanent income. Incomplete markets, however, feature borrowing constraints and uninsurable shocks, creating heterogeneity and precautionary savings, leading to wealth disparities and more volatile consumption patterns (Aiyagari 1994; Bewley 1986). Thus, one fundamental way real economies depart from the idealized benchmark is that markets are incomplete. In a complete market environment, individuals can fully insure against uncertainties and can borrow or save freely at a given interest rate. In that ideal case, people smooth their consumption over time nearly perfectly. Consumption ends up being much less volatile than income, because during bad times individuals can borrow or draw on savings, and during good times they can save extra income for the future. In fact, theory predicts that with complete markets, consumption at any point reflects an individual’s permanent income (the expected long-term average income), not the transitory ups and downs of current income. A direct implication is that temporary policy measures (like one-time stimulus checks or short-lived tax cuts) would have only a small effect on consumption—because rational consumers know such windfalls are transitory, they prefer to save a large portion of them, aiming to maintain a stable consumption path. In a complete market, households effectively pool risks and smooth out idiosyncratic shocks; as a result, their spending is steady and mainly influenced by changes in expected lifetime resources rather than short-term liquidity fluctuations.\nBy contrast, in incomplete market settings, individuals do not have access to perfect insurance or unlimited borrowing. They face idiosyncratic income shocks (e.g., job loss, illness) that they must largely bear on their own. Additionally, they may encounter liquidity constraints or borrowing limits that prevent them from smoothing consumption fully. The models developed by Truman Bewley, S. Rao Aiyagari, and others formalize this situation. In these models, all agents are ex ante identical (they have the same preferences and potential income distribution), but they become ex post heterogeneous because each experiences different income shocks over time and they cannot completely insure against these shocks (Bewley 1986; Aiyagari 1994). Households thus engage in precautionary saving—they tend to save more when they have income, building a buffer of assets to self-insure against future bad draws. Consumption is no longer completely smooth; when a negative shock hits a liquidity-constrained household, it may have to cut consumption sharply because it cannot borrow easily. Conversely, a positive shock to a hand-to-mouth household leads to a spike in consumption if they were previously constrained.\nIncomplete markets therefore produce higher marginal propensities to consume out of transitory income changes—in other words, constrained people would spend a larger fraction of any temporary income windfall than they would under complete markets (Aiyagari 1994). This is consistent with empirical data showing many households, especially those with low wealth, quickly spend stimulus payments or bonuses, as they have unmet needs or debts to pay.\nAnother key difference is that incomplete markets generate a non-trivial distribution of wealth. Since each individual’s asset accumulation depends on their history of shocks and their precautionary saving motive, over time the economy develops inequality in wealth and consumption. Some agents will build up sizable precautionary balances (if they experience good income luck or have frugal preferences), while others might remain near the borrowing constraint with minimal savings. The wealth distribution in such models is typically highly skewed, capturing the fact that a small fraction of people may hold a large share of total assets – a feature very much in line with real-world data. By contrast, in a representative agent or complete markets model, distributional issues are either absent or of no consequence, since everyone effectively pools risks together. Incomplete markets thus bring distributional considerations to the forefront of macroeconomic analysis.\nFor policymakers, these differences mean that monetary and fiscal policy can have uneven effects across the population and can influence aggregate demand through channels that are muted in complete-market models. For example, an interest rate cut in an incomplete market setting might stimulate borrowing and spending for some agents, but for others it mainly reduces their interest income (if they are savers), potentially widening inequality. Likewise, a government stimulus targeted at liquidity-constrained households could yield a relatively large boost to consumption (due to their high propensity to consume out of additional income), whereas the same payment to a wealthier, fully insured household might just be saved. In summary, incomplete markets make the macroeconomy less “frictionless” and more sensitive to distribution and credit conditions. We next examine specific models that incorporate these features, to draw out their policy insights.\n\nFriedman Consumption Smoothing Model (Complete Markets)\nMilton Friedman’s model of consumption – known as the Permanent Income Hypothesis (PIH) – suggests consumption depends primarily on permanent rather than temporary income changes. Hence, monetary policy must focus on long-term credibility rather than short-run stimulus (Friedman 1957; Hall 1978). This is a cornerstone of the neoclassical view on consumption behavior. Friedman proposed that an individual’s consumption at any given time is determined not by current income alone, but by their permanent income, which is the expected long-term average income. Temporary fluctuations in income, according to this theory, have only a small effect on consumption because people use saving and borrowing to smooth out those fluctuations. In other words, households act like long-term planners: if they receive an unexpectedly high income this year, they will not dramatically raise their spending, understanding that the extra income may not last. Instead, they will save most of it (or pay down debt), spreading the benefit over future years. Conversely, if income dips briefly, they can draw on past savings or borrow to maintain their usual consumption level, expecting to repay when income recovers. This behavior leads to relatively stable consumption paths, as illustrated by Friedman’s famous observation that consumption is much smoother than the often volatile income streams that individuals experience year to year.\nFriedman’s model assumes that credit markets function well (people can borrow against future income) and that consumers are forward-looking and rational. Under these conditions, monetary and fiscal policy have limited ability to alter consumption unless they affect expected long-term income. For example, a one-time tax rebate or a temporarily lower interest rate might not stimulate much extra spending – consumers recognize that this is a short-term change. Indeed, a key takeaway of the permanent income theory is that policies which only increase current income without raising expected future income will mostly lead to higher saving rather than higher spending. Friedman contrasted this with the Keynesian view in which consumers have a high marginal propensity to consume out of current income (perhaps because they are myopic or liquidity-constrained). He argued that the Keynesian assumption was flawed in ignoring forward-looking behavior. Empirical puzzles of the mid-20th century (such as why consumption didn’t rise one-for-one with income gains from, say, war-time fiscal expansions) could be explained by PIH: people understood those income gains were temporary and saved much of them.\nIn policy terms, the Friedman consumption model supports a rather conservative use of demand management. A central bank that rapidly expands money or lowers interest rates might not trigger a large consumption boom unless people believe those actions will persist and raise their permanent income or wealth. Similarly, a government stimulus check will be partly saved if households treat it as a transitory windfall. An important implication is that discretionary policy “surprises” are not a reliable way to boost aggregate demand – rational agents will react mainly to the expected persistent components of policy. Monetarists like Friedman instead advocated rule-based policies (such as steadily growing the money supply at a fixed rate) to provide a stable environment for consumers and investors to plan. If policy is erratic, it could even be counterproductive: for instance, trying to exploit a short-run trade-off by pushing unemployment lower than its natural rate would just raise inflation expectations, with little lasting benefit to output (this is essentially Friedman’s adaptive expectations version of the Phillips Curve argument). In summary, Friedman’s complete-market consumption model underscores the importance of expectations and permanent income. It suggests that monetary policy should focus on the long-term nominal stability (controlling inflation) and avoid frequent discretionary shifts, because people will see through those shifts and adjust their saving behavior accordingly. It also implies that fiscal stabilization (e.g. stimulus payments) will be most effective when aimed at households likely to be liquidity-constrained, a point that becomes clearer once we consider incomplete market models.\n\n\nConsumption-Investment Trade-off under Liquidity Constraints\nLiquidity constraints disrupt optimal consumption-investment trade-offs. Constrained agents cannot invest sufficiently during downturns, weakening monetary policy’s effectiveness, particularly in stimulating investment or consumption among constrained households. For example, consider a household facing liquidity constraints and uncertain future income. An interest rate cut reduces the returns on their precautionary savings, forcing the household to reduce the buffer stock meant to protect against income fluctuations. Consequently, this household may have limited resources available for productive investments such as education or small business expansion. Younger or non-saver households facing liquidity constraints prioritize current consumption over future consumption due to diminishing marginal returns of future utility. Thus, when interest rates decrease, these households are more likely to immediately spend additional available funds rather than accumulate precautionary savings or invest in long-term productive assets. In contrast, a wealthier, unconstrained household may use lower interest rates to cheaply finance additional investment opportunities, potentially increasing their wealth relative to constrained households.\nA central theme in a standard intertemporal choice problem is the trade-off between consuming today and investing for tomorrow. In a frictionless world, consumers equate the marginal benefit of spending an extra dollar today with the marginal benefit of saving that dollar (investing it to spend later). This optimality condition (often called the Euler equation in macroeconomics) ensures that resources are allocated to their most valued use over time. However, in reality many households and firms face liquidity constraints or borrowing limits that prevent them from freely making this trade-off. Such constraints are a key imperfection that alters the impact of monetary policy and other shocks.\nWhen agents are liquidity-constrained, they cannot borrow as much as they would like against future income. This means in bad times they might want to maintain consumption or invest in opportunities (human capital, business expansion, etc.), but they simply lack the funds or credit access to do so. Consequently, current consumption may fall below the level that would be chosen under complete markets, and valuable investments might be foregone. For instance, a skilled worker who becomes unemployed may cut back sharply on consumption – not because their lifetime income prospects are shattered, but because in that moment they don’t have liquid assets or credit to smooth over the gap. Likewise, a small business might pass up a profitable investment because banks refuse credit due to the firm’s lack of collateral. These scenarios lead to a suboptimal allocation of resources over time, amplifying short-run fluctuations and causing longer-run consequences (lost growth from underinvestment, etc.).\nFrom a policy perspective, liquidity constraints mean that monetary policy may have an asymmetric effect. If the central bank raises interest rates, it generally cools off borrowing and spending – both unconstrained and constrained agents will cut back (the former by choice, the latter perhaps by necessity as credit becomes more expensive or scarce). But if the central bank lowers interest rates to stimulate the economy, those who are constrained might still be unable to borrow (banks may not lend to them even at low rates, if their balance sheet is weak or job uncertain) and thus cannot increase consumption or investment. In other words, there is a segment of the population for whom monetary easing doesn’t translate into more spending because they were not borrowing in the first place (they were at their borrowing limit). Instead, the stimulus might mainly induce already well-capitalized agents to borrow or invest more – which can have distributional effects.\nOn the other hand, consider fiscal policy: a transfer (like a stimulus check or unemployment benefit extension) given to a liquidity-constrained household is likely to be spent in large part, precisely because that household’s consumption was suppressed by lack of funds. Empirical evidence and incomplete-market models both find that households with little liquid wealth have high marginal propensities to consume (MPCs) out of such transfers. This contrasts with the near-zero MPC out of a transitory income increase for a fully smoothed consumer in Friedman’s framework. Therefore, liquidity constraints reconcile why Keynesian-style demand stimulus can work in practice (many people do spend most of an extra dollar if they were cash-strapped), even though Friedman’s theory might suggest they shouldn’t. Modern heterogeneous agent models incorporate this insight by showing that when a large fraction of consumers are hand-to-mouth or buffer-stock savers, aggregate consumption is sensitive to the distribution of income and cash-on-hand.\nFor investment, liquidity constraints imply that not all investment opportunities are realized, especially among smaller firms or entrepreneurs, if external finance is costly or unavailable. In a recession, even if the central bank slashes interest rates, banks may be risk-averse and tighten lending standards, so only the safest borrowers benefit from low rates. This can lead to a situation often described as “pushing on a string,” where monetary policy loses traction in stimulating additional private investment or consumption because the bottleneck is in credit access, not the cost of credit per se.\nIn summary, the consumption-investment trade-off under liquidity constraints highlights that market imperfections can dampen or distort the transmission of monetary policy. A perfectly rational, unconstrained agent might respond to lower interest rates by optimally borrowing and spending more (since the opportunity cost of funds is lower). But a constrained agent does nothing (they can’t borrow anyway), and an unconstrained wealthy agent might already be satiated in consumption and only shift their portfolio. These dynamics mean that in downturns, monetary policy might need support from fiscal measures that target constrained agents to be fully effective. It also means that policymakers should be aware of credit conditions and possibly use regulatory tools to ensure that rate cuts get passed through to borrowers. The general principle is that in the presence of liquidity constraints, short-run fluctuations can have long-run costs (foregone investment, lower human capital accumulation) and policies should aim to alleviate these constraints during bad times.\n\n\nBewley Model: Precautionary Savings in an Incomplete Market\n\nAssumes heterogeneous agents face idiosyncratic income shocks and borrowing limits.\nExogenous interest rates.\nGenerates wealth inequality through precautionary savings.\nHighlights the importance of social safety nets and targeted fiscal policies for macroeconomic stability (Bewley 1986).\n\nThe Bewley model (named after economist Truman Bewley) is a foundational framework for analyzing incomplete markets with heterogeneous agents. In Bewley’s setup, we consider a large number of infinitely-lived consumers who face idiosyncratic income shocks in each period. These shocks are uninsurable – there is no complete set of insurance markets for them – and consumers can only trade a single risk-free asset (such as a bond or money) to self-insure. Moreover, consumers face a borrowing limit (they cannot have debt beyond a certain level, often this ad hoc level is set to zero for simplicity). Despite all consumers having the same preferences and income process ex ante, the randomness of shocks makes them heterogeneous ex post in terms of their asset holdings and current income. This type of model is often called a heterogeneous agent incomplete-markets model, or simply a Bewley model, after the seminal work in (Bewley 1986). It has become a workhorse for understanding consumption, saving, and wealth distribution under uncertainty.\nIn the Bewley model, each consumer solves a consumption-saving problem: how much to consume today versus save as a buffer for future uncertainty. A typical finding is the emergence of a precautionary saving motive – people save not just for lifecycle reasons (retirement, etc.) but also to buffer against income risk. Those who experience good shocks build up assets, while those hit by bad shocks draw down assets or if they have none, they hit the borrowing constraint and their consumption drops. Over time, the model reaches an equilibrium where the cross-sectional distribution of wealth is stationary (in a statistical sense): some fraction of the population has high wealth, some has low wealth, with persistent inequality generated purely from idiosyncratic risk and saving behavior. This equilibrium typically features a fat-tailed distribution, meaning there are some very high-wealth individuals (who had a run of good shocks or especially strong saving discipline) and a significant mass of low-wealth individuals who might be frequently at the edge of the borrowing constraint. Quantitatively, such models can generate wealth concentration that qualitatively resembles that observed in real economies (though matching the extreme concentration in actual data often requires adding other elements like heterogeneity in earnings ability or rates of return).\nOne key aspect of the Bewley model is that the interest rate is treated as exogenous (or determined outside the model, say by a central bank or a global capital market). In other words, Bewley’s original formulation is a partial equilibrium analysis: it looks at an individual’s optimal saving given an interest rate, but does not necessarily determine that interest rate from within the model. This is akin to studying a small open economy where people can save or borrow at a fixed world interest rate, or a situation with a perfectly elastic supply of funds. Under this fixed interest rate, not everyone can dissave indefinitely because of the borrowing constraint, so in aggregate there will typically be positive net saving (since precautionary motives induce people to hold assets). If the interest rate is high relative to people’s time preference and risk, the low-wealth agents will borrow up to the limit and the high-wealth will save a lot, and an equilibrium wealth distribution forms. If the interest rate is too high, precautionary saving might not be enough to sustain it (people try to borrow too much); if it’s too low, people accumulate assets and the economy might reach a point where the lowest wealth is at the borrowing limit and highest is still saving – typically there is some interest rate that balances asset demand and the “excess” of precautionary saving.\nWhile the technical details can be involved, the intuition gleaned from the Bewley model is powerful for policy. It shows how incomplete markets alone (without any price rigidity or aggregate shocks) can lead to under-consumption by some and the accumulation of large buffers by others. This has implications for long-run growth and inequality. If many people are constrained and cannot invest in their education or businesses, the economy might underperform its potential. It also implies that policies like social insurance (unemployment insurance, social security, etc.) can affect aggregate outcomes: for example, providing more generous unemployment benefits might reduce the need for precautionary saving, which could actually stimulate consumption among lower-wealth households and reduce inequality. On the flip side, too generous a safety net could reduce the incentive to save at all. Bewley-type models have been used to examine optimal policy in this context, such as what level of unemployment insurance optimally trades off providing insurance versus maintaining incentives.\nAn important extension of the Bewley model is to use it for wealth distribution insights. The model clarifies that even if everyone has identical earning potential, incomplete markets will generate inequality simply due to luck and precautionary behavior. This suggests that some observed inequality is not due to differences in skill or hard work, but due to insufficient insurance against life’s risks. Policymakers concerned with excessive inequality might draw on this insight to justify progressive taxation or public insurance programs that effectively do what missing markets would have – help smooth incomes and consumption across states of the world. Indeed, one policy implication highlighted in such models is that improving access to credit for credit-worthy but constrained households, or providing more public insurance, could make the overall economy better off by allowing more efficient consumption and investment choices (though there are always trade-offs and moral hazard issues to consider).\nIn summary, the Bewley model provides a micro-founded explanation for why some people end up liquidity-constrained and how that influences their behavior. For monetary policy, it warns that aggregate demand may be more sensitive to the distribution of wealth and income than traditional models would suggest – if a recession hits the lower-wealth population hard, their consumption will contract strongly (since they can’t borrow), potentially deepening the downturn. Purely focusing on interest rates as a lever might be insufficient; fiscal redistributive tools or direct transfers could be more potent in such scenarios. The model’s relevance has grown as economists recognize the limitations of the representative-agent paradigm and seek to incorporate heterogeneous agent effects into macroeconomic policy analysis.\n\n\nAiyagari Model: General Equilibrium with Incomplete Markets\n\nIncorporates endogenous determination of interest rates through production equilibrium.\nDemonstrates “excess capital accumulation” due to precautionary motives.\nAdvocates capital income taxation for improved welfare and highlights the distributional consequences of monetary policy changes (Aiyagari 1994).\n\nS. Rao Aiyagari’s model builds directly on the Bewley framework but adds an important layer: a production economy that yields a general equilibrium determination of prices (interest rate and possibly wages). In the Aiyagari (1994) model, we still have infinitely-lived agents with idiosyncratic income shocks and borrowing constraints (precisely the Bewley setup on the household side), but now those households supply savings to, and borrow from, a productive sector with capital. In essence, Aiyagari embeds the precautionary savings behavior into a full macroeconomic model with capital accumulation. The result is a self-contained macroeconomic equilibrium where the interest rate is endogenously determined by the supply and demand for capital, rather than being fixed externally. Households’ collective saving (driven by precautionary motives) feeds into the capital stock, and firms’ demand for capital (based on productivity and diminishing returns) determines the equilibrium interest rate that clears the capital market.\nOne of Aiyagari’s key findings is that in an economy with uninsurable income risk, the equilibrium interest rate will generally be lower than it would be in a comparable complete-markets economy. Intuitively, because households value holding assets as a buffer (beyond what they would in a no-risk scenario), they tend to save more, which pushes down the return to capital. In other words, there is excess aggregate saving due to precautionary motives, leading to a larger capital stock and lower interest rate than the classical model without income risk would predict. This is sometimes referred to as the “Aiyagari excess capital result.” It implies that the laissez-faire outcome might not be socially optimal – there could be “too much” capital from a certain perspective, because individuals don’t internalize that by saving so much for themselves, they depress the return for everyone. One practical implication Aiyagari pointed out is that a government could improve welfare by taxing capital income and redistributing it (or using it to fund social insurance) in such an economy. By doing so, it reduces the need for individuals to self-insure via excessive capital accumulation, potentially moving the economy closer to the golden-rule level of capital (where consumption is maximized). This was a striking result since in a standard frictionless model, capital taxation is often detrimental in the long run – but here, moderate capital taxation can correct an inefficiency arising from incomplete markets.\nThe Aiyagari model also provides insight into the interplay between inequality and aggregate production. Unlike the Bewley model, which was partial equilibrium, here the distribution of wealth affects aggregate supply (through capital accumulation). If the wealth is concentrated in fewer hands, the aggregate consumption could be lower (since wealthy individuals have lower MPCs, they might save a lot of their income), and the aggregate capital might be higher (since those with excess wealth invest it). This has led to extensive research on the quantitative impact of redistributive policies on growth and output. For instance, if you redistribute wealth from the rich (low MPC) to the poor (high MPC), you might raise current consumption but reduce saving and thus future capital – whether that is good or bad for long-run output depends on parameters, but in some cases it can actually increase output if the economy was above the golden rule level of capital to start with (Marcet, Obiols-Homs, and Weil 2007).\nAnother aspect is the feedback of interest rates on inequality. In Aiyagari’s equilibrium, the interest rate settles at a level where households are indifferent between saving and not saving (on the margin). If interest rates are very low, borrowing is cheap, but also the reward for saving is low, which could discourage some saving. However, typically in these models many households still save because of risk aversion and precaution. The low interest rate also means that those who are borrowing-constrained are not paying a huge interest burden (assuming they can borrow at that rate), but many cannot borrow much anyway due to the constraint. Overall, compared to a representative-agent model, the Aiyagari model predicts different responses to monetary policy. For example, if the central bank lowers the interest rate (below the equilibrium that would prevail from just technology and time preference), it transfers resources from savers to borrowers. In an economy with inequality, this has non-neutral effects: borrowers (often poorer agents) gain relief and might consume more, while savers (wealthier agents) earn less on their assets and might consume less (or seek riskier investments). The distributional effects of monetary policy come into play. Recent research in heterogeneous agent New Keynesian (HANK) models builds on this by adding nominal rigidities, but even in the basic Aiyagari model, one can see that monetary policy is not just about one representative agent’s intertemporal choice – it will create winners and losers due to heterogeneity in assets and consumption propensities.\nFor policymakers, Aiyagari’s work underscores a few points: (1) Monetary neutrality may not hold cleanly in the short run even if prices are flexible, because redistributions caused by interest rate changes can affect aggregate demand; (2) there may be a role for permanent fiscal policy (like capital taxation or debt issuance) to influence the long-run capital stock and interest rate in a way that improves welfare, countering the incomplete-market externality; (3) evaluating monetary policy requires understanding the underlying wealth distribution – for instance, a low interest rate environment will tend to benefit borrowers and younger households (via cheaper credit, higher asset values) while hurting those who rely on interest income (like pensioners or wealthier rentiers). If mismanaged, prolonged ultra-low rates can contribute to asset price inflation (as savers seek returns in real estate or stocks), thereby widening wealth inequality if only the already-wealthy hold those appreciating assets. Indeed, some attribute the rise in asset valuations and wealth concentration in recent decades partly to very low global real interest rates and ample liquidity, consistent with the mechanisms in Aiyagari-type models.\nIn conclusion, the Aiyagari model enriches our understanding by marrying heterogeneity with production. It reminds us that macroeconomic policy cannot be divorced from distributional considerations. The long-term natural rate of interest, the effectiveness of fiscal redistribution, and the impact of monetary policy all look different once we acknowledge that not everyone is alike in the economy. By capturing how liquidity constraints and precautionary savings influence aggregate capital, this model provides guidance on questions like whether and how to tax wealth, and how aggressive monetary policy should be in, say, pushing interest rates to very low levels. Policymakers drawing on these insights might strive for a balance: ensuring there is enough aggregate saving for investment and growth, but not so much that it reflects unmet social insurance needs or creates financial imbalances.\n\n\nHarrison & Kreps (1978) Model: Asset Pricing with Heterogeneous Beliefs\n\nHeterogeneous investor beliefs combined with short-sale constraints can cause speculative bubbles, elevating asset prices above fundamental values.\nSuggests improving market completeness and transparency to curb speculation and volatility (Harrison and Kreps 1978).\n\nThe Harrison and Kreps (1978) model introduces a different kind of market imperfection into macro-finance: heterogeneous beliefs among investors, combined with constraints on short selling. Unlike the previous models (which focused on borrowing constraints and income risk), this model lives in the world of asset trading and speculation. Harrison and Kreps asked what happens in an asset market when investors have differing opinions about an asset’s value and they are not allowed to short sell the asset freely. Their answer was groundbreaking: even if all investors are rational (in that they update beliefs consistently with their own information), the mere diversity of opinion can lead to asset prices exceeding the valuation of even the most optimistic individual investor.\nHere’s the intuition: suppose some investors (“optimists”) believe a stock or house will be very valuable in the future, while others (“pessimists”) believe it will not. If short selling is constrained (pessimists cannot easily borrow the asset to sell it short), the market price will be determined largely by the optimists’ willingness to pay. Now add the element of speculation – investors may buy an asset not just for its fundamental value (like dividends or rent) but also for the option to resell it in the future. Harrison & Kreps showed that when beliefs differ, an investor might pay more than their own estimate of the asset’s fundamental value because they anticipate that someone even more optimistic might buy it at a higher price later. In effect, a resale option is priced in. This leads to what we might call a speculative premium on the asset price. The price can rise above the level that any single investor would pay if they had to hold the asset forever. In their words, the right to resell makes investors willing to pay more than the asset’s “hold-to-maturity” value. The inability of pessimists to short sell means nothing counteracts this upward pressure – the pessimists simply sit out of the market rather than actively pushing the price down by shorting. Thus, the market price reflects an over-optimistic valuation, driven by the most bullish views and the prospect of flipping the asset.\nThis mechanism helps explain phenomena like asset price bubbles or situations where market prices seem to detach from fundamental values. Real estate is a commonly cited example: investors might buy houses at high prices not only because they expect rising rents or income (fundamentals), but because they think they can later sell the house to someone else at an even higher price (speculation). If enough people believe housing prices will keep rising, and skeptics can’t effectively short the housing market, the result is a self-reinforcing price boom. The Harrison-Kreps model formalized how even fully rational agents with rational expectations (each given their own belief) can end up trading at prices that embed a speculative component. It doesn’t require irrational exuberance; it only requires disagreement and some friction (short-sale constraints) that prevents full arbitrage. In their equilibrium, everyone understands the price is above their own fundamental valuation, but they also know someone else might be willing to pay even more, so it can still be rational to buy now and plan to sell later – a clear parallel to the greater fool theory, but derived in a rigorous way.\nPolicy implications from the Harrison & Kreps model revolve around financial market regulation and information disclosure. One implication is that short-selling constraints can fuel overpricing. If regulators make short selling too restrictive (perhaps in an attempt to curb volatility or prevent speculative attacks), they might inadvertently remove a balancing force that keeps prices close to fundamentals. The model would suggest that allowing more short selling (with proper oversight to avoid abuse) could actually lead to more informative, less one-sided pricing. Another implication is the value of transparency and common information. In the model, beliefs are heterogeneous and “dogmatic” – each trader sticks to their prior and interprets signals in their own way. If public information can help align beliefs (or at least inform the pessimists and optimists of each other’s views), it might reduce the degree of disagreement. However, complete agreement is unrealistic; differences in models, data interpretation, or risk appetite will always create some dispersion of opinion.\nFrom a monetary policy perspective, one might not immediately see a connection, since H&K is about asset pricing in a frictional financial market. But there are subtle links. Central banks today pay close attention to asset markets – housing, equities, etc. – because large deviations of asset prices from fundamentals can pose risks to financial stability and the broader economy. For instance, if low interest rates contribute to a speculative housing boom (by making borrowing cheap and encouraging optimistic beliefs about ongoing price growth), a subsequent crash could harm banks and consumers, leading to a recession. The H&K model suggests that a booming asset market is not necessarily a sign of solid fundamentals; it could be a sign of constrained pessimism and resale-driven pricing. Policymakers, therefore, should be cautious in interpreting asset price signals. It also provides an argument for macroprudential policies: tools that directly address asset market excess (for example, tighter loan-to-value ratios in mortgage lending during a housing boom, or stricter margin requirements in stock trading). These can be seen as ways to mitigate the speculative dynamics – essentially pricking bubbles before they grow too large. By making it harder to purely speculate (through leverage restrictions) or by encouraging more two-sided markets (perhaps by permitting certain derivatives or short positions), regulators might reduce the likelihood of severe mispricings.\nIn summary, the Harrison & Kreps model adds another layer to our understanding: market outcomes can be inefficient not just because of real-side frictions (like incomplete insurance) but also because of financial-side frictions (like trading constraints and belief dispersion). It is a reminder that even with rational actors, markets may need regulatory oversight to ensure they reflect true economic value. For a policymaker, being aware of this mechanism is important. It cautions against assuming that all investors have the same expectations (they don’t), and it illustrates why asset price booms can develop even without obvious irrationality. Recognizing a speculative bubble early is notoriously difficult, but understanding models like this helps officials appreciate the warning signs (e.g., when asset prices only make sense under very optimistic scenarios and buyers cite the ability to resell as justification). It also supports measures to improve market completeness – such as permitting more sophisticated financial instruments – because a more complete market (ability to hedge, to short, etc.) ironically may prevent the wild swings that incomplete markets allow.",
    "crumbs": [
      "교육",
      "분열",
      "Incomplete Markets"
    ]
  },
  {
    "objectID": "market_incomplete.html#policy-implications-of-models",
    "href": "market_incomplete.html#policy-implications-of-models",
    "title": "Incomplete Markets",
    "section": "Policy Implications of Models",
    "text": "Policy Implications of Models\n\nMonetarism emphasizes stable, predictable policy frameworks.\nBewley and Aiyagari models highlight social insurance and targeted redistribution.\nHarrison & Kreps stress regulation of speculative financial markets.\n\nEach theoretical framework provides distinct policy insights for designing monetary and economic strategies. Key implications can be summarized clearly:\n\nFriedman/Monetarist (Complete Markets)\nMonetarist models emphasize predictable and stable monetary policies. Temporary stimulus measures are ineffective since consumers respond mainly to permanent income changes. Monetary policy should thus follow clear, credible rules (e.g., a fixed money growth rate or inflation targeting), focusing on long-term price stability. Fiscal prudence is advised, as large deficits can be counterproductive due to anticipated future taxation (Ricardian equivalence). Ultimately, monetary policy should avoid attempts to permanently influence real variables like unemployment or income distribution directly, which are better addressed through structural and fiscal reforms.\nBewley (Incomplete Markets)\nThe Bewley model underscores the importance of social insurance and targeted redistribution. Because individuals face uninsurable risks and liquidity constraints, they maintain precautionary savings, limiting investment and consumption in downturns. Thus, policies providing social safety nets (unemployment insurance, healthcare, targeted stimulus payments) help stabilize aggregate demand and prevent severe economic downturns. Furthermore, targeted redistribution or improved credit access can mitigate inequality-driven demand shortfalls, addressing systemic underconsumption and secular stagnation risks. Effective policy involves balancing adequate insurance to stabilize demand without overly dampening incentives to work or save.\nAiyagari (Incomplete Markets & General Equilibrium)\nExtending Bewley’s framework, Aiyagari emphasizes that precautionary saving leads to excessive capital accumulation and lower equilibrium interest rates. A crucial policy implication is that moderate capital income taxation, combined with redistribution, can improve overall welfare. Monetary policy, particularly interest-rate adjustments, significantly affects wealth distribution—low rates benefit borrowers (often younger or lower-wealth groups) at the expense of savers. Policymakers should therefore coordinate monetary and fiscal policies to achieve optimal capital allocation, moderate inequality, and promote balanced, sustainable economic growth.\nHarrison & Kreps (Speculative Markets)\nThe Harrison & Kreps model highlights the necessity of financial market regulation and transparency to address speculative bubbles driven by heterogeneous beliefs and short-sale constraints. Policy interventions should facilitate market completeness (e.g., by allowing regulated short selling or derivatives trading) and enforce transparency through robust disclosure standards. During speculative booms, targeted macroprudential tools (loan-to-value ratios, capital buffers) and proactive central-bank communication can mitigate bubbles without overly aggressive monetary tightening, thus integrating financial stability concerns effectively into monetary policy.\n\nIn summary, a robust monetary policy approach integrates these perspectives, recognizing the importance of stable monetary frameworks, adequate social insurance, fiscal coordination, and prudent financial regulation. Successful policy practice involves using complementary tools (monetary, fiscal, regulatory) and adapting dynamically based on the interplay of inflation stability, wealth distribution, and financial stability. Historical experiences, such as the 2008 crisis and the COVID-19 response, illustrate the effectiveness of combining these insights into comprehensive, multidimensional policy strategies.",
    "crumbs": [
      "교육",
      "분열",
      "Incomplete Markets"
    ]
  },
  {
    "objectID": "market_incomplete.html#bridging-the-gap-between-theory-and-reality",
    "href": "market_incomplete.html#bridging-the-gap-between-theory-and-reality",
    "title": "Incomplete Markets",
    "section": "Bridging the Gap Between Theory and Reality",
    "text": "Bridging the Gap Between Theory and Reality\nModels should guide, not dictate, policy. No single model fully captures the complexity of real economies; thus, policymakers must integrate insights across multiple frameworks—complete markets, incomplete markets, and speculative dynamics—to design effective policies.\nReal economies simultaneously involve heterogeneous agents, liquidity constraints, price rigidities, and speculative market behavior. Friedman’s monetarism emphasizes stable rules and credible expectations; Bewley and Aiyagari highlight liquidity constraints and precautionary savings; Harrison & Kreps emphasize speculation driven by belief heterogeneity. Modern macroeconomic approaches increasingly integrate these elements (e.g., Heterogeneous Agent New Keynesian models).\nHowever, real-world policymaking faces challenges not fully accounted for by theory:\n\nExpectations and Behavioral Factors: Individuals often deviate from rational expectations, resulting in behavioral biases and herd behaviors. Effective policy should therefore actively manage expectations through credible communication (forward guidance).\nPolitical Economy and Credibility: Optimal theoretical policies may not align with political realities. Policymakers thus rely on robust frameworks—such as inflation targeting and automatic stabilizers—that remain effective even under delayed or politically compromised responses.\nFinancial and Global Contexts: Traditional models underestimated financial sector impacts on policy effectiveness. Policymakers must explicitly account for banking sector stability and international capital flows in policy design.\n\nIn practice, policymakers should stress-test policies using multiple models, employ targeted empirical approaches informed by microdata, and regularly update strategies based on observed outcomes. For instance, contrary to initial views, the monetary, fiscal, and financial-market policies implemented during the COVID-19 pandemic—particularly in the U.S.—highlight a failure of effective coordination, as evidenced by subsequent extreme wealth inequality and persistent inflation. Many experts criticize the lack of coherent policy integration, arguing it exacerbated economic distortions and reduced overall policy effectiveness.\nIn summary, bridging theory and reality requires flexible and empirically informed policy strategies, recognizing theoretical insights and practical limitations while ensuring effective coordination to mitigate unintended adverse outcomes.",
    "crumbs": [
      "교육",
      "분열",
      "Incomplete Markets"
    ]
  },
  {
    "objectID": "market_incomplete.html#empirical-review-evaluating-policy-recommendations-in-practice",
    "href": "market_incomplete.html#empirical-review-evaluating-policy-recommendations-in-practice",
    "title": "Incomplete Markets",
    "section": "Empirical Review: Evaluating Policy Recommendations in Practice",
    "text": "Empirical Review: Evaluating Policy Recommendations in Practice\nHistorical experience provides valuable tests for theoretical predictions, highlighting both successes and failures in policy implementation:\n\nGreat Depression: Friedman and Schwartz demonstrated that monetary contraction deepened the Depression. The period validated the importance of maintaining stable aggregate demand and money supply, while illustrating risks of debt-deflation spirals. It emphasized roles for both monetary stabilization and fiscal intervention (e.g., New Deal, WWII spending), leading to institutional innovations like deposit insurance and lender-of-last-resort roles for central banks.\nPostwar Keynesian-Monetarist Debate & Stagflation: The stagflation of the 1970s validated Friedman’s critique of simplistic Keynesian demand management and the natural rate hypothesis. Volcker’s aggressive monetary tightening demonstrated that inflation control requires credible commitment and acceptance of short-term economic pain. This period underscored the difficulty of targeting monetary aggregates directly, resulting in central banks shifting towards inflation targeting regimes with more flexible interest rate rules.\nGreat Moderation and 2008 Financial Crisis: The New Keynesian synthesis (price-stickiness, monetary rules, and representative agents) initially seemed successful during the Great Moderation, but failed to predict growing inequality and financial vulnerabilities that triggered the 2008 crisis. The crisis validated incomplete-market frameworks (Bewley/Aiyagari) and speculative asset-price models (Harrison-Kreps), highlighting the critical need to include financial frictions and heterogeneity in macroeconomic analysis. Quantitative easing stabilized financial markets, but disproportionately benefited wealthier asset holders, intensifying inequality.\nCOVID-19 Pandemic and Inflation Spike: The aggressive fiscal and monetary responses during COVID-19 prevented immediate economic collapse but led to persistent inflation and increased wealth inequality due to poor coordination and mis-targeted stimulus. In the U.S., particularly, stimulus measures lacked coherence, causing asset bubbles and skewed benefits to the wealthy. Current inflation challenges highlight monetarist concerns about excessive stimulus and underline the importance of coordinated fiscal-monetary policy frameworks.\n\nKey empirical lessons:\n\nCredible monetary policy frameworks that stabilize inflation are critical for long-run economic stability.\nMarket imperfections (liquidity constraints, incomplete markets, financial fragility) significantly affect policy effectiveness and must be explicitly considered.\nDistributional issues matter: Monetary policy can inadvertently exacerbate inequality, stressing the need for complementary fiscal redistributive policies.\nExpectations and clear communication are essential for policy effectiveness, reinforcing the rational expectations perspective on policy transparency.\n\nIn sum, successful policy in practice requires combining disciplined monetary frameworks, proactive management of market imperfections and inequalities, and flexibility to adapt based on evolving empirical evidence and economic conditions.",
    "crumbs": [
      "교육",
      "분열",
      "Incomplete Markets"
    ]
  },
  {
    "objectID": "market_incomplete.html#conclusion",
    "href": "market_incomplete.html#conclusion",
    "title": "Incomplete Markets",
    "section": "Conclusion",
    "text": "Conclusion\nEffective monetary policy requires a balance between long-run credibility and stability (as emphasized by neoclassical and monetarist models) and short-run interventions addressing market imperfections and inequality. Stable monetary frameworks—such as credible inflation targets and rule-based policies—anchor expectations, reduce uncertainty, and support sustainable economic growth. However, real economies exhibit significant market frictions, requiring complementary fiscal and regulatory interventions.\nIncomplete-market models (Bewley, Aiyagari) highlight the necessity of policies that alleviate liquidity constraints and promote broader economic participation. Financial-market models (Harrison & Kreps) stress the importance of regulatory vigilance to prevent speculative excesses. Recent empirical experiences—particularly the uncoordinated COVID-19 policy response in the U.S., which contributed to extreme wealth inequality and persistent inflation—underscore the risks of fragmented policy approaches.\nThe long-term orientation of neoclassical models also reminds us that economic growth and efficiency are paramount for raising living standards broadly. Sustainable reductions in wealth inequality are best achieved through inclusive growth—ensuring more people can participate in economic advancement via skill-building, access to capital, and productive investments. Policies that enhance productivity—education, infrastructure, and innovation—are crucial complements to monetary policy. While these are typically fiscal or structural measures, monetary policy plays a role by maintaining a low-inflation, stable macroeconomic backdrop and preventing the misallocation of capital into unproductive speculative ventures.\nTo mitigate wealth inequality without sacrificing efficiency, policymakers should consider measures such as:\n\nProgressive taxation and wealth taxes to fund public goods and transfers efficiently (as Aiyagari’s framework suggests, moderate capital taxation can be welfare-enhancing).\n\nPublic investment in education, healthcare, and economic opportunity to foster broad-based growth and human capital development, reducing inequality over time.\n\nEncouraging broad-based asset ownership through mechanisms like employee stock ownership plans or tax incentives for retirement savings, ensuring more households benefit from asset appreciation.\n\nMaintaining low and stable inflation to protect real incomes, particularly for lower-income households who lack financial hedges against rising prices.\n\nUltimately, the most effective policy approach integrates monetary stability, fiscal coordination, and regulatory prudence to foster inclusive, sustainable economic prosperity while ensuring that short-term interventions do not undermine long-term growth and efficiency.",
    "crumbs": [
      "교육",
      "분열",
      "Incomplete Markets"
    ]
  },
  {
    "objectID": "market_incomplete.html#appendix",
    "href": "market_incomplete.html#appendix",
    "title": "Incomplete Markets",
    "section": "Appendix",
    "text": "Appendix\nThe appendix provides technical details for the theoretical models referenced. Each section includes the formal derivation of key equations, calibration and simulation exercises, and case analyses illustrating the implications of each model.\n\nFriedman’s Permanent Income Model\n\n1. Theoretical Formulation\nFriedman’s Permanent Income Hypothesis (PIH) posits that an individual’s consumption at time \\(t\\) is based on expected lifetime income rather than current income. The standard model assumes perfect capital markets, quadratic utility, and rational expectations.\nLet the consumer’s lifetime utility function be: \\[\nU = \\sum_{t=0}^{\\infty} \\beta^t u(c_t),\n\\] where \\(\\beta\\) is the subjective discount factor and \\(u(c_t)\\) is quadratic utility: \\[\nu(c_t) = -\\frac{1}{2} (c_t - c^*)^2.\\]\nThe budget constraint is: \\[\nA_{t+1} = (1+r)(A_t + Y_t - C_t),\n\\] where \\(A_t\\) is assets, \\(r\\) is the interest rate, \\(Y_t\\) is income, and \\(C_t\\) is consumption. Given rational expectations, Hall (1978) derived that optimal consumption follows a martingale process: \\[\nE_t[c_{t+1}] = c_t.\n\\] This implies that changes in consumption are unpredictable: \\[\nE_t[c_{t+1} - c_t] = 0.\n\\] Thus, only permanent changes in income affect consumption significantly, while transitory changes are mostly saved.\n\n\n2. Calibration and Simulation\nWe calibrate the model using standard parameter values: - Discount factor: \\(\\beta = 0.96\\) - Interest rate: \\(r = 0.04\\) - Income process: \\(Y_t = Y_p + Y_t^T\\), where \\(Y_p\\) is permanent income and \\(Y_t^T\\) is a transitory shock. - Variance of transitory income shock: \\(\\sigma_T^2 = 0.02\\) - Variance of permanent income shock: \\(\\sigma_P^2 = 0.01\\)\nSimulation results illustrate consumption smoothing. Given a one-time positive income shock of \\(\\Delta Y_T\\), consumption increases only modestly: \\[\n\\Delta C_t \\approx \\frac{\\sigma_T^2}{\\sigma_P^2 + \\sigma_T^2} \\Delta Y_T.\n\\] This highlights that temporary income changes have minimal effects on consumption compared to permanent income changes.\n\n\n3. Case Analysis: Impact of Temporary vs. Permanent Income Changes\nTo empirically validate the model, we compare U.S. consumption responses to temporary stimulus payments vs. permanent tax cuts:\n\n2001 U.S. Tax Rebate (temporary): Studies found that only 20-40% of the rebate was spent immediately, consistent with PIH predictions.\n1980s Reagan Tax Cuts (permanent): Consumption increased significantly, aligning with the model’s implication that permanent income shifts drive behavior.\n\nThis supports the policy conclusion that temporary monetary stimulus has limited consumption effects, reinforcing the monetarist argument for stable, rules-based policy frameworks over discretionary interventions.\n\n\n\nBewley Model\n\n1. Theoretical Formulation\nThe Bewley model describes a heterogeneous agent economy with idiosyncratic income shocks and liquidity constraints. Agents optimize consumption and savings in response to uncertain income paths, leading to precautionary savings behavior.\nThe agent maximizes expected lifetime utility: \\[\nU = \\sum_{t=0}^{\\infty} \\beta^t u(c_t),\n\\] subject to the budget constraint: \\[\nA_{t+1} = (1+r)A_t + Y_t - C_t,\n\\] and a borrowing constraint: \\[\nA_t \\geq 0.\n\\] The income process follows a stochastic evolution: \\[\nY_t = Y_p + \\varepsilon_t,\n\\] where \\(Y_p\\) is the persistent component and \\(\\varepsilon_t\\) is a transitory shock.\nThe Euler equation governs optimal consumption: \\[\nE_t \\left[ u'(c_t) \\right] = \\beta (1+r) E_t \\left[ u'(c_{t+1}) \\right].\n\\] Binding borrowing constraints lead to higher marginal propensities to consume (MPC), distinguishing this model from the complete-market benchmark.\n\n\n2. Calibration and Simulation\nWe calibrate the model using empirically relevant parameters:\n\nRisk aversion: \\(\\sigma = 2\\)\nDiscount factor: \\(\\beta = 0.96\\)\nInterest rate: \\(r = 0.04\\)\nIncome process variance: \\(\\sigma_Y^2 = 0.02\\)\n\nSimulating the model shows the emergence of a stationary wealth distribution. Agents with lower wealth levels exhibit high MPCs, while wealthier agents accumulate savings to self-insure against income shocks.\n\n\n3. Case Analysis: Wealth Distribution and Policy Implications\nEmpirical evidence shows that wealth inequality observed in real economies is well captured by Bewley models. For example:\n\nU.S. Wealth Distribution: The model reproduces the fact that the top 10% of wealth holders own a disproportionately large share of total assets.\nImpact of Credit Market Access: Relaxing borrowing constraints leads to higher consumption smoothing, reducing excess precautionary savings.\nEffects of Monetary Policy: Interest rate cuts primarily benefit liquidity-constrained households, increasing consumption, while wealthier agents respond weakly due to already accumulated assets.\n\nThis model underscores the importance of credit access and social insurance policies to counteract excessive precautionary savings and stabilize aggregate demand.\n\n\n\nAiyagari Model\n\n1. Theoretical Formulation\nBuilding on the Bewley model, Aiyagari introduces production and general equilibrium. Households supply capital and labor, and firms use a Cobb-Douglas production function: \\[\nY = K^\\alpha L^{1-\\alpha}.\n\\] The interest rate \\(r\\) and wage \\(w\\) adjust to ensure market clearing: \\[\nK = \\sum A_i, \\quad L = \\sum l_i.\n\\] The stationary distribution of wealth results in an endogenous equilibrium interest rate lower than in the complete markets model due to precautionary savings.\n\n\n2. Calibration and Simulation\n\nCapital share: \\(\\alpha = 0.36\\)\nDiscount factor: \\(\\beta = 0.96\\)\nDepreciation: \\(\\delta = 0.08\\)\nRisk aversion: \\(\\sigma = 2\\)\n\nSimulations confirm excess capital accumulation, implying that moderate capital taxation can improve welfare by reallocating excess savings towards consumption.\n\n\n3. Case Analysis: Redistribution and Policy Implications\n\nEffect of Redistribution: Moderate taxation on capital income reallocates resources and increases aggregate welfare.\nImpact on Interest Rates: The equilibrium interest rate is lower than in the complete markets model, reflecting the precautionary savings motive.\n\nThis model highlights the distributional consequences of monetary and fiscal policy and suggests that targeted redistribution can enhance efficiency without major losses in output.\n\n\n\nHarrison & Kreps Model\n\n1. Theoretical Formulation\nThe Harrison & Kreps (1978) model explores how heterogeneous beliefs among investors can lead to speculative bubbles in asset pricing. The key insight is that optimistic investors dominate the pricing mechanism when short-selling constraints exist, leading to equilibrium prices that can exceed fundamental values.\nConsider a two-period model where investors have differing subjective probabilities about a risky asset’s payoff in period \\(t=2\\). The risky asset pays \\(X_H\\) with probability \\(p_H\\) (optimists’ belief) and \\(X_L\\) with probability \\(1 - p_H\\). Similarly, pessimists believe the probabilities are \\(p_L\\) and \\(1 - p_L\\), where \\(p_H &gt; p_L\\).\nThe price of the asset in period \\(t=1\\) is determined by the highest bidder, given short-sale constraints: \\[\nP_1 = \\max\\{ P_H, P_L \\},\n\\] where \\(P_H\\) and \\(P_L\\) are the optimists’ and pessimists’ valuation of the asset, respectively: \\[\nP_H = \\frac{p_H X_H + (1 - p_H) X_L}{1 + r}, \\quad P_L = \\frac{p_L X_H + (1 - p_L) X_L}{1 + r}.\n\\] If short selling is not allowed, the price is set by the optimists, even if pessimists believe it to be overvalued.\nIn period \\(t=2\\), the true payoff \\(X\\) is realized, and prices adjust accordingly: \\[\nP_2 = X.\n\\] If the optimists’ belief was overly optimistic, a crash occurs, showing that speculative bubbles arise due to heterogeneous expectations rather than fundamental mispricing alone.\n\n\n2. Calibration and Simulation\nTo simulate this model, we set:\n\nRisk-free rate: \\(r = 0.03\\)\nPayoffs: \\(X_H = 120\\), \\(X_L = 80\\)\nBeliefs: \\(p_H = 0.8\\), \\(p_L = 0.5\\)\n\nIf short selling is restricted, the period-1 price reflects the optimists’ valuation: \\[\nP_1 = \\frac{0.8(120) + 0.2(80)}{1.03} = 97.09.\n\\] If short selling is allowed, the pessimists’ valuation influences pricing: \\[\nP_1 = \\frac{0.5(120) + 0.5(80)}{1.03} = 97.09.\n\\]\nSimulated Results:\n\nIf \\(p_H\\) is too optimistic, asset prices rise above fundamentals, and when reality sets in at \\(t=2\\), prices drop, mimicking bubble dynamics.\nIf short-selling is unrestricted, prices better reflect fundamentals, reducing volatility.\n\n\n\n3. Case Analysis: Speculative Bubbles and Policy Implications\nEmpirical applications of the Harrison & Kreps model highlight key cases of speculative booms and busts:\n\nDot-com Bubble (1999–2000): Investor optimism, combined with limited mechanisms for short-selling, led to massive overvaluation.\nHousing Market Crash (2008): Housing assets were driven by speculative demand and optimistic credit assessments; when reality hit, prices crashed.\n\nPolicy takeaways:\n\nMarket Transparency: Improving information flow reduces belief dispersion, dampening bubbles.\nShort-Selling Mechanisms: Allowing short positions prevents excessive speculative premiums.\nMacroprudential Policies: Loan-to-value and capital requirements mitigate credit-fueled bubbles.\n\nThis model underscores how heterogeneous beliefs and market constraints drive speculative price deviations, necessitating a balanced approach to financial regulation.",
    "crumbs": [
      "교육",
      "분열",
      "Incomplete Markets"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Projects",
    "section": "",
    "text": "성장과 분배의 균형? 1.",
    "crumbs": [
      "교육",
      "Projects"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Projects",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n균형 좋아하시네. 낙수효과? 참을 만큼 참았습니다. 이제는 명확히 말해야 할 때입니다. 성장은 면죄부가 아닙니다. 오히려 지난 수십 년간의 자본집중적 성장은 분배의 실패를 구조화했고, 경제적 자유를 극소수의 특권으로 전락시켰습니다. 정의(Justice) 없는 권력(Power)은 폭정이고, 권력 없는 정의는 무능입니다. 지금 필요한 것은 ‘모두를 위한 성장’ 같은 착한 구호가 아니라, 기울어진 시소의 중심축을 강제로라도 옮기는 구조적 재분배입니다. 지금 이 사회에 필요한 건 성장이 아니라, 정의로운 힘의 재구성입니다.↩︎",
    "crumbs": [
      "교육",
      "Projects"
    ]
  },
  {
    "objectID": "dichotomy.html",
    "href": "dichotomy.html",
    "title": "Identifying Dichotomy",
    "section": "",
    "text": "Wealth concentration and inequality have long been central issues in economics, attracting renewed attention as wealth disparities continue to widen. The well-documented Pareto Principle suggests that the top 20% holds approximately 80% of total wealth, but empirical studies indicate that this ratio has become even more skewed over time (Piketty and Saez 2003; Alvaredo et al. 2013; Saez and Zucman 2016). Such trends raise fundamental questions about the underlying dynamics of wealth distribution: Is wealth accumulation a zero-sum game? If so, what is the most evident dichotomy that best exposes this phenomenon?\nAt every moment, newly created capital is introduced into the economic system through production, subsequently distributed among economic agents, and then either dissipated through consumption or retained through investment. Unlike a static framework where total wealth is fixed, real-world economic systems operate as dynamic entities where total wealth evolves over time. The interplay between production, distribution, consumption, and investment determines how wealth is allocated and reallocated across different socioeconomic groups. If the system maintains a stable and proportional allocation of new wealth over time, the relative shares of net worth for different groups remain unchanged. However, if allocation mechanisms fluctuate, then the wealth share of one group necessarily increases at the expense of another—reinforcing a zero-sum dynamic.\nThe key question, therefore, is: How should we partition the population to most clearly expose this zero-sum characteristic? In other words, at which wealth cutoff percentile do we observe the highest absolute correlation between two complementary wealth groups? Should we divide the population into the top 10% versus the bottom 90%? Or is a more extreme division, such as top 0.1% versus bottom 99.9%, more effective in revealing these dynamics?",
    "crumbs": [
      "교육",
      "분열",
      "Identifying Dichotomy"
    ]
  },
  {
    "objectID": "dichotomy.html#introduction",
    "href": "dichotomy.html#introduction",
    "title": "Identifying Dichotomy",
    "section": "",
    "text": "Wealth concentration and inequality have long been central issues in economics, attracting renewed attention as wealth disparities continue to widen. The well-documented Pareto Principle suggests that the top 20% holds approximately 80% of total wealth, but empirical studies indicate that this ratio has become even more skewed over time (Piketty and Saez 2003; Alvaredo et al. 2013; Saez and Zucman 2016). Such trends raise fundamental questions about the underlying dynamics of wealth distribution: Is wealth accumulation a zero-sum game? If so, what is the most evident dichotomy that best exposes this phenomenon?\nAt every moment, newly created capital is introduced into the economic system through production, subsequently distributed among economic agents, and then either dissipated through consumption or retained through investment. Unlike a static framework where total wealth is fixed, real-world economic systems operate as dynamic entities where total wealth evolves over time. The interplay between production, distribution, consumption, and investment determines how wealth is allocated and reallocated across different socioeconomic groups. If the system maintains a stable and proportional allocation of new wealth over time, the relative shares of net worth for different groups remain unchanged. However, if allocation mechanisms fluctuate, then the wealth share of one group necessarily increases at the expense of another—reinforcing a zero-sum dynamic.\nThe key question, therefore, is: How should we partition the population to most clearly expose this zero-sum characteristic? In other words, at which wealth cutoff percentile do we observe the highest absolute correlation between two complementary wealth groups? Should we divide the population into the top 10% versus the bottom 90%? Or is a more extreme division, such as top 0.1% versus bottom 99.9%, more effective in revealing these dynamics?",
    "crumbs": [
      "교육",
      "분열",
      "Identifying Dichotomy"
    ]
  },
  {
    "objectID": "dichotomy.html#data",
    "href": "dichotomy.html#data",
    "title": "Identifying Dichotomy",
    "section": "Data",
    "text": "Data\nOur empirical analysis is based on FRED (Federal Reserve Economic Data), covering quarterly observations from 1989 to 2024. The dataset is structured into wealth brackets representing net wealth shares at different percentile levels:\nGroups (stars)\n\n\\(X_4(t)\\): Share of Net Worth Held by the Top 0.1% (99.9th to 100th Wealth Percentiles) (WFRBSTP1300)\n\\(X_3(t)\\): Share of Net Worth Held by the 99th to 99.9th Wealth Percentiles (WFRBS99T999273)\n\nc.f. Share of Net Worth Held by the Top 1% (99th to 100th Wealth Percentiles) (WFRBST01134)\n\n\\(X_2(t)\\): Share of Net Worth Held by the 90th to 99th Wealth Percentiles (WFRBSN09161)\n\\(X_1(t)\\): Share of Net Worth Held by the 50th to 90th Wealth Percentiles (WFRBSN40188)\n\\(X_0(t)\\): Share of Net Worth Held by the Bottom 50% (1st to 50th Wealth Percentiles) (WFRBSB50215)\n\nAdditionally, we reference wealth cutoff amount to identify the minimum level of wealth required to belong to specific top percentile groups:\nCutoff Levels (bins)\n\n\\(p_4\\) or 99.9th: Minimum Wealth Cutoff for the Top 0.1% (99.9th to 100th Wealth Percentiles) (WFRBLTP1311)\n\\(p_3\\) or 99th: Minimum Wealth Cutoff for the 99th to 99.9th Wealth Percentiles (WFRBL99T999309)\n\\(p_2\\) or 90th: Minimum Wealth Cutoff for the 90th to 99th Wealth Percentiles (WFRBLN09304)\n\\(p_1\\) or 50th: Minimum Wealth Cutoff for the 50th to 90th Wealth Percentiles (WFRBLN40302)",
    "crumbs": [
      "교육",
      "분열",
      "Identifying Dichotomy"
    ]
  },
  {
    "objectID": "dichotomy.html#methodology",
    "href": "dichotomy.html#methodology",
    "title": "Identifying Dichotomy",
    "section": "Methodology",
    "text": "Methodology\nGiven that total share of net wealth must always sum to one, any partition of the population into two groups remains complementary: \\[\nX_0 + X_1 + X_2 + X_3 + X_4 = 1.\n\\]\nTo quantify the most evident dichotomy, we define two complementary wealth groups for different percentile cutoffs:\n\nWhen \\(p = p_1\\):\n\\[\n  a(t) = X_4(t)+X_3(t)+X_2(t)+X_1(t), \\quad b(t) = X_0(t).\n\\]\nWhen \\(p = p_2\\):\n\\[\n  a(t) = X_4(t)+X_3(t)+X_2(t), \\quad b(t) = X_1(t)+X_0(t).\n\\]\nWhen \\(p = p_3\\):\n\\[\n  a(t) = X_4(t)+X_3(t), \\quad b(t) = X_2(t)+X_1(t)+X_0(t).\n\\]\nWhen \\(p = p_4\\):\n\\[\n  a(t) = X_4(t), \\quad b(t) = X_3(t)+X_2(t)+X_1(t)+X_0(t).\n\\]\n\nFor each cutoff \\(p\\), we compute the correlation:\n\\[\n   y(p) = \\mathrm{corr}\\bigl(a(t),\\,b(t)\\bigr).\n\\]\nWe seek the wealth cutoff \\(p\\) that maximizes the absolute correlation \\(|y(p)|\\), revealing the strongest inverse relationship between the two resulting wealth groups. A high absolute correlation suggests that fluctuations in one group’s net worth share are systematically mirrored by the other, reinforcing the zero-sum nature of wealth accumulation. This dichotomy provides insight into how different capital accumulation mechanisms—through labor or capital investment—shape long-term wealth distribution.\nStrong inverse correlations at certain percentiles may indicate critical thresholds where redistribution policies—such as capital taxation or inheritance taxation—could have the most pronounced effects (Piketty 2011; Piketty, Saez, and Stantcheva 2014; Saez and Zucman 2016).",
    "crumbs": [
      "교육",
      "분열",
      "Identifying Dichotomy"
    ]
  },
  {
    "objectID": "dichotomy.html#empirical-results",
    "href": "dichotomy.html#empirical-results",
    "title": "Identifying Dichotomy",
    "section": "Empirical Results",
    "text": "Empirical Results\nAfter processing the quarterly dataset (excluding missing values), we find that the 90th percentile cutoff (\\(p_2\\)) exhibits the highest absolute correlation between the two complementary wealth groups. Specifically, as of 2022-07-01, the minimum wealth required to be in the top 10% was approximately $2,152,788.\nThis suggests that dividing the population into top 10% vs. bottom 90% most effectively reveals the zero-sum nature of wealth redistribution, compared to other partitions such as top 50% vs. bottom 50% or top 0.1% vs. bottom 99.9%.\nThese findings imply that the most structurally significant wealth division occurs between the top 10% and the rest, rather than between the ultra-rich and lower percentiles. This observation aligns with broader discussions on wealth polarization, where the top 10% increasingly dominates capital ownership while the bottom 90% exhibits a more recessive trajectory.",
    "crumbs": [
      "교육",
      "분열",
      "Identifying Dichotomy"
    ]
  },
  {
    "objectID": "dichotomy.html#conclusions",
    "href": "dichotomy.html#conclusions",
    "title": "Identifying Dichotomy",
    "section": "Conclusions",
    "text": "Conclusions\nThis study demonstrates that partitioning the population at the 90th wealth percentile provides the most evident dichotomy in revealing the zero-sum nature of capital accumulation. Over time, wealth redistribution mechanisms result in strong inverse correlations between net worth shares of different groups, underscoring the struggling aspects of capital accumulation. The analysis suggests that the structural division between the top 10% and the bottom 90% is more significant than commonly assumed top 1% vs. bottom 99% splits, reinforcing the notion that wealth concentration extends beyond the ultra-rich and affects broader socioeconomic strata.\nThese findings hold important implications for public policy, particularly in debates surrounding progressive taxation, capital gains policies, and inheritance tax structures. A strong inverse correlation at the 90th percentile threshold suggests that redistributive policies targeted at this level could have significant implications for long-term wealth dynamics. This aligns with prior research emphasizing the role of tax policy in shaping wealth accumulation patterns and mitigating excessive concentration of economic power (Piketty, Saez, and Stantcheva 2014; Saez and Zucman 2016).\nWhile this study primarily focuses on empirical correlation analysis, future research should explore additional macroeconomic variables to refine our understanding of wealth distribution dynamics. Incorporating GDP growth rates, investment patterns, labor market structures, and monetary policy changes may provide further insights into how systemic wealth flows evolve in response to economic shocks and policy interventions. Additionally, extending the dataset to include international comparisons could offer a broader perspective on whether the 90th percentile threshold serves as a critical inflection point for wealth inequality across different economies. Further research integrating both empirical and theoretical approaches will be essential in developing more effective strategies for addressing wealth concentration and economic mobility.\n\n\nCode\nimport pandas as pd\nimport pandas_datareader.data as web\nimport matplotlib.pyplot as plt\n\n# 데이터 기간 설정\nstart_date = '1989-07-01'\nend_date = '2024-07-01'\n\n# FRED 데이터 가져오기\nseries_ids = {\n    'X_4': 'WFRBSTP1300',\n    'X_3': 'WFRBS99T999273',\n    'X_2': 'WFRBSN09161',\n    'X_1': 'WFRBSN40188',\n    'X_0': 'WFRBSB50215'\n}\n\ndata = pd.DataFrame()\nfor name, series_id in series_ids.items():\n    data[name] = web.DataReader(series_id, 'fred', start_date, end_date)\n\n# 결측값 제거\ndata.dropna(inplace=True)\n\n\n# FRED에서 wealth level 데이터 가져오기\nwealth_level_ids = {\n    1: 'WFRBLN40302',\n    2: 'WFRBLN09304',\n    3: 'WFRBL99T999309',\n    4: 'WFRBLTP1311'\n}\n\nwealth_levels = {}\nfor p, series_id in wealth_level_ids.items():\n    latest_data = web.DataReader(series_id, 'fred', start_date, end_date).dropna().iloc[-1]\n    wealth_levels[p] = (latest_data.name, latest_data.iloc[0])  # 날짜와 값을 함께 저장\n    \n# 총 관측치 수 출력\nprint(f\"Total number of observations after removing NaN values: {len(data)}\")\n\n\nTotal number of observations after removing NaN values: 141\n\n\n\n\nCode\n# 상관관계 계산 함수\ndef calculate_correlation(a, b):\n    return a.corr(b)\n\n# 상관관계 계산\ncorrelations = {\n    1: calculate_correlation(data['X_4'] + data['X_3'] + data['X_2'] + data['X_1'], data['X_0']),\n    2: calculate_correlation(data['X_4'] + data['X_3'] + data['X_2'], data['X_1'] + data['X_0']),\n    3: calculate_correlation(data['X_4'] + data['X_3'], data['X_2'] + data['X_1'] + data['X_0']),\n    4: calculate_correlation(data['X_4'], data['X_3'] + data['X_2'] + data['X_1'] + data['X_0'])\n}\n\n# 결과 출력\nfor p, y in correlations.items():\n    print(f\"Correlation for p_{p}: {y:.4f}\")\n\n# 최소 상관관계를 가지는 wealth percentile 찾기\nmin_corr_p = min(correlations, key=correlations.get)\npercentile_map = {1: \"50th\", 2: \"90th\", 3: \"99th\", 4: \"99.9th\"}\nprint(f\"Minimum correlation is at p_{min_corr_p}: {percentile_map[min_corr_p]}\")\nwealth_date, wealth_level = wealth_levels[min_corr_p]\nprint(f\"Wealth level for {percentile_map[min_corr_p]} percentile on {wealth_date.date()}: ${wealth_level:,.0f}\")\n\n\n# 그래프 표현\np_values = list(correlations.keys())\ny_values = list(correlations.values())\n\nplt.plot(p_values, y_values, marker='o')\nplt.xlabel('Wealth Cutoff (p)')\nplt.ylabel('Correlation (y(p))')\nplt.title('Wealth Cutoff vs Correlation')\nplt.grid(True)\nplt.show()\n\n\nCorrelation for p_1: -0.9983\nCorrelation for p_2: -0.9998\nCorrelation for p_3: -0.9996\nCorrelation for p_4: -0.9989\nMinimum correlation is at p_2: 90th\nWealth level for 90th percentile on 2022-07-01: $2,153,046",
    "crumbs": [
      "교육",
      "분열",
      "Identifying Dichotomy"
    ]
  },
  {
    "objectID": "capm_industry.html",
    "href": "capm_industry.html",
    "title": "Industry CAPM",
    "section": "",
    "text": "This study investigates the empirical validity of the single-factor Capital Asset Pricing Model (CAPM) when applied to value-weighted industry portfolios over a multi-decade horizon (1999–2023). While the CAPM remains a foundational model in asset pricing theory, its assumptions—such as static equilibrium and constant risk loadings (betas)—may be unrealistic in dynamic, evolving markets. In this context, we assess whether the model adequately explains cross-sectional return differences at the industry level.\nWe focus on three core empirical challenges: 1. The time-variation in industry-specific market betas, which undermines the model’s assumption of stable covariance structures. 2. The uncertainty in estimating the market risk premium, which can distort expected returns and cost-of-capital estimations. 3. The presence of persistent pricing errors (alphas) that suggest structural mispricing and potentially omitted risk factors.\nOur approach draws on the empirical framework pioneered by Fama and French (1997), extended with contemporary data and methods. Using firm-level CRSP data and SIC-based industry classification, we construct monthly value-weighted portfolios and implement rolling beta estimations, cross-sectional regressions, and alpha decomposition to evaluate the model’s performance.\nReferences:\n\nFama, Eugene F., and Kenneth R. French. “Industry costs of equity.” Journal of Financial Economics 43.2 (1997): 153–193.\n\nFama-French Data Library. CRSP and SIC Classification Methodologies.",
    "crumbs": [
      "교육",
      "설명? 예측?",
      "Industry CAPM"
    ]
  },
  {
    "objectID": "capm_industry.html#introduction",
    "href": "capm_industry.html#introduction",
    "title": "Industry CAPM",
    "section": "",
    "text": "This study investigates the empirical validity of the single-factor Capital Asset Pricing Model (CAPM) when applied to value-weighted industry portfolios over a multi-decade horizon (1999–2023). While the CAPM remains a foundational model in asset pricing theory, its assumptions—such as static equilibrium and constant risk loadings (betas)—may be unrealistic in dynamic, evolving markets. In this context, we assess whether the model adequately explains cross-sectional return differences at the industry level.\nWe focus on three core empirical challenges: 1. The time-variation in industry-specific market betas, which undermines the model’s assumption of stable covariance structures. 2. The uncertainty in estimating the market risk premium, which can distort expected returns and cost-of-capital estimations. 3. The presence of persistent pricing errors (alphas) that suggest structural mispricing and potentially omitted risk factors.\nOur approach draws on the empirical framework pioneered by Fama and French (1997), extended with contemporary data and methods. Using firm-level CRSP data and SIC-based industry classification, we construct monthly value-weighted portfolios and implement rolling beta estimations, cross-sectional regressions, and alpha decomposition to evaluate the model’s performance.\nReferences:\n\nFama, Eugene F., and Kenneth R. French. “Industry costs of equity.” Journal of Financial Economics 43.2 (1997): 153–193.\n\nFama-French Data Library. CRSP and SIC Classification Methodologies.",
    "crumbs": [
      "교육",
      "설명? 예측?",
      "Industry CAPM"
    ]
  },
  {
    "objectID": "capm_industry.html#data-and-methodology",
    "href": "capm_industry.html#data-and-methodology",
    "title": "Industry CAPM",
    "section": "Data and Methodology",
    "text": "Data and Methodology\n\nData Source:\n\nCRSP: Monthly time-series market capitalization data for public stocks listed on Nasdaq, NYSE, and AMEX from 1994-01 to 2023-12.\nFama-French Data Library: Market excess returns (Rm-Rf) and one-month Treasury bill rates as the proxy for the risk-free rate.\n\nTime Frequency and Period: Monthly, covering 30 years (1994-01 to 2023-12), utilizing 5-year rolling windows to estimate monthly industry-specific betas (initial estimation starts from 1999-02).\nIndustry Classification: Ten major industries defined based on SIC codes:\n\nAgriculture, Mining, Construction, Manufacturing, Transportation, Utilities, Wholesale, Retail, Finance, and Services.\nAdjustment1: Reclassified ‘Public’ to ‘Service’, excluded ‘Missing’.",
    "crumbs": [
      "교육",
      "설명? 예측?",
      "Industry CAPM"
    ]
  },
  {
    "objectID": "capm_industry.html#model-specification",
    "href": "capm_industry.html#model-specification",
    "title": "Industry CAPM",
    "section": "Model Specification",
    "text": "Model Specification\nThe single-factor CAPM model used:\n\\[\n(E[r_i] - r_f)=  \\alpha_i + \\beta_i (E[r_m] - r_f)\n\\]\nWhere:\n\n\\(E[r_i]\\): Time-averaged return (i.e., realized net growth rate) of industry portfolio ( i )\n\\(r_f\\): Time-averaged return of the risk-free asset (e.g., one-month T-bill)\n\\(E[r_m]\\): Time-averaged return of the market portfolio, defined as a value-weighted convex combination of all industry portfolios\n\\(\\beta_i\\): Industry-specific market beta, representing the linear projection coefficient onto the market excess return under a single-factor model. It is traditionally assumed to be constant under static equilibrium conditions.\n\\(\\alpha_i\\): Orthogonal component of the industry portfolio’s return relative to the market factor; equivalently, the mean residual from an orthogonal projection onto the market return. This term captures either pricing errors under CAPM or the effects of omitted risk factors.\n\n\nThe Alpha\nThe interpretation of \\(\\alpha_i\\) aligns with the residual term in the linear projection:\n\\[\nr_i - r_f = \\alpha_i + \\beta_i (r_m - r_f) + \\varepsilon_i\n\\]\nwhere \\(\\alpha_i\\) is the average component of \\(\\varepsilon_i\\), and \\(\\varepsilon_i\\) is orthogonal to the regressor. Under ideal CAPM assumptions, \\(\\alpha_i = 0\\), but empirically it often deviates from zero due to model misspecification or omitted risk factors.\n\n\nSimple Logical Analysis\nTo better understand the structural dynamics behind industry-driven market behavior, we consider two stylized scenarios:\n\nFirst, imagine a situation where a single dominant industry—such as Services—accounts for a disproportionately large share of total market capitalization (e.g., 70%). In this case, the market portfolio, defined as a value-weighted aggregate, would exhibit a very high linear correlation with the dominant industry’s return. This undermines diversification and implies that market-wide movements are largely driven by a single sector.\nSecond, consider a scenario where two large industries, each accounting for ~45% of the market, are perfectly negatively correlated. Such a structure would lead to very low overall market volatility, as gains in one sector would offset losses in the other, thus creating strong hedging opportunities and enhancing the benefits of diversification.\n\nIn reality, however, the two industries that collectively dominate the market—Services and Manufacturing—exhibit strong positive correlation. As a result, market volatility is amplified, and hedging opportunities are limited, especially in passive value-weighted portfolios. This concentration and correlation structure challenge one of CAPM’s implicit assumptions: that the market portfolio is a well-diversified proxy for systematic risk.",
    "crumbs": [
      "교육",
      "설명? 예측?",
      "Industry CAPM"
    ]
  },
  {
    "objectID": "capm_industry.html#from-1999-to-2023",
    "href": "capm_industry.html#from-1999-to-2023",
    "title": "Industry CAPM",
    "section": "From 1999 to 2023",
    "text": "From 1999 to 2023\n\nHistorical excess risk premiums of the US stock market\n\n\nCode\n# 30 years of crsp_monthly\n# start_date = \"1994-01-31\" # i.e. '1994-02-01'\n# end_date = \"2023-12-31\"\n\n# Because of 5 year rolling estimation of monthly beta\nstart_date = \"1999-01-31\"\nend_date = \"2023-12-31\"\n\nprint(f\"Start Date: {start_date}\")\nprint(f\"End Date: {end_date}\")\n\n\nStart Date: 1999-01-31\nEnd Date: 2023-12-31\n\n\n\n\nCode\n#@title Libraries and Time-window\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.formula.api as smf\n\nimport sqlite3\n\ntidy_finance = sqlite3.connect(\"../../colab/tidy_finance_python.sqlite\")\n\nfactors_ff3_monthly = pd.read_sql_query(\n  sql=\"SELECT month, mkt_excess, rf FROM factors_ff3_monthly\",\n  con=tidy_finance,\n  parse_dates={\"month\"}\n)\n\n# 1994-01-01 indicates mktcap at 1994-01-31 which is the start date\n# the first return is calculated\ncrsp_monthly = pd.read_sql_query(\n  sql=\"SELECT permno, month, ret, ret_excess, mktcap, mktcap_lag, siccd, industry, exchange FROM crsp_monthly\",\n  con=tidy_finance,\n  parse_dates={\"month\"}\n)\n\n# 5 year rolling estimated beta is available from 1999-01-01\nbeta = (pd.read_sql_query(\n    sql=\"SELECT permno, month, beta_monthly FROM beta\",\n    con=tidy_finance,\n    parse_dates={\"month\"})\n)\n\nbeta_lag = (beta\n  .assign(month = lambda x: x[\"month\"] + pd.DateOffset(months=1))\n  .get([\"permno\", \"month\", \"beta_monthly\"])\n  .rename(columns={\"beta_monthly\": \"beta_lag\"})\n  .dropna()\n)\n\n# Calculate 12-month moving average\nfactors_ff3_monthly['mkt_excess_ma12'] = factors_ff3_monthly['mkt_excess'].rolling(window=12).mean()\n\n# Plot: Market Excess Return with 12-month Moving Average\nplt.figure(figsize=(12, 5))\nplt.plot(factors_ff3_monthly['month'], factors_ff3_monthly['mkt_excess'], label='Monthly Excess Return', color='lightsteelblue')\nplt.plot(factors_ff3_monthly['month'], factors_ff3_monthly['mkt_excess_ma12'], label='12-Month Moving Average', color='darkblue')\nplt.axhline(0, color='gray', linestyle='--', linewidth=1)\nplt.title('Monthly Market Excess Return with 12-Month Moving Average', fontsize=14)\nplt.xlabel('Date')\nplt.ylabel('Excess Return')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nStructural Shifts in Industry Concentration\n\nInter-Industry Divergence: Growing disparity in concentration levels across industries\nIntra-Industry Consolidation: Increasing dominance of top firms within each industry\n\n\n\nCode\n#@title Number of Firms in Industry Portfolios\n# First, create a dummy column for counting\ncrsp_monthly['count'] = 1\n\n# Create the pivot table\npfo_number = crsp_monthly.pivot_table(\n    values='count',  # The column to aggregate (count in this case)\n    index='month',    # The column to use as index\n    columns='industry', # The column to use as columns\n    aggfunc='sum',    # The aggregation function to use (sum in this case)\n    fill_value=0      # Fill NaN values with 0\n)\n\nsorted_columns = pfo_number.mean().sort_values(ascending=False).index\npfo_number[sorted_columns].plot(\n    kind='line',\n    xlabel='month',\n    ylabel='number of firms',\n    title='Number of Firms in Industry'\n)\nplt.legend(bbox_to_anchor=(1.0, 1.0)) # legend outside\nplt.show()\n\npfo_number[['Missing','Public']].plot(\n    kind='line',\n    xlabel='month',\n    ylabel='number of firms',\n    title='Number of Firms in Industry'\n)\nplt.show()\n\nprint('The average number of firms in Missing industry is', pfo_number['Missing'].mean().round(2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe average number of firms in Missing industry is 2.82\n\n\n\n\nCode\n#@title Industry Concentration Dynamics\n\n# Average Firm Size in Industry Portfolios (Public in Black)\n\npfo_size = crsp_monthly.pivot_table(\n    index='month',\n    columns='industry',\n    values='mktcap',\n    aggfunc='mean'\n)\n\nsorted_columns = pfo_size.mean().sort_values(ascending=False).index\n\nax = pfo_size[sorted_columns].plot(\n    kind='line',\n    xlabel='month',\n    ylabel='mktcap',\n    title='Average Firm Size in Industry Portfolios',\n    linewidth=1.5\n)\n\n# Set Public line to black\nfor line, col in zip(ax.get_lines(), sorted_columns):\n    if col == \"Public\":\n        line.set_color('black')\n        line.set_linewidth(2.0)\n\nplt.legend(bbox_to_anchor=(1.0, 1.0))  # legend outside\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n#@title 산업 내 HHI (Herfindahl-Hirschman Index)\n\n# Step 1: 각 월, 각 산업 내 기업별 시가총액 비중 계산\ncrsp_monthly['mktcap_share'] = (\n    crsp_monthly\n    .groupby(['month', 'industry'], group_keys=False)['mktcap']\n    .transform(lambda x: x / x.sum())\n)\n\n# Step 2: HHI 계산 (각 산업의 각 월에 대해)\nindustry_hhi = (\n    crsp_monthly\n    .assign(mktcap_share_sq=lambda x: x['mktcap_share'] ** 2)\n    .groupby(['month', 'industry'], group_keys=False)['mktcap_share_sq']\n    .sum()\n    .unstack()\n    .sort_index()\n)\n\n# 산업 내 Top-5 Market Cap Share 계산 \ndef top5_share_func(df):\n    # group에는 'month', 'industry'가 포함되므로 사용하지 않음\n    top5_sum = df.nlargest(5, 'mktcap')['mktcap'].sum()\n    total = df['mktcap'].sum()\n    return top5_sum / total if total != 0 else np.nan\n\n# Step: 그룹핑 컬럼을 index로 빼서 apply의 group에서 제거\ntop5_share = (\n    crsp_monthly\n    .sort_values(['month', 'industry', 'mktcap'], ascending=[True, True, False])\n    .set_index(['month', 'industry'])  # &lt;-- group에 포함되지 않게 index로 설정\n    .groupby(['month', 'industry'], group_keys=False)\n    .apply(top5_share_func)  # group에 month/industry 포함되지 않음\n    .unstack()  # 산업별 column\n    .sort_index()\n)\n\nselected_industries = ['Transportation', 'Utilities', 'Retail', 'Manufacturing', 'Services']\n\n# HHI plot\nindustry_hhi[selected_industries].plot(\n    figsize=(12, 5),\n    title='HHI: Industry Concentration Over Time',\n    ylabel='Herfindahl Index',\n    xlabel='Month'\n)\nplt.legend(bbox_to_anchor=(1.0, 1.0))\nplt.show()\n\n# Top-5 Share plot\ntop5_share[selected_industries].plot(\n    figsize=(12, 5),\n    title='Top-5 Market Cap Share in Each Industry Over Time',\n    ylabel='Top-5 Share',\n    xlabel='Month'\n)\nplt.legend(bbox_to_anchor=(1.0, 1.0))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe sharp increase in Top-5 market capitalization share since the post-2009 period highlights a structural shift toward greater industry concentration—particularly within the transportation, utilities, retail, manufacturing, and services sectors. This trend indicates that a small number of dominant firms increasingly account for a disproportionate share of total industry market value.\nWhile average firm size already suggested this pattern, the Top-5 share offers a direct and quantifiable measure. Notably, the Services sector has experienced a persistent rise in concentration since 2011, likely driven by digital transformation, platform-based business models, and network effects. The Manufacturing sector, meanwhile, remained relatively stable until 2019 before undergoing a rapid increase in dominance, possibly due to technology-driven scale economies.\nThese developments coincide with macroeconomic shifts following the 2008 financial crisis, including accommodative policies like quantitative easing (QE), which may have reinforced the “winner-takes-most” dynamics. Importantly, this rising concentration may partially explain observed deviations from CAPM predictions, as industry-level returns become increasingly shaped by a few large-cap firms with idiosyncratic risk-return profiles.\n\n\nEvolution of Industry Market Cap Shares (1999–2023)\n\n\nCode\n#@title df: Drop industry 'Missing' and Re-classify industry 'Public' to 'Services'\n\n# Copy original\ndf = crsp_monthly.copy()\n\n# Drop Missing\ndf = df[df['industry'] != 'Missing']\n\n# Reclassify Public → Services\ndf.loc[df['industry'] == 'Public', 'industry'] = 'Services'\n\n# Merge with factor data and beta\ndf = (df\n  .merge(beta, how=\"inner\", on=[\"permno\", \"month\"])\n  .merge(beta_lag, how=\"inner\", on=[\"permno\", \"month\"])\n  .merge(factors_ff3_monthly, how=\"inner\", on=[\"month\"])\n)\n\n\n\n\nCode\n#@title Market Cap Share of industry portfolios\npfo_share = df.pivot_table(index='month', columns='industry', values='mktcap', aggfunc='sum')\n\n# Normalize pfo_share to sum to 1 for each row\npfo_share[:] = pfo_share.div(pfo_share.sum(axis=1), axis=0)\n\nsorted_columns = pfo_share.mean().sort_values(ascending=False).index\npfo_share[sorted_columns].plot(\n    kind='line',\n    xlabel='month',\n    ylabel='mktcap',\n    title='Market Cap Share of industry portfolios'\n)\nplt.legend(bbox_to_anchor=(1.0, 1.0)) # legend outside\nplt.show()\n\n\n\n\n\n\n\n\n\nThe market portfolio’s composition—measured by the value-weighted market capitalization share of each industry—has experienced notable structural changes since 1999. While the majority of industries have remained relatively small in terms of aggregate market weight, three sectors—Manufacturing, Services, and Finance—have consistently dominated.\nIn particular, the Manufacturing sector’s dominance has gradually declined from nearly 50% in 1999 to below 40% in 2023. Conversely, the Services sector, especially after absorbing “Public” firms, has expanded significantly, rising from under 20% to over 30% in the same period. The Finance sector saw a sharp decline following the 2008 financial crisis and has since stabilized at a lower level.\nThese compositional shifts reflect evolving patterns in industrial dominance and have direct implications for the systematic risk profile of the aggregate market portfolio. As sectoral weights change, so too does the market beta composition underlying the CAPM framework.\n\n\nTime-Varying Systematic Risk by Industry\n\n\nCode\n#@title Time-varying industry Market Betas\n\n# ===============================================\n# 1. Market Cap-weighted Industry Beta (Value-Weighted Beta)\n# ===============================================\n# CAPM의 factor loading인 beta는 산업 내 대형 기업일수록 시장과의 공분산에 더 큰 영향을 미치므로,\n# 산업별 단순 평균 beta는 산업의 실제 systematic risk를 과소/과대평가할 수 있습니다.\n# 따라서 각 기업의 시가총액으로 가중평균한 value-weighted beta를 계산합니다.\n\n# Step 1: Beta weighted by market cap\ndf['beta_weighted'] = df['beta_monthly'] * df['mktcap']\n\n# Step 2: Group by month and industry to compute weighted beta\npfo_beta_weighted = (\n    df.groupby(['month', 'industry'])[['beta_weighted', 'mktcap']]\n      .sum()\n      .assign(beta_vw=lambda x: x['beta_weighted'] / x['mktcap'])\n      .reset_index()\n      .pivot(index='month', columns='industry', values='beta_vw')\n)\n\n# ===============================================\n# 2. Time-Series Plot of Value-Weighted Industry Betas\n# ===============================================\nsorted_columns = pfo_beta_weighted.mean().sort_values(ascending=False).index\n\npfo_beta_weighted[sorted_columns].plot(\n    kind='line',\n    figsize=(12, 6),\n    xlabel='Month',\n    ylabel='Value-weighted Beta',\n    title='Time-varying Value-weighted Industry Beta'\n)\nplt.legend(bbox_to_anchor=(1.0, 1.0))\nplt.show()\n\n# ===============================================\n# 3. Boxplot of Value-Weighted Industry Betas\n# ===============================================\n# Melt for seaborn\npfo_beta_weighted_melted = pd.melt(\n    pfo_beta_weighted.reset_index(),\n    id_vars=['month'],\n    value_vars=pfo_beta_weighted.columns\n)\npfo_beta_weighted_melted.columns = ['month', 'industry', 'beta']\n\n# Sort industries by average beta\nmean_beta_vw = pfo_beta_weighted_melted.groupby('industry')['beta'].mean().sort_values(ascending=False)\n\n# Boxplot\nplt.figure(figsize=(10, 6))\nsns.boxplot(\n    y='industry',\n    x='beta',\n    data=pfo_beta_weighted_melted,\n    order=mean_beta_vw.index,\n    orient='h'\n)\nplt.title('Boxplot of Value-weighted Beta for Each Industry (Sorted by Mean)')\nplt.show()\n\n# ===============================================\n# 4. Scatter Plot: Mean Beta vs. Mean Market Cap Share\n# ===============================================\n# Mean industry beta (value-weighted)\nbeta_mean = pfo_beta_weighted.mean()\n\n# Mean market cap share (already normalized)\nmktcap_share_mean = pfo_share.mean()\n\n# Scatter Plot\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=beta_mean, y=mktcap_share_mean)\nfor industry in beta_mean.index:\n    plt.text(beta_mean[industry], mktcap_share_mean[industry], industry, fontsize=9)\nplt.xlabel('Mean Industry Beta (Value-weighted)')\nplt.ylabel('Mean Market Cap Share')\nplt.title('Mean Industry Beta vs. Mean Market Cap Share')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of value-weighted industry betas uncovers several important dynamics in the evolution of systematic risk exposures across sectors.\nFirst, the time-series plot shows a random-walk-like behavior in beta trajectories, suggesting that industry-level risk exposure is far from stable and must be modeled as time-varying. The boxplot reinforces this heterogeneity:\n\nIndustries such as Manufacturing and Retail exhibit narrow beta distributions, indicating consistent risk exposure among firms in these sectors.\n\nIn contrast, Mining and Construction show wide dispersion, pointing to greater intra-industry variability in systematic risk.\n\nA comparison of beta levels also reveals structural asymmetries:\n\nRetail, Utilities, and Agriculture maintain beta values consistently below one, aligning with their roles as defensive sectors.\n\nConversely, the Services sector displays beta values above one, along with a rising market cap share—suggesting it has become a key driver of market returns.\n\nThis imbalance implies that a value-weighted market portfolio—heavily exposed to high-beta sectors—offers limited hedging potential, especially in downturns.\nThe scatter plot of mean beta vs. mean market cap share further illustrates this, with Manufacturing standing out as a structural outlier: it holds an average beta near 1 but dominates in market share. These findings support a more strategic allocation to low-beta sectors, particularly in anticipation of macroeconomic risks. This aligns with recent investment behavior by Buffett, who has increased exposure to retail-sector firms like Ulta Beauty, likely as a hedge against cyclical downturns.\n\n\nEmpirical Testing of CAPM Using Fama-MacBeth Regressions\n\n\nCode\n#@title 10 Value-Weighted industry pfos\n\ndef weighted_avg(x, weights):\n    \"\"\"Calculates the weighted average of a series.\"\"\"\n    return np.average(x, weights=weights)\n\n# Apply weighted_avg function to pivot_table\npfo_vw_ret_excess = df.pivot_table(\n    index='month',\n    columns='industry',\n    values='ret_excess',\n    aggfunc=lambda x: weighted_avg(x, df.loc[x.index, 'mktcap'])\n)\n\npfo_vw_beta_lag = df.pivot_table(\n    index='month',\n    columns='industry',\n    values='beta_lag',\n    aggfunc=lambda x: weighted_avg(x, df.loc[x.index, 'mktcap'])\n)\n\nmean_vw_beta_lag = pfo_vw_beta_lag.mean().rename('mean_beta_lag')\nmean_vw_ret_excess = pfo_vw_ret_excess.mean().rename('mean_ret_excess')\n\nmkt_excess = factors_ff3_monthly['mkt_excess'].mean()\nrf = factors_ff3_monthly['rf'].mean()\n\n\n\n\nCode\n#@title Cross-sectional regressions for each month\n\n# Fama-MacBeth (1973) two-pass procedure \n\nrisk_premiums = (df\n  .groupby(\"month\")[['ret_excess', 'beta_lag']]\n  .apply(lambda x: smf.ols(formula=\"ret_excess ~ beta_lag\", data=x).fit().params)\n  .reset_index()\n)\n\n# Time-series Aggregation (i.e. average)\n# average across the time-series dimension to get the mean risk premium for each characteristic\n# calculate t-test statistics for each regressor,\n# critical values of 1.96 (at 5% significance) or 2.576 (at 1% significance) for two-tailed significance tests\n\nmean_premiums = (risk_premiums\n  .melt(id_vars=\"month\", var_name=\"factor\", value_name=\"estimate\")\n  .groupby(\"factor\")[\"estimate\"]\n  .apply(lambda x: pd.Series({\n      \"mean_premium\": 100*x.mean(),\n      \"t_statistic\": x.mean()/x.std()*np.sqrt(len(x))\n    })\n  )\n  .reset_index()\n  .pivot(index=\"factor\", columns=\"level_1\", values=\"estimate\")\n  .reset_index()\n)\n\n# reporting standard errors of risk premiums, after adjusting for autocorrelation (Newey and West (1987) standard errors)\n\nmean_premiums_newey_west = (risk_premiums\n  .melt(id_vars=\"month\", var_name=\"factor\", value_name=\"estimate\")\n  .groupby(\"factor\")\n  .apply(lambda x: (\n      x[\"estimate\"].mean()/\n        smf.ols(\"estimate ~ 1\", x)\n        .fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 6}).bse\n    ), include_groups=False\n  )\n  .reset_index()\n  .rename(columns={\"Intercept\": \"t_statistic_newey_west\"})\n)\n\nfm_reg = (mean_premiums\n  .merge(mean_premiums_newey_west, on=\"factor\")\n  .round(3)\n)\nfm_reg['mean_premium'] = fm_reg['mean_premium']*12\n\nprint('Annual Risk Premium of Market Beta')\nfm_reg\n\n\nAnnual Risk Premium of Market Beta\n\n\n\n\n\n\n\n\n\nfactor\nmean_premium\nt_statistic\nt_statistic_newey_west\n\n\n\n\n0\nIntercept\n9.432\n3.891\n3.046\n\n\n1\nbeta_lag\n1.860\n0.766\n0.725\n\n\n\n\n\n\n\nWe employ the Fama-MacBeth (1973) two-pass regression method to estimate the annualized market risk premium under the single-factor CAPM framework. The first pass consists of estimating rolling betas for each firm, which are then aggregated into value-weighted industry betas. The second pass involves running monthly cross-sectional regressions of industry excess returns on lagged betas from 1999 to 2023. To account for possible serial correlation, we report both naïve t-statistics and Newey-West (1987) adjusted statistics.\nThe results show a striking pattern:\n\nThe estimated intercept (alpha) averages 9.43% annually, and this deviation from the theoretical risk-free rate is statistically significant (t = 3.89; NW-adjusted t = 3.05).\n\nThe estimated beta risk premium, on the other hand, is only 1.86% annually, with no statistical significance (t = 0.77; NW-adjusted t = 0.73).\n\nThis finding leads to two critical implications:\n\nThe CAPM fails to explain a substantial portion of the cross-sectional variation in industry returns.\nThere is likely a mispricing component or omitted factor structure that the single-factor model cannot capture.\n\nFrom a modeling perspective, the coexistence of a strong alpha and weak beta suggests that estimation errors are compounding: both the time-varying nature of betas and the instability of risk premia contribute to the overall model misspecification. These results are consistent with the view that industry-specific risk profiles may involve multiple dimensions of risk, and that static CAPM assumptions are empirically untenable over long horizons.\n\n\nSecurity Market Line and Conditional Alpha\n\n\nCode\n#@title CAPM SML prediction plot\n\nimport matplotlib.ticker as mtick\n\n# Combine beta and return\npfo_sml = pd.concat([mean_vw_beta_lag, mean_vw_ret_excess], axis=1)\npfo_sml = pfo_sml.reset_index().rename(columns={'index': 'industry'})\n\n# CAPM Regression Line (fitted to 10 points)\nmodel = smf.ols('mean_ret_excess ~ mean_beta_lag', data=pfo_sml).fit()\nintercept_capm_fit = model.params['Intercept']\n\n# SML: CAPM predicted line (Rf intercept)\nintercept_capm_theory = rf\n\n# SML: Fama-MacBeth implied line (intercept from fm_reg table)\nintercept_fm = fm_reg.loc[fm_reg['factor'] == 'Intercept', 'mean_premium'].values[0] / 100 / 12  # monthly rate\n\n# Start plot\nplt.figure(figsize=(8, 6))\n\n# Scatter plot of 10 industries\nfor _, row in pfo_sml.iterrows():\n    plt.scatter(row['mean_beta_lag'], row['mean_ret_excess'], color='black')\n    plt.annotate(row['industry'], (row['mean_beta_lag'] + 0.01, row['mean_ret_excess']), fontsize=9)\n\n# Draw SMLs\n# Theoretical CAPM SML (Rf, slope = E[Rm - Rf])\nplt.axline((0, intercept_capm_theory), slope=mkt_excess, linestyle='dashed', color='black', label='CAPM (Rf Intercept)')\n\n# Regression fit line (OLS over 10 industry points)\nplt.axline((0, intercept_capm_fit), slope=mkt_excess, linestyle='dashed', color='red', label='OLS Fit on 10 Points')\n\n# Fama-MacBeth implied line (Intercept from FM regression)\nplt.axline((0, intercept_fm), slope=mkt_excess, linestyle='dashed', color='blue', label='Fama-MacBeth Intercept')\n\n# Format\nplt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\nplt.xlabel('Mean Beta (Lagged)')\nplt.ylabel('Mean Excess Return (Monthly)')\nplt.title('Unconditional Security Market Line (Industry-Level CAPM)')\nplt.legend(loc='lower right')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n#@title 개별 산업의 mispricing 정도를 파악\n\n# 1. Calculate conditional alpha\npfo_sml['capm_pred'] = pfo_sml['mean_beta_lag'] * mkt_excess\npfo_sml['alpha'] = pfo_sml['mean_ret_excess'] - pfo_sml['capm_pred']\n\n# 2. Sort industries by alpha\npfo_sml_sorted = pfo_sml.sort_values(by='alpha', ascending=False)\n\n# 3. Barplot of alpha\nimport seaborn as sns\nplt.figure(figsize=(10, 6))\nsns.barplot(data=pfo_sml_sorted, x='alpha', y='industry', hue='industry', palette='coolwarm', dodge=False)\nplt.axvline(0, color='black', linestyle='--')\nplt.gca().xaxis.set_major_formatter(mtick.PercentFormatter(1.0))\nplt.xlabel('Conditional Alpha (Monthly)')\nplt.title('Industry-level Conditional Alpha under CAPM')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nWe visualize the unconditional Security Market Line (SML) implied by the single-factor CAPM across 10 value-weighted industry portfolios. Each point on the plot corresponds to an industry, placed according to its average market beta (horizontal axis) and realized average excess return (vertical axis) over the sample period.\nThree lines are shown for comparison:\n\nThe black dashed line represents the theoretical CAPM SML, where the intercept equals the average risk-free rate and the slope equals the average market risk premium.\nThe red dashed line is a simple OLS fit across the 10 industry points, which minimizes cross-sectional error.\nThe blue dashed line uses the Fama-MacBeth estimated intercept, incorporating pricing errors in the CAPM framework.\n\nMost industry points lie between the CAPM-predicted line and the Fama-MacBeth-adjusted line. This suggests that while the risk-return relationship remains approximately linear, the CAPM fails to account for substantial pricing errors, as reflected in large intercept terms.\nTo quantify these deviations more precisely, we calculate conditional alphas for each industry. These alphas represent the difference between realized and CAPM-predicted returns, conditional on the industry’s average beta.\n\nPositive alpha implies the industry earned more than predicted by its systematic risk exposure.\nNegative alpha suggests overvaluation relative to CAPM expectations.\n\nThe results reveal persistent mispricing across several sectors, reinforcing earlier conclusions about the model’s empirical inadequacy. The CAPM may still serve as a baseline pricing model, but the presence of large unexplained returns calls for either a multi-factor extension or a fundamental rethinking of the linear risk-return paradigm.",
    "crumbs": [
      "교육",
      "설명? 예측?",
      "Industry CAPM"
    ]
  },
  {
    "objectID": "capm_industry.html#conclusions",
    "href": "capm_industry.html#conclusions",
    "title": "Industry CAPM",
    "section": "Conclusions",
    "text": "Conclusions\nThis short study evaluates the empirical validity of the single-factor CAPM using value-weighted industry portfolios over a 25-year period (1999–2023). Several key findings emerge:\n\nIndustry-specific market betas exhibit substantial time variation, contradicting the CAPM’s assumption of stable factor loadings. This instability weakens the model’s explanatory power over long horizons and complicates its use in asset pricing and cost-of-capital estimation.\nThe market risk premium, when estimated empirically, shows large uncertainty and wide confidence bounds, limiting its practical usefulness in capital budgeting and valuation decisions.\nFama-MacBeth regressions reveal economically large and statistically significant intercepts (alphas), while the estimated risk premium on beta is both small and statistically insignificant. This suggests that the single-factor CAPM omits important pricing components or fails to capture cross-sectional return dynamics.\nIndustry-level CAPM predictions show structural deviations from theoretical SML predictions. Finance and Transportation exhibit relatively low pricing errors, potentially due to regulatory distortions (e.g., “Too Big to Fail”) or reduced market responsiveness.\nLastly, the increasing dominance of a few large-cap firms—particularly in Services and Manufacturing—implies that market-wide returns are increasingly shaped by concentrated industry dynamics. This structural concentration further limits the diversification benefits assumed under standard portfolio theory.\n\nOverall, while CAPM remains a foundational framework in asset pricing, this analysis highlights its limitations in capturing the complexities of modern equity markets—particularly when applied at the industry level over long horizons.",
    "crumbs": [
      "교육",
      "설명? 예측?",
      "Industry CAPM"
    ]
  },
  {
    "objectID": "capm_industry.html#footnotes",
    "href": "capm_industry.html#footnotes",
    "title": "Industry CAPM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn this study, firms originally categorized under the “Public” industry in the CRSP database are reassigned to the “Services” industry group. This decision is based on a review of the largest firms within the Public category—such as Tesla, Zoom, Airbnb, PayPal, and Coinbase—which predominantly operate in service-driven, software-based, or platform-oriented business models. Although a few firms like Tesla or Kraft Heinz engage in manufacturing, the overall structure and revenue sources of the Public group are better aligned with the characteristics of modern service industries. This reassignment enhances interpretability in CAPM-based industry comparisons while maintaining consistency in economic logic.↩︎",
    "crumbs": [
      "교육",
      "설명? 예측?",
      "Industry CAPM"
    ]
  },
  {
    "objectID": "capm.html",
    "href": "capm.html",
    "title": "Revisiting the CAPM",
    "section": "",
    "text": "Introduction\nThe Capital Asset Pricing Model (CAPM), developed by Sharpe (1964), Lintner (1965), and Mossin (1966), remains a cornerstone of modern finance, linking expected returns to risk. It classifies risk into two categories: systematic risk, which stems from market-wide factors and cannot be diversified away, and unsystematic risk, which is asset-specific and can be mitigated through diversification. The CAPM posits that only systematic risk, measured by beta, justifies a return premium, as investors can eliminate unsystematic risk by holding a diversified portfolio, ideally approximating the market portfolio. This principle aligns with broader linear factor models like the Arbitrage Pricing Theory (APT) (Ross 1976), which extends the CAPM by incorporating multiple systematic risk factors while similarly dismissing diversifiable risk under no-arbitrage conditions.\nHowever, the CAPM’s empirical validity has been contested. Banz (1981) documented the size effect, where small-cap stocks outperform CAPM predictions, while Basu (1977) identified the value effect, showing excess returns for stocks with high earnings-to-price ratios. These anomalies spurred the development of multifactor models, such as the Fama-French three-factor model (Fama and French 1993), which augment beta with size and value factors. Beyond these, market concentration has emerged as a critical lens for understanding asset pricing deviations. Hou and Robinson (2006) found that firms in concentrated industries earn higher returns, attributing this to economic rents from market power. Edmans (2009) linked ownership concentration to superior performance, while Choi et al. (2017) showed that institutional investors with concentrated portfolios outperform diversified ones. Neuhann and Sockin (2024) explored how financial market concentration distorts capital allocation, and Bustamante and Donangelo (2017) tied product market concentration to industry returns.\nFrom a theoretical perspective, Magill and Quinzii (1996) argued that incomplete markets—lacking sufficient contingent claims—prevent full risk hedging, challenging CAPM assumptions. Cochrane (1996) emphasized the role of investment-based pricing, while Campbell (1992) critiqued volatility as an incomplete risk measure. Socioeconomic analyses, such as Saez and Zucman (2016), further connect market concentration to wealth inequality, highlighting broader implications. This rich body of literature suggests that market structure and concentration significantly complicate the CAPM’s risk-return framework, necessitating a deeper examination.\n\n\nMain\n\nEmpirical and Mathematical Foundations of Diversification\nDiversification’s risk-reducing power is well-established empirically and mathematically. Elton and Gruber (1977) analyzed 3,290 securities, demonstrating that a portfolio of just four stocks markedly reduces variance compared to a single stock, underscoring diversification’s practical utility.\nMathematically, consider an equally weighted portfolio of \\(n\\) securities. The portfolio variance \\(\\sigma_p^2\\) is given by:\n\\[\n\\sigma_p^2 = \\frac{1}{n} \\bar{\\sigma}^2 + \\frac{n-1}{n} \\bar{\\rho} \\bar{\\sigma}^2\n\\]\nwhere:\n\n\\(\\bar{\\sigma}^2\\) = average variance of individual securities\n\n\\(\\bar{\\rho}\\) = average correlation between securities\n\nAs \\(n\\) increases, \\(\\sigma_p^2\\) converges to \\(\\bar{\\rho} \\bar{\\sigma}^2\\), indicating that covariance, not individual variance, dominates portfolio risk. When \\(\\bar{\\rho} &lt; 1\\), diversification lowers volatility, resembling how higher-order terms in Taylor polynomials diminish to smooth a function or how fractal geometry simplifies irregularities with scale.\n\n\nA Critique of Risk as Volatility\nThe CAPM equates risk with volatility, but this assumption is narrow. Long-term investors may prioritize structural risks—e.g., economic shifts or sector obsolescence—over short-term price swings. A volatile growth stock might be less “risky” to them than a stable but declining asset. Campbell (1992) supports this critique, arguing that volatility oversimplifies the multifaceted nature of risk, a view echoed by behavioral finance perspectives (Shiller 2003).\n\n\nDiversification and the Risk-Return Trade-Off\nThe CAPM ties diversification to the risk-return trade-off, suggesting investors can eliminate unsystematic risk while earning returns proportional to systematic risk exposure. In a stochastic setting, diverse agents (e.g., farmers, energy producers) share idiosyncratic risks, enhancing welfare (Cochrane 2009). Yet, this assumes a broad, competitive market. When the market portfolio—say, the S&P 500—is dominated by a few highly correlated stocks (e.g., tech giants), diversification falters. High \\(\\bar{\\rho}\\) reduces variance’s sensitivity to \\(n\\), undermining the CAPM’s benefits (Grullon, Larkin, and Michaely 2019).\n\n\nMarket Concentration and the Upper Frontier\nIn the standard arbitrage-free asset pricing framework, the upper mean-variance frontier assets correlate perfectly (negatively) with the stochastic discount factor (SDF). In concentrated markets, dominant firms with economic moats (Bustamante and Donangelo 2017) act as principal components, compressing the payoff space. For example, if the Herfindahl-Hirschman Index (HHI) measures this dominance or concentration, then a high HHI signals reliance on few assets, limiting diversification. Investors may then seek arbitrage in these stocks, amplifying concentration (Valta 2012).\n\n\nImplications for Investors and Market Stability\nIn concentrated markets, diversification yields to capturing rents from dominant firms. These firms use primary-market capital to reinforce moats, distributing profits rather than fostering competition (Hou and Robinson 2006). Secondary-market trading becomes zero-sum, redistributing wealth without value creation. Early investors in concentrated stocks gain disproportionately, widening inequality (Saez and Zucman 2016) and challenging the CAPM’s traditional data-driven risk-return logic.\n\n\nBroader Socioeconomic Consequences\nConcentration reduces contingent claim diversity, impairing hedging capacity (Magill and Quinzii 1996). A shock to a dominant sector triggers systemic ripples, increasing instability. This homogeneity shifts markets from managing uncertainty to rewarding market power, exacerbating wealth gaps and contradicting the CAPM’s egalitarian risk-sharing ideal.\n\n\n\nConclusion\nThe CAPM’s diversification-driven risk-return framework faces significant challenges from market concentration. As diversification weakens, investors prioritize rents over risk reduction, simplifying markets into systems dominated by a few firms. This shift threatens stability, equity, and hedging capacity, urging a rethinking of the CAPM and policies to enhance market diversity.\n\n\n\n\n\nReferences\n\nBanz, Rolf W. 1981. “The Relationship Between Return and Market Value of Common Stocks.” Journal of Financial Economics 9 (1): 3–18.\n\n\nBasu, Sanjoy. 1977. “Investment Performance of Common Stocks in Relation to Their Price‐earnings Ratios: A Test of the Efficient Market Hypothesis.” The Journal of Finance 32 (3): 663–82.\n\n\nBustamante, M Cecilia, and Andres Donangelo. 2017. “Product Market Competition and Industry Returns.” The Review of Financial Studies 30 (12): 4216–66.\n\n\nCampbell, John Y. 1992. “Intertemporal Asset Pricing Without Consumption Data.” National Bureau of Economic Research Cambridge, Mass., USA.\n\n\nChoi, Nicole, Mark Fedenia, Hilla Skiba, and Tatyana Sokolyk. 2017. “Portfolio Concentration and Performance of Institutional Investors Worldwide.” Journal of Financial Economics 123 (1): 189–208.\n\n\nCochrane, John H. 1996. “A Cross-Sectional Test of an Investment-Based Asset Pricing Model.” Journal of Political Economy 104 (3): 572–621.\n\n\n———. 2009. Asset Pricing: Revised Edition. Princeton university press.\n\n\nEdmans, Alex. 2009. “Blockholder Trading, Market Efficiency, and Managerial Myopia.” The Journal of Finance 64 (6): 2481–2513.\n\n\nElton, Edwin J, and Martin J Gruber. 1977. “Risk Reduction and Portfolio Size: An Analytical Solution.” The Journal of Business 50 (4): 415–37.\n\n\nFama, Eugene F, and Kenneth R French. 1993. “Common Risk Factors in the Returns on Stocks and Bonds.” Journal of Financial Economics 33 (1): 3–56.\n\n\nGrullon, Gustavo, Yelena Larkin, and Roni Michaely. 2019. “Are US Industries Becoming More Concentrated?” Review of Finance 23 (4): 697–743.\n\n\nHou, Kewei, and David T Robinson. 2006. “Industry Concentration and Average Stock Returns.” The Journal of Finance 61 (4): 1927–56.\n\n\nLintner, John. 1965. “The Valuation of Risk Assets and the Selection of Risky Investments in Stock Portfolios and Capital Budgets.” The Review of Economics and Statistics 47 (1): 13–37.\n\n\nMagill, Michael, and Martine Quinzii. 1996. “Incomplete Markets over an Infinite Horizon: Long-Lived Securities and Speculative Bubbles.” Journal of Mathematical Economics 26 (1): 133–70.\n\n\nMossin, Jan. 1966. “Equilibrium in a Capital Asset Market.” Econometrica 34 (4): 768–83.\n\n\nNeuhann, Daniel, and Michael Sockin. 2024. “Financial Market Concentration and Misallocation.” Journal of Financial Economics 159: 103875.\n\n\nRoss, Stephen A. 1976. “The Arbitrage Theory of Capital Asset Pricing.” Journal of Economic Theory 13 (3): 341–60.\n\n\nSaez, Emmanuel, and Gabriel Zucman. 2016. “Wealth Inequality in the United States Since 1913: Evidence from Capitalized Income Tax Data.” The Quarterly Journal of Economics 131 (2): 519–78.\n\n\nSharpe, William F. 1964. “Capital Asset Prices: A Theory of Market Equilibrium Under Conditions of Risk.” The Journal of Finance 19 (3): 425–42.\n\n\nShiller, Robert J. 2003. “From Efficient Markets Theory to Behavioral Finance.” Journal of Economic Perspectives 17 (1): 83–104.\n\n\nValta, Philip. 2012. “Competition and the Cost of Debt.” Journal of Financial Economics 105 (3): 661–82.",
    "crumbs": [
      "교육",
      "설명? 예측?",
      "Revisiting the CAPM"
    ]
  },
  {
    "objectID": "corner_solution.html",
    "href": "corner_solution.html",
    "title": "Corner Solutions",
    "section": "",
    "text": "1. Introduction\nIn standard economic theory, both consumer preferences and production sets are generally assumed to exhibit convexity (Arrow and Debreu 1954; Debreu 1959). This assumption supports foundational results, including the existence and uniqueness of equilibrium and the efficiency of market allocations. In practice, however, features such as network externalities (Katz and Shapiro 1985; Rochet and Tirole 2003), rent-seeking (Shleifer and Vishny 1993), and multiple equilibria—often culminating in pronounced market dominance—can produce outcomes resembling non-convex preferences (Arthur 1994). In many cases, corner solutions and path-dependent equilibria emerge from winner-takes-all dynamics, concentrated economic power, and barriers to entry.\n\n\n2. Convexity in Economic Theory\n2.1 Convex Preferences and Production Sets\n\nConsumer preferences are typically modeled with quasi-concave utility functions, yielding convex (or “bowl-shaped”) indifference curves. This setup implies a preference for diversity in consumption, rather than extreme or corner solutions (Debreu 1959).\nProducers are often assumed to face diminishing marginal returns, reflected in a convex production possibility set. Under such conditions, output expansions follow a predictable pattern, and average costs rise eventually.\n\n2.2 Existence and Efficiency of Equilibrium\n\nWith convexity, free market entry, symmetric information, and price-taking behavior, perfectly competitive markets are shown to possess a stable equilibrium that is Pareto efficient (Arrow and Debreu 1954).\nThese results typically rely on fixed-point theorems and the properties of convex sets, ensuring both the existence of equilibrium prices and (in many cases) uniqueness or stability (Debreu 1959).\n\n2.3 Normative Implications\n\nConvexity underpins the normative stance that, absent significant market failures, competitive markets gravitate toward Pareto-efficient resource allocations.\nConsequently, government interventions usually aim to correct externalities, public goods issues, or information asymmetries within a broader context of largely convex preferences and production sets.\n\n\n\n3. Non-Convexities in Reality\n3.1 Network Externalities and Increasing Returns to Scale\n\nIn contrast to diminishing returns, many digital or platform-based markets exhibit network externalities, or increasing returns to scale (Katz and Shapiro 1985; Rochet and Tirole 2003). As additional users join a platform, its value to each user grows, often driving corner solutions in both production and consumption.\nInstead of smoothly concave utility or production functions, certain markets feature segments of increasing marginal returns, leading to “winner-takes-all” or “winner-take-most” dynamics.\n\n3.2 Coordination Games and Multiple Equilibria\n\nNetwork externalities commonly create coordination games, where each agent’s optimal choice depends on the choices of others. Small initial advantages or random shocks may tip the market toward a specific product or standard, resulting in lock-in (Arthur 1994).\nSuch scenarios can produce multiple Nash equilibria, for instance everyone choosing Product A or everyone choosing Product B, with potentially large welfare differences between them.\n\n3.3 Extreme or Corner Solutions in Consumption and Production\n\nWith robust network effects, consumers or producers may converge on a single brand, platform, or location, effectively marginalizing other options—even if those alternatives might have been preferred under purely convex preferences.\nThese corner solutions deviate from the classical idea that diversification in consumption and moderate scales in production yield optimal outcomes.\n\n3.4 Rent-Seeking and Incumbent Power\n\nDominant firms or groups can exploit political influence—through lobbying or regulatory capture—to fortify their positions, reinforcing non-convex outcomes by stifling competition (Tirole 1988; Shleifer and Vishny 1993).\nRent-seeking intensifies the misallocation of resources, as efforts are diverted to defending or reinforcing incumbents’ power, often via barriers to entry, reduced competition, and growing inequalities.\n\n\n\n4. Government Interventions\n4.1 Theoretical View: Correcting Market Imperfections\n\nTraditionally, policy interventions focus on addressing market failures, assuming that preferences and technologies remain fundamentally convex and that interventions are limited and transparent.\n\n4.2 Empirical Evidence: Policy Amplifies Non-Convexities\n\nIn reality, incumbents can wield outsized influence through lobbying and political capture, prompting policies that strengthen market concentration (Tirole 1988).\nInstead of fostering genuinely competitive markets, such policies may lock in non-convex outcomes, creating a vicious cycle of entrenched monopolistic power and limited competition.\n\n4.3 Lock-in and Path Dependence\n\nWhen policy-making aligns with incumbent interests, even minor advantages can become self-reinforcing (Arthur 1994).\nConsequently, once a market tips toward a specific firm, region, or product, effective competition may prove infeasible without sweeping policy reforms or disruptive innovation.\n\n\n\n5. Conclusion\nAlthough classical economic models lean on convex preferences and technologies to assert the existence of unique, efficient equilibria, real-world dynamics often revolve around non-convex phenomena. Network externalities, coordination failures, and rent-seeking can drive corner solutions, multiple equilibria, and lock-in that preserve incumbent advantages. Far from mitigating these issues, government policies sometimes exacerbate them through preferential treatment of dominant actors. Recognizing these non-convex realities is crucial for crafting policy frameworks that transcend purely theoretical assumptions of convexity and address the path-dependent complexity characterizing modern markets.\n\n\nAppendix: Utilitarian Objective function\n\n\nCode\n#@title Utilitarian objective function\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom scipy.optimize import minimize\n\n# 함수 정의\ndef z_function(x, y, a, b):\n  return y * (x**a) + (1 - y) * ((b - x)**a)\n\n# x, y 범위 및 매개변수 설정\na = 0.3  # 매개변수 a 값 (0과 1 사이)\nb = 20  # 매개변수 b 값\n\nx = np.linspace(0, b, 100)  # x 범위: 0부터 20까지 100개의 점\ny = np.linspace(0, 1, 100)  # y 범위: 0부터 1까지 100개의 점\nX, Y = np.meshgrid(x, y)  # x, y 좌표 격자 생성\n\n\n# Z 값 계산\nZ = z_function(X, Y, a, b)\n\n\ndef negative_z_function(params):\n    x, y = params\n    return -z_function(x, y, a, b)  # 최솟값을 찾기 위해 음수 값 반환\n\n# 초기값 설정 (interior 범위 내)\ninitial_guess = [b / 2, 0.5]\n\n# 경계 조건 설정\nbounds = [(0, b), (0, 1)]\n\n# 최적화 실행\nresult = minimize(negative_z_function, initial_guess, bounds=bounds)\n\n# 결과 추출\nextreme_point_x, extreme_point_y = result.x\nextreme_point_z = z_function(extreme_point_x, extreme_point_y, a, b)\n\nprint(\"Extreme Point (x, y, z):\", extreme_point_x, extreme_point_y, extreme_point_z)\n\n# Calculate Hessian matrix\ndef hessian_matrix(x, y, a, b):\n  \"\"\"Calculates the Hessian matrix of the z_function.\"\"\"\n  d2z_dx2 = a * (a - 1) * (y * (x**(a - 2)) + (1 - y) * ((b - x)**(a - 2)))\n  d2z_dy2 = 0  # Second derivative with respect to y is 0\n  d2z_dxdy = a * (x**(a - 1) - (b - x)**(a - 1))\n  d2z_dydx = d2z_dxdy  # Mixed partial derivatives are equal\n\n  return [[d2z_dx2, d2z_dxdy], [d2z_dydx, d2z_dy2]]\n\n# Determine the type of extreme point\nhessian = hessian_matrix(extreme_point_x, extreme_point_y, a, b)\ndeterminant = np.linalg.det(hessian)\n\nif determinant &gt; 0 and hessian[0][0] &gt; 0:\n  extreme_type = \"Minimum\"\nelif determinant &gt; 0 and hessian[0][0] &lt; 0:\n  extreme_type = \"Maximum\"\nelse:\n  extreme_type = \"Saddle\"\n\nprint(\"Extreme Point Type:\", extreme_type)\n\n\n# 3D 그래프 그리기\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(X, Y, Z)\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('z')\nplt.title('3D Graph of z = y*x^a + (1-y)(b-x)^a')\n\n\n# global interior extreme point 표시\nax.scatter(extreme_point_x, extreme_point_y, extreme_point_z, color='red', marker='o', s=100)\nax.text(extreme_point_x, extreme_point_y, extreme_point_z, f'Extreme Point ({extreme_type})', color='red')\n\nplt.show()\n\n# Contour Plot 그리기\nfig, ax = plt.subplots()\ncontour = ax.contour(X, Y, Z)\nax.set_xlabel('x')\nax.set_ylabel('y')\nplt.title('Contour Plot of z = y*x^a + (1-y)(b-x)^a')\nplt.clabel(contour, inline=1, fontsize=10)\n\n# global interior extreme point 표시\nax.scatter(extreme_point_x, extreme_point_y, color='red', marker='o', s=100)\nax.text(extreme_point_x, extreme_point_y, 'Extreme Point', color='red')\n\nplt.show()\n\n\nExtreme Point (x, y, z): 10.0 0.5 1.9952623149688795\nExtreme Point Type: Saddle\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAppendix: Homogeneous function of degree 1\n\n\nCode\n# Define a return to scale\nscale = 1 # Constant return to scale, i.e. Homogeneous function of degree 1\n\n# Define parameter a\na = 1/4\n\n# total wealth of x\nk_x = 2\n# total wealth of y\nk_y = 2\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 경고 메시지 숨기기\nnp.seterr(invalid='ignore')\n\ndef numerical_derivative(f, X, Y, h=1e-5):\n    \"\"\" Compute numerical partial derivatives using central difference method.\"\"\"\n    dfdx = (f(X + h, Y) - f(X - h, Y)) / (2 * h)  # ∂f/∂x\n    dfdy = (f(X, Y + h) - f(X, Y - h)) / (2 * h)  # ∂f/∂y\n    return dfdx, dfdy\n\n# Define functions u_1(x,y) = x^a * y^(1-a) and u_2(x,y) = (2-x)(2-y)\ndef u1(x, y):\n    return x**(scale*a) * y**(scale*(1-a))\n\ndef u2(x, y):\n    return (k_x - x)**(scale*a) * (k_y - y)**(scale*(1-a))\n\n# Define the grid\nx = np.linspace(0, k_x, 15)\ny = np.linspace(0, k_y, 15)\nX, Y = np.meshgrid(x, y)\n\n# Compute the numerical derivatives (vector field components)\nU1, V1 = numerical_derivative(u1, X, Y)\nU2, V2 = numerical_derivative(u2, X, Y)\n\n# Reduce the density of vectors for better visualization\nx_sparse = np.linspace(0, k_x, 8)\ny_sparse = np.linspace(0, k_y, 8)\nX_sparse, Y_sparse = np.meshgrid(x_sparse, y_sparse)\nU1_sparse, V1_sparse = numerical_derivative(u1, X_sparse, Y_sparse)\nU2_sparse, V2_sparse = numerical_derivative(u2, X_sparse, Y_sparse)\n\n# Plot the combined vector fields and contour plots\n#plt.figure(figsize=(8, 8))\n\n# Contour plots of u_1 and u_2 (level curves only)\ncontour1 = plt.contour(X, Y, u1(X, Y), colors='blue', linestyles='solid', linewidths=1)\ncontour2 = plt.contour(X, Y, u2(X, Y), colors='red', linestyles='dashed', linewidths=1)\n\n# Overlay vector fields\nplt.quiver(X_sparse, Y_sparse, U1_sparse, V1_sparse, color='b', angles='xy', label='∇$u_1$')\nplt.quiver(X_sparse, Y_sparse, U2_sparse, V2_sparse, color='r', angles='xy', label='∇$u_2$')\n\n# Labels and grid\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Gradient Vector Fields & Level Curves of $u_1$ and $u_2$')\nplt.legend()\nplt.grid('scaled')\nplt.axis('square')\n\nplt.tight_layout()\n# Show the plot\nplt.show()\n\n# Compute the sum of gradients\nU_sum = U1 + U2\nV_sum = V1 + V2\n\n# Reduce the density of vectors for better visualization\nU_sum_sparse, V_sum_sparse = numerical_derivative(lambda x, y: u1(x, y) + u2(x, y), X_sparse, Y_sparse)\n\n# Plot the combined vector fields and contour plots\n#plt.figure(figsize=(8, 8))\n\n# Contour plots of u_1 and u_2 (level curves only)\ncontour1 = plt.contour(X, Y, u1(X, Y), colors='blue', linestyles='solid', linewidths=1)\ncontour2 = plt.contour(X, Y, u2(X, Y), colors='red', linestyles='dashed', linewidths=1)\n\n# Overlay sum of gradient vector fields\nplt.quiver(X_sparse, Y_sparse, U_sum_sparse, V_sum_sparse, color='g', angles='xy', label='∇($u_1 + u_2$)')\n\n# Labels and grid\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Sum of Gradient Vector Fields & Level Curves of $u_1$ and $u_2$')\nplt.legend()\n\nplt.grid('scaled')\nplt.axis('square')\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAppendix: Sigmoid utility function\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Define constants\nkx = (np.pi**3 / 2) ** (1/3)\nky = (2**(1/2)) * ((np.pi**3 / 2) ** (1/3))\n\n# Define the grid\nx = np.linspace(0, kx, 1000)\ny = np.linspace(0, ky, 1000)\nX, Y = np.meshgrid(x, y)\n\n# Define the functions\nu1 = 1 - np.cos(X**(1/3) * Y**(2/3))\nu2 = 1 - np.cos((kx - X)**(1/3) * (ky - Y)**(2/3))\n\n# Find intersection points where u1 == u2\nthreshold = 1e-3  # Numerical tolerance for equality\nintersection_mask = np.abs(u1 - u2) &lt; threshold\nX_intersect = X[intersection_mask]\nY_intersect = Y[intersection_mask]\nZ_intersect = u1[intersection_mask]  # u1 and u2 are nearly equal\n\n# Create 3D plot\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection='3d')\n\n# Plot intersection line\nax.scatter(X_intersect, Y_intersect, Z_intersect, color='black', s=10, label='Intersection Line')\n\n# Surface plots for reference\nax.plot_surface(X, Y, u1, cmap='Blues', alpha=0.5)\nax.plot_surface(X, Y, u2, cmap='Reds', alpha=0.5)\n\n# Labels and title\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\nax.set_title('3D Intersection of $u_1$ and $u_2$')\nax.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nArrow, Kenneth J., and Gerard Debreu. 1954. “Existence of an Equilibrium for a Competitive Economy.” Econometrica 22 (3): 265–90.\n\n\nArthur, W. Brian. 1994. Increasing Returns and Path Dependence in the Economy. Ann Arbor, MI: University of Michigan Press.\n\n\nDebreu, Gerard. 1959. Theory of Value: An Axiomatic Analysis of Economic Equilibrium. New Haven, CT: Yale University Press.\n\n\nKatz, Michael L., and Carl Shapiro. 1985. “Network Externalities, Competition, and Compatibility.” The American Economic Review 75 (3): 424–40.\n\n\nRochet, Jean-Charles, and Jean Tirole. 2003. “Platform Competition in Two-Sided Markets.” Journal of the European Economic Association 1 (4): 990–1029.\n\n\nShleifer, Andrei, and Robert W. Vishny. 1993. “Corruption.” The Quarterly Journal of Economics 108 (3): 599–617.\n\n\nTirole, Jean. 1988. The Theory of Industrial Organization. Cambridge, MA: MIT Press.",
    "crumbs": [
      "교육",
      "분열",
      "Corner Solutions"
    ]
  },
  {
    "objectID": "gilded_age.html",
    "href": "gilded_age.html",
    "title": "New Gilded Age",
    "section": "",
    "text": "The term ‘Gilded Age’ was originally coined by Mark Twain in his novel The Gilded Age: A Tale of Today (Twain and Warner 1873), describing an era characterized by rapid economic expansion, extreme wealth concentration, and political corruption. A similar dynamic is emerging today, where financial and technological elites dominate economic output while wealth inequality reaches historic highs (Piketty 2014). The late 19th century saw industrial monopolies like Standard Oil and U.S. Steel controlling markets; today, tech giants such as Amazon, Apple, and Google exhibit similar dominance (Zucman 2019).\nSimultaneously, the Federal Reserve’s response to financial instability, particularly through excessive monetary expansion, contrasts with past policy mistakes that led to severe economic contractions due to monetary shrinkage (Bernanke 2000). If current economic trends persist—marked by the increasing concentration of wealth, hyperinflation risks, and geopolitical tensions—then the U.S. may be heading toward another crisis akin to the 1929 stock market collapse (Kindleberger 1978).",
    "crumbs": [
      "교육",
      "분열",
      "New Gilded Age"
    ]
  },
  {
    "objectID": "gilded_age.html#extreme-wealth-concentration-and-economic-disparities",
    "href": "gilded_age.html#extreme-wealth-concentration-and-economic-disparities",
    "title": "New Gilded Age",
    "section": "Extreme Wealth Concentration and Economic Disparities",
    "text": "Extreme Wealth Concentration and Economic Disparities\nIn the late 19th century, “Robber Barons” controlled vast industrial empires while working-class Americans suffered under exploitative labor conditions (Irwin 2017). Today, the economic landscape reflects a similar dynamic: the top 1% of Americans hold over 30% of total U.S. wealth, and financial markets remain dominated by a handful of institutional investors and corporations (Saez and Zucman 2020). If historical trends hold, wealth concentration at this level often precedes financial and political crises.",
    "crumbs": [
      "교육",
      "분열",
      "New Gilded Age"
    ]
  },
  {
    "objectID": "gilded_age.html#financial-market-distortions-due-to-federal-reserve-policies",
    "href": "gilded_age.html#financial-market-distortions-due-to-federal-reserve-policies",
    "title": "New Gilded Age",
    "section": "Financial Market Distortions Due to Federal Reserve Policies",
    "text": "Financial Market Distortions Due to Federal Reserve Policies\nHistorically, the Federal Reserve’s failure to manage monetary policy effectively has exacerbated financial downturns. During the Great Depression, the Fed allowed the money supply to contract, worsening deflation (Friedman and Schwartz 1993). Conversely, in the 2008 financial crisis, the Fed implemented massive QE programs to avoid liquidity shortages (Gopinath and Gourinchas 2020). If the Fed continues expanding the money supply unchecked while maintaining low interest rates, it could trigger runaway inflation or asset bubbles (Reinhart and Rogoff 2010).",
    "crumbs": [
      "교육",
      "분열",
      "New Gilded Age"
    ]
  },
  {
    "objectID": "gilded_age.html#protectionist-policies-and-global-trade-disruptions",
    "href": "gilded_age.html#protectionist-policies-and-global-trade-disruptions",
    "title": "New Gilded Age",
    "section": "Protectionist Policies and Global Trade Disruptions",
    "text": "Protectionist Policies and Global Trade Disruptions\nIn response to financial instability, the U.S. may turn to protectionist measures similar to those seen in the early 20th century, such as the Smoot-Hawley Tariff Act (Irwin 2017). If the U.S. imposes broad tariffs on allies like Canada, Mexico, and the EU (excluding the UK), retaliatory tariffs could significantly reduce global trade, accelerating economic fragmentation (Acker 2020).",
    "crumbs": [
      "교육",
      "분열",
      "New Gilded Age"
    ]
  },
  {
    "objectID": "gilded_age.html#capital-controls",
    "href": "gilded_age.html#capital-controls",
    "title": "New Gilded Age",
    "section": "Capital Controls",
    "text": "Capital Controls\nTo prevent capital flight, the U.S. government might implement capital controls, restricting the movement of funds outside the country (Dornbusch 1996). Such policies could initially stabilize domestic financial markets by preventing liquidity outflows, but they would ultimately deter foreign investment and reduce the credibility of the U.S. dollar (Prasad 2021). If capital controls are implemented alongside protectionist trade policies, the global financial system could realign, reducing reliance on the dollar (Eichengreen 2019).",
    "crumbs": [
      "교육",
      "분열",
      "New Gilded Age"
    ]
  },
  {
    "objectID": "gilded_age.html#internal-conflict",
    "href": "gilded_age.html#internal-conflict",
    "title": "New Gilded Age",
    "section": "Internal Conflict",
    "text": "Internal Conflict\nWith increasing partisan division, the U.S. could experience state-led resistance against federal economic policies. Democratic-led states might oppose Republican federal mandates, leading to legal disputes over taxation, social policies, and trade regulations (Levitsky and Ziblatt 2018). In extreme cases, states like California could advocate for economic or political autonomy, mirroring secessionist movements of the 19th century, while Texas, despite its strong Republican leanings, might push for greater state sovereignty in response to federal overreach or shifting national policies (Acker 2020).",
    "crumbs": [
      "교육",
      "분열",
      "New Gilded Age"
    ]
  },
  {
    "objectID": "gilded_age.html#u.s.-dollar-as-the-global-reserve-currency",
    "href": "gilded_age.html#u.s.-dollar-as-the-global-reserve-currency",
    "title": "New Gilded Age",
    "section": "U.S. Dollar as the Global Reserve Currency",
    "text": "U.S. Dollar as the Global Reserve Currency\nThe decline of the British pound post-World War II illustrates how global reserve currencies can lose dominance due to internal and external economic shifts (Eichengreen 2019). If U.S. political instability continues, central banks worldwide may accelerate diversification away from dollar holdings, increasing reliance on alternative financial networks such as BRICS payment systems, Bitcoin, and other emerging digital currencies. (Prasad 2021).",
    "crumbs": [
      "교육",
      "분열",
      "New Gilded Age"
    ]
  },
  {
    "objectID": "market_conditions.html",
    "href": "market_conditions.html",
    "title": "Free Market Conditions",
    "section": "",
    "text": "This appendix examines the interdependence of key theoretical conditions in economics and finance: Perfectly Competitive Markets (PCM), the Capital Asset Pricing Model (CAPM), the Efficient Market Hypothesis (EMH), and the No-Arbitrage (NA) condition. While these concepts share common foundations in equilibrium and efficiency, they serve distinct purposes. Their structural assumptions determine how prices are formed, how risk is allocated, and whether arbitrage opportunities persist in the long run.\n\nPerfectly Competitive Market (PCM) and Its Implications for Financial Markets\nPCM represents an idealized economic structure where prices are determined by market forces under strict competition. The defining characteristics include:\n\nMany buyers and sellers, ensuring a high degree of competition.\n\nPrice-taking behavior, where no single participant influences market prices.\n\nHomogeneous goods, meaning that products are perfect substitutes.\n\nInformation symmetry, ensuring that all participants have equal access to relevant data.\n\nNo friction on entry and exit, allowing participants to freely enter or exit without cost.\n\nZero long-run economic profits, as competition eliminates excess returns in equilibrium.\n\nThe welfare theorem states that if all traders have convex preferences in PCM, then the equilibrium allocation of limited resources is Pareto efficient. However, PCM does not explicitly incorporate risk, distinguishing it from financial models such as CAPM. Although PCM shares price-taking behavior and information symmetry with CAPM, it does not inherently account for risk-return trade-offs, which are central to financial market equilibrium.\n\n\nThe Capital Asset Pricing Model (CAPM) and Systematic Risk Pricing\nCAPM is a single-factor asset pricing model that determines equilibrium asset prices based on systematic risk exposure. It extends the competitive market framework to financial markets under the following assumptions:\n\nMean-variance optimization, where investors maximize expected utility based on risk-return trade-offs.\n\nRisk characterization through two moments, assuming that asset returns are fully described by their mean and variance.\n\nHomogeneous assets, meaning financial assets can be interpreted as contingent claims (i.e., claims on future payoffs that can be normalized using a risk-free currency), ensuring that the law of one price holds.\n\nInformation symmetry, allowing all investors to have identical expectations about risk and returns.\n\nFrictionless markets, with no transaction costs, taxes, or constraints on borrowing and lending.\n\nMarket portfolio as the tangency portfolio, implying that all investors hold a combination of the risk-free asset and the market portfolio.\n\nWhile CAPM shares PCM’s assumption of competitive equilibrium, it explicitly incorporates systematic risk pricing, which is absent in traditional PCM models.\n\n\nNo-Arbitrage (NA) and the Role of Market Structures\nThe NA condition is fundamental to financial market sustainability. Unlike PCM and CAPM, which describe equilibrium-based price formation, NA ensures that risk-free arbitrage does not persist indefinitely.\nPersistent arbitrage leads to:\n\nCapital concentration, where wealth accumulates disproportionately among arbitrageurs, reducing market competitiveness.\n\nInefficient risk-sharing, as distortions in capital allocation prevent the effective pricing of idiosyncratic risk.\n\nMarket failure, where financial markets lose their role as efficient allocators of capital.\n\nNA is not a direct consequence of PCM or CAPM but rather a structural requirement for a well-functioning financial system. The absence of arbitrage opportunities is necessary for risk-adjusted returns to reflect systematic risk rather than mispricing.\n\n\nThe Efficient Market Hypothesis (EMH) as an Informational Condition for NA\nEMH provides an informational framework under which NA can hold. NA requires that market participants act on available information to eliminate arbitrage. This is feasible if:\n\nInvestors are rational, meaning they respond optimally to profit opportunities.\n\nInformation is symmetric, ensuring that arbitrage opportunities do not persist due to asymmetric knowledge.\n\nHowever, empirical evidence suggests that NA does not always hold due to:\n\nBehavioral biases, where sentiment-driven trading creates arbitrage opportunities.\n\nMarket microstructure effects, where asymmetric information and liquidity constraints delay arbitrage elimination.\n\nWhile EMH supports NA in principle, it is not sufficient for ensuring arbitrage-free markets in the presence of bounded rationality and institutional frictions.\n\n\nEmpirical Implications of Arbitrage Persistence\nEmpirical deviations from NA and EMH do not necessarily invalidate these theories but highlight structural inefficiencies in financial markets. Persistent arbitrage opportunities suggest:\n\nInvestor irrationality, supporting behavioral finance explanations.\n\nInformation asymmetry, consistent with market microstructure theories.\n\nRegulatory distortions, such as too-big-to-fail policies, short-selling bans, and liquidity constraints.\n\nIf arbitrage persists over long time horizons, this may signal failures in market structure that undermine competition and risk allocation. The consequences include:\n\nExcessive capital concentration, reinforcing financial monopolies.\n\nDeterioration in risk-sharing mechanisms, reducing market efficiency.\n\nNA is not just an empirical observation but a necessary structural condition for financial market stability.\n\n\nThe Role of Institutional Frameworks\nThe interplay between NA, EMH, and CAPM depends on regulatory and institutional safeguards. While theoretical models assume that arbitrage disappears naturally, real-world markets require institutional mechanisms to enforce NA and maintain EMH. Key mechanisms include:\n\nFinancial disclosure requirements, ensuring that relevant information is accessible to all participants.\n\nMarket integrity measures, such as circuit breakers and trade halts, preventing extreme mispricings.\n\nRegulations on leverage and short-selling, reducing excessive arbitrage concentration.\n\nLiquidity provisions and market-making incentives, ensuring that mispricings do not persist due to temporary liquidity shortages.\n\nWithout these structural conditions, arbitrage can persist, distorting the intended function of financial markets. The ability of CAPM to predict asset prices, the validity of EMH, and the enforcement of NA all depend on institutional frameworks that mitigate market failures.\n\n\nConceptual Summary\n\n\n\n\n\n\n\n\n\n\nFeature\nPCM\nCAPM\nNA\nEMH\n\n\n\n\nMarket Type\nCompetitive goods market\nCompetitive capital markets\nStructural equilibrium condition\nInformational efficiency condition\n\n\nPrice Mechanism\nSupply and demand equilibrium\nRisk-return equilibrium (beta-based)\nElimination of risk-free arbitrage\nPrices fully reflect information\n\n\nInformation Symmetry\nYes\nYes\nNot required, but supports NA\nNecessary for strong-form EMH\n\n\nMarket Frictions\nNo friction on entry & exit\nUnlimited borrowing/lending assumption\nPrevents arbitrage persistence\nCan be affected by liquidity constraints\n\n\n\nNA, EMH, and CAPM all rely on market structures to function effectively. While PCM provides the foundation for competitive equilibrium, it lacks a risk framework, which CAPM introduces. NA serves as a constraint that ensures risk-free arbitrage does not disrupt financial markets, while EMH posits that prices reflect available information, a condition necessary but not sufficient for NA. These theories, though distinct, are interconnected through institutional safeguards and regulatory mechanisms that sustain financial market efficiency.",
    "crumbs": [
      "교육",
      "분열",
      "Free Market Conditions"
    ]
  },
  {
    "objectID": "modern_matthew.html",
    "href": "modern_matthew.html",
    "title": "Modern Matthew Effect",
    "section": "",
    "text": "This report extends my empirical findings from the ME5 PRIOR research portfolios (of Fama and French) into a broader philosophical and socioeconomic interpretation. I draw connections between large-cap momentum dynamics in capital markets and foundational ideas from ethics, social choice theory, and political philosophy.\nThe core phenomenon observed—persistent outperformance among firms that are already the largest and most successful—bears striking resemblance to the biblical and sociological concept of the Matthew Effect:\n\n“For to every one who has, more will be given…” — Matthew 25:29\n\nIn financial markets, this is not merely a parable—it is measurable, structural, and persistent. It reveals that what appears to be a free, fair, and competitive economy may in fact be a self-reinforcing regime of privilege, cloaked under the language of freedom, equality, fairness, and consent—the founding pillars of liberal democracy.",
    "crumbs": [
      "교육",
      "설명? 예측?",
      "Modern Matthew Effect"
    ]
  },
  {
    "objectID": "modern_matthew.html#introduction",
    "href": "modern_matthew.html#introduction",
    "title": "Modern Matthew Effect",
    "section": "",
    "text": "This report extends my empirical findings from the ME5 PRIOR research portfolios (of Fama and French) into a broader philosophical and socioeconomic interpretation. I draw connections between large-cap momentum dynamics in capital markets and foundational ideas from ethics, social choice theory, and political philosophy.\nThe core phenomenon observed—persistent outperformance among firms that are already the largest and most successful—bears striking resemblance to the biblical and sociological concept of the Matthew Effect:\n\n“For to every one who has, more will be given…” — Matthew 25:29\n\nIn financial markets, this is not merely a parable—it is measurable, structural, and persistent. It reveals that what appears to be a free, fair, and competitive economy may in fact be a self-reinforcing regime of privilege, cloaked under the language of freedom, equality, fairness, and consent—the founding pillars of liberal democracy.",
    "crumbs": [
      "교육",
      "설명? 예측?",
      "Modern Matthew Effect"
    ]
  },
  {
    "objectID": "modern_matthew.html#from-empirical-momentum-to-structural-inequality",
    "href": "modern_matthew.html#from-empirical-momentum-to-structural-inequality",
    "title": "The Modern Matthew Effect",
    "section": "From Empirical Momentum to Structural Inequality",
    "text": "From Empirical Momentum to Structural Inequality\nOur analysis of the ME5 PRIOR portfolios revealed the following:\n\nEven within the top 20% of firms by market capitalization (ME5), those that performed best in the prior 11 months (PRIOR5) tend to continue outperforming in the subsequent period.\nThis effect is particularly strong post-2010, aligning with macroeconomic changes such as quantitative easing, the rise of passive investing, and capital concentration.\nThis implies a reinforcing feedback loop: prior success draws more capital, more capital enables further success, and performance becomes increasingly decoupled from fundamentals.\n\nThe analogy to social mobility is immediate: those who have, get more—not just relatively, but absolutely and increasingly.",
    "crumbs": [
      "교육",
      "설명? 예측?",
      "The Modern Matthew Effect"
    ]
  },
  {
    "objectID": "modern_matthew.html#implication-beyond-firms-the-matthew-effect-at-the-household-level",
    "href": "modern_matthew.html#implication-beyond-firms-the-matthew-effect-at-the-household-level",
    "title": "The Modern Matthew Effect",
    "section": "Implication Beyond Firms: The Matthew Effect at the Household Level",
    "text": "Implication Beyond Firms: The Matthew Effect at the Household Level\nIf capital lock-in dynamics operate not only at the firm level but also across households—a plausible and empirically supported assumption—the implications are far-reaching:\n\nWealthy households benefit disproportionately from capital gains and passive income\nLower- and middle-income households rely predominantly on labor income, with limited upside\nThe result is a declining probability of upward mobility, and increasing wealth stratification\n\nThis structure undermines the ideal of a meritocratic liberal democracy where opportunity is equitably distributed and fairness is institutionalized. In reality, the capital system begins to resemble a rigged equilibrium, where the initial balance is illusionary—a symbolic counterweight with no real effect.",
    "crumbs": [
      "교육",
      "설명? 예측?",
      "The Modern Matthew Effect"
    ]
  },
  {
    "objectID": "modern_matthew.html#the-philosophical-lens-arrow-spinoza-and-pascal",
    "href": "modern_matthew.html#the-philosophical-lens-arrow-spinoza-and-pascal",
    "title": "Modern Matthew Effect",
    "section": "The Philosophical Lens: Arrow, Spinoza, and Pascal",
    "text": "The Philosophical Lens: Arrow, Spinoza, and Pascal\nArrow’s utilitarian social welfare framework assumes at least some degree of fairness and fluidity among agents. But in a setting of structural inequality and declining mobility:\n\nThe expected utility of society does not necessarily increase, even if total capital grows\nThe bottom half of society may experience static or declining utility, overwhelmed by the persistent rise of the elite\n\nFrom a Spinozist viewpoint, this is a failure of a rational, self-sustaining society—it lacks balance and cohesion. For Pascal, it raises the existential irony of a system designed for all, yet serving few.\nAnd if we apply Rawls’ difference principle, we see an institutional failure to justify inequality based on benefit to the least advantaged. Liberal democracy, viewed from the perspective of financial capitalism, begins to fracture: the ideals of liberty, fairness, and equality become ceremonial rather than structural.",
    "crumbs": [
      "교육",
      "설명? 예측?",
      "Modern Matthew Effect"
    ]
  },
  {
    "objectID": "modern_matthew.html#toward-a-new-interpretation-of-market-performance",
    "href": "modern_matthew.html#toward-a-new-interpretation-of-market-performance",
    "title": "Modern Matthew Effect",
    "section": "Toward a New Interpretation of Market Performance",
    "text": "Toward a New Interpretation of Market Performance\nIn light of this, the ME5 PRIOR5 outperformance is no longer just a quantitative anomaly or a smart strategy. It is a signal—a structural symptom of capitalistic reinforcement, not unlike a planetary orbit deepening into a gravity well.\nThus, our empirical findings do more than guide portfolio construction. They challenge how we: - Model efficiency in capital markets - Measure welfare in society - Justify inequality under modern capitalism",
    "crumbs": [
      "교육",
      "설명? 예측?",
      "Modern Matthew Effect"
    ]
  },
  {
    "objectID": "modern_matthew.html#conclusion",
    "href": "modern_matthew.html#conclusion",
    "title": "Modern Matthew Effect",
    "section": "Conclusion",
    "text": "Conclusion\n\nThe Matthew Effect is not just a biblical principle—it is a lived, structural condition of our financial and social reality.\n\nThe capital markets amplify this dynamic, and unless counterbalanced by social, policy, or philosophical intervention, it threatens not only equity, but long-term stability and collective welfare.",
    "crumbs": [
      "교육",
      "설명? 예측?",
      "Modern Matthew Effect"
    ]
  },
  {
    "objectID": "modern_matthew.html#appendix-empirical-evidence-from-me5-prior-portfolios",
    "href": "modern_matthew.html#appendix-empirical-evidence-from-me5-prior-portfolios",
    "title": "The Modern Matthew Effect",
    "section": "Appendix: Empirical Evidence from ME5 PRIOR Portfolios",
    "text": "Appendix: Empirical Evidence from ME5 PRIOR Portfolios\nIn this report, we examine the persistence of momentum within the top market-cap group (ME5) using the Fama-French 25 Portfolios sorted by size and prior return. Specifically, we analyze how the performance of five portfolios (PRIOR1 to PRIOR5), sorted by prior 11-month returns within the ME5 group, has evolved between two distinct market regimes: pre-2010 and post-2010.\nThe results are striking.\n\nEven among the richest, largest companies (top 20% by market cap), those that performed best over the prior year continued to outperform in the next month—especially in the post-2010 market regime.\n\nWhat initially seemed like a behavioral or statistical curiosity turns out to reflect a deeper transformation of the market’s internal structure—a transformation where capital flows, visibility, and narrative dominance now reinforce past winners among the already-rich. In this setting, the language of freedom, fairness, and opportunity—so essential to liberal democracy—becomes less about equal access and more about locked-in advantage. The market becomes a mirror, not of fair competition, but of Matthew Effect dynamics, where “to those who have, more shall be given.”\nFor risk-aware investors engaged in short-term, monthly rebalancing, the implications are concrete:\n\nPRIOR3 offers exceptional consistency with lower volatility and downside exposure\nPRIOR5 delivers superior absolute return with tolerable risk\nBoth are based on highly liquid, mega-cap stocks, making them operationally feasible and institutionally scalable\n\n\nIn short: Even among the richest firms, past winners tend to keep winning.\nMomentum is no longer a temporary anomaly. It is part of the architecture.\n\nThis realization challenges equilibrium-based pricing models and calls into question our normative assumptions about economic fairness. If capital accumulates in a non-reverting way among the already-powerful, mobility collapses—and with it, the utilitarian social welfare that undergirds Arrow’s idealized world of dynamic fairness. What remains is a hollow pendulum: a financial system that swings, but never truly balances.\n\nMethodology\nWe compute a comprehensive set of performance metrics (e.g., annualized return, Sharpe, Sortino, Calmar, Omega, expected CRRA utility) for each ME5 PRIOR portfolio across two structural periods:\n\nPre-2010: January 1996 to December 2009\nPost-2010: January 2010 to December 2023\n\nAll returns are monthly and expressed as decimals (net of compounding). Metrics are calculated for value-weighted portfolios of NYSE-listed firms within ME5, using prior 11-month cumulative returns (excluding the most recent month) to define momentum quintiles.\n\n\nEmpirical Summary and Interpretation\n\nPre-2010:\n\nPRIOR4 (moderate momentum) was most effective for risk-adjusted returns (Sharpe 0.66, Sortino 0.94)\n\nPRIOR5 had higher return, but more volatility and negative skew\n\nPRIOR1 severely underperformed (negative average return, deep drawdowns)\n\nPost-2010:\n\nMomentum dominance intensifies\n\nPRIOR5 exhibits top raw returns (13.7%), Sortino (1.41), and CRRA utility\n\nPRIOR3 shows best Sharpe (0.91) and lowest volatility (15%)\n\nBoth dominate the risk-return landscape with resilience and predictability\n\n\nThe market is no longer cyclical, but directional; no longer egalitarian, but reinforcing. Passive capital flows, QE-induced yield suppression, and index-based exposure collectively sustain a regime where “winners win more, losers lag longer.” If the ideal of equal opportunity was once the balancing pendulum of a free market society, the modern financial system now swings in only one direction.\n\n\nME5 PRIOR Performance Comparison\n\n\nCode\nimport pandas as pd\nimport pandas_datareader.data as pdr\nimport numpy as np\nfrom scipy.stats import skew, kurtosis\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning) # FutureWarning 제거\npd.set_option('display.width', None)\npd.set_option('display.max_columns', 15)\n\n# (1) Load Fama-French data\nstart_date = '1996-01-01'\nend_date = '2023-12-31'\n\n# Define evaluation functions\np = 0.005 # 0.5% 월간 수익률 threshold\neta = 3 # CRRA risk-aversion parameter\n\nff = pdr.DataReader(\"25_Portfolios_ME_Prior_12_2\", \"famafrench\", start_date, end_date)\nff_df = ff[0]\n# print(ff[0].columns.tolist())\n\n# (2) Extract ME5 PRIOR portfolios and convert to net return\nme5_prior = pd.DataFrame({\n    'PRIOR1': ff_df['BIG LoPRIOR'],\n    'PRIOR2': ff_df['ME5 PRIOR2'],\n    'PRIOR3': ff_df['ME5 PRIOR3'],\n    'PRIOR4': ff_df['ME5 PRIOR4'],\n    'PRIOR5': ff_df['BIG HiPRIOR'],\n}) / 100  # convert % to net return\n\nme5_prior.index = me5_prior.index.to_timestamp()\n\n# (3) Split: pre-2010 vs post-2010\nsplit_date = pd.to_datetime(\"2010-01-01\")\nme5_prior_pre = me5_prior[me5_prior.index &lt; split_date]\nme5_prior_post = me5_prior[me5_prior.index &gt;= split_date]\n\n# (4) Metric definitions\ndef annualized_return(r): return (1 + r).prod()**(12 / len(r)) - 1\ndef annualized_volatility(r): return r.std() * np.sqrt(12)\ndef sharpe_ratio(r): return (r.mean() / r.std()) * np.sqrt(12)\ndef sortino_ratio(r): return (r.mean() / r[r &lt; 0].std()) * np.sqrt(12)\ndef omega_ratio(r, threshold=p):\n    gains = r[r &gt; threshold] - threshold\n    losses = threshold - r[r &lt; threshold]\n    return gains.sum() / losses.sum() if losses.sum() &gt; 0 else np.nan\ndef max_drawdown(r):\n    cum = (1 + r).cumprod()\n    peak = cum.cummax()\n    dd = (cum - peak) / peak\n    return dd.min()\ndef calmar_ratio(r):\n    ar = annualized_return(r)\n    mdd = max_drawdown(r)\n    return ar / abs(mdd) if mdd &lt; 0 else np.nan\ndef expected_crra(r, eta=eta):\n    x = 1 + r\n    util = (x**(1 - eta) - 1) / (1 - eta)\n    return util.mean()\ndef fisher_skewness(r): return skew(r, bias=False)\ndef excess_kurtosis(r): return kurtosis(r, fisher=True, bias=False)\n\nmetrics = {\n    \"Annualized Return\": annualized_return,\n    \"Annualized Volatility\": annualized_volatility,\n    \"Sharpe Ratio\": sharpe_ratio,\n    \"Sortino Ratio\": sortino_ratio,\n    \"Omega Ratio\": omega_ratio,\n    \"Calmar Ratio\": calmar_ratio,\n    \"Expected CRRA\": expected_crra,\n    \"Fisher Skewness\": fisher_skewness,\n    \"Excess Kurtosis\": excess_kurtosis\n}\n\n# (5) Evaluate for each period\ndef evaluate_metrics(df):\n    return pd.DataFrame({\n        port: {name: func(df[port]) for name, func in metrics.items()}\n        for port in df.columns\n    }).T\n\nresult_pre = evaluate_metrics(me5_prior_pre)\nresult_post = evaluate_metrics(me5_prior_post)\n\n\n\nPerformance Metrics for ME5 PRIOR Portfolios\n\n\n\n\n\n\nPerformance Metrics (Pre-2010)\n\n\n\nAnnualized Return\nAnnualized Volatility\nSharpe Ratio\nSortino Ratio\nOmega Ratio\nCalmar Ratio\nExpected CRRA\nFisher Skewness\nExcess Kurtosis\n\n\n\n\nPRIOR1\n-0.0138\n0.3050\n0.1064\n0.1498\n0.9271\n-0.0166\n-0.0092\n0.2258\n2.3524\n\n\nPRIOR2\n0.0604\n0.2042\n0.3896\n0.5606\n1.0790\n0.1015\n0.0014\n-0.0223\n2.0126\n\n\nPRIOR3\n0.0598\n0.1584\n0.4472\n0.5899\n1.0570\n0.1265\n0.0027\n-0.3123\n2.4509\n\n\nPRIOR4\n0.0877\n0.1435\n0.6603\n0.9439\n1.1981\n0.2205\n0.0052\n-0.5270\n0.9767\n\n\nPRIOR5\n0.0835\n0.1851\n0.5283\n0.7321\n1.1629\n0.1670\n0.0037\n-0.5999\n0.9929\n\n\n\n\n\n\n\n\n\n\n\n\n\nPerformance Metrics (Post-2010)\n\n\n\nAnnualized Return\nAnnualized Volatility\nSharpe Ratio\nSortino Ratio\nOmega Ratio\nCalmar Ratio\nExpected CRRA\nFisher Skewness\nExcess Kurtosis\n\n\n\n\nPRIOR1\n0.0954\n0.2640\n0.4775\n0.7371\n1.2210\n0.2337\n0.0016\n0.1463\n2.9558\n\n\nPRIOR2\n0.1378\n0.1727\n0.8370\n1.2415\n1.4502\n0.5904\n0.0082\n-0.1295\n1.3204\n\n\nPRIOR3\n0.1342\n0.1512\n0.9129\n1.3653\n1.4582\n0.5231\n0.0085\n-0.3910\n0.4723\n\n\nPRIOR4\n0.1232\n0.1500\n0.8526\n1.3902\n1.4047\n0.5853\n0.0078\n-0.0354\n0.4905\n\n\nPRIOR5\n0.1372\n0.1633\n0.8721\n1.4108\n1.4592\n0.5302\n0.0084\n-0.0665\n0.2892\n\n\n\n\n\n\n\nRisk-adjusted metrics highlight structural momentum persistence after 2010.\n\n\nPerformance comparison of ME5 PRIOR portfolios\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnnualized return, Sharpe ratio (r_f=0), and Expected CRRA (eta=3) show stronger momentum post-2010.",
    "crumbs": [
      "교육",
      "설명? 예측?",
      "The Modern Matthew Effect"
    ]
  },
  {
    "objectID": "structure_cointegration.html",
    "href": "structure_cointegration.html",
    "title": "Time-Varying Cointegration",
    "section": "",
    "text": "This study investigates whether key U.S. economic indicators exhibit time-varying cointegration and how structural breaks affect their long-run relationships. The analysis aims to:\n\nIdentify which variables are cointegrated in the long term.\nTrack how these relationships evolve over time.\nDetect and interpret structural breaks in equilibrium relationships.\nExtend beyond traditional models by using fractional and threshold cointegration frameworks.",
    "crumbs": [
      "교육",
      "설명? 예측?",
      "Time-Varying Cointegration"
    ]
  },
  {
    "objectID": "structure_cointegration.html#research-overview",
    "href": "structure_cointegration.html#research-overview",
    "title": "Time-Varying Cointegration",
    "section": "",
    "text": "This study investigates whether key U.S. economic indicators exhibit time-varying cointegration and how structural breaks affect their long-run relationships. The analysis aims to:\n\nIdentify which variables are cointegrated in the long term.\nTrack how these relationships evolve over time.\nDetect and interpret structural breaks in equilibrium relationships.\nExtend beyond traditional models by using fractional and threshold cointegration frameworks.",
    "crumbs": [
      "교육",
      "설명? 예측?",
      "Time-Varying Cointegration"
    ]
  },
  {
    "objectID": "structure_cointegration.html#preliminaries",
    "href": "structure_cointegration.html#preliminaries",
    "title": "Time-Varying Cointegration",
    "section": "2. Preliminaries",
    "text": "2. Preliminaries\n\nSpurious correlation vs. Cointegration: High correlation between non-stationary variables may be misleading unless the variables are cointegrated.\nCointegration: A linear combination of I(1) variables that is stationary (I(0)) implies a stable long-term equilibrium.\nInterpretation: Variables sharing a cointegration relationship tend to move together over time, despite individual stochastic trends.\nEmpirical Rule: In long-term data (e.g., &gt;10 years), a correlation coefficient &gt; 0.7 between I(1) series may suggest stable equilibrium.\nNelson & Plosser (1982): U.S. macroeconomic series often follow stochastic trends, cautioning against naive regression without testing for cointegration.",
    "crumbs": [
      "교육",
      "설명? 예측?",
      "Time-Varying Cointegration"
    ]
  },
  {
    "objectID": "structure_cointegration.html#research-questions",
    "href": "structure_cointegration.html#research-questions",
    "title": "Time-Varying Cointegration",
    "section": "3. Research Questions",
    "text": "3. Research Questions\n\nWhich economic indicators form long-term relationships?\nHow have these relationships changed over time?\nDo identified structural breaks correspond to major economic shocks (e.g., 2008, COVID-19, inflation)?",
    "crumbs": [
      "교육",
      "설명? 예측?",
      "Time-Varying Cointegration"
    ]
  },
  {
    "objectID": "structure_cointegration.html#data-and-preprocessing",
    "href": "structure_cointegration.html#data-and-preprocessing",
    "title": "Time-Varying Cointegration",
    "section": "4. Data and Preprocessing",
    "text": "4. Data and Preprocessing\nPeriod: January 1990 – December 2024 (34 years)\nFrequency: Monthly\n\n\n\n\n\n\n\n\n\n\nCategory\nVariable\nSource\nStart Year\nNotes\n\n\n\n\nEquity\nSPY, NDX\nYahoo Finance\n1993, 1985\nDaily/Monthly\n\n\nCurrency\nDXY\nFRED/Yahoo\n1973\nDaily/Monthly\n\n\nBonds\nFed Funds Rate, 10Y Treasury Yield\nFRED\n1954, 1953\nMonthly\n\n\nMoney Supply\nM2\nFRED\n1959\nMonthly\n\n\nCommodities\nGold Price\nYahoo\n1975\nDaily/Monthly\n\n\nInflation\nCPI\nFRED\n1947\nMonthly\n\n\nConsumption\nConsumer Sentiment\nFRED\n1978\nMonthly\n\n\nInvestment\nReal GPDIC1\nFRED\n1960\nOriginally quarterly; interpolated monthly",
    "crumbs": [
      "교육",
      "설명? 예측?",
      "Time-Varying Cointegration"
    ]
  },
  {
    "objectID": "structure_cointegration.html#methodology",
    "href": "structure_cointegration.html#methodology",
    "title": "Time-Varying Cointegration",
    "section": "5. Methodology",
    "text": "5. Methodology\n\nA. Testing for Cointegration\n\nStep 1: Unit Root Testing\n\nEnsure variables are I(1)\nMethods:\n\nADF Test\nPhillips-Perron Test\nADF-GLS (ERS)\nKPSS\n\n\n\n\nStep 2: Cointegration Existence\n\nApply only if variables are I(1)\nMethods:\n\nJohansen Test (multivariate)\nEngle-Granger Test (pairwise)\n\nIf cointegration fails: consider VAR or short-run models\n\n\n\n\nB. Detecting Structural Breaks\n\nStep 1: Break Detection in Traditional Cointegration\n\nMethods:\n\nBai-Perron Test\nQuandt-Andrews Test\nRolling Johansen Test\nCUSUM & CUSUMSQ Tests\n\n\n\n\nStep 2: Qualitative Mapping to Events\n\n\n\nBreakpoint\nLikely Cause\n\n\n\n\n2008-Q3\nGlobal Financial Crisis\n\n\n2011-Q3\nEuropean Debt Crisis\n\n\n2020-Q1\nCOVID-19 Shock\n\n\n2022-Q1\nInflation & Fed Rate Hikes\n\n\n\nOverlay structural breaks with macroeconomic shocks, policy shifts, and global market events.\n\n\n\nC. Fractional Cointegration Extension\n\nStep 1: Testing\n\nEstimate fractional differencing order (\\(d\\)) via:\n\nGPH Test\nRobinson Test\n\n\n\n\nStep 2: Detecting Breaks\n\nUse methods for long-memory models:\n\nRolling estimates of \\(d\\)\nWavelet-based structural break detection\nRolling Hurst exponent analysis\n\n\n\n\n\nD. Comparison of Breakpoints (Traditional vs. Fractional)\n\nCommon breakpoints strengthen the validity of structural shifts.\nTraditional: discrete shifts; Fractional: gradual long-memory transitions.\n\n\n\nE. Threshold Cointegration Models\n\nStep 1: Apply TECM\n\nEstimate threshold level (\\(\\gamma\\))\nModel: \\[\n\\Delta Y_t =\n\\begin{cases}\n\\alpha_1 (Y_{t-1} - \\beta X_{t-1}) + \\epsilon_t, & \\text{if } |Y_{t-1} - \\beta X_{t-1}| &gt; \\gamma \\\\\n\\alpha_2 (Y_{t-1} - \\beta X_{t-1}) + \\epsilon_t, & \\text{otherwise}\n\\end{cases}\n\\]\n\n\n\nStep 2: Interpret Regime-dependent Adjustments\n\nUse Sup-Wald test for significance\nEvaluate asymmetry in adjustment speeds (\\(\\alpha_1 \\ne \\alpha_2\\))\n\n\n\n\nF. Threshold Fractional Cointegration (TFECM)\n\nStep 1: Estimation\n\nCombine fractional differencing with threshold effects: \\[\n\\Delta Y_t =\n\\begin{cases}\n(1 - L)^{d_1} X_t + \\epsilon_t, & \\text{if } |X_t - \\beta Y_t| &gt; \\gamma \\\\\n(1 - L)^{d_2} Y_t + \\eta_t, & \\text{otherwise}\n\\end{cases}\n\\]\n\n\n\nStep 2: Interpretation\n\nCaptures memory-driven and threshold-based nonlinearity\nUse Sup LM test for threshold significance",
    "crumbs": [
      "교육",
      "설명? 예측?",
      "Time-Varying Cointegration"
    ]
  },
  {
    "objectID": "structure_cointegration.html#summary-evaluation",
    "href": "structure_cointegration.html#summary-evaluation",
    "title": "Time-Varying Cointegration",
    "section": "6. Summary Evaluation",
    "text": "6. Summary Evaluation\nStrengths: - Systematic, step-by-step progression from standard to advanced models - Combination of linear and nonlinear, short-memory and long-memory models - Identifies persistent shifts and gradual regime changes\nChallenges: - Data-driven thresholds may introduce bias - Advanced methods require substantial computational resources - Fractional and nonlinear models need theoretical grounding for interpretation\nConclusion: This framework offers a rigorous, flexible, and empirically grounded approach to studying evolving long-run relationships in macroeconomic data, with wide applications in investment strategy, economic forecasting, and policy evaluation.",
    "crumbs": [
      "교육",
      "설명? 예측?",
      "Time-Varying Cointegration"
    ]
  },
  {
    "objectID": "modern_matthew.html#structural-inequality-at-the-firm-level",
    "href": "modern_matthew.html#structural-inequality-at-the-firm-level",
    "title": "Modern Matthew Effect",
    "section": "Structural Inequality at the Firm level",
    "text": "Structural Inequality at the Firm level\nMy analysis of the ME5 PRIOR portfolios revealed the following:\n\nEven within the top 20% of firms by market capitalization (ME5), those that performed best in the prior 11 months (PRIOR5) tend to continue outperforming in the subsequent period.\nThis effect is particularly strong post-2010, aligning with macroeconomic changes such as quantitative easing, the rise of passive investing, and capital concentration.\nThis implies a reinforcing feedback loop: prior success draws more capital, more capital enables further success, and performance becomes increasingly decoupled from diversification.\n\nThe analogy to social mobility is immediate: those who have, get more—not just relatively, but absolutely and increasingly.",
    "crumbs": [
      "교육",
      "설명? 예측?",
      "Modern Matthew Effect"
    ]
  },
  {
    "objectID": "modern_matthew.html#the-matthew-effect-at-the-household-level",
    "href": "modern_matthew.html#the-matthew-effect-at-the-household-level",
    "title": "Modern Matthew Effect",
    "section": "The Matthew Effect at the Household Level",
    "text": "The Matthew Effect at the Household Level\nIf capital lock-in dynamics operate not only at the firm level but also across households—a plausible and empirically supported assumption—the implications are far-reaching:\n\nWealthy households benefit disproportionately from capital gains and passive income\nLower- and middle-income households rely predominantly on labor income, with limited upside\nThe result is a declining probability of upward mobility, and increasing wealth stratification\n\nThis structure undermines the ideal of a meritocratic liberal democracy where opportunity is equitably distributed and fairness is institutionalized. In reality, the capital system begins to resemble a rigged equilibrium, where the initial balance is illusionary—a symbolic counterweight with no real effect.",
    "crumbs": [
      "교육",
      "설명? 예측?",
      "Modern Matthew Effect"
    ]
  },
  {
    "objectID": "modern_matthew.html#appendix-me5-prior-portfolios",
    "href": "modern_matthew.html#appendix-me5-prior-portfolios",
    "title": "Modern Matthew Effect",
    "section": "Appendix: ME5 PRIOR Portfolios",
    "text": "Appendix: ME5 PRIOR Portfolios\nIn this report, I examined the persistence of momentum within the top market-cap group (ME5) using the Fama-French 25 Portfolios sorted by size and prior return. Specifically, I analyzed how the performance of five portfolios (PRIOR1 to PRIOR5), sorted by prior 11-month returns within the ME5 group, has evolved between two distinct market regimes: pre-2010 and post-2010.\nThe results are striking.\n\nEven among the largest companies (top 20% by market cap), those that performed best over the prior year continued to outperform in the next month—both in the pre and post-2010 market regime.\n\nWhat initially seemed like a behavioral or statistical curiosity turns out to reflect a deeper transformation of the market’s internal structure—a transformation where capital flows, visibility, and narrative dominance now reinforce past winners among the already-rich. In this setting, the language of freedom, fairness, and opportunity—so essential to liberal democracy—becomes less about equal access and more about locked-in advantage. The market becomes a mirror, not of fair competition, but of Matthew Effect dynamics, where “to those who have, more shall be given.”\nFor risk-aware investors engaged in short-term, monthly rebalancing, the implications are concrete:\n\nPRIOR3 offers exceptional consistency with lower volatility and downside exposure\nPRIOR5 delivers superior absolute return with tolerable risk\nBoth are based on highly liquid, mega-cap stocks, making them operationally feasible and institutionally scalable\n\n\nIn short: Even among the richest firms, past winners tend to keep winning.\nMomentum is no longer a temporary anomaly. It is part of the architecture.\n\nThis realization challenges equilibrium-based pricing models and calls into question our normative assumptions about economic fairness. If capital accumulates in a non-reverting way among the already-powerful, mobility collapses—and with it, the utilitarian social welfare that undergirds Arrow’s idealized world of dynamic fairness. What remains is a hollow pendulum: a financial system that swings, but never truly balances.\n\nMethodology\nWe compute a comprehensive set of performance metrics (e.g., annualized return, Sharpe, Sortino, Calmar, Omega, expected CRRA utility) for each ME5 PRIOR portfolio across two structural periods:\n\nPre-2010: January 1996 to December 2009\nPost-2010: January 2010 to December 2023\n\nAll returns are monthly and expressed as decimals (net of compounding). Metrics are calculated for value-weighted portfolios of NYSE-listed firms within ME5, using prior 11-month cumulative returns (excluding the most recent month) to define momentum quintiles.\n\n\nEmpirical Summary and Interpretation\n\nPre-2010:\n\nPRIOR4 (moderate momentum) was most effective for risk-adjusted returns (Sharpe 0.66, Sortino 0.94)\n\nPRIOR5 had higher return, but more volatility and negative skew\n\nPRIOR1 severely underperformed (negative average return, deep drawdowns)\n\nPost-2010:\n\nMomentum dominance intensifies\n\nPRIOR5 exhibits top raw returns (13.7%), Sortino (1.41), and CRRA utility\n\nPRIOR3 shows best Sharpe (0.91) and second lowest volatility (15%)\n\nBoth dominate the risk-return landscape with resilience and predictability\n\n\nThe market is no longer cyclical, but directional; no longer egalitarian, but reinforcing. Passive capital flows, QE-induced yield suppression, and index-based exposure collectively sustain a regime where “winners win more, losers lag longer.” If the ideal of equal opportunity was once the balancing pendulum of a free market society, the modern financial system now swings in only one direction.\n\n\nME5 PRIOR Performance Comparison\n\n\nCode\nimport pandas as pd\nimport pandas_datareader.data as pdr\nimport numpy as np\nfrom scipy.stats import skew, kurtosis\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning) # FutureWarning 제거\npd.set_option('display.width', None)\npd.set_option('display.max_columns', 15)\n\n# (1) Load Fama-French data\nstart_date = '1996-01-01'\nend_date = '2023-12-31'\n\n# Define evaluation functions\np = 0.005 # 0.5% 월간 수익률 threshold\neta = 3 # CRRA risk-aversion parameter\n\nff = pdr.DataReader(\"25_Portfolios_ME_Prior_12_2\", \"famafrench\", start_date, end_date)\nff_df = ff[0]\n# print(ff[0].columns.tolist())\n\n# (2) Extract ME5 PRIOR portfolios and convert to net return\nme5_prior = pd.DataFrame({\n    'PRIOR1': ff_df['BIG LoPRIOR'],\n    'PRIOR2': ff_df['ME5 PRIOR2'],\n    'PRIOR3': ff_df['ME5 PRIOR3'],\n    'PRIOR4': ff_df['ME5 PRIOR4'],\n    'PRIOR5': ff_df['BIG HiPRIOR'],\n}) / 100  # convert % to net return\n\nme5_prior.index = me5_prior.index.to_timestamp()\n\n# (3) Split: pre-2010 vs post-2010\nsplit_date = pd.to_datetime(\"2010-01-01\")\nme5_prior_pre = me5_prior[me5_prior.index &lt; split_date]\nme5_prior_post = me5_prior[me5_prior.index &gt;= split_date]\n\n# (4) Metric definitions\ndef annualized_return(r): return (1 + r).prod()**(12 / len(r)) - 1\ndef annualized_volatility(r): return r.std() * np.sqrt(12)\ndef sharpe_ratio(r): return (r.mean() / r.std()) * np.sqrt(12)\ndef sortino_ratio(r): return (r.mean() / r[r &lt; 0].std()) * np.sqrt(12)\ndef omega_ratio(r, threshold=p):\n    gains = r[r &gt; threshold] - threshold\n    losses = threshold - r[r &lt; threshold]\n    return gains.sum() / losses.sum() if losses.sum() &gt; 0 else np.nan\ndef max_drawdown(r):\n    cum = (1 + r).cumprod()\n    peak = cum.cummax()\n    dd = (cum - peak) / peak\n    return dd.min()\ndef calmar_ratio(r):\n    ar = annualized_return(r)\n    mdd = max_drawdown(r)\n    return ar / abs(mdd) if mdd &lt; 0 else np.nan\ndef expected_crra(r, eta=eta):\n    x = 1 + r\n    util = (x**(1 - eta) - 1) / (1 - eta)\n    return util.mean()\ndef fisher_skewness(r): return skew(r, bias=False)\ndef excess_kurtosis(r): return kurtosis(r, fisher=True, bias=False)\n\nmetrics = {\n    \"Annualized Return\": annualized_return,\n    \"Annualized Volatility\": annualized_volatility,\n    \"Sharpe Ratio\": sharpe_ratio,\n    \"Sortino Ratio\": sortino_ratio,\n    \"Omega Ratio\": omega_ratio,\n    \"Calmar Ratio\": calmar_ratio,\n    \"Expected CRRA\": expected_crra,\n    \"Fisher Skewness\": fisher_skewness,\n    \"Excess Kurtosis\": excess_kurtosis\n}\n\n# (5) Evaluate for each period\ndef evaluate_metrics(df):\n    return pd.DataFrame({\n        port: {name: func(df[port]) for name, func in metrics.items()}\n        for port in df.columns\n    }).T\n\nresult_pre = evaluate_metrics(me5_prior_pre)\nresult_post = evaluate_metrics(me5_prior_post)\n\n\n\nPerformance Metrics for ME5 PRIOR Portfolios\n\n\n\n\n\n\nPerformance Metrics (Pre-2010)\n\n\n\nAnnualized Return\nAnnualized Volatility\nSharpe Ratio\nSortino Ratio\nOmega Ratio\nCalmar Ratio\nExpected CRRA\nFisher Skewness\nExcess Kurtosis\n\n\n\n\nPRIOR1\n-0.0138\n0.3050\n0.1064\n0.1498\n0.9271\n-0.0166\n-0.0092\n0.2258\n2.3524\n\n\nPRIOR2\n0.0604\n0.2042\n0.3896\n0.5606\n1.0790\n0.1015\n0.0014\n-0.0223\n2.0126\n\n\nPRIOR3\n0.0598\n0.1584\n0.4472\n0.5899\n1.0570\n0.1265\n0.0027\n-0.3123\n2.4509\n\n\nPRIOR4\n0.0877\n0.1435\n0.6603\n0.9439\n1.1981\n0.2205\n0.0052\n-0.5270\n0.9767\n\n\nPRIOR5\n0.0835\n0.1851\n0.5283\n0.7321\n1.1629\n0.1670\n0.0037\n-0.5999\n0.9929\n\n\n\n\n\n\n\n\n\n\n\n\n\nPerformance Metrics (Post-2010)\n\n\n\nAnnualized Return\nAnnualized Volatility\nSharpe Ratio\nSortino Ratio\nOmega Ratio\nCalmar Ratio\nExpected CRRA\nFisher Skewness\nExcess Kurtosis\n\n\n\n\nPRIOR1\n0.0954\n0.2640\n0.4775\n0.7371\n1.2210\n0.2337\n0.0016\n0.1463\n2.9558\n\n\nPRIOR2\n0.1378\n0.1727\n0.8370\n1.2415\n1.4502\n0.5904\n0.0082\n-0.1295\n1.3204\n\n\nPRIOR3\n0.1342\n0.1512\n0.9129\n1.3653\n1.4582\n0.5231\n0.0085\n-0.3910\n0.4723\n\n\nPRIOR4\n0.1232\n0.1500\n0.8526\n1.3902\n1.4047\n0.5853\n0.0078\n-0.0354\n0.4905\n\n\nPRIOR5\n0.1372\n0.1633\n0.8721\n1.4108\n1.4592\n0.5302\n0.0084\n-0.0665\n0.2892\n\n\n\n\n\n\n\nRisk-adjusted metrics highlight structural momentum persistence.\n\n\nPerformance comparison of ME5 PRIOR portfolios\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnnualized return, Sharpe ratio (r_f=0), and Expected CRRA (eta=3) show consistent momentum even among the top 20% in size.",
    "crumbs": [
      "교육",
      "설명? 예측?",
      "Modern Matthew Effect"
    ]
  }
]