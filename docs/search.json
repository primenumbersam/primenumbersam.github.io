[
  {
    "objectID": "time-series/yield_curve.html",
    "href": "time-series/yield_curve.html",
    "title": "Yield curve",
    "section": "",
    "text": "채권 가격 (bond price)과 시장 금리(yield rate, 채권 수익률)는 역의 관계(inverse relationship)\n\n금리가 오르면 채권 가격은 하락하고,\n금리가 내리면 채권 가격은 상승한다.\n\n왜?\n\n시장 금리가 상승하면 기존의 낮은 금리를 가진 채권의 상대적 매력이 떨어지므로 가격이 하락하고,\n시장 금리가 하락하면 기존의 높은 금리를 가진 채권이 더 매력적으로 보이므로 가격이 상승한다.\n\n채권의 현재 가치(PV)를 수식으로 표현하면 다음과 같다.\n\\[\nP = \\sum_{t=1}^{T} \\frac{C}{(1 + r)^t} + \\frac{F}{(1 + r)^T}\n\\]\n여기서,\n- \\(P\\) : 채권 가격 (Present Value)\n- \\(C\\) : 매년 지급되는 쿠폰 이자 (Coupon Payment)\n- \\(F\\) : 만기 시 원금(Face Value)\n- \\(r\\) : 시장 금리 (Yield Rate)\n- \\(T\\) : 잔존 만기\n시장금리(\\(r\\))가 증가하면 할인율이 커져서 내가 가진 채권의 현재 가치(\\(P\\))가 낮아짐\n\n\n가정:\n\n채권의 액면가: 1,000달러\n쿠폰 이자율: 5% (연간)\n만기: 3년\n현재 시장 금리: 5% → 2% (3% 하락)\n\n인플레이션 때문에 기준금리가 내려가서 시장 금리가 하락하면,\n새로 발행되는 2% 금리의 채권보다 기존 5%의 쿠폰을 지급하는 채권이 더 매력적으로 보이므로,\n기존 채권의 가격이 상승한다.\n\n\n\n채권 금리는 단순히 하나의 고정된 값이 아니라, 만기에 따라 다르게 형성됨.\n\n단기 채권(Short-term bonds, 1년 이하): 단기 금리는 중앙은행의 정책금리(예: 연준의 FFR, 한국은행의 기준금리)와 직접적으로 연결됨.\n중기 채권(Mid-term bonds, 2~10년): 경제 성장률, 인플레이션 기대치, 신용 위험 등에 영향을 받음.\n장기 채권(Long-term bonds, 10년 이상): 장기 인플레이션 기대, 국가 신용, 경기 사이클 등에 따라 수익률이 결정됨.\n\n\n\n\n채권은 듀레이션(Duration)이 길수록 금리 변화에 대한 가격 민감도가 커진다.\n- 장기 채권: 금리 변화에 더 민감\n- 단기 채권: 금리 변화에 덜 민감\n\n\n\nTerm Structure는 채권 시장에서 만기(term)에 따라 형성되는 금리 구조\nTerm Structure는 주로 Yield Curve(수익률 곡선)의 형태로 시각화된다.\n\n\n\n\n금리 상승 (Yield 상승) 예상 → 채권 가격 하락 → 기존 장기 채권 보유자는 손해. 기존 장기 채권을 매도하고, 새로운 금리가 반영된 채권을 매수. 기준금리 상승때문에 단기 채권은 괜찮음.\n금리 하락 (Yield 하락) 에상 → 채권 가격 상승 → 기존 장기 채권 보유자는 이득. 기존 채권을 보유하여 가격 상승에 따른 자본 이득(capital gain) 확보. 장기 채권을 보유하는 것이 유리."
  },
  {
    "objectID": "time-series/yield_curve.html#채권-가격과-시장-금리의-관계",
    "href": "time-series/yield_curve.html#채권-가격과-시장-금리의-관계",
    "title": "Yield curve",
    "section": "",
    "text": "채권 가격 (bond price)과 시장 금리(yield rate, 채권 수익률)는 역의 관계(inverse relationship)\n\n금리가 오르면 채권 가격은 하락하고,\n금리가 내리면 채권 가격은 상승한다.\n\n왜?\n\n시장 금리가 상승하면 기존의 낮은 금리를 가진 채권의 상대적 매력이 떨어지므로 가격이 하락하고,\n시장 금리가 하락하면 기존의 높은 금리를 가진 채권이 더 매력적으로 보이므로 가격이 상승한다.\n\n채권의 현재 가치(PV)를 수식으로 표현하면 다음과 같다.\n\\[\nP = \\sum_{t=1}^{T} \\frac{C}{(1 + r)^t} + \\frac{F}{(1 + r)^T}\n\\]\n여기서,\n- \\(P\\) : 채권 가격 (Present Value)\n- \\(C\\) : 매년 지급되는 쿠폰 이자 (Coupon Payment)\n- \\(F\\) : 만기 시 원금(Face Value)\n- \\(r\\) : 시장 금리 (Yield Rate)\n- \\(T\\) : 잔존 만기\n시장금리(\\(r\\))가 증가하면 할인율이 커져서 내가 가진 채권의 현재 가치(\\(P\\))가 낮아짐\n\n\n가정:\n\n채권의 액면가: 1,000달러\n쿠폰 이자율: 5% (연간)\n만기: 3년\n현재 시장 금리: 5% → 2% (3% 하락)\n\n인플레이션 때문에 기준금리가 내려가서 시장 금리가 하락하면,\n새로 발행되는 2% 금리의 채권보다 기존 5%의 쿠폰을 지급하는 채권이 더 매력적으로 보이므로,\n기존 채권의 가격이 상승한다.\n\n\n\n채권 금리는 단순히 하나의 고정된 값이 아니라, 만기에 따라 다르게 형성됨.\n\n단기 채권(Short-term bonds, 1년 이하): 단기 금리는 중앙은행의 정책금리(예: 연준의 FFR, 한국은행의 기준금리)와 직접적으로 연결됨.\n중기 채권(Mid-term bonds, 2~10년): 경제 성장률, 인플레이션 기대치, 신용 위험 등에 영향을 받음.\n장기 채권(Long-term bonds, 10년 이상): 장기 인플레이션 기대, 국가 신용, 경기 사이클 등에 따라 수익률이 결정됨.\n\n\n\n\n채권은 듀레이션(Duration)이 길수록 금리 변화에 대한 가격 민감도가 커진다.\n- 장기 채권: 금리 변화에 더 민감\n- 단기 채권: 금리 변화에 덜 민감\n\n\n\nTerm Structure는 채권 시장에서 만기(term)에 따라 형성되는 금리 구조\nTerm Structure는 주로 Yield Curve(수익률 곡선)의 형태로 시각화된다.\n\n\n\n\n금리 상승 (Yield 상승) 예상 → 채권 가격 하락 → 기존 장기 채권 보유자는 손해. 기존 장기 채권을 매도하고, 새로운 금리가 반영된 채권을 매수. 기준금리 상승때문에 단기 채권은 괜찮음.\n금리 하락 (Yield 하락) 에상 → 채권 가격 상승 → 기존 장기 채권 보유자는 이득. 기존 채권을 보유하여 가격 상승에 따른 자본 이득(capital gain) 확보. 장기 채권을 보유하는 것이 유리."
  },
  {
    "objectID": "time-series/yield_curve.html#yield-curve수익률-곡선",
    "href": "time-series/yield_curve.html#yield-curve수익률-곡선",
    "title": "Yield curve",
    "section": "Yield Curve(수익률 곡선)",
    "text": "Yield Curve(수익률 곡선)\nYield Curve는 만기별 수익률을 연결한 곡선으로, 주요 형태는 3가지:\n\n정상적인 형태(Normal Yield Curve): 장기 금리가 단기 금리보다 높음 → 경제 성장이 기대되는 경우\n역전된 형태(Inverted Yield Curve): 장기 금리가 단기 금리보다 낮음 → 경기 침체 신호\n평평한 형태(Flat Yield Curve): 단기와 장기 금리 차이가 거의 없음 → 경기 전환기\n\n이렇게 다양한 형태가 나타나는 이유를 설명하는 이론들에는, 금리 기대이론(Expectations Hypothesis), 유동성 프리미엄 이론(Liquidity Premium Theory), 시장 분할 이론(Market Segmentation Theory) 등이 있음.\nYield Curve(수익률 곡선)는 본질적으로 \\((\\text{만기}, \\text{yield})\\) 쌍에 대한 Conditional Expectation Model을 적용한 결과라고 볼 수 있다.\n채권 시장에서 다양한 만기의 국채(예: 1년, 2년, 5년, 10년, 30년 등)의 수익률 데이터를 수집하면, 기본적으로 (만기, yield) 쌍의 산점도(scatter plot)를 얻을 수 있다.\n이후, Yield Curve는 이러한 데이터를 조건부 기대값 모형 (Conditional Expectation model)을 사용하여 스무딩(Smoothing)하거나 추정(Fitting)한 것이라고 볼 수 있다.\n수익률 곡선을 추정하는 대표적인 방법으로 다음과 같은 모델들이 있습니다.\n\nPolynomial Regression\n\n가장 기본적인 방법은 2차 또는 3차 다항식을 사용하여 스무딩한 곡선을 추정.\n\\[\n\\mathbb{E}[Y | T] = \\beta_0 + \\beta_1 T + \\beta_2 T^2 + \\beta_3 T^3 + \\epsilon\n\\] 여기서:\n\n\\(T\\)는 채권의 만기,\n\\(Y\\)는 수익률(Yield),\n\\(\\mathbb{E}[Y | T]\\)는 조건부 기대값.\n\n\n\n\nSpline Regression\n\nCubic Spline 또는 B-spline을 사용하여 여러 구간에서 스무딩된 Yield Curve를 만듬.\n\n\n\nNelson-Siegel & Svensson model\n\n실무에서 많이 사용하는 Nelson-Siegel Model의 기본 식 \\[\nY(T) = \\beta_0 + \\beta_1 \\frac{1 - e^{-T/\\tau}}{T/\\tau} + \\beta_2 \\left(\\frac{1 - e^{-T/\\tau}}{T/\\tau} - e^{-T/\\tau} \\right)\n\\]\n여기서:\n\n\\(\\beta_0, \\beta_1, \\beta_2\\)는 모델의 파라미터\n\\(\\tau\\)는 시간 척도 조정 파라미터\n\\(T\\)는 만기 (term)\n\n\n\n\nGaussian Process, Neural Networks\n\nGaussian Process Regression (GPR) 또는 딥러닝 모델(Neural Networks)을 활용하여 Yield Curve를 추정하는 방법"
  },
  {
    "objectID": "time-series/yield_curve.html#code-scatter-plot-yield-curve",
    "href": "time-series/yield_curve.html#code-scatter-plot-yield-curve",
    "title": "Yield curve",
    "section": "Code: Scatter Plot + Yield Curve",
    "text": "Code: Scatter Plot + Yield Curve\n(만기, 수익률) 데이터 산점도를 그린 후, 스무딩된 Yield Curve를 적용.\n\n\nCode\n# Yield Curve(수익률 곡선)**는 본질적으로 (term,yield) 쌍에 대한 Conditional Expectation Model을 적용한 결과\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\n\n# 예제 데이터 (만기, 수익률)\nmaturity = np.array([1, 2, 3, 5, 7, 10, 20, 30])  # 만기 (years)\nyield_rates = np.array([3.2, 3.4, 3.5, 3.7, 3.8, 4.0, 4.2, 4.3])  # 수익률 (%)\n\n# Nelson-Siegel 모델 함수 정의\ndef nelson_siegel(T, beta0, beta1, beta2, tau):\n    return beta0 + beta1 * (1 - np.exp(-T/tau)) / (T/tau) + beta2 * ((1 - np.exp(-T/tau)) / (T/tau) - np.exp(-T/tau))\n\n# 초기값 설정 및 최적화\npopt, _ = curve_fit(nelson_siegel, maturity, yield_rates, p0=[4, -1, 1, 2])\n\n# 스무딩된 곡선 생성\nT_fit = np.linspace(0.5, 30, 100)  # 연속적인 만기 값\nY_fit = nelson_siegel(T_fit, *popt)\n\n# 산점도 및 수익률 곡선 그래프\nplt.figure(figsize=(10, 6))\nplt.scatter(maturity, yield_rates, color='blue', label=\"Observed Data (Scatter)\")\nplt.plot(T_fit, Y_fit, color='red', linestyle='-', label=\"Fitted Yield Curve (Nelson-Siegel)\")\nplt.xlabel(\"Maturity (Years)\")\nplt.ylabel(\"Yield (%)\")\nplt.title(\"Yield Curve: Scatter Plot with Nelson-Siegel Fit\")\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "time-series/indicator_growth.html",
    "href": "time-series/indicator_growth.html",
    "title": "경제 성장을 대표하는 지표?",
    "section": "",
    "text": "중앙은행이 대기업(‘고래’ 기업)들에게만 몰빵으로 현금 유동성을 공급하고, 이 기업들은 새로운 회사를 설립하고 그 자금을 투자에 사용한다. 투자된 돈이 중위 소득 상승과 소비로 연결되지 않고 다시 주식시장으로 유입되어 주가가 상승하면, 명목 GDP(Nominal GDP)는 증가할 수 있다. 하지만 이러한 명목 GDP 증가는 실제 경제 성장(real growth)을 반영하는 것일까?\n\n\n\n현금 유동성 공급으로 새롭게 만들어진 기업들이 소비자물가지수(CPI) 산정에 포함되지 않는 사업(B2B, 금융, AI, 데이터센터 등)을 한다면, CPI는 거의 변하지 않을 가능성이 매우 높다. 결과적으로, 명목 GDP 증가가 실질 GDP(Real GDP) 증가로 보이는 착시가 발생할 수 있다. 즉, 통계적으로 경제가 성장한 것처럼 보일 수 있지만, 실질적으로 대다수의 생활 수준은 나아지지 않아졌을 가능성이 매우 높다.\n\n\n\n공리주의 사회에서 경제 성장의 핵심 목표는 단순한 GDP 증가가 아니라 대다수 국민의 실질적 생활 수준 향상이다. 이를 더 잘 반영하는 지표는 Real GDP per capita 보다 Real Median Wage(실질 중위임금) 이 될 수 있다.\n\nReal GDP per capita는 평균적인 경제 생산량 증가를 보여주지만, 실제로 국민 개개인의 소득 분배를 고려하지 않는다.\n\nReal Median Wage는 국민 대다수가 체감하는 실질 소득의 변화를 나타내므로, 보다 정확한 경제 성장 지표가 될 수 있다.\n\n만약 명목 GDP가 증가하지만 중위임금이 정체되거나 하락한다면, 이는 대기업과 자산가 중심의 성장일 뿐, 일반 국민들에게 혜택이 돌아가지 않는 성장일 가능성이 크다.\n\n\n\n\n경제 성장의 지속 가능성을 예측하는 데 있어, 현재 사용되는 Natural GDP Growth Rate(자연 경제 성장률) 모델은 구조적인 문제를 가질 수 있다.\n- 이런 모델들은 과거 데이터를 기반으로 경제 성장률을 예측하지만, 금융시장 및 소비자 행동의 비선형적 변화를 제대로 반영하지 못한다.\n- 반면 Consumer Sentiment Index(소비자 심리 지수) 는 실시간으로 소비자들의 기대와 시장 심리를 측정한다.\n- 즉, 미래의 경제 성장 가능성을 예측하는 데 있어, 소비자들이 체감하는 경제 신뢰도와 구매 의향이 더 실질적인 지표가 될 수 있다.\n\n\n\n\n명목 GDP와 실질 GDP만으로 경제 성장을 평가하는 것은 한계가 있다.\n\nReal Median Wage가 상승하지 않는다면, GDP 성장은 일부 계층에만 혜택을 주는 비대칭적 성장일 가능성이 크다.\n경제 전망을 판단할 때, Consumer Sentiment Index와 같은 체감 지표가 전통적인 성장률 모델보다 더 현실적인 통찰을 제공할 수 있다.\n\n결국, 우리가 고민해야 할 것은 단순한 GDP 증가가 아니라 ’국민 개개인이 체감하는 실질적 경제 성장’이다."
  },
  {
    "objectID": "time-series/indicator_growth.html#real-gdp-vs.-real-median-wage",
    "href": "time-series/indicator_growth.html#real-gdp-vs.-real-median-wage",
    "title": "경제 성장을 대표하는 지표?",
    "section": "",
    "text": "중앙은행이 대기업(‘고래’ 기업)들에게만 몰빵으로 현금 유동성을 공급하고, 이 기업들은 새로운 회사를 설립하고 그 자금을 투자에 사용한다. 투자된 돈이 중위 소득 상승과 소비로 연결되지 않고 다시 주식시장으로 유입되어 주가가 상승하면, 명목 GDP(Nominal GDP)는 증가할 수 있다. 하지만 이러한 명목 GDP 증가는 실제 경제 성장(real growth)을 반영하는 것일까?\n\n\n\n현금 유동성 공급으로 새롭게 만들어진 기업들이 소비자물가지수(CPI) 산정에 포함되지 않는 사업(B2B, 금융, AI, 데이터센터 등)을 한다면, CPI는 거의 변하지 않을 가능성이 매우 높다. 결과적으로, 명목 GDP 증가가 실질 GDP(Real GDP) 증가로 보이는 착시가 발생할 수 있다. 즉, 통계적으로 경제가 성장한 것처럼 보일 수 있지만, 실질적으로 대다수의 생활 수준은 나아지지 않아졌을 가능성이 매우 높다.\n\n\n\n공리주의 사회에서 경제 성장의 핵심 목표는 단순한 GDP 증가가 아니라 대다수 국민의 실질적 생활 수준 향상이다. 이를 더 잘 반영하는 지표는 Real GDP per capita 보다 Real Median Wage(실질 중위임금) 이 될 수 있다.\n\nReal GDP per capita는 평균적인 경제 생산량 증가를 보여주지만, 실제로 국민 개개인의 소득 분배를 고려하지 않는다.\n\nReal Median Wage는 국민 대다수가 체감하는 실질 소득의 변화를 나타내므로, 보다 정확한 경제 성장 지표가 될 수 있다.\n\n만약 명목 GDP가 증가하지만 중위임금이 정체되거나 하락한다면, 이는 대기업과 자산가 중심의 성장일 뿐, 일반 국민들에게 혜택이 돌아가지 않는 성장일 가능성이 크다.\n\n\n\n\n경제 성장의 지속 가능성을 예측하는 데 있어, 현재 사용되는 Natural GDP Growth Rate(자연 경제 성장률) 모델은 구조적인 문제를 가질 수 있다.\n- 이런 모델들은 과거 데이터를 기반으로 경제 성장률을 예측하지만, 금융시장 및 소비자 행동의 비선형적 변화를 제대로 반영하지 못한다.\n- 반면 Consumer Sentiment Index(소비자 심리 지수) 는 실시간으로 소비자들의 기대와 시장 심리를 측정한다.\n- 즉, 미래의 경제 성장 가능성을 예측하는 데 있어, 소비자들이 체감하는 경제 신뢰도와 구매 의향이 더 실질적인 지표가 될 수 있다.\n\n\n\n\n명목 GDP와 실질 GDP만으로 경제 성장을 평가하는 것은 한계가 있다.\n\nReal Median Wage가 상승하지 않는다면, GDP 성장은 일부 계층에만 혜택을 주는 비대칭적 성장일 가능성이 크다.\n경제 전망을 판단할 때, Consumer Sentiment Index와 같은 체감 지표가 전통적인 성장률 모델보다 더 현실적인 통찰을 제공할 수 있다.\n\n결국, 우리가 고민해야 할 것은 단순한 GDP 증가가 아니라 ’국민 개개인이 체감하는 실질적 경제 성장’이다."
  },
  {
    "objectID": "time-series/indicator_growth.html#부록-gdp-성장률의-한계와-대안적-경제지표",
    "href": "time-series/indicator_growth.html#부록-gdp-성장률의-한계와-대안적-경제지표",
    "title": "경제 성장을 대표하는 지표?",
    "section": "부록: GDP 성장률의 한계와 대안적 경제지표",
    "text": "부록: GDP 성장률의 한계와 대안적 경제지표\n\nGDP 성장률의 측정과 한계\nGross Domestic Product(GDP)는 전통적으로 국가 경제 성장을 나타내는 핵심 지표로 활용되어 왔다. 그러나 최근 학계에서는 GDP 중심의 성장률 평가가 실제 국민의 삶의 질을 정확히 반영하지 못할 수 있다는 우려가 꾸준히 제기되고 있다(Kubiszewski et al., 2013; Stiglitz et al., 2009). 특히 금융 및 자산시장 중심의 성장 패턴이 강해질수록, GDP가 증가함에도 불구하고 소득 격차가 벌어지고 중산층의 생활 수준은 오히려 하락할 수 있다(Piketty, 2014).\n\nStiglitz, Sen, and Fitoussi (2009) 는 ’GDP가 단순한 생산물의 양적 증가만을 측정할 뿐, 불평등 심화와 실제 국민들이 체감하는 경제적 안녕(well-being)을 제대로 반영하지 못한다’는 문제를 공식적으로 제기한 바 있다. 이들은 GDP의 한계를 극복할 새로운 경제지표가 필요하다고 주장했다.\nCoyle (2014) 역시 GDP 수치 자체가 금융시장의 과열 현상으로 인해 실질적 경제 생산성 및 개개인의 복지 향상과 크게 괴리될 가능성을 지적했다. 그는 특히 금융시장의 투자 활동이 명목 GDP를 상승시키지만, 이러한 상승이 곧 일반 대중의 삶의 질 향상과 연결되지 않을 수 있음을 명시했다.\n\n\n\n대안적 경제지표로서의 실질 중위임금(Real Median Wage)\n경제학자들은 경제 성과 측정의 대안으로 소득분포의 중위값(median) 또는 중간층의 실질 소득 변화에 주목할 필요가 있다고 강조한다(Saez & Zucman, 2016; Chetty et al., 2017). 특히 중위임금(Median Wage)은 경제 성장이 국민 개개인의 생활 수준에 실제로 기여하는지를 명확히 나타내는 핵심적 지표로 주목받는다.\n\nChetty et al. (2017) 의 연구에 따르면, 미국 경제에서 GDP의 꾸준한 상승에도 불구하고 최근 30년간 실질 중위 소득이 정체된 사례가 발견되었으며, 이는 GDP 중심의 경제 성장이 반드시 대다수의 국민에게 이득을 가져다주지 않는다는 사실을 보여준다. 이 연구는 중위 소득을 핵심적 지표로 삼아 경제 정책의 우선순위를 재조정할 필요가 있다고 제안한다.\nSaez and Zucman (2016) 은 소득불평등의 증가가 GDP 성장률과 무관하게 진행되며, 중위 소득의 정체 또는 감소가 발생하면 경제 성장은 지속가능성을 잃게 된다고 경고한다. 그들은 중위 소득과 실질 생활 수준을 함께 고려하는 새로운 정책 프레임워크의 필요성을 강조한다.\n\n\n\n경제적 기대지표: Consumer Sentiment Index의 우월성\n경제 성장의 미래 전망을 평가할 때, 일반적으로 Central Bank 및 국제기구는 자연 경제 성장률(natural GDP rate)과 같은 구조적 모형 기반의 지표를 선호해왔다. 하지만 최근 연구들은 구조적 모형이 금융시장과 실물경제의 복잡한 상호작용을 잘 반영하지 못할 가능성이 있다고 지적한다(Sims, 2010; Coibion & Gorodnichenko, 2015). 이에 따라 소비자심리지수(Consumer Sentiment Index)가 경제 전망의 보다 정확한 선행지표로 주목받기 시작했다.\n\nCoibion and Gorodnichenko (2015) 는 소비자심리지수가 GDP 성장률, 특히 실질 소비지출의 미래 경로를 잘 예측하는 능력을 가졌음을 실증적으로 증명했다. 이 연구는 특히 불확실성이 높은 시기일수록 소비자심리지수가 공식적인 성장률 모형보다 경제 예측력에서 뛰어남을 보였다.\nSims (2010) 는 구조적 경제 예측 모형이 과거 데이터의 패턴에 과도하게 의존하여 금융 위기와 같은 비선형적 충격을 놓치는 경향이 있다고 지적했다. 반면 소비자 기대감은 그러한 경제적 충격을 보다 신속히 반영하며, 실물경제의 미래 변화에 대한 중요한 통찰력을 제공한다.\n\n\n\n시사점\n위 연구들의 공통된 결론은 명확하다. 전통적 GDP 성장률 측정 방식은 경제 성장과 국민 생활 수준 향상을 반드시 보장하지 않으며, 때로는 현실과 심각한 괴리를 일으킬 수 있다. 실질 중위임금(Real Median Wage)은 국민들의 실제 생활수준 개선 여부를 평가하는 데 있어 GDP보다 더욱 정확한 지표이며, 미래 경제에 대한 소비자의 심리를 측정하는 소비자심리지수는 구조적 예측모델보다 현실적 통찰력을 제공한다.\n경제 정책은 GDP라는 숫자 증가에 매몰되지 않고, 개개인의 실질 생활수준과 소비자의 체감적 경제 기대감을 더 정확히 반영하는 지표 중심으로 전환해야 한다."
  },
  {
    "objectID": "time-series/indicator_growth.html#참고문헌",
    "href": "time-series/indicator_growth.html#참고문헌",
    "title": "경제 성장을 대표하는 지표?",
    "section": "참고문헌",
    "text": "참고문헌\n\nChetty, R., Grusky, D., Hell, M., Hendren, N., Manduca, R., & Narang, J. (2017). The fading American dream: Trends in absolute income mobility since 1940. Science, 356(6336), 398-406.\nCoibion, O., & Gorodnichenko, Y. (2015). Information Rigidity and the Expectations Formation Process: A Simple Framework and New Facts. American Economic Review, 105(8), 2644-2678.\nCoyle, D. (2014). GDP: A brief but affectionate history. Princeton University Press.\nKubiszewski, I., Costanza, R., Franco, C., Lawn, P., Talberth, J., Jackson, T., & Aylmer, C. (2013). Beyond GDP: Measuring and achieving global genuine progress. Ecological Economics, 93, 57-68.\nPiketty, T. (2014). Capital in the Twenty-First Century. Harvard University Press.\nSaez, E., & Zucman, G. (2016). Wealth inequality in the United States since 1913: Evidence from capitalized income tax data. Quarterly Journal of Economics, 131(2), 519-578.\nSims, C. A. (2010). Rational Inattention and Monetary Economics. Handbook of Monetary Economics, 3, 155-181.\nStiglitz, J., Sen, A., & Fitoussi, J. P. (2009). Report by the commission on the measurement of economic performance and social progress. Paris: Commission on the Measurement of Economic Performance and Social Progress."
  },
  {
    "objectID": "projects/pricing_equal/pricing_equal.html",
    "href": "projects/pricing_equal/pricing_equal.html",
    "title": "Pricing the seemingly Equal Opportunities under Structural Inequality",
    "section": "",
    "text": "By applying the EMM-based asset pricing approach, this study quantifies the impact of structural inequalities on the valuation of ostensibly equal opportunities. The results emphasize the importance of considering underlying economic disparities when evaluating the fairness and effectiveness of qualification standards in society."
  },
  {
    "objectID": "projects/pricing_equal/pricing_equal.html#conclusion",
    "href": "projects/pricing_equal/pricing_equal.html#conclusion",
    "title": "Pricing the seemingly Equal Opportunities under Structural Inequality",
    "section": "",
    "text": "By applying the EMM-based asset pricing approach, this study quantifies the impact of structural inequalities on the valuation of ostensibly equal opportunities. The results emphasize the importance of considering underlying economic disparities when evaluating the fairness and effectiveness of qualification standards in society."
  },
  {
    "objectID": "projects/pricing_equal/pricing_equal.html#main",
    "href": "projects/pricing_equal/pricing_equal.html#main",
    "title": "Pricing the seemingly Equal Opportunities under Structural Inequality",
    "section": "Main",
    "text": "Main\n\nModel Scenario\nIn short, we consider a society divided into two distinct classes:\n\nProletariat (P): Individuals with access to general opportunities.\nCapitalist (C): Individuals with access to both general and exclusive special opportunities.\n\nInternally, both classes operate within perfectly competitive markets that adhere to the no-arbitrage principle. However, due to capital constraints, members of the P class cannot access the special opportunities available to the C class, rendering these opportunities unattainable. Consequently, while the absolute value of special opportunities is equal to or greater than that of general opportunities, the P class cannot exploit potential arbitrage opportunities due to these constraints. Both classes have offspring who inherit the economic outcomes of their parents’ investments. To claim these inherited assets, all offspring must meet a minimum qualification standard (e.g., educational credentials), analogous to the strike price (\\(K\\)) in option pricing. Until this qualification is met, the offspring hold a call option on their parents’ assets.\nIn detail, consider two distinct social classes: the proletariat (P), representing working-class individuals, and the capitalist class (C), representing wealthy individuals. These two classes face fundamentally different opportunity sets due to deep-seated structural inequalities. Such societal barriers, from both democratic and utilitarian viewpoints, constitute a significant social inefficiency.\nTo analyze the structural origins of this inefficiency, we apply the concept of Limits to Arbitrage Theory, which suggests markets are not always fully efficient. This inefficiency arises because of various constraints, such as behavioral biases, risk constraints, and notably, capital constraints. Capital constraints imply that significant capital is required to engage in arbitrage, effectively isolating classes economically. We assume, therefore, that the proletariat and capitalist classes are economically segregated due to these capital constraints.\nWithin each class, economic opportunities exist in a perfectly competitive market, satisfying the no-arbitrage condition internally. Members of the proletariat (P) freely select from a common set of general opportunities available to them. Conversely, members of the capitalist class (C) not only share these general opportunities but also exclusively access additional “special opportunities.” From the viewpoint of proletariat class members, these special opportunities represent unattainable benefits (“grapes beyond reach”), carrying higher or equal absolute value compared to general opportunities.\nTheoretically, if members of the capitalist class could short-sell general opportunities and simultaneously long-position special opportunities, they could realize arbitrage profits. However, due to aforementioned capital constraints (“limits to arbitrage”), such arbitrage trading is practically impossible within this model.\nBoth classes have offspring who inherit the economic results (absolute asset values) of their parents’ investment choices. To claim these inherited assets, children from both classes must meet identical minimum qualification standards (e.g., university diplomas, basic educational credentials). Although the qualification standard is identical for both, outcomes differ significantly due to inherited assets. For instance, a business administration graduate from the capitalist class inherits and manages substantial capital (businesses), while an identically qualified individual from the proletariat class works as an employee, earning wages in capitalist-owned enterprises.\nThe minimum qualification standard can be understood as a fixed barrier or strike price (K) of a call option. Until the qualification requirement is fulfilled, the children effectively hold call options on their parents’ assets. This model aims to quantify the absolute value of these call options for each class, reflecting the inherited economic outcomes accessible to the offspring.\n\n\nEMM-based Call Option Valuation Theory\nAssume a simple 1 period setting with only two possible states (e.g. good or bad). Under the Equivalent Martingale Measure (EMM), the fair market value of a call option at time 0, denoted as \\(\\hat{f}_0\\), must satisfy:\n\\[\\hat{f_0}\\cdot R = E^q[f_1]\\] where \\(f_1=max(S_0 \\cdot U-K,0)\\)\n\\[S_0\\cdot R=E^q[S_1]\\] where \\(S_0\\) = the current price of risky underlying asset\nFrom these two equations, we have the EMM or the state price density for good state \\(q\\) as: \\[\nq = \\frac{R - D}{U - D}\n\\]\nThus, the present fair value of the call option is given by:\n\\[\n\\hat{f}_0 = \\frac{1}{R}\\left[ \\frac{R - D}{U - D}(S_0 \\cdot U - K) + \\frac{U - R}{U - D} \\cdot 0 \\right] = \\frac{(R - D)(U - 1)}{R(U - D)}\n\\]\nassuming that\n\nInitial Asset Price (normalized): \\(S_0=1\\)\n\nStrike Price (qualification threshold): \\(K=1\\)\n\nMaturity: 18 years (quarterly steps = 72 periods)\n\nThis formula indicates a clear proportional relationship for estimating current fair values representing inherited qualification-based claims for each class over a 18-year maturity period. While this valuation could be approximated using continuous distributions (e.g., Black-Scholes under symmetric assumptions \\(U \\cdot D=1\\)), our discrete binomial model allows straightforward interpretation without loss of economic intuition.\n\n\nEmpirical Analysis\nWe employ distinct underlying assets for each class. Using the historical dataset (Q1 1982–Q4 2019, 152 quarterly observations), we estimated the parameters and their associated fair values of call options for each class.\nFor Proletariat (P) Class Children:\n\nRisky Asset: US Median usual weekly Real earnings (LES1252881600Q)\n\nRisk-Free Asset: US Real GDP per capita (A939RX0Q048SBEA)\n\nParameters:\n\n\\(U_p\\):= 75th percentile growth of wage (risky asset)\n\n\\(D_p\\):= 25th percentile growth of wage\n\n\\(R_p\\):= Median growth rate of Real GDP per capita\n\n\nFor Capitalist (C) Class Children:\n\nRisky Asset: S&P 500 equity (SPX)\n\nRisk-Free Asset: US 10-Year Treasury Bond\nParameters:\n\n\\(U_c\\):= 75th percentile quarterly growth of the equity index\n\n\\(D_c\\):= 25th percentile quarterly growth of the equity index\n\n\\(R_c\\):= Median of quarterly US 10-Year Treasury Bond Yield (DGS10)\n\n\nEmpirical Results:\n\nFor Proletariat (P) class children, \\(\\hat{P}_0=?\\)\n\nFor Capitalist (C) class children, \\(\\hat{C}_0=?\\)\n\n\n\nCode\nimport yfinance as yf\nimport pandas_datareader.data as web\nimport pandas as pd\nimport numpy as np\n\n# 데이터 기간 설정\nstart_date = '1982-01-01'\nend_date = '2019-12-31'\n\n# S&P 500 데이터 가져오기\nsp500 = yf.download(\"^GSPC\", start=start_date, end=end_date, interval=\"1d\")\nsp500_q = sp500['Close'].resample('QE').last()  # 분기별 종가 데이터\n\n# FRED 데이터 가져오기\nus_median_weekly_earnings = web.DataReader('LES1252881600Q', 'fred', start_date, end_date)\nus_real_gdp_per_capita = web.DataReader('A939RX0Q048SBEA', 'fred', start_date, end_date)\nus_10yr_treasury_yield = web.DataReader('DGS10', 'fred', start_date, end_date)\n\n# 인덱스를 맞추기 위해 분기별로 재샘플링\nus_median_weekly_earnings = us_median_weekly_earnings.resample('QE').last()\nus_real_gdp_per_capita = us_real_gdp_per_capita.resample('QE').last()\nus_10yr_treasury_yield = us_10yr_treasury_yield.resample('QE').last()\n\n# 데이터프레임으로 변환\ndata = pd.DataFrame({\n    'SP500': sp500_q.squeeze(),\n    'Median_Weekly_Earnings': us_median_weekly_earnings['LES1252881600Q'].squeeze(),\n    'Real_GDP_per_Capita': us_real_gdp_per_capita['A939RX0Q048SBEA'].squeeze(),\n    '10yr_Treasury_Yield': us_10yr_treasury_yield['DGS10'].squeeze()\n}, index=sp500_q.index)\n\n# 결측값 제거\ndata.dropna(inplace=True)\n\n# 수익률 계산\ndata['SP500_Return'] = data['SP500'].pct_change()\ndata['Earnings_Growth'] = data['Median_Weekly_Earnings'].pct_change()\ndata['GDP_Growth'] = data['Real_GDP_per_Capita'].pct_change()\n\n# 통계치 계산\nU_p = data['Earnings_Growth'].quantile(0.75)+1\nD_p = data['Earnings_Growth'].quantile(0.25)+1\nR_p = data['GDP_Growth'].median()+1\n\nU_c = data['SP500_Return'].quantile(0.75)+1\nD_c = data['SP500_Return'].quantile(0.25)+1\nR_c = data['10yr_Treasury_Yield'].median()\nR_c = R_c / 100 +1\n\nprint(f\"Proletariat Class Children Parameters:\")\nprint(f\"U_p: {U_p:.2f}\")\nprint(f\"D_p: {D_p:.2f}\")\nprint(f\"R_p: {R_p:.2f}\")\n\nprint(f\"\\nCapitalist Class Children Parameters:\")\nprint(f\"U_c: {U_c:.2f}\")\nprint(f\"D_c: {D_c:.2f}\")\nprint(f\"R_c: {R_c:.2f}\")\n\n\n# Binomial Option Pricing Model\ndef binomial_option_pricing(K, S_0, T, U, D, R, dt):\n    \"\"\"\n    K: Strike price\n    S_0: Initial stock price\n    T: Time to maturity (in years)\n    U: Up factor\n    D: Down factor\n    R: Risk-free rate\n    dt: Number of steps for each year\n    \"\"\"\n    n = T*dt # Number of steps in the binomial tree\n    q = (R - D) / (U - D)\n\n    # Initialize option values at maturity\n    option_values = np.zeros((n + 1, 1))\n    for i in range(n + 1):\n        ST = S_0 * (U ** i) * (D ** (n - i))\n        option_values[i] = max(0, ST - K)\n\n    # Backward recursion for option values\n    for j in range(n - 1, -1, -1):\n        for i in range(j + 1):\n            option_values[i] = (q * option_values[i + 1] + (1 - q) * option_values[i]) / R\n\n    return option_values[0, 0]\n\n\n# Parameters\nK = 1  # Strike price\nS_0 = 1  # Initial stock price\nT = 18  # Time to maturity (18 years)\ndt = 4 # Number of steps for each year\n\n# Calculate option prices\noption_price_proletariat = binomial_option_pricing(K, S_0, T, U_p, D_p, R_p, dt)\noption_price_capitalist = binomial_option_pricing(K, S_0, T, U_c, D_c, R_c, dt)\n\nprint(f\"\\nFair price of Call Option, held by Proletariat Class Children:\\n {option_price_proletariat:.2f}\")\nprint(f\"Fair price of Call Option, held by Capitalist Class Children:\\n {option_price_capitalist:.2f}\")\n\n\nYF.download() has changed argument auto_adjust default to True\n\n\n[*********************100%***********************]  1 of 1 completed\n\n\nProletariat Class Children Parameters:\nU_p: 1.01\nD_p: 1.00\nR_p: 1.01\n\nCapitalist Class Children Parameters:\nU_c: 1.07\nD_c: 0.99\nR_c: 1.05\n\nFair price of Call Option, held by Proletariat Class Children:\n 0.30\nFair price of Call Option, held by Capitalist Class Children:\n 0.97\n\n\n\n\nDiscussion\nThis model clarifies the stark inequality underlying “ostensibly equal” qualification standards. Although formally identical, the call options’ absolute valuations significantly diverge, reflecting distinct economic inheritances accessible to each class. This disparity highlights structural inefficiencies and deep-rooted inequalities, persisting despite nominally identical qualification standards.\nUltimately, this analysis underscores how asset-based class differentiation profoundly impacts the perceived and realized absolute value of educational and economic opportunities, illuminating critical implications for economic policy, educational equity, and social justice frameworks."
  },
  {
    "objectID": "projects/pricing_equal/pricing_equal.html#introduction",
    "href": "projects/pricing_equal/pricing_equal.html#introduction",
    "title": "Pricing the seemingly Equal Opportunities under Structural Inequality",
    "section": "Introduction",
    "text": "Introduction\nIn societies characterized by pronounced economic stratification, opportunities presented as equal often yield disparate outcomes across different social strata. This disparity arises from inherent structural inefficiencies that restrict access to certain opportunities based on class. The Limits to Arbitrage Theory posits that market inefficiencies can persist due to various constraints, including capital limitations, preventing rational traders from correcting mispricings (Shleifer and Vishny 1997). This paper explores how such constraints contribute to the unequal valuation of opportunities between the proletariat (P) and capitalist (C) classes."
  },
  {
    "objectID": "projects/pricing_equal/pricing_equal.html#literature-review",
    "href": "projects/pricing_equal/pricing_equal.html#literature-review",
    "title": "Pricing the seemingly Equal Opportunities under Structural Inequality",
    "section": "Literature Review",
    "text": "Literature Review\nThe theoretical foundation relies primarily on the limits to arbitrage theory, initially articulated by Shleifer and Vishny (Shleifer and Vishny 1997). Their seminal work shows how market inefficiencies persist due to practical constraints, particularly capital constraints, restricting the ability of arbitrageurs to exploit and correct mispricings. These constraints arise from significant capital requirements that effectively segregate participants into distinct economic spheres, as emphasized by subsequent studies on financially constrained arbitrageurs (Gromb and Vayanos 2002; Xiong 2001).\nGeanakoplos’ research introduces the leverage cycle, which explains how fluctuations in leverage and capital availability perpetuate systemic inequality and financial instability (Geanakoplos 2010). Complementary studies, such as that by Gromb and Vayanos, also demonstrate the welfare implications of constrained arbitrageurs operating under capital limitations, further exacerbating persistent inequality (Gromb and Vayanos 2002). Moreover, Barberis and Thaler’s comprehensive survey on behavioral finance indicates how cognitive and behavioral constraints exacerbate market inefficiencies, reinforcing the structural barriers that differentiate economic outcomes across social strata (Barberis and Thaler 2003).\nExtending beyond purely financial contexts, sociological and economic research provides additional perspectives on structural inequalities and opportunity valuation. Bourdieu’s concept of social reproduction underscores how cultural capital perpetuates socioeconomic inequalities across generations (Bourdieu 1973). Recent empirical evidence by Stansbury (Stansbury 2024) and findings by the Social Mobility Commission (Social Mobility Commission 2023) further demonstrate how economic capital inherited through generations shapes differential outcomes, even when individuals ostensibly possess identical qualifications.\nChetty et al. provide compelling evidence linking parental economic conditions to children’s educational outcomes and future earnings, strongly supporting the relevance of inherited economic positions in determining opportunity valuations (Chetty et al. 2014). Similarly, Piketty’s influential book highlights the crucial role inherited wealth plays in perpetuating structural economic disparities, emphasizing the critical nature of capital inheritance in shaping individuals’ economic trajectories and their access to opportunities (Piketty 2014).\nPolicy implications regarding these structural inequalities and efforts to enhance social mobility have been explored extensively by institutions such as the OECD. Their analyses suggest policy frameworks that might alleviate the persistent inequalities discussed herein (OECD 2018). Reeves’ concept of the “glass floor” further illustrates how affluent socioeconomic backgrounds systematically maintain class advantages despite equal or even lesser merit-based qualifications (Reeves 2017).\nCollectively, these studies underline a coherent narrative: ostensibly equal opportunities often conceal significant disparities rooted in inherited structural inequalities, persistent capital constraints, and behavioral limitations to arbitrage. Our model complements this literature by quantitatively evaluating how these structural factors systematically influence the absolute value of identical qualification standards across distinct socioeconomic groups."
  },
  {
    "objectID": "projects/incomplete/incomplete.html",
    "href": "projects/incomplete/incomplete.html",
    "title": "Monetary Policy and Market Imperfections",
    "section": "",
    "text": "Neoclassical economics emphasizes that rational individuals base decisions primarily on long-term trends rather than short-term fluctuations. In this view, rational agents with forward-looking expectations base decisions on an infinite-horizon optimization, meaning they will not invest in assets or projects that have a declining long-term trajectory or negative net present value. Market prices, in turn, are thought to reflect these rational expectations about future fundamentals. This philosophy is exemplified in models of Milton Friedman and his successors, where individuals smooth consumption over time and focus on permanent income, and in modern macroeconomic models that assume agents plan for the long run. The neoclassical mindset links naturally with Friedman’s monetarism, which asserts that steady, rules-based growth of the money supply yields better outcomes than reactive fine-tuning. Both frameworks assume that people anticipate the future well, so erratic policy shifts mainly lead to price changes rather than lasting gains in output or employment. Representative models include Friedman’s Permanent Income Hypothesis, Bewley’s incomplete-market model, Aiyagari’s general equilibrium model with heterogeneous agents, and the speculative asset-pricing model by Harrison & Kreps (Friedman 1957; Bewley 1986; Aiyagari 1994; Harrison and Kreps 1978).\nFriedman’s monetarism closely aligns with neoclassical thinking, promoting predictable, rules-based monetary policy to ensure market stability and long-term neutrality of money. In contrast, Keynesian and New Keynesian frameworks highlight the importance of short-term interventions due to market frictions such as price stickiness. New Keynesians agree that people are forward-looking, but they highlight that wages and prices can be “sticky” (slow to adjust), which prevents markets from clearing quickly. As a result, even rational agents may be temporarily unable to reach optimal outcomes, and monetary policy can have powerful real effects in the short term. For example, if prices cannot adjust immediately, an unexpected cut in interest rates may boost real spending and output rather than just raising prices. New Keynesian models therefore rationalize active stabilization policy – they contend that without it, the economy can suffer prolonged recessions or unemployment that a well-timed policy intervention could ameliorate. This stands in contrast to the monetarist/neoclassical view that markets self-correct relatively quickly. Despite these differences, both schools provide valuable insight: neoclassical and monetarist models offer clarity about long-run tendencies and private sector behavior under rational expectations, while Keynesian models stress the importance of short-run dynamics and coordination failures. The analysis in this paper bridges these perspectives by using theoretical models to examine how market imperfections – such as incomplete markets and heterogeneous expectations – modify the standard predictions. Although the models are abstract, each provides important long-term policy guidance. Understanding their lessons can help policymakers design monetary strategies that foster stability without creating new problems in the process."
  },
  {
    "objectID": "projects/incomplete/incomplete.html#introduction",
    "href": "projects/incomplete/incomplete.html#introduction",
    "title": "Monetary Policy and Market Imperfections",
    "section": "",
    "text": "Neoclassical economics emphasizes that rational individuals base decisions primarily on long-term trends rather than short-term fluctuations. In this view, rational agents with forward-looking expectations base decisions on an infinite-horizon optimization, meaning they will not invest in assets or projects that have a declining long-term trajectory or negative net present value. Market prices, in turn, are thought to reflect these rational expectations about future fundamentals. This philosophy is exemplified in models of Milton Friedman and his successors, where individuals smooth consumption over time and focus on permanent income, and in modern macroeconomic models that assume agents plan for the long run. The neoclassical mindset links naturally with Friedman’s monetarism, which asserts that steady, rules-based growth of the money supply yields better outcomes than reactive fine-tuning. Both frameworks assume that people anticipate the future well, so erratic policy shifts mainly lead to price changes rather than lasting gains in output or employment. Representative models include Friedman’s Permanent Income Hypothesis, Bewley’s incomplete-market model, Aiyagari’s general equilibrium model with heterogeneous agents, and the speculative asset-pricing model by Harrison & Kreps (Friedman 1957; Bewley 1986; Aiyagari 1994; Harrison and Kreps 1978).\nFriedman’s monetarism closely aligns with neoclassical thinking, promoting predictable, rules-based monetary policy to ensure market stability and long-term neutrality of money. In contrast, Keynesian and New Keynesian frameworks highlight the importance of short-term interventions due to market frictions such as price stickiness. New Keynesians agree that people are forward-looking, but they highlight that wages and prices can be “sticky” (slow to adjust), which prevents markets from clearing quickly. As a result, even rational agents may be temporarily unable to reach optimal outcomes, and monetary policy can have powerful real effects in the short term. For example, if prices cannot adjust immediately, an unexpected cut in interest rates may boost real spending and output rather than just raising prices. New Keynesian models therefore rationalize active stabilization policy – they contend that without it, the economy can suffer prolonged recessions or unemployment that a well-timed policy intervention could ameliorate. This stands in contrast to the monetarist/neoclassical view that markets self-correct relatively quickly. Despite these differences, both schools provide valuable insight: neoclassical and monetarist models offer clarity about long-run tendencies and private sector behavior under rational expectations, while Keynesian models stress the importance of short-run dynamics and coordination failures. The analysis in this paper bridges these perspectives by using theoretical models to examine how market imperfections – such as incomplete markets and heterogeneous expectations – modify the standard predictions. Although the models are abstract, each provides important long-term policy guidance. Understanding their lessons can help policymakers design monetary strategies that foster stability without creating new problems in the process."
  },
  {
    "objectID": "projects/incomplete/incomplete.html#core-assumptions-of-neoclassical-economics-and-monetarism",
    "href": "projects/incomplete/incomplete.html#core-assumptions-of-neoclassical-economics-and-monetarism",
    "title": "Monetary Policy and Market Imperfections",
    "section": "Core Assumptions of Neoclassical Economics and Monetarism",
    "text": "Core Assumptions of Neoclassical Economics and Monetarism\nThree core assumptions underpin the neoclassical and monetarist perspective in macroeconomics:\n\nRational expectations and infinite-horizon optimization: Individuals and firms are assumed to form expectations about the future in a rational way (using all available information) and to optimize their decisions over an infinite time horizon. In practice, this means economic agents base current consumption, saving, and investment on the expected present value of long-term outcomes, rather than overreacting to short-lived changes. They anticipate the future effects of policies, so systematic policy actions are largely already “priced in” to decisions. This assumption was emphasized by the new classical economists who followed Friedman, such as Robert Lucas, in arguing that only unexpected policy moves affect real behavior in the short run. Under rational expectations, people won’t persistently spend windfalls or chase assets whose fundamentals don’t justify their price, since they foresee the eventual reversion to fundamental value.\nMarket clearing in the long run (flexible prices): Neoclassical models typically assume that prices of goods, services, and factors adjust to equilibrate supply and demand, at least in the long run. While short-term frictions can occur, the long-run default is an economy at full employment with resources fully utilized. Any deviations (recessions or booms) are seen as temporary, provided policy does not introduce long-term distortions. This view contrasts with Keynesian models where wages or prices might remain out of equilibrium for an extended period. The neoclassical stance is that given enough time, economic forces will push the economy back to its potential output with stable growth. Monetarists, too, believed that “markets naturally move toward a stable center” in the absence of big shocks. Thus, they argue against aggressive intervention that attempts to exploit short-run trade-offs (like pumping up output at the cost of higher inflation), because eventually prices adjust and only inflation remains.\nNeutrality of money regarding real economic outcomes in the long run:(Lucas Jr 1972; Friedman 1968). A cornerstone of monetarism and neoclassical thought is that changes in the money supply only have transient effects on real variables (output, employment) and no effect in the long run. In the long run, an exogenous increase in the money stock is reflected in higher nominal prices and wages, but real consumption, investment, and output return to their original path. In other words, money is “neutral” with respect to real economic activity once prices have fully adjusted. Most economists agree that this long-run neutrality holds approximately true in practice – doubling the money supply eventually doubles the price level – and monetarists place great importance on it. This assumption underlies the monetarist recommendation to avoid monetary surprises: any attempt to permanently boost employment by printing money will just create inflation once people’s expectations catch up. Rational agents, thinking in an infinite-horizon framework, will not be tricked for long; they come to expect higher inflation, negating any output gains. Monetary policy, therefore, is seen primarily as a tool for controlling inflation and nominal variables, not as a way to engineer long-term higher growth.\n\nThese core assumptions shape the policy mindset in the neoclassical/monetarist framework. If agents are highly forward-looking and markets tend to clear, discretionary stabilization policy has limited power – it might only cause short-term blips or even destabilize expectations. Instead, maintaining credible, consistent policy (such as a steady money growth rule or inflation target) is viewed as the optimal approach for long-run welfare. In the next sections, we examine how relaxing some of these assumptions – by introducing incomplete markets, borrowing constraints, or heterogeneous beliefs – changes the conclusions and policy implications."
  },
  {
    "objectID": "projects/incomplete/incomplete.html#complete-vs.-incomplete-markets",
    "href": "projects/incomplete/incomplete.html#complete-vs.-incomplete-markets",
    "title": "Monetary Policy and Market Imperfections",
    "section": "Complete vs. Incomplete Markets",
    "text": "Complete vs. Incomplete Markets\nComplete markets allow full insurance against risks and unlimited borrowing, resulting in smooth consumption aligned with permanent income. Incomplete markets, however, feature borrowing constraints and uninsurable shocks, creating heterogeneity and precautionary savings, leading to wealth disparities and more volatile consumption patterns (Aiyagari 1994; Bewley 1986). Thus, one fundamental way real economies depart from the idealized benchmark is that markets are incomplete. In a complete market environment, individuals can fully insure against uncertainties and can borrow or save freely at a given interest rate. In that ideal case, people smooth their consumption over time nearly perfectly. Consumption ends up being much less volatile than income, because during bad times individuals can borrow or draw on savings, and during good times they can save extra income for the future. In fact, theory predicts that with complete markets, consumption at any point reflects an individual’s permanent income (the expected long-term average income), not the transitory ups and downs of current income. A direct implication is that temporary policy measures (like one-time stimulus checks or short-lived tax cuts) would have only a small effect on consumption – because rational consumers know such windfalls are transitory, they prefer to save a large portion of them, aiming to maintain a stable consumption path. In a complete market, households effectively pool risks and smooth out idiosyncratic shocks; as a result, their spending is steady and mainly influenced by changes in expected lifetime resources rather than short-term liquidity fluctuations.\nBy contrast, in incomplete market settings, individuals do not have access to perfect insurance or unlimited borrowing. They face idiosyncratic income shocks (e.g. job loss, illness) that they must largely bear on their own. Additionally, they may encounter liquidity constraints or borrowing limits that prevent them from smoothing consumption fully. The models developed by Truman Bewley, S. Rao Aiyagari, and others formalize this situation. In these models, all agents are ex ante identical (they have the same preferences and potential income distribution), but they become ex post heterogeneous because each experiences different income shocks over time and they cannot completely insure against these shocks (54. The Aiyagari Model — Quantitative Economics with Julia). Households thus engage in precautionary saving – they tend to save more when they have income, building a buffer of assets to self-insure against future bad draws. Consumption is no longer completely smooth; when a negative shock hits a liquidity-constrained household, it may have to cut consumption sharply because it cannot borrow easily. Conversely, a positive shock to a hand-to-mouth household leads to a spike in consumption if they were previously constrained. Incomplete markets therefore produce higher marginal propensities to consume out of transitory income changes – in other words, people spend a larger fraction of any temporary income windfall than they would under complete markets (Lecture 1 Standard Incomplete Markets Steady State). This is consistent with empirical data showing many households, especially those with low wealth, quickly spend stimulus payments or bonuses, as they have unmet needs or debts to pay.\nAnother key difference is that incomplete markets generate a non-trivial distribution of wealth. Since each individual’s asset accumulation depends on their history of shocks and their precautionary saving motive, over time the economy develops inequality in wealth and consumption. Some agents will build up sizable precautionary balances (if they experience good income luck or have frugal preferences), while others might remain near the borrowing constraint with minimal savings. The wealth distribution in such models is typically highly skewed, capturing the fact that a small fraction of people may hold a large share of total assets – a feature very much in line with real-world data. By contrast, in a representative agent or complete markets model, distributional issues are either absent or of no consequence, since everyone effectively pools risks together. Incomplete markets thus bring distributional considerations to the forefront of macroeconomic analysis.\nFor policymakers, these differences mean that monetary and fiscal policy can have uneven effects across the population and can influence aggregate demand through channels that are muted in complete-market models. For example, an interest rate cut in an incomplete market setting might stimulate borrowing and spending for some agents, but for others it mainly reduces their interest income (if they are savers), potentially widening inequality. Likewise, a government stimulus targeted at liquidity-constrained households could yield a relatively large boost to consumption (due to their high propensity to consume out of additional income), whereas the same payment to a wealthier, fully insured household might just be saved. In summary, incomplete markets make the macroeconomy less “frictionless” and more sensitive to distribution and credit conditions. We next examine specific models that incorporate these features, to draw out their policy insights."
  },
  {
    "objectID": "projects/incomplete/incomplete.html#friedman-consumption-smoothing-model-complete-markets",
    "href": "projects/incomplete/incomplete.html#friedman-consumption-smoothing-model-complete-markets",
    "title": "Monetary Policy and Market Imperfections",
    "section": "Friedman Consumption Smoothing Model (Complete Markets)",
    "text": "Friedman Consumption Smoothing Model (Complete Markets)\nMilton Friedman’s model of consumption – known as the Permanent Income Hypothesis (PIH) – suggests consumption depends primarily on permanent rather than temporary income changes. Hence, monetary policy must focus on long-term credibility rather than short-run stimulus (Friedman 1957; Hall 1978). This is a cornerstone of the neoclassical view on consumption behavior. Friedman proposed that an individual’s consumption at any given time is determined not by current income alone, but by their permanent income, which is the expected long-term average income. Temporary fluctuations in income, according to this theory, have only a small effect on consumption because people use saving and borrowing to smooth out those fluctuations. In other words, households act like long-term planners: if they receive an unexpectedly high income this year, they will not dramatically raise their spending, understanding that the extra income may not last. Instead, they will save most of it (or pay down debt), spreading the benefit over future years. Conversely, if income dips briefly, they can draw on past savings or borrow to maintain their usual consumption level, expecting to repay when income recovers. This behavior leads to relatively stable consumption paths, as illustrated by Friedman’s famous observation that consumption is much smoother than the often volatile income streams that individuals experience year to year.\n(60. The Permanent Income Model — Intermediate Quantitative Economics with Python) Example of consumption smoothing in a simulation of the permanent income model – the green line shows fluctuating income, while the black line shows consumption, which varies much less due to saving/borrowing. The blue line (debt) rises when income is below average and falls (becoming savings) when income is above average. Rational consumers use credit markets to smooth out income shocks, keeping consumption on a steady trajectory.\nFriedman’s model assumes that credit markets function well (people can borrow against future income) and that consumers are forward-looking and rational. Under these conditions, monetary and fiscal policy have limited ability to alter consumption unless they affect expected long-term income. For example, a one-time tax rebate or a temporarily lower interest rate might not stimulate much extra spending – consumers recognize that this is a short-term change. Indeed, a key takeaway of the permanent income theory is that policies which only increase current income without raising expected future income will mostly lead to higher saving rather than higher spending. Friedman contrasted this with the Keynesian view in which consumers have a high marginal propensity to consume out of current income (perhaps because they are myopic or liquidity-constrained). He argued that the Keynesian assumption was flawed in ignoring forward-looking behavior. Empirical puzzles of the mid-20th century (such as why consumption didn’t rise one-for-one with income gains from, say, war-time fiscal expansions) could be explained by PIH: people understood those income gains were temporary and saved much of them.\nIn policy terms, the Friedman consumption model supports a rather conservative use of demand management. A central bank that rapidly expands money or lowers interest rates might not trigger a large consumption boom unless people believe those actions will persist and raise their permanent income or wealth. Similarly, a government stimulus check will be partly saved if households treat it as a transitory windfall. An important implication is that discretionary policy “surprises” are not a reliable way to boost aggregate demand – rational agents will react mainly to the expected persistent components of policy. Monetarists like Friedman instead advocated rule-based policies (such as steadily growing the money supply at a fixed rate) to provide a stable environment for consumers and investors to plan. If policy is erratic, it could even be counterproductive: for instance, trying to exploit a short-run trade-off by pushing unemployment lower than its natural rate would just raise inflation expectations, with little lasting benefit to output (this is essentially Friedman’s adaptive expectations version of the Phillips Curve argument). In summary, Friedman’s complete-market consumption model underscores the importance of expectations and permanent income. It suggests that monetary policy should focus on the long-term nominal stability (controlling inflation) and avoid frequent discretionary shifts, because people will see through those shifts and adjust their saving behavior accordingly. It also implies that fiscal stabilization (e.g. stimulus payments) will be most effective when aimed at households likely to be liquidity-constrained, a point that becomes clearer once we consider incomplete market models."
  },
  {
    "objectID": "projects/incomplete/incomplete.html#consumption-investment-trade-off-under-liquidity-constraints",
    "href": "projects/incomplete/incomplete.html#consumption-investment-trade-off-under-liquidity-constraints",
    "title": "Monetary Policy and Market Imperfections",
    "section": "Consumption-Investment Trade-off under Liquidity Constraints",
    "text": "Consumption-Investment Trade-off under Liquidity Constraints\nLiquidity constraints disrupt optimal consumption-investment trade-offs. Constrained agents cannot invest sufficiently during downturns, weakening monetary policy’s effectiveness, particularly in stimulating investment or consumption among constrained households.\nA central theme in a standard intertemporal choice problem is the trade-off between consuming today and investing for tomorrow. In a frictionless world, consumers equate the marginal benefit of spending an extra dollar today with the marginal benefit of saving that dollar (investing it to spend later). This optimality condition (often called the Euler equation in macroeconomics) ensures that resources are allocated to their most valued use over time. However, in reality many households and firms face liquidity constraints or borrowing limits that prevent them from freely making this trade-off. Such constraints are a key imperfection that alters the impact of monetary policy and other shocks.\nWhen agents are liquidity-constrained, they cannot borrow as much as they would like against future income. This means in bad times they might want to maintain consumption or invest in opportunities (human capital, business expansion, etc.), but they simply lack the funds or credit access to do so. Consequently, current consumption may fall below the level that would be chosen under complete markets, and valuable investments might be foregone. For instance, a skilled worker who becomes unemployed may cut back sharply on consumption – not because their lifetime income prospects are shattered, but because in that moment they don’t have liquid assets or credit to smooth over the gap. Likewise, a small business might pass up a profitable investment because banks refuse credit due to the firm’s lack of collateral. These scenarios lead to a suboptimal allocation of resources over time, amplifying short-run fluctuations and causing longer-run consequences (lost growth from underinvestment, etc.).\nFrom a policy perspective, liquidity constraints mean that monetary policy may have an asymmetric effect. If the central bank raises interest rates, it generally cools off borrowing and spending – both unconstrained and constrained agents will cut back (the former by choice, the latter perhaps by necessity as credit becomes more expensive or scarce). But if the central bank lowers interest rates to stimulate the economy, those who are constrained might still be unable to borrow (banks may not lend to them even at low rates, if their balance sheet is weak or job uncertain) and thus cannot increase consumption or investment. In other words, there is a segment of the population for whom monetary easing doesn’t translate into more spending because they were not borrowing in the first place (they were at their borrowing limit). Instead, the stimulus might mainly induce already well-capitalized agents to borrow or invest more – which can have distributional effects.\nOn the other hand, consider fiscal policy: a transfer (like a stimulus check or unemployment benefit extension) given to a liquidity-constrained household is likely to be spent in large part, precisely because that household’s consumption was suppressed by lack of funds. Empirical evidence and incomplete-market models both find that households with little liquid wealth have high marginal propensities to consume (MPCs) out of such transfers. This contrasts with the near-zero MPC out of a transitory income increase for a fully smoothed consumer in Friedman’s framework. Therefore, liquidity constraints reconcile why Keynesian-style demand stimulus can work in practice (many people do spend most of an extra dollar if they were cash-strapped), even though Friedman’s theory might suggest they shouldn’t. Modern heterogeneous agent models incorporate this insight by showing that when a large fraction of consumers are hand-to-mouth or buffer-stock savers, aggregate consumption is sensitive to the distribution of income and cash-on-hand.\nFor investment, liquidity constraints imply that not all investment opportunities are realized, especially among smaller firms or entrepreneurs, if external finance is costly or unavailable. In a recession, even if the central bank slashes interest rates, banks may be risk-averse and tighten lending standards, so only the safest borrowers benefit from low rates. This can lead to a situation often described as “pushing on a string,” where monetary policy loses traction in stimulating additional private investment or consumption because the bottleneck is in credit access, not the cost of credit per se.\nIn summary, the consumption-investment trade-off under liquidity constraints highlights that market imperfections can dampen or distort the transmission of monetary policy. A perfectly rational, unconstrained agent might respond to lower interest rates by optimally borrowing and spending more (since the opportunity cost of funds is lower). But a constrained agent does nothing (they can’t borrow anyway), and an unconstrained wealthy agent might already be satiated in consumption and only shift their portfolio. These dynamics mean that in downturns, monetary policy might need support from fiscal measures that target constrained agents to be fully effective. It also means that policymakers should be aware of credit conditions and possibly use regulatory tools to ensure that rate cuts get passed through to borrowers. The general principle is that in the presence of liquidity constraints, short-run fluctuations can have long-run costs (foregone investment, lower human capital accumulation) and policies should aim to alleviate these constraints during bad times."
  },
  {
    "objectID": "projects/incomplete/incomplete.html#bewley-model-precautionary-savings-in-an-incomplete-market",
    "href": "projects/incomplete/incomplete.html#bewley-model-precautionary-savings-in-an-incomplete-market",
    "title": "Monetary Policy and Market Imperfections",
    "section": "Bewley Model: Precautionary Savings in an Incomplete Market",
    "text": "Bewley Model: Precautionary Savings in an Incomplete Market\n\nAssumes heterogeneous agents face idiosyncratic income shocks and borrowing limits.\nExogenous interest rates.\nGenerates wealth inequality through precautionary savings.\nHighlights the importance of social safety nets and targeted fiscal policies for macroeconomic stability (Bewley 1986).\n\nThe Bewley model (named after economist Truman Bewley) is a foundational framework for analyzing incomplete markets with heterogeneous agents. In Bewley’s setup, we consider a large number of infinitely-lived consumers who face idiosyncratic income shocks in each period. These shocks are uninsurable – there is no complete set of insurance markets for them – and consumers can only trade a single risk-free asset (such as a bond or money) to self-insure. Moreover, consumers face a borrowing limit (they cannot have debt beyond a certain level, often this is set to zero for simplicity). Despite all consumers having the same preferences and income process ex ante, the randomness of shocks makes them heterogeneous ex post in terms of their asset holdings and current income. This type of model is often called a heterogeneous agent incomplete-markets model, or simply a Bewley model, after the seminal work in (Bewley 1986). It has become a workhorse for understanding consumption, saving, and wealth distribution under uncertainty.\nIn the Bewley model, each consumer solves a consumption-saving problem: how much to consume today versus save as a buffer for future uncertainty. A typical finding is the emergence of a precautionary saving motive – people save not just for lifecycle reasons (retirement, etc.) but also to buffer against income risk. Those who experience good shocks build up assets, while those hit by bad shocks draw down assets or if they have none, they hit the borrowing constraint and their consumption drops. Over time, the model reaches an equilibrium where the cross-sectional distribution of wealth is stationary (in a statistical sense): some fraction of the population has high wealth, some has low wealth, with persistent inequality generated purely from idiosyncratic risk and saving behavior. This equilibrium typically features a fat-tailed distribution, meaning there are some very high-wealth individuals (who had a run of good shocks or especially strong saving discipline) and a significant mass of low-wealth individuals who might be frequently at the edge of the borrowing constraint. Quantitatively, such models can generate wealth concentration that qualitatively resembles that observed in real economies (though matching the extreme concentration in actual data often requires adding other elements like heterogeneity in earnings ability or rates of return).\nOne key aspect of the Bewley model is that the interest rate is treated as exogenous (or determined outside the model, say by a central bank or a global capital market). In other words, Bewley’s original formulation is a partial equilibrium analysis: it looks at an individual’s optimal saving given an interest rate, but does not necessarily determine that interest rate from within the model. This is akin to studying a small open economy where people can save or borrow at a fixed world interest rate, or a situation with a perfectly elastic supply of funds. Under this fixed interest rate, not everyone can dissave indefinitely because of the borrowing constraint, so in aggregate there will typically be positive net saving (since precautionary motives induce people to hold assets). If the interest rate is high relative to people’s time preference and risk, the low-wealth agents will borrow up to the limit and the high-wealth will save a lot, and an equilibrium wealth distribution forms. If the interest rate is too high, precautionary saving might not be enough to sustain it (people try to borrow too much); if it’s too low, people accumulate assets and the economy might reach a point where the lowest wealth is at the borrowing limit and highest is still saving – typically there is some interest rate that balances asset demand and the “excess” of precautionary saving.\nWhile the technical details can be involved, the intuition gleaned from the Bewley model is powerful for policy. It shows how incomplete markets alone (without any price rigidity or aggregate shocks) can lead to under-consumption by some and the accumulation of large buffers by others. This has implications for long-run growth and inequality. If many people are constrained and cannot invest in their education or businesses, the economy might underperform its potential. It also implies that policies like social insurance (unemployment insurance, social security, etc.) can affect aggregate outcomes: for example, providing more generous unemployment benefits might reduce the need for precautionary saving, which could actually stimulate consumption among lower-wealth households and reduce inequality. On the flip side, too generous a safety net could reduce the incentive to save at all. Bewley-type models have been used to examine optimal policy in this context, such as what level of unemployment insurance optimally trades off providing insurance versus maintaining incentives.\nAn important extension of the Bewley model is to use it for wealth distribution insights. The model clarifies that even if everyone has identical earning potential, incomplete markets will generate inequality simply due to luck and precautionary behavior. This suggests that some observed inequality is not due to differences in skill or hard work, but due to insufficient insurance against life’s risks. Policymakers concerned with excessive inequality might draw on this insight to justify progressive taxation or public insurance programs that effectively do what missing markets would have – help smooth incomes and consumption across states of the world. Indeed, one policy implication highlighted in such models is that improving access to credit for credit-worthy but constrained households, or providing more public insurance, could make the overall economy better off by allowing more efficient consumption and investment choices (though there are always trade-offs and moral hazard issues to consider).\nIn summary, the Bewley model provides a micro-founded explanation for why some people end up liquidity-constrained and how that influences their behavior. For monetary policy, it warns that aggregate demand may be more sensitive to the distribution of wealth and income than traditional models would suggest – if a recession hits the lower-wealth population hard, their consumption will contract strongly (since they can’t borrow), potentially deepening the downturn. Purely focusing on interest rates as a lever might be insufficient; fiscal redistributive tools or direct transfers could be more potent in such scenarios. The model’s relevance has grown as economists recognize the limitations of the representative-agent paradigm and seek to incorporate heterogeneous agent effects into macroeconomic policy analysis."
  },
  {
    "objectID": "projects/incomplete/incomplete.html#aiyagari-model-general-equilibrium-with-incomplete-markets",
    "href": "projects/incomplete/incomplete.html#aiyagari-model-general-equilibrium-with-incomplete-markets",
    "title": "Monetary Policy and Market Imperfections",
    "section": "Aiyagari Model: General Equilibrium with Incomplete Markets",
    "text": "Aiyagari Model: General Equilibrium with Incomplete Markets\n\nIncorporates endogenous determination of interest rates through production equilibrium.\nDemonstrates “excess capital accumulation” due to precautionary motives.\nAdvocates capital income taxation for improved welfare and highlights the distributional consequences of monetary policy changes (Aiyagari 1994).\n\nS. Rao Aiyagari’s model builds directly on the Bewley framework but adds an important layer: a production economy that yields a general equilibrium determination of prices (interest rate and possibly wages). In the Aiyagari (1994) model, we still have infinitely-lived agents with idiosyncratic income shocks and borrowing constraints (precisely the Bewley setup on the household side), but now those households supply savings to, and borrow from, a productive sector with capital. In essence, Aiyagari embeds the precautionary savings behavior into a full macroeconomic model with capital accumulation. The result is a self-contained macroeconomic equilibrium where the interest rate is endogenously determined by the supply and demand for capital, rather than being fixed externally. Households’ collective saving (driven by precautionary motives) feeds into the capital stock, and firms’ demand for capital (based on productivity and diminishing returns) determines the equilibrium interest rate that clears the capital market.\nOne of Aiyagari’s key findings is that in an economy with uninsurable income risk, the equilibrium interest rate will generally be lower than it would be in a comparable complete-markets economy. Intuitively, because households value holding assets as a buffer (beyond what they would in a no-risk scenario), they tend to save more, which pushes down the return to capital. In other words, there is excess aggregate saving due to precautionary motives, leading to a larger capital stock and lower interest rate than the classical model without income risk would predict (Marcet, Obiols-Homs, and Weil 2007). This is sometimes referred to as the “Aiyagari excess capital result.” It implies that the laissez-faire outcome might not be socially optimal – there could be “too much” capital from a certain perspective, because individuals don’t internalize that by saving so much for themselves, they depress the return for everyone. One practical implication Aiyagari pointed out is that a government could improve welfare by taxing capital income and redistributing it (or using it to fund social insurance) in such an economy. By doing so, it reduces the need for individuals to self-insure via excessive capital accumulation, potentially moving the economy closer to the golden-rule level of capital (where consumption is maximized). This was a striking result since in a standard frictionless model, capital taxation is often detrimental in the long run – but here, moderate capital taxation can correct an inefficiency arising from incomplete markets.\nThe Aiyagari model also provides insight into the interplay between inequality and aggregate production. Unlike the Bewley model, which was partial equilibrium, here the distribution of wealth affects aggregate supply (through capital accumulation). If the wealth is concentrated in fewer hands, the aggregate consumption could be lower (since wealthy individuals have lower MPCs, they might save a lot of their income), and the aggregate capital might be higher (since those with excess wealth invest it). This has led to extensive research on the quantitative impact of redistributive policies on growth and output. For instance, if you redistribute wealth from the rich (low MPC) to the poor (high MPC), you might raise current consumption but reduce saving and thus future capital – whether that is good or bad for long-run output depends on parameters, but in some cases it can actually increase output if the economy was above the golden rule level of capital to start with (Marcet, Obiols-Homs, and Weil 2007).\nAnother aspect is the feedback of interest rates on inequality. In Aiyagari’s equilibrium, the interest rate settles at a level where households are indifferent between saving and not saving (on the margin). If interest rates are very low, borrowing is cheap, but also the reward for saving is low, which could discourage some saving. However, typically in these models many households still save because of risk aversion and precaution. The low interest rate also means that those who are borrowing-constrained are not paying a huge interest burden (assuming they can borrow at that rate), but many cannot borrow much anyway due to the constraint. Overall, compared to a representative-agent model, the Aiyagari model predicts different responses to monetary policy. For example, if the central bank lowers the interest rate (below the equilibrium that would prevail from just technology and time preference), it transfers resources from savers to borrowers. In an economy with inequality, this has non-neutral effects: borrowers (often poorer agents) gain relief and might consume more, while savers (wealthier agents) earn less on their assets and might consume less (or seek riskier investments). The distributional effects of monetary policy come into play. Recent research in heterogeneous agent New Keynesian (HANK) models builds on this by adding nominal rigidities, but even in the basic Aiyagari model, one can see that monetary policy is not just about one representative agent’s intertemporal choice – it will create winners and losers due to heterogeneity in assets and consumption propensities.\nFor policymakers, Aiyagari’s work underscores a few points: (1) Monetary neutrality may not hold cleanly in the short run even if prices are flexible, because redistributions caused by interest rate changes can affect aggregate demand; (2) there may be a role for permanent fiscal policy (like capital taxation or debt issuance) to influence the long-run capital stock and interest rate in a way that improves welfare, countering the incomplete-market externality; (3) evaluating monetary policy requires understanding the underlying wealth distribution – for instance, a low interest rate environment will tend to benefit borrowers and younger households (via cheaper credit, higher asset values) while hurting those who rely on interest income (like pensioners or wealthier rentiers). If mismanaged, prolonged ultra-low rates can contribute to asset price inflation (as savers seek returns in real estate or stocks), thereby widening wealth inequality if only the already-wealthy hold those appreciating assets. Indeed, some attribute the rise in asset valuations and wealth concentration in recent decades partly to very low global real interest rates and ample liquidity, consistent with the mechanisms in Aiyagari-type models.\nIn conclusion, the Aiyagari model enriches our understanding by marrying heterogeneity with production. It reminds us that macroeconomic policy cannot be divorced from distributional considerations. The long-term natural rate of interest, the effectiveness of fiscal redistribution, and the impact of monetary policy all look different once we acknowledge that not everyone is alike in the economy. By capturing how liquidity constraints and precautionary savings influence aggregate capital, this model provides guidance on questions like whether and how to tax wealth, and how aggressive monetary policy should be in, say, pushing interest rates to very low levels. Policymakers drawing on these insights might strive for a balance: ensuring there is enough aggregate saving for investment and growth, but not so much that it reflects unmet social insurance needs or creates financial imbalances."
  },
  {
    "objectID": "projects/incomplete/incomplete.html#harrison-kreps-1978-model-asset-pricing-with-heterogeneous-beliefs",
    "href": "projects/incomplete/incomplete.html#harrison-kreps-1978-model-asset-pricing-with-heterogeneous-beliefs",
    "title": "Monetary Policy and Market Imperfections",
    "section": "Harrison & Kreps (1978) Model: Asset Pricing with Heterogeneous Beliefs",
    "text": "Harrison & Kreps (1978) Model: Asset Pricing with Heterogeneous Beliefs\n\nHeterogeneous investor beliefs combined with short-sale constraints can cause speculative bubbles, elevating asset prices above fundamental values.\nSuggests improving market completeness and transparency to curb speculation and volatility (Harrison and Kreps 1978).\n\nThe Harrison and Kreps (1978) model introduces a different kind of market imperfection into macro-finance: heterogeneous beliefs among investors, combined with constraints on short selling. Unlike the previous models (which focused on borrowing constraints and income risk), this model lives in the world of asset trading and speculation. Harrison and Kreps asked what happens in an asset market when investors have differing opinions about an asset’s value and they are not allowed to short sell the asset freely. Their answer was groundbreaking: even if all investors are rational (in that they update beliefs consistently with their own information), the mere diversity of opinion can lead to asset prices exceeding the valuation of even the most optimistic individual investor.\nHere’s the intuition: suppose some investors (“optimists”) believe a stock or house will be very valuable in the future, while others (“pessimists”) believe it will not. If short selling is constrained (pessimists cannot easily borrow the asset to sell it short), the market price will be determined largely by the optimists’ willingness to pay. Now add the element of speculation – investors may buy an asset not just for its fundamental value (like dividends or rent) but also for the option to resell it in the future. Harrison & Kreps showed that when beliefs differ, an investor might pay more than their own estimate of the asset’s fundamental value because they anticipate that someone even more optimistic might buy it at a higher price later. In effect, a resale option is priced in. This leads to what we might call a speculative premium on the asset price. The price can rise above the level that any single investor would pay if they had to hold the asset forever. In their words, the right to resell makes investors willing to pay more than the asset’s “hold-to-maturity” value. The inability of pessimists to short sell means nothing counteracts this upward pressure – the pessimists simply sit out of the market rather than actively pushing the price down by shorting. Thus, the market price reflects an over-optimistic valuation, driven by the most bullish views and the prospect of flipping the asset.\nThis mechanism helps explain phenomena like asset price bubbles or situations where market prices seem to detach from fundamental values. Real estate is a commonly cited example: investors might buy houses at high prices not only because they expect rising rents or income (fundamentals), but because they think they can later sell the house to someone else at an even higher price (speculation). If enough people believe housing prices will keep rising, and skeptics can’t effectively short the housing market, the result is a self-reinforcing price boom. The Harrison-Kreps model formalized how even fully rational agents with rational expectations (each given their own belief) can end up trading at prices that embed a speculative component. It doesn’t require irrational exuberance; it only requires disagreement and some friction (short-sale constraints) that prevents full arbitrage. In their equilibrium, everyone understands the price is above their own fundamental valuation, but they also know someone else might be willing to pay even more, so it can still be rational to buy now and plan to sell later – a clear parallel to the greater fool theory, but derived in a rigorous way.\nPolicy implications from the Harrison & Kreps model revolve around financial market regulation and information disclosure. One implication is that short-selling constraints can fuel overpricing. If regulators make short selling too restrictive (perhaps in an attempt to curb volatility or prevent speculative attacks), they might inadvertently remove a balancing force that keeps prices close to fundamentals. The model would suggest that allowing more short selling (with proper oversight to avoid abuse) could actually lead to more informative, less one-sided pricing. Another implication is the value of transparency and common information. In the model, beliefs are heterogeneous and “dogmatic” – each trader sticks to their prior and interprets signals in their own way. If public information can help align beliefs (or at least inform the pessimists and optimists of each other’s views), it might reduce the degree of disagreement. However, complete agreement is unrealistic; differences in models, data interpretation, or risk appetite will always create some dispersion of opinion.\nFrom a monetary policy perspective, one might not immediately see a connection, since H&K is about asset pricing in a frictional financial market. But there are subtle links. Central banks today pay close attention to asset markets – housing, equities, etc. – because large deviations of asset prices from fundamentals can pose risks to financial stability and the broader economy. For instance, if low interest rates contribute to a speculative housing boom (by making borrowing cheap and encouraging optimistic beliefs about ongoing price growth), a subsequent crash could harm banks and consumers, leading to a recession. The H&K model suggests that a booming asset market is not necessarily a sign of solid fundamentals; it could be a sign of constrained pessimism and resale-driven pricing. Policymakers, therefore, should be cautious in interpreting asset price signals. It also provides an argument for macroprudential policies: tools that directly address asset market excess (for example, tighter loan-to-value ratios in mortgage lending during a housing boom, or stricter margin requirements in stock trading). These can be seen as ways to mitigate the speculative dynamics – essentially pricking bubbles before they grow too large. By making it harder to purely speculate (through leverage restrictions) or by encouraging more two-sided markets (perhaps by permitting certain derivatives or short positions), regulators might reduce the likelihood of severe mispricings.\nIn summary, the Harrison & Kreps model adds another layer to our understanding: market outcomes can be inefficient not just because of real-side frictions (like incomplete insurance) but also because of financial-side frictions (like trading constraints and belief dispersion). It is a reminder that even with rational actors, markets may need regulatory oversight to ensure they reflect true economic value. For a policymaker, being aware of this mechanism is important. It cautions against assuming that all investors have the same expectations (they don’t), and it illustrates why asset price booms can develop even without obvious irrationality. Recognizing a speculative bubble early is notoriously difficult, but understanding models like this helps officials appreciate the warning signs (e.g., when asset prices only make sense under very optimistic scenarios and buyers cite the ability to resell as justification). It also supports measures to improve market completeness – such as permitting more sophisticated financial instruments – because a more complete market (ability to hedge, to short, etc.) ironically may prevent the wild swings that incomplete markets allow."
  },
  {
    "objectID": "projects/incomplete/incomplete.html#policy-implications-of-models",
    "href": "projects/incomplete/incomplete.html#policy-implications-of-models",
    "title": "Monetary Policy and Market Imperfections",
    "section": "Policy Implications of Models",
    "text": "Policy Implications of Models\n\nMonetarism emphasizes stable, predictable policy frameworks.\nBewley and Aiyagari models highlight social insurance and targeted redistribution.\nHarrison & Kreps stress regulation of speculative financial markets.\n\nEach of the models discussed offers distinct insights for the design of monetary policy and related economic policies. We summarize the key policy implications from each framework:\n\nFriedman/Monetarist Consumption Smoothing (Complete Markets): The policy lesson here is to avoid erratic or overly activist monetary policy, and instead commit to clear, long-term rules. Since consumers base spending on permanent income, temporary boosts from monetary expansions will mostly be saved, providing at best a short-lived stimulus. Trying to exploit short-run trade-offs (like printing money to lower unemployment) is ineffective once people adjust their expectations, and it risks creating inflation with no long-term gain in output. Thus, monetarists argue for steady money supply growth or an inflation target that is credible, so that businesses and consumers can make plans without fearing surprises. In practice, this translated to proposals like Friedman’s k-percent rule (grow the money supply by a fixed percent each year) and, later, to inflation targeting regimes adopted by many central banks. Fiscal policy in this view should also be prudent: large deficits might lead people to expect future taxes (Ricardian equivalence), so they save any tax cuts. Overall, the Friedman model supports policy credibility and consistency. A lack of credibility – say if a central bank is suspected of financing deficits or pursuing political goals – could unmoor expectations and cause instability. To avoid such outcomes, many countries gave central banks independent status and clear mandates (e.g. 2% inflation target). The monetarist perspective also cautions against using monetary policy to address issues like unemployment beyond its natural rate or wealth inequality directly; those should be tackled with structural and fiscal tools, as monetary policy’s role is limited to nominal stabilization in the long run.\nBewley Incomplete-Market Model: A major implication from the Bewley model is the importance of the social safety net and credit market policies. Because individuals cannot fully insure against job loss or income drops, they self-insure by accumulating precautionary savings and cutting consumption in bad times. Policies that provide insurance – unemployment benefits, food assistance, public healthcare, etc. – can alleviate the most severe constraints and prevent sharp collapses in consumption among those hit by shocks. This not only has a humanitarian rationale but also a macroeconomic one: by softening the blow of recessions on individuals, these policies help stabilize aggregate demand (people with insurance don’t cut spending as drastically when something bad happens). However, overly generous benefits could discourage work or saving, so the model implies an optimal balance. Furthermore, ensuring access to credit for credit-worthy households (for example, through student loan programs or small business loans) can improve efficiency: it allows productive investments (education, entrepreneurship) that might not happen if people are liquidity-constrained. From a monetary policy perspective, if many consumers are liquidity-constrained, fiscal tools might be more direct in boosting demand – central banks may encourage governments to step in when needed (as seen in the 2020 COVID crisis, where direct payments had a large effect on spending). Another insight is that inequality in wealth can reduce aggregate demand, since the wealthy spend a smaller fraction of each extra dollar than the poor. So, extremely skewed wealth distribution (which the Bewley model shows arising from incomplete markets) could lead to a chronic shortfall in demand (some economists link this to secular stagnation). In such cases, redistributive fiscal policies or higher inflation targets (to erode real debt and boost spending) might be considered. The Bewley model thus broadens the mandate of policymakers: stable inflation is important, but so is the distribution of resources, as it feeds into macro stability.\nAiyagari General Equilibrium Model: Building on Bewley, Aiyagari adds that monetary and fiscal policy can influence the long-run capital stock and interest rate in an economy with incomplete markets. One direct policy implication from Aiyagari (1995) was that moderate capital income taxation can be welfare-improving. Taxing capital and redistributing income (or f## Empirical Review: Evaluating Policy Recommendations in Practice\nHistory provides a testing ground for the theories discussed, and the record reveals both successes and challenges for various policy approaches. We review a few notable episodes and empirical findings to see how the theoretical predictions held up and what lessons were learned:\nThe Great Depression and its aftermath: Monetarists, led by Friedman and Schwartz, famously argued that the Federal Reserve’s failure to prevent a sharp contraction of the money supply was the primary cause of the Great Depression in the 1930s. They documented how bank failures and Fed inaction led to deflation and collapsing output. This case strongly validated the importance of stabilizing the money supply (or more broadly, aggregate demand) – a point on which Keynesians and monetarists eventually agreed, even if their recommended methods differed. The Depression also underscored the dangers of debt-deflation (Irving Fisher’s theory): once prices fell, the real burden of debt skyrocketed, squeezing borrowers (a scenario an incomplete-markets model with many bankruptcies would show vividly). The policy response in the 1930s was mixed – initially, very little was done by monetary authorities; later, fiscal New Deal programs provided relief and the abandonment of the gold standard allowed some monetary expansion. The eventual massive fiscal stimulus of World War II finally ended the Depression. In theoretical retrospect, the Great Depression illustrates that when monetary policy is constrained (or mismanaged), fiscal policy and other interventions become crucial to restore confidence and demand. It also led to institutional changes: deposit insurance was introduced (addressing a Bewley-model type issue of self-insurance by preventing bank runs), and central banks became more attuned to their role as lenders of last resort.\nPost-WWII Keynesian vs Monetarist debates: In the postwar decades, Keynesian demand management (using fiscal deficits or surpluses to smooth the business cycle) was in vogue. This worked with moderate success in the 1950s and 60s when economies were often below full capacity and inflation was low. However, the late 1960s and 1970s brought stagflation – high inflation and high unemployment – which was a rude awakening that simple Keynesian policies had limits. Friedman’s prediction of the natural rate of unemployment (and the breakdown of the Phillips Curve trade-off) was confirmed when governments trying to push unemployment too low ended up with accelerating inflation but no further gains in employment. Monetarists advocated focusing on inflation and letting unemployment find its natural level. This led to the famous Volcker disinflation in the early 1980s: the Fed (and other central banks) raised interest rates dramatically, inducing recessions that tamed inflation expectations. The cost was high in terms of lost output in the short run, but the benefit was decades of more stable prices thereafter. The success of this strategy gave credibility to rule-like behavior (Volcker essentially followed a monetarist playbook initially, targeting money growth, then later switching to direct inflation targeting). It also vindicated the rational expectations view to some extent – once the policy regime changed credibly, inflation fell faster than old models predicted, as people adjusted expectations. However, there was also learning that pure monetarist targeting of monetary aggregates proved difficult in practice (the relationship between money supply measures and the economy broke down in the 1980s), so central banks shifted to interest rate targets and broad-based judgment. The empirical lesson: controlling inflation is crucial and possible, but requires willingness to endure short-term pain, and policy must adapt to structural changes (financial innovation changed money demand, for example).\nRise of New Keynesian Consensus (1990s–2000s): By the 1990s, a synthesis emerged: a baseline New Keynesian model (which is essentially an RBC core with price-stickiness and a monetary policy rule) became the standard for many central banks. This model, often featuring a representative agent, did well for normal business cycle fluctuations and was validated by the period of the Great Moderation (mid-1980s to 2007) when inflation and output volatility were relatively low. Central banks fine-tuned interest rates in response to output gaps and inflation deviations (Taylor rule behavior) and generally succeeded in keeping economies stable. However, this era also saw growing inequality in many countries (due to globalization, technological change, and possibly financial asset booms). Mainstream macro models largely ignored inequality, assuming it didn’t affect aggregate outcomes. Empirically though, questions arose: was the glut of global savings partly a result of rising wealth concentration and emerging-market reserve accumulation (an incomplete-market global effect)? Were low interest rates fueling a series of asset bubbles (tech stocks in the 90s, housing in the 2000s) that could have devastating effects when they burst? These questions became painfully acute with the 2008 Global Financial Crisis. The crisis was triggered by a combination of factors aligned with multiple models: a Harrison-Kreps style speculative bubble in housing and complex financial products, a huge build-up of debt by households (many of whom turned out to be liquidity-constrained and defaulted when house prices fell), and a banking sector that was under-capitalized and had assumed house prices could only go up. When the bubble burst, it led to a cascade of failures (incomplete markets suddenly very incomplete as credit froze). Central banks cut rates to zero and beyond (some did QE), and governments provided fiscal stimulus and bailouts. The recovery was slow, and in some places inequality worsened as those with assets recovered faster (stocks rebounded) than those without. The aftermath spawned a rethinking: macro models now incorporate financial frictions and heterogeneity much more.\nEmpirical studies on quantitative easing (QE) found that it did boost asset prices (bond purchases raised bond prices and lowered yields, and likely spilled over to equities and real estate). This helped stabilize the financial system and likely prevented an even worse downturn. But it also had a side effect: by making assets more expensive, QE disproportionately benefited wealthy asset holders, contributing to wealth inequality. Central bankers like Janet Yellen and Mario Draghi acknowledged these effects, though they argued that doing nothing would have hurt the poor even more via higher unemployment. This nuanced outcome is very much in line with Aiyagari-type reasoning – there are distributional consequences, but one must weigh them against macro stabilization gains. Research generally shows that while monetary policy can temporarily widen income and wealth gaps (for example, a surprise rate cut tends to increase asset values), the long-term distributional effects are smaller compared to other forces. Still, the perception of unequal gains can erode public support for central banks, a reality policymakers must manage via communication and, ideally, coordination with fiscal policy (e.g., fiscal tools can redistribute some of the gains more broadly).\nPandemic response (2020) and current outlook: The COVID-19 shock led to an even more aggressive policy response than 2008, in part because lessons were learned. Governments globally unleashed fiscal stimulus of unprecedented scale, much of it directed at households and businesses in need (basically applying the Keynesian and incomplete-markets playbook to support incomes). Central banks slashed rates and bought assets again, but this time fiscal policy did the heavy lifting in sustaining demand. The result was a surprisingly fast recovery, but also some side effects: inflation spiked in 2021-2022 as supply chain issues and rapid demand recovery outstripped supply. Some argued that policies over-shot, citing monetarist warnings about too much money chasing too few goods. Others noted that without the big response, economies could have spiraled downward. We are effectively watching another test: can central banks withdraw excess stimulus and tame inflation without causing a new recession? The outcome will inform the debate between those who prioritize aggressive stabilization and those who emphasize caution and long-run neutrality. Already, the surge in inflation has reminded everyone that even decades of stable prices do not mean the problem of inflation is permanently solved – vigilance is required, echoing Friedman’s point that inflation can reignite if policy loses focus.\n\nIn terms of case studies: countries that followed more rigid rules versus those that used discretion provide comparisons. For instance, Germany has historically been very inflation-averse (a monetarist ethos) and also implements strong fiscal rules. This has kept its inflation low but perhaps at the cost of slower demand growth at times. On the other hand, countries like the US and UK were quicker to use unorthodox policies in the crises, arguably recovering faster, but now facing higher inflation that needs wrangling. Emerging markets often learned the hard way that failing to anchor expectations (through credible policies) leads to currency crashes and high inflation – many adopted inflation targeting and improved fiscal discipline in the 2000s, which helped them weather shocks better. However, emerging economies also illustrate incomplete markets externally – they accumulate reserves as self-insurance (precautionary saving at a national level), which ties into global imbalances affecting interest rates.\nEmpirical evaluations of specific policies show mixed results: for example, the fiscal multipliers (effect of government spending on output) tend to be larger when liquidity constraints are binding (like in a recession or at the zero lower bound), confirming the heterogeneous-agent/New Keynesian synergy view. Similarly, studies find that unconventional monetary policies like QE have diminishing returns and potential risks if used for too long – aligning with the idea that you can’t permanently stimulate real activity with monetary expansion (sooner or later, it’s neutral and just affects prices or financial stability).\nOne critique of policy based on these models is that models can be “gamed” to justify almost any policy if one is not careful – i.e., results can be sensitive to assumptions. For example, one could fine-tune an incomplete markets model to show a very large benefit to a certain tax policy, while another calibration might not. This is why empirical validation is key. Policymakers rely on teams of economists to confront models with data: how much did consumers actually spend out of the stimulus? Did inequality actually rise after that rate cut? These questions can be answered with data, lending confidence to or raising doubts about theoretical prescriptions.\nIn conclusion, empirical experience broadly supports a few robust lessons: 1. Stable and credible monetary policy (low, predictable inflation) provides the foundation for economic growth – losing that stability (as seen in various inflationary episodes) leads to hardship and requires painful corrections. 2. Market imperfections are real and matter – ignoring them can lead to blind spots. The 2008 crisis was in part due to ignoring financial fragility; the slow recoveries when relying only on monetary policy show the limits of representative-agent models; the successful use of fiscal transfers in 2020 demonstrated the power of targeting liquidity-constrained households. 3. Wealth and income distribution influence macro outcomes. Extreme inequality can dampen demand and fuel financial imbalances. While central banks can’t solve inequality, their actions can exacerbate or alleviate it at the margins. Coordination with fiscal policy (which can directly redistribute or invest in broad-based gains) is important to achieve inclusive growth. 4. Expectations and communication are key. Miscommunication or unpredictable shifts cause real economic costs (taper tantrums, etc.), validating the rational expectations focus on clear guidance.\nThe case studies reinforce that a balanced approach – taking into account both neoclassical discipline (fiscal/monetary restraint when needed) and Keynesian activism (stimulus when needed) – tends to perform best. Countries with strong institutions that can do both (stimulate in busts, constrain in booms) have generally done better over the long haul. And finally, flexibility and learning are crucial: policymakers must be willing to update their strategies as new information and research become available, much as our theoretical understanding has evolved over time.unding public insurance) effectively substitutes for some private precautionary saving, leading the economy to operate with slightly less capital but potentially higher overall utility. This runs counter to the policy of zero capital taxation that emerges from some complete-market models (per the Atkinson-Stiglitz or Chamley-Judd results), showing how market imperfections alter policy prescriptions. For monetary policy, the Aiyagari model reinforces the idea that policy has distributional effects that should not be ignored. Persistently low interest rates, for instance, help borrowers (often younger or less wealthy participants who may start businesses or buy homes) but hurt savers (often retirees or wealthy rentiers). This can contribute to wealth inequality if rising asset prices (stocks, real estate) accompany the low rates, benefiting asset owners. Policymakers might need to monitor such side effects; while the central bank’s primary goals are inflation and employment, awareness of how its actions impact wealth distribution is growing. Some central banks now openly discuss inequality in their reports, even if they lack a formal mandate to address it. In terms of coordination, Aiyagari’s work suggests monetary and fiscal policy should be coordinated in the long run. For example, if monetary policy is keeping rates very low to stimulate demand (perhaps because inequality and precautionary savings are depressing demand), fiscal policy could step in either by redistributing income (to boost demand directly) or by borrowing for public investment (utilizing the high savings available). In short, the model implies that macroeconomic stabilization might require a mix of tools – interest rate adjustments alone won’t solve problems rooted in incomplete markets.\n\nHarrison & Kreps Speculative Markets Model: The primary policy implications here concern financial market regulation and transparency. To prevent asset price bubbles driven by speculative dynamics, policymakers can consider easing short-sale constraints or providing alternative means for pessimistic views to be expressed (such as futures markets or other derivatives). If, for instance, there had been more instruments for investors to bet on a decline in housing prices in the mid-2000s, the housing bubble in some countries might not have grown as large (or it might have at least signaled trouble sooner). Of course, the growth of mortgage-backed securities and derivatives also contributed to the financial crisis – the issue was they were misused and misunderstood. So the lesson is nuanced: it’s not simply “allow all short selling,” but rather create well-regulated avenues for dissenting opinions in markets, which can mitigate one-sided optimism. Additionally, policy can aim to reduce extreme heterogeneity of beliefs through better information. Requiring higher standards of disclosure for firms and financial products can make it more likely that investors have similar assessments of value, rather than wildly diverging estimates. During episodes of obvious speculative fervor, authorities face a tough choice: intervene (prick the bubble) or stand aside. Harrison & Kreps would say that in a purely rational bubble, everyone knows it’s a bubble but is riding it – this might be popped by a policy signal that changes collective expectations (for example, a central bank hinting that asset prices are overvalued, or using macroprudential tools to curtail excessive borrowing for speculation). Indeed, central banks and regulators increasingly use jawboning and targeted measures to address pockets of exuberance. The model also suggests a need for caution in monetary policy: if low interest rates are contributing to a speculative boom, a central bank might consider that in its decision-making (the so-called “leaning against the wind” approach, which is debated). While targeting asset prices directly is not the primary job of monetary policy, the fallout of a burst bubble often does become the central bank’s problem (as in 2008). Therefore, integrating financial stability concerns with monetary policy is an important implication. This has led to more collaboration between central banks and regulatory bodies, and the use of countercyclical capital buffers for banks, limits on loan-to-value ratios for mortgages in hot housing markets, etc., to dampen speculation.\n\nIn sum, each model teaches that policy cannot be one-dimensional. A steadfast commitment to low and stable inflation (from Friedman’s legacy) remains crucial – history shows that losing control of inflation can be very costly to fix. But equally, ignoring market imperfections can lead to other problems: incomplete markets call for insurance and redistribution mechanisms; heterogeneous agent effects call for coordination of monetary and fiscal responses; and speculative dynamics call for prudential regulation. A comprehensive policy framework in the real world needs to blend these lessons. As a concrete example, consider the aftermath of the 2008 financial crisis and the 2020 pandemic shock: central banks not only cut rates and did quantitative easing (monetary tools), but governments also enacted large fiscal packages (insurance and redistribution), and regulators adjusted rules to keep credit flowing (financial stability measures). That multifaceted approach reflects, at least in spirit, the insights from the range of models above."
  },
  {
    "objectID": "projects/incomplete/incomplete.html#bridging-the-gap-between-theory-and-reality",
    "href": "projects/incomplete/incomplete.html#bridging-the-gap-between-theory-and-reality",
    "title": "Monetary Policy and Market Imperfections",
    "section": "Bridging the Gap Between Theory and Reality",
    "text": "Bridging the Gap Between Theory and Reality\nModels must inform rather than dictate policy. Policymakers should integrate insights from multiple models (complete markets, incomplete markets, and speculative bubbles) and adapt strategies based on empirical data and practical considerations (such as political constraints, credibility, and expectations management).\nBridging theory and reality involves recognizing the limitations of each model and combining their strengths.\nFirstly, no single model captures all aspects of the economy. Friedman’s consumption model assumes away heterogeneity and price stickiness; Bewley and Aiyagari models abstract from nominal frictions and often from aggregate demand fluctuations (they typically focus on steady states or perfect foresight transitions); Harrison-Kreps focuses on asset trading, holding other macro factors constant. Actual economies have all these features simultaneously: heterogeneous agents and sticky prices and speculative financial markets, all interacting. In reality, a recession is often caused by a mix of factors – e.g., a financial bubble burst (à la Harrison-Kreps) that leads to a drop in demand, which is then amplified by liquidity-constrained consumers (à la Bewley) cutting spending, and firms facing sticky prices that can’t adjust quickly (à la New Keynesian models). Policymakers need to understand the big picture: each model is like a partial lens, and together they form a more complete view. Modern macroeconomic research is moving toward integrating these elements (for instance, Heterogeneous Agent New Keynesian models attempt to combine incomplete markets with price stickiness, and some models incorporate belief heterogeneity in a macro context). The practical upshot is that central banks and finance ministries should employ a suite of models and empirical analyses, not just a single paradigm, when assessing policy.\nSecondly, there are factors like expectations formation and behavioral biases that the neoclassical models often assume away with rational expectations. In reality, people may not be fully rational or may use heuristics; expectations can become unanchored if policy lacks credibility. For example, Friedman assumes people eventually adjust to inflation correctly, but if a central bank suddenly pursues a radically different regime (say, to finance a war or large deficit), expectations might overshoot or become chaotic. Similarly, the Harrison-Kreps model assumes rational agents, but real speculative bubbles might involve some herd behavior or extrapolative expectations (people thinking “prices have always risen, so they will keep rising”). Incorporating these could either exacerbate the model’s implications (bubbles get even bigger if some investors are over-optimistic in a biased way) or introduce new dynamics (crashes triggered by panic, etc.). Policymakers thus should also pay attention to market sentiment indicators, surveys of expectations, and so forth, rather than assuming everyone is calculating expected present values with perfect logic. Sometimes, taking pre-emptive action or communication to shape expectations is as important as the policy action itself – a lesson well understood in modern central bank forward guidance strategy.\nAnother practical consideration is political economy and credibility. The models often assume policies can be set optimally by benevolent planners. In reality, central banks and governments are institutions that need to maintain trust. The monetarist advice for rules-based policy partly arises from this: a rule guards against the temptation of short-termism or political interference. Discretionary fiscal policy might be optimal in a model (e.g., send checks to liquidity-constrained households in a recession), but implementing that in time and in the right amount can be challenging politically. There’s often a delay (recognition lag, legislative lag) and sometimes mis-targeting of stimulus due to political compromises. This means that while theory might say “in a downturn, do X,” the reality might be that X arrives late or in diluted form. Policymakers thus also think about robustness – policies that will do reasonably well even if the ideal policy isn’t feasible. For instance, an inflation-targeting central bank provides a stable nominal anchor (robust against many shocks) even if it doesn’t address every problem; a strong automatic stabilizer (like unemployment insurance) kicks in during recessions without needing new legislation each time, providing timely support aligned with Bewley/Aiyagari insights.\nThe limitations of the models in real-world application include calibration issues (what numbers to plug in), and sometimes they omit important sectors (like the banking sector, which was conspicuously absent in many models before the 2008 crisis). The financial system can amplify or dampen policy effectiveness. The 2008 crisis taught that having banks in distress undermines monetary transmission – no matter how low the Fed set interest rates, if banks wouldn’t lend, businesses and consumers couldn’t borrow. That led to a surge of interest in models that include banks and credit constraints at a macro level. Similarly, international factors (exchange rates, capital flows) are abstracted from in a closed-economy model; in reality, a country’s monetary policy can be affected by global capital markets (the “global savings glut” possibly contributing to low interest rates, for example). So policymakers also must consider the global context – something a domestic model might miss.\nHow can policymakers apply theoretical insights to improve outcomes? One approach is stress-testing policies against multiple models. If a proposed policy (say, a new tool like negative interest rates or helicopter money) is run through a standard DSGE model, a heterogeneous agent model, and a financial model, and it appears beneficial in all, that builds confidence in its robustness. If one model shows a potential problem (e.g., helicopter money could unanchor inflation expectations in the monetarist view), the policy can be tweaked (commit to doing it only in a controlled manner). Central banks now regularly use projections that incorporate both rep-agent DSGE results and judgments informed by heterogeneous agent data analysis.\nAnother approach is pilot programs and data-driven adjustments. For instance, if the government considers a new type of fiscal transfer to boost consumption, it can look at microdata to estimate MPCs (taking a page from incomplete-market theory) and design the program accordingly – perhaps targeting those with low incomes or little wealth for maximum impact. Then, as the program is rolled out, they collect data and see if consumption responds as expected. In essence, treat policy like an experiment informed by theory.\nA concrete example of bridging theory and practice is how central banks responded to the COVID-19 pandemic in 2020. The shock was unique, but policymakers drew on multiple theoretical insights: they knew from Keynesian models that a massive demand shock needed a strong response; from monetarist logic that keeping credit flowing would prevent a deflationary spiral; from heterogeneous agent models that direct payments to households and lending to small businesses would be critical (since those were most constrained and likely to spend quickly); and from financial models that ensuring markets didn’t freeze (through liquidity facilities, asset purchases) would prevent a speculative fire-sale spiral in asset prices. The result was an all-out, multi-pronged policy response – essentially applying “whatever it takes” in various dimensions. This kind of response arguably prevented a far worse downturn and is a testament to learning from past theoretical and empirical work.\nIn summary, bridging theory and reality means using theory as a guide, not a straitjacket. Policymakers benefit from understanding the mechanisms highlighted by models – for example, the importance of expectations (Friedman), the impact of liquidity constraints (Bewley/Aiyagari), the role of speculation (Harrison-Kreps), and the significance of price rigidities (New Keynesian). But they must also weigh real-world considerations like implementation delays, political constraints, and unforeseen shocks. The best outcomes likely arise when theory-informed intuition is combined with flexibility and empirical feedback. As one economist quipped, “models are to be used, not believed.” The aim is to ensure that when making decisions, policymakers are aware of pitfalls like time inconsistency, distributional effects, or financial instability, and have contingency plans informed by a broad understanding of how the economy functions in various states."
  },
  {
    "objectID": "projects/incomplete/incomplete.html#empirical-review-evaluating-policy-recommendations-in-practice",
    "href": "projects/incomplete/incomplete.html#empirical-review-evaluating-policy-recommendations-in-practice",
    "title": "Monetary Policy and Market Imperfections",
    "section": "Empirical Review: Evaluating Policy Recommendations in Practice",
    "text": "Empirical Review: Evaluating Policy Recommendations in Practice\nHistory provides a testing ground for the theories discussed, and the record reveals both successes and challenges for various policy approaches. We review a few notable episodes and empirical findings to see how the theoretical predictions held up and what lessons were learned:\n\nThe Great Depression and its aftermath: Monetarists, led by Friedman and Schwartz, famously argued that the Federal Reserve’s failure to prevent a sharp contraction of the money supply was the primary cause of the Great Depression in the 1930s. They documented how bank failures and Fed inaction led to deflation and collapsing output. This case strongly validated the importance of stabilizing the money supply (or more broadly, aggregate demand) – a point on which Keynesians and monetarists eventually agreed, even if their recommended methods differed. The Depression also underscored the dangers of debt-deflation (Irving Fisher’s theory): once prices fell, the real burden of debt skyrocketed, squeezing borrowers (a scenario an incomplete-markets model with many bankruptcies would show vividly). The policy response in the 1930s was mixed – initially, very little was done by monetary authorities; later, fiscal New Deal programs provided relief and the abandonment of the gold standard allowed some monetary expansion. The eventual massive fiscal stimulus of World War II finally ended the Depression. In theoretical retrospect, the Great Depression illustrates that when monetary policy is constrained (or mismanaged), fiscal policy and other interventions become crucial to restore confidence and demand. It also led to institutional changes: deposit insurance was introduced (addressing a Bewley-model type issue of self-insurance by preventing bank runs), and central banks became more attuned to their role as lenders of last resort.\nPost-WWII Keynesian vs Monetarist debates: In the postwar decades, Keynesian demand management (using fiscal deficits or surpluses to smooth the business cycle) was in vogue. This worked with moderate success in the 1950s and 60s when economies were often below full capacity and inflation was low. However, the late 1960s and 1970s brought stagflation – high inflation and high unemployment – which was a rude awakening that simple Keynesian policies had limits. Friedman’s prediction of the natural rate of unemployment (and the breakdown of the Phillips Curve trade-off) was confirmed when governments trying to push unemployment too low ended up with accelerating inflation but no further gains in employment. Monetarists advocated focusing on inflation and letting unemployment find its natural level. This led to the famous Volcker disinflation in the early 1980s: the Fed (and other central banks) raised interest rates dramatically, inducing recessions that tamed inflation expectations. The cost was high in terms of lost output in the short run, but the benefit was decades of more stable prices thereafter. The success of this strategy gave credibility to rule-like behavior (Volcker essentially followed a monetarist playbook initially, targeting money growth, then later switching to direct inflation targeting). It also vindicated the rational expectations view to some extent – once the policy regime changed credibly, inflation fell faster than old models predicted, as people adjusted expectations. However, there was also learning that pure monetarist targeting of monetary aggregates proved difficult in practice (the relationship between money supply measures and the economy broke down in the 1980s), so central banks shifted to interest rate targets and broad-based judgment. The empirical lesson: controlling inflation is crucial and possible, but requires willingness to endure short-term pain, and policy must adapt to structural changes (financial innovation changed money demand, for example).\nRise of New Keynesian Consensus (1990s–2000s): By the 1990s, a synthesis emerged: a baseline New Keynesian model (which is essentially an RBC core with price-stickiness and a monetary policy rule) became the standard for many central banks. This model, often featuring a representative agent, did well for normal business cycle fluctuations and was validated by the period of the Great Moderation (mid-1980s to 2007) when inflation and output volatility were relatively low. Central banks fine-tuned interest rates in response to output gaps and inflation deviations (Taylor rule behavior) and generally succeeded in keeping economies stable. However, this era also saw growing inequality in many countries (due to globalization, technological change, and possibly financial asset booms). Mainstream macro models largely ignored inequality, assuming it didn’t affect aggregate outcomes. Empirically though, questions arose: was the glut of global savings partly a result of rising wealth concentration and emerging-market reserve accumulation (an incomplete-market global effect)? Were low interest rates fueling a series of asset bubbles (tech stocks in the 90s, housing in the 2000s) that could have devastating effects when they burst? These questions became painfully acute with the 2008 Global Financial Crisis. The crisis was triggered by a combination of factors aligned with multiple models: a Harrison-Kreps style speculative bubble in housing and complex financial products, a huge build-up of debt by households (many of whom turned out to be liquidity-constrained and defaulted when house prices fell), and a banking sector that was under-capitalized and had assumed house prices could only go up. When the bubble burst, it led to a cascade of failures (incomplete markets suddenly very incomplete as credit froze). Central banks cut rates to zero and beyond (some did QE), and governments provided fiscal stimulus and bailouts. The recovery was slow, and in some places inequality worsened as those with assets recovered faster (stocks rebounded) than those without. The aftermath spawned a rethinking: macro models now incorporate financial frictions and heterogeneity much more.\nEmpirical studies on quantitative easing (QE) found that it did boost asset prices (bond purchases raised bond prices and lowered yields, and likely spilled over to equities and real estate). This helped stabilize the financial system and likely prevented an even worse downturn. But it also had a side effect: by making assets more expensive, QE disproportionately benefited wealthy asset holders, contributing to wealth inequality. Central bankers like Janet Yellen and Mario Draghi acknowledged these effects, though they argued that doing nothing would have hurt the poor even more via higher unemployment. This nuanced outcome is very much in line with Aiyagari-type reasoning – there are distributional consequences, but one must weigh them against macro stabilization gains. Research generally shows that while monetary policy can temporarily widen income and wealth gaps (for example, a surprise rate cut tends to increase asset values), the long-term distributional effects are smaller compared to other forces. Still, the perception of unequal gains can erode public support for central banks, a reality policymakers must manage via communication and, ideally, coordination with fiscal policy (e.g., fiscal tools can redistribute some of the gains more broadly).\nPandemic response (2020) and current outlook: The COVID-19 shock led to an even more aggressive policy response than 2008, in part because lessons were learned. Governments globally unleashed fiscal stimulus of unprecedented scale, much of it directed at households and businesses in need (basically applying the Keynesian and incomplete-markets playbook to support incomes). Central banks slashed rates and bought assets again, but this time fiscal policy did the heavy lifting in sustaining demand. The result was a surprisingly fast recovery, but also some side effects: inflation spiked in 2021-2022 as supply chain issues and rapid demand recovery outstripped supply. Some argued that policies over-shot, citing monetarist warnings about too much money chasing too few goods. Others noted that without the big response, economies could have spiraled downward. We are effectively watching another test: can central banks withdraw excess stimulus and tame inflation without causing a new recession? The outcome will inform the debate between those who prioritize aggressive stabilization and those who emphasize caution and long-run neutrality. Already, the surge in inflation has reminded everyone that even decades of stable prices do not mean the problem of inflation is permanently solved – vigilance is required, echoing Friedman’s point that inflation can reignite if policy loses focus.\n\nIn terms of case studies: countries that followed more rigid rules versus those that used discretion provide comparisons. For instance, Germany has historically been very inflation-averse (a monetarist ethos) and also implements strong fiscal rules. This has kept its inflation low but perhaps at the cost of slower demand growth at times. On the other hand, countries like the US and UK were quicker to use unorthodox policies in the crises, arguably recovering faster, but now facing higher inflation that needs wrangling. Emerging markets often learned the hard way that failing to anchor expectations (through credible policies) leads to currency crashes and high inflation – many adopted inflation targeting and improved fiscal discipline in the 2000s, which helped them weather shocks better. However, emerging economies also illustrate incomplete markets externally – they accumulate reserves as self-insurance (precautionary saving at a national level), which ties into global imbalances affecting interest rates.\nEmpirical evaluations of specific policies show mixed results: for example, the fiscal multipliers (effect of government spending on output) tend to be larger when liquidity constraints are binding (like in a recession or at the zero lower bound), confirming the heterogeneous-agent/New Keynesian synergy view. Similarly, studies find that unconventional monetary policies like QE have diminishing returns and potential risks if used for too long – aligning with the idea that you can’t permanently stimulate real activity with monetary expansion (sooner or later, it’s neutral and just affects prices or financial stability).\nOne critique of policy based on these models is that models can be “gamed” to justify almost any policy if one is not careful – i.e., results can be sensitive to assumptions. For example, one could fine-tune an incomplete markets model to show a very large benefit to a certain tax policy, while another calibration might not. This is why empirical validation is key. Policymakers rely on teams of economists to confront models with data: how much did consumers actually spend out of the stimulus? Did inequality actually rise after that rate cut? These questions can be answered with data, lending confidence to or raising doubts about theoretical prescriptions.\nIn conclusion, empirical experience broadly supports a few robust lessons: 1. Stable and credible monetary policy (low, predictable inflation) provides the foundation for economic growth – losing that stability (as seen in various inflationary episodes) leads to hardship and requires painful corrections. 2. Market imperfections are real and matter – ignoring them can lead to blind spots. The 2008 crisis was in part due to ignoring financial fragility; the slow recoveries when relying only on monetary policy show the limits of representative-agent models; the successful use of fiscal transfers in 2020 demonstrated the power of targeting liquidity-constrained households. 3. Wealth and income distribution influence macro outcomes. Extreme inequality can dampen demand and fuel financial imbalances. While central banks can’t solve inequality, their actions can exacerbate or alleviate it at the margins. Coordination with fiscal policy (which can directly redistribute or invest in broad-based gains) is important to achieve inclusive growth. 4. Expectations and communication are key. Miscommunication or unpredictable shifts cause real economic costs (taper tantrums, etc.), validating the rational expectations focus on clear guidance.\nThe case studies reinforce that a balanced approach – taking into account both neoclassical discipline (fiscal/monetary restraint when needed) and Keynesian activism (stimulus when needed) – tends to perform best. Countries with strong institutions that can do both (stimulate in busts, constrain in booms) have generally done better over the long haul. And finally, flexibility and learning are crucial: policymakers must be willing to update their strategies as new information and research become available, much as our theoretical understanding has evolved over time."
  },
  {
    "objectID": "projects/incomplete/incomplete.html#conclusion",
    "href": "projects/incomplete/incomplete.html#conclusion",
    "title": "Monetary Policy and Market Imperfections",
    "section": "Conclusion",
    "text": "Conclusion\nEffective monetary policy balances credibility and long-run neutrality (neoclassical insights) with short-run interventions recognizing market imperfections and heterogeneous agents. Policymakers should implement coordinated monetary, fiscal, and regulatory policies to foster inclusive, stable growth.\nThe analysis presented in this paper highlights that effective monetary policy requires a nuanced understanding of economic foundations as well as market imperfections. From the neoclassical and monetarist perspective, we learned the importance of credibility, rules, and the long-run view: money is ultimately neutral, and attempting to exploit short-run effects through discretionary surprises can backfire, leading to inflation or distorted markets. This calls for commitment to low and stable inflation and avoidance of policies that generate uncertainty or arbitrary redistributions of wealth (such as surprise inflations which hurt savers and help debtors). A clear framework – whether a constant money growth rule or an inflation-targeting regime – helps anchor expectations and fosters an environment conducive to investment and growth.\nAt the same time, integrating insights from models of incomplete markets and heterogeneous agents leads to several recommendations for mitigating wealth inequality and market inefficiencies. One key recommendation is for policymakers to address liquidity constraints and lack of insurance in the economy, through complementary fiscal and regulatory policies. For example, strengthening unemployment insurance, facilitating access to healthcare and credit, or even implementing automatic stabilizers that kick in during downturns can prevent the kind of spiraling cutbacks in spending that deepen recessions. By doing so, the economy operates more like the complete-market ideal where consumption doesn’t collapse for those hit by shocks. This not only protects vulnerable households (reducing inequality in bad times), but also stabilizes aggregate demand which makes it easier for monetary policy to do its job.\nAnother recommendation is to incorporate distributional analysis into policy decisions. Central banks could monitor how their interest rate changes affect different income and wealth groups – for instance, via internal research on heterogeneity (many central banks have set up departments to study inequality and finance). While the mandate may remain focused on inflation and employment, if a policy path is likely to create much larger inequality with only marginal benefit for aggregate outcomes, policymakers might seek alternative tools or coordinate with fiscal authorities to offset the impact. For example, if quantitative easing is boosting asset prices significantly, a government could consider fiscal measures (like temporary wealth taxes or increased social spending funded by the gains from higher asset prices) to redistribute some of the benefits. Such coordination can ensure that economic stabilization does not come at the cost of a more divided society, because a widespread loss of social cohesion can also undermine long-term growth and complicate future policy (public opposition can limit what central banks can do).\nFinancial market imperfections, as underscored by the Harrison & Kreps model, imply that regulatory vigilance is needed to maintain efficient and stable markets. Policy recommendations here include maintaining prudent loan-to-value ratios, margin requirements, and ensuring transparency in financial products. Rather than using blunt monetary tightening to pop an asset bubble (which can harm the whole economy), targeted macroprudential measures can be more efficient. However, if a dangerous bubble is inflating and macroprudential measures are insufficient, central banks should not entirely shy away from “leaning against the wind” – a slight adjustment to account for financial stability could be justified. The main point is that preventing severe boom-bust cycles will help avoid sudden spikes in inequality and long-term damage (since crashes often throw many out of work, while some wealthy actors might even profit by buying assets at fire-sale prices, widening disparities). Thus, a stable financial environment is a public good that policy should aim to provide.\nThe long-term orientation of neoclassical models also reminds us that economic growth and efficiency are paramount for raising living standards broadly. Therefore, policies that enhance productivity – education, infrastructure, innovation – are crucial complements to monetary policy. These are often fiscal or structural policies, but monetary policy contributes by providing a low-inflation, stable backdrop and by avoiding distortions (for instance, preventing capital from being misallocated into unproductive speculative ventures via bubble prevention). If wealth inequality is to be reduced, it is best done by inclusive growth – enabling more people to partake in the economy’s advancement (through skill-building, access to capital, etc.) – rather than by short-sighted inflationary policies that might erode the real value of debts at the cost of overall economic health.\nIn practical terms, policy coordination and clarity in roles will yield the best results. Monetary authorities should communicate their limits: for example, “We can manage inflation and support employment, but we alone cannot solve structural unemployment or inequality.” This honest communication can prod other branches of government to act where they must (like reforming education or taxation). Conversely, fiscal authorities can design their interventions (tax, spend, regulate) in a way that complements the central bank’s efforts – e.g., using expansionary fiscal policy in a liquidity trap when monetary policy is out of ammunition, or conversely, restraining fiscal excess in boom times to not put the central bank in a bind. When each policy arm respects the insights of the various models, the overall economic management improves.\nTo specifically mitigate wealth inequality without sacrificing efficiency, policymakers might consider measures such as: - Progressive taxation and wealth taxes used judiciously to fund public goods and transfers (Aiyagari’s work suggests some taxation on capital can be optimal. - Public investment in education, health, and opportunities for the less wealthy, enabling broader participation in growth (thus reducing inequality in a generation and boosting human capital). - Encouraging broad-based asset ownership (for instance, employee stock ownership plans, or incentives for retirement savings for low-income workers) so that more households benefit from asset price increases rather than just a narrow set of investors. - Maintaining low inflation which protects the real incomes of the poor (who have less access to hedges like stocks or real estate) – inflation can act as a regressive tax, so controlling it is pro-poor. - Avoiding high unemployment through timely policy action, since job loss can have permanent scarring effects on workers and increases inequality (those who keep jobs vs. those who don’t). This aligns with using all available tools in a severe downturn to hasten recovery, as seen in 2020.\nAll these recommendations flow naturally from combining the models’ insights: stable money, attention to heterogeneity, and prudent oversight of markets. None of these is easy to implement – each has political and technical challenges. But the cost of inaction can be high. If policy is mismanaged, as we argued, the consequences can be self-defeating: attempts to overstimulate can lead to runaway inflation that hurts everyone (especially the poor), failure to consider distribution can produce political backlash and social unrest, and neglecting financial stability can lead to crises that wipe out wealth and jobs.\nIn closing, the overarching message is one of balance and foresight. Neoclassical economics teaches the value of consistency and respecting long-run constraints; Keynesian and incomplete-market perspectives teach the value of responding to short-run needs and human realities. A policymaker who is informed by both will neither be complacent in good times (ignoring building risks) nor panic in bad times (resorting to unanchored policies). They will strive for inclusive prosperity: economic growth that is stable, broadly shared, and resilient to shocks. By designing monetary and fiscal policies with these principles in mind, we can hope to moderate the boom-bust cycles, temper wealth inequalities, and achieve more stable economic progress. This synthesis of ideas – turning theoretical precision into practical wisdom – is ultimately what leads to sound policy and improved economic outcomes for society as a whole."
  },
  {
    "objectID": "projects/incomplete/incomplete.html#appendix",
    "href": "projects/incomplete/incomplete.html#appendix",
    "title": "Monetary Policy and Market Imperfections",
    "section": "Appendix",
    "text": "Appendix\nThe appendix provides technical details on the theoretical models referenced. We include outlines of the mathematical formulation of each model and key derivations:\n\nFriedman’s Permanent Income Model: We derive the consumption function under the assumption of perfect capital markets and quadratic utility (following Hall (1978)’s rational expectations version). This shows formally that \\(c_t\\) depends on expected lifetime resources and that \\(c_{t+1}-c_t\\) is unpredictable (a martingale) if consumers optimize. We also present a figure illustrating consumption smoothing in response to a one-time income boost (consumption increases only modestly, with most of the boost saved).\nBewley Model: We set up the infinite-horizon dynamic programming problem for the consumer: \\(V(a,y)\\) with asset level \\(a\\) and income state \\(y\\). We show the Euler equation with an occasionally binding borrowing constraint and discuss the existence of a stationary wealth distribution. We include calibrated parameter values (e.g., risk aversion, income shock variance) from a standard calibration and report the resulting Gini coefficients for wealth and consumption. This demonstrates how precautionary savings behavior emerges and how borrowing limits cause deviations from the complete markets benchmark (e.g., higher MPC for low \\(a\\) agents).\nAiyagari Model: Building on the Bewley setup, we add a production function (Cobb-Douglas) and define general equilibrium: the interest rate \\(r\\) and wage \\(w\\) adjust so that the aggregate capital supply (sum of individual assets) equals capital demand by firms, and labor supply equals labor demand. We show the equilibrium conditions and compute the steady-state \\(r\\) which satisfies the market clearing (illustrated by a diagram of the capital supply curve from households and capital demand curve from the firm – the intersection gives \\(r^*\\)). We then compare this \\(r^*\\) to the one in a complete markets Ramsey model with the same technology, confirming \\(r^*_{incomplete} &lt; r^*_{complete}\\) under typical calibrations. We also simulate the model to show the effect of a one-time wealth redistribution on capital and output, illustrating Aiyagari’s point that moderate redistribution can increase aggregate consumption without much loss in output if the economy was over-accumulating capital.\nHarrison & Kreps Model: We outline the example from their paper: two periods, two types of investors with different beliefs about the probability of a high payoff. We illustrate how, with no short selling, the price in the first period reflects the higher valuation (optimist’s valuation plus the option value of resale) and can exceed even the optimist’s expected value of fundamentals. If short-selling is allowed, the pessimist would short at any price above their valuation, which would push the price down closer to fundamental values. We generalize to their infinite-horizon speculative equilibrium concept and provide intuition with a chart showing the pricing function under heterogeneous beliefs. We also note subsequent literature that extended their model (e.g., considering risk aversion or learning, which can either dampen or amplify the speculative premium).\n\nCalibration and Simulation: The appendix includes tables of parameter values used in simulations (time preference, income shock distribution, production function parameters, etc.) and discusses how these are chosen to match empirical moments (like savings rate, wealth Gini, etc.). We then present simulation results comparing, for example, a scenario of an expansionary monetary policy shock in a representative agent model vs. in a heterogeneous agent model. The latter shows a distribution of consumption responses – with liquidity-constrained agents responding strongly and wealthy agents less so – and a different aggregate outcome. This helps illustrate quantitatively the importance of heterogeneity for policy multipliers.\nPolicy Case Studies Analysis: We provide additional data and charts complementing the empirical review. For instance, a figure showing the U.S. unemployment and inflation in the 1970s and 1980s to visualize the Phillips Curve breakdown and Volcker disinflation; a chart of asset holdings by wealth percentiles to indicate how QE gains might concentrate; and cross-country comparisons of inequality measures before and after major policy regimes (e.g., comparing inequality trends in countries that adopted inflation targeting vs those that had high inflation). These data support the claims made in the main text about policy impacts on inequality and stability."
  },
  {
    "objectID": "projects/dichotomy/dichotomy.html",
    "href": "projects/dichotomy/dichotomy.html",
    "title": "Identifying the Most Significant Dichotomy",
    "section": "",
    "text": "Wealth concentration and inequality have long been central issues in economics, attracting renewed attention as wealth disparities continue to widen. The well-documented Pareto Principle suggests that the top 20% holds approximately 80% of total wealth, but empirical studies indicate that this ratio has become even more skewed over time (Piketty and Saez 2003; Alvaredo et al. 2013; Saez and Zucman 2016). Such trends raise fundamental questions about the underlying dynamics of wealth distribution: Is wealth accumulation a zero-sum game? If so, what is the most evident dichotomy that best exposes this phenomenon?\nAt every moment, newly created capital is introduced into the economic system through production, subsequently distributed among economic agents, and then either dissipated through consumption or retained through investment. Unlike a static framework where total wealth is fixed, real-world economic systems operate as dynamic entities where total wealth evolves over time. The interplay between production, distribution, consumption, and investment determines how wealth is allocated and reallocated across different socioeconomic groups. If the system maintains a stable and proportional allocation of new wealth over time, the relative shares of net worth for different groups remain unchanged. However, if allocation mechanisms fluctuate, then the wealth share of one group necessarily increases at the expense of another—reinforcing a zero-sum dynamic.\nThe key question, therefore, is: How should we partition the population to most clearly expose this zero-sum characteristic? In other words, at which wealth cutoff percentile do we observe the highest absolute correlation between two complementary wealth groups? Should we divide the population into the top 10% versus the bottom 90%? Or is a more extreme division, such as top 0.1% versus bottom 99.9%, more effective in revealing these dynamics?"
  },
  {
    "objectID": "projects/dichotomy/dichotomy.html#introduction",
    "href": "projects/dichotomy/dichotomy.html#introduction",
    "title": "Identifying the Most Significant Dichotomy",
    "section": "",
    "text": "Wealth concentration and inequality have long been central issues in economics, attracting renewed attention as wealth disparities continue to widen. The well-documented Pareto Principle suggests that the top 20% holds approximately 80% of total wealth, but empirical studies indicate that this ratio has become even more skewed over time (Piketty and Saez 2003; Alvaredo et al. 2013; Saez and Zucman 2016). Such trends raise fundamental questions about the underlying dynamics of wealth distribution: Is wealth accumulation a zero-sum game? If so, what is the most evident dichotomy that best exposes this phenomenon?\nAt every moment, newly created capital is introduced into the economic system through production, subsequently distributed among economic agents, and then either dissipated through consumption or retained through investment. Unlike a static framework where total wealth is fixed, real-world economic systems operate as dynamic entities where total wealth evolves over time. The interplay between production, distribution, consumption, and investment determines how wealth is allocated and reallocated across different socioeconomic groups. If the system maintains a stable and proportional allocation of new wealth over time, the relative shares of net worth for different groups remain unchanged. However, if allocation mechanisms fluctuate, then the wealth share of one group necessarily increases at the expense of another—reinforcing a zero-sum dynamic.\nThe key question, therefore, is: How should we partition the population to most clearly expose this zero-sum characteristic? In other words, at which wealth cutoff percentile do we observe the highest absolute correlation between two complementary wealth groups? Should we divide the population into the top 10% versus the bottom 90%? Or is a more extreme division, such as top 0.1% versus bottom 99.9%, more effective in revealing these dynamics?"
  },
  {
    "objectID": "projects/dichotomy/dichotomy.html#data",
    "href": "projects/dichotomy/dichotomy.html#data",
    "title": "Identifying the Most Significant Dichotomy",
    "section": "Data",
    "text": "Data\nOur empirical analysis is based on FRED (Federal Reserve Economic Data), covering quarterly observations from 1989 to 2024. The dataset is structured into wealth brackets representing net wealth shares at different percentile levels:\nGroups (stars)\n\n\\(X_4(t)\\): Share of Net Worth Held by the Top 0.1% (99.9th to 100th Wealth Percentiles) (WFRBSTP1300)\n\\(X_3(t)\\): Share of Net Worth Held by the 99th to 99.9th Wealth Percentiles (WFRBS99T999273)\n\nc.f. Share of Net Worth Held by the Top 1% (99th to 100th Wealth Percentiles) (WFRBST01134)\n\n\\(X_2(t)\\): Share of Net Worth Held by the 90th to 99th Wealth Percentiles (WFRBSN09161)\n\\(X_1(t)\\): Share of Net Worth Held by the 50th to 90th Wealth Percentiles (WFRBSN40188)\n\\(X_0(t)\\): Share of Net Worth Held by the Bottom 50% (1st to 50th Wealth Percentiles) (WFRBSB50215)\n\nAdditionally, we reference wealth cutoff amount to identify the minimum level of wealth required to belong to specific top percentile groups:\nCutoff Levels (bins)\n\n\\(p_4\\) or 99.9th: Minimum Wealth Cutoff for the Top 0.1% (99.9th to 100th Wealth Percentiles) (WFRBLTP1311)\n\\(p_3\\) or 99th: Minimum Wealth Cutoff for the 99th to 99.9th Wealth Percentiles (WFRBL99T999309)\n\\(p_2\\) or 90th: Minimum Wealth Cutoff for the 90th to 99th Wealth Percentiles (WFRBLN09304)\n\\(p_1\\) or 50th: Minimum Wealth Cutoff for the 50th to 90th Wealth Percentiles (WFRBLN40302)"
  },
  {
    "objectID": "projects/dichotomy/dichotomy.html#methodology",
    "href": "projects/dichotomy/dichotomy.html#methodology",
    "title": "Identifying the Most Significant Dichotomy",
    "section": "Methodology",
    "text": "Methodology\nGiven that total share of net wealth must always sum to one, any partition of the population into two groups remains complementary: \\[\nX_0 + X_1 + X_2 + X_3 + X_4 = 1.\n\\]\nTo quantify the most evident dichotomy, we define two complementary wealth groups for different percentile cutoffs:\n\nWhen \\(p = p_1\\):\n\\[\n  a(t) = X_4(t)+X_3(t)+X_2(t)+X_1(t), \\quad b(t) = X_0(t).\n\\]\nWhen \\(p = p_2\\):\n\\[\n  a(t) = X_4(t)+X_3(t)+X_2(t), \\quad b(t) = X_1(t)+X_0(t).\n\\]\nWhen \\(p = p_3\\):\n\\[\n  a(t) = X_4(t)+X_3(t), \\quad b(t) = X_2(t)+X_1(t)+X_0(t).\n\\]\nWhen \\(p = p_4\\):\n\\[\n  a(t) = X_4(t), \\quad b(t) = X_3(t)+X_2(t)+X_1(t)+X_0(t).\n\\]\n\nFor each cutoff \\(p\\), we compute the correlation:\n\\[\n   y(p) = \\mathrm{corr}\\bigl(a(t),\\,b(t)\\bigr).\n\\]\nWe seek the wealth cutoff \\(p\\) that maximizes the absolute correlation \\(|y(p)|\\), revealing the strongest inverse relationship between the two resulting wealth groups. A high absolute correlation suggests that fluctuations in one group’s net worth share are systematically mirrored by the other, reinforcing the zero-sum nature of wealth accumulation. This dichotomy provides insight into how different capital accumulation mechanisms—through labor or capital investment—shape long-term wealth distribution.\nStrong inverse correlations at certain percentiles may indicate critical thresholds where redistribution policies—such as capital taxation or inheritance taxation—could have the most pronounced effects (Piketty 2011; Piketty, Saez, and Stantcheva 2014; Saez and Zucman 2016)."
  },
  {
    "objectID": "projects/dichotomy/dichotomy.html#empirical-results",
    "href": "projects/dichotomy/dichotomy.html#empirical-results",
    "title": "Identifying the Most Significant Dichotomy",
    "section": "Empirical Results",
    "text": "Empirical Results\nAfter processing the quarterly dataset (excluding missing values), we find that the 90th percentile cutoff (\\(p_2\\)) exhibits the highest absolute correlation between the two complementary wealth groups. Specifically, as of 2022-07-01, the minimum wealth required to be in the top 10% was approximately $2,152,788.\nThis suggests that dividing the population into top 10% vs. bottom 90% most effectively reveals the zero-sum nature of wealth redistribution, compared to other partitions such as top 50% vs. bottom 50% or top 0.1% vs. bottom 99.9%.\nThese findings imply that the most structurally significant wealth division occurs between the top 10% and the rest, rather than between the ultra-rich and lower percentiles. This observation aligns with broader discussions on wealth polarization, where the top 10% increasingly dominates capital ownership while the bottom 90% exhibits a more recessive trajectory."
  },
  {
    "objectID": "projects/dichotomy/dichotomy.html#conclusions",
    "href": "projects/dichotomy/dichotomy.html#conclusions",
    "title": "Identifying the Most Significant Dichotomy",
    "section": "Conclusions",
    "text": "Conclusions\nThis study demonstrates that partitioning the population at the 90th wealth percentile provides the most evident dichotomy in revealing the zero-sum nature of capital accumulation. Over time, wealth redistribution mechanisms result in strong inverse correlations between net worth shares of different groups, underscoring the struggling aspects of capital accumulation. The analysis suggests that the structural division between the top 10% and the bottom 90% is more significant than commonly assumed top 1% vs. bottom 99% splits, reinforcing the notion that wealth concentration extends beyond the ultra-rich and affects broader socioeconomic strata.\nThese findings hold important implications for public policy, particularly in debates surrounding progressive taxation, capital gains policies, and inheritance tax structures. A strong inverse correlation at the 90th percentile threshold suggests that redistributive policies targeted at this level could have significant implications for long-term wealth dynamics. This aligns with prior research emphasizing the role of tax policy in shaping wealth accumulation patterns and mitigating excessive concentration of economic power (Piketty, Saez, and Stantcheva 2014; Saez and Zucman 2016).\nWhile this study primarily focuses on empirical correlation analysis, future research should explore additional macroeconomic variables to refine our understanding of wealth distribution dynamics. Incorporating GDP growth rates, investment patterns, labor market structures, and monetary policy changes may provide further insights into how systemic wealth flows evolve in response to economic shocks and policy interventions. Additionally, extending the dataset to include international comparisons could offer a broader perspective on whether the 90th percentile threshold serves as a critical inflection point for wealth inequality across different economies. Further research integrating both empirical and theoretical approaches will be essential in developing more effective strategies for addressing wealth concentration and economic mobility.\n\n\nCode\nimport pandas as pd\nimport pandas_datareader.data as web\nimport matplotlib.pyplot as plt\n\n# 데이터 기간 설정\nstart_date = '1989-07-01'\nend_date = '2024-07-01'\n\n# FRED 데이터 가져오기\nseries_ids = {\n    'X_4': 'WFRBSTP1300',\n    'X_3': 'WFRBS99T999273',\n    'X_2': 'WFRBSN09161',\n    'X_1': 'WFRBSN40188',\n    'X_0': 'WFRBSB50215'\n}\n\ndata = pd.DataFrame()\nfor name, series_id in series_ids.items():\n    data[name] = web.DataReader(series_id, 'fred', start_date, end_date)\n\n# 결측값 제거\ndata.dropna(inplace=True)\n\n\n# FRED에서 wealth level 데이터 가져오기\nwealth_level_ids = {\n    1: 'WFRBLN40302',\n    2: 'WFRBLN09304',\n    3: 'WFRBL99T999309',\n    4: 'WFRBLTP1311'\n}\n\nwealth_levels = {}\nfor p, series_id in wealth_level_ids.items():\n    latest_data = web.DataReader(series_id, 'fred', start_date, end_date).dropna().iloc[-1]\n    wealth_levels[p] = (latest_data.name, latest_data.iloc[0])  # 날짜와 값을 함께 저장\n    \n# 총 관측치 수 출력\nprint(f\"Total number of observations after removing NaN values: {len(data)}\")\n\n\nTotal number of observations after removing NaN values: 141\n\n\n\n\nCode\n# 상관관계 계산 함수\ndef calculate_correlation(a, b):\n    return a.corr(b)\n\n# 상관관계 계산\ncorrelations = {\n    1: calculate_correlation(data['X_4'] + data['X_3'] + data['X_2'] + data['X_1'], data['X_0']),\n    2: calculate_correlation(data['X_4'] + data['X_3'] + data['X_2'], data['X_1'] + data['X_0']),\n    3: calculate_correlation(data['X_4'] + data['X_3'], data['X_2'] + data['X_1'] + data['X_0']),\n    4: calculate_correlation(data['X_4'], data['X_3'] + data['X_2'] + data['X_1'] + data['X_0'])\n}\n\n# 결과 출력\nfor p, y in correlations.items():\n    print(f\"Correlation for p_{p}: {y:.4f}\")\n\n# 최소 상관관계를 가지는 wealth percentile 찾기\nmin_corr_p = min(correlations, key=correlations.get)\npercentile_map = {1: \"50th\", 2: \"90th\", 3: \"99th\", 4: \"99.9th\"}\nprint(f\"Minimum correlation is at p_{min_corr_p}: {percentile_map[min_corr_p]}\")\nwealth_date, wealth_level = wealth_levels[min_corr_p]\nprint(f\"Wealth level for {percentile_map[min_corr_p]} percentile on {wealth_date.date()}: ${wealth_level:,.0f}\")\n\n\n# 그래프 표현\np_values = list(correlations.keys())\ny_values = list(correlations.values())\n\nplt.plot(p_values, y_values, marker='o')\nplt.xlabel('Wealth Cutoff (p)')\nplt.ylabel('Correlation (y(p))')\nplt.title('Wealth Cutoff vs Correlation')\nplt.grid(True)\nplt.show()\n\n\nCorrelation for p_1: -0.9980\nCorrelation for p_2: -0.9998\nCorrelation for p_3: -0.9996\nCorrelation for p_4: -0.9987\nMinimum correlation is at p_2: 90th\nWealth level for 90th percentile on 2022-07-01: $2,152,788"
  },
  {
    "objectID": "projects/corner_solution/corner_solution.html",
    "href": "projects/corner_solution/corner_solution.html",
    "title": "Corner Solutions in Optimization Model",
    "section": "",
    "text": "1. Introduction\nIn standard economic theory, both consumer preferences and production sets are generally assumed to exhibit convexity (Arrow and Debreu 1954; Debreu 1959). This assumption supports foundational results, including the existence and uniqueness of equilibrium and the efficiency of market allocations. In practice, however, features such as network externalities (Katz and Shapiro 1985; Rochet and Tirole 2003), rent-seeking (Shleifer and Vishny 1993), and multiple equilibria—often culminating in pronounced market dominance—can produce outcomes resembling non-convex preferences (Arthur 1994). In many cases, corner solutions and path-dependent equilibria emerge from winner-takes-all dynamics, concentrated economic power, and barriers to entry.\n\n\n2. Convexity in Economic Theory\n2.1 Convex Preferences and Production Sets\n\nConsumer preferences are typically modeled with quasi-concave utility functions, yielding convex (or “bowl-shaped”) indifference curves. This setup implies a preference for diversity in consumption, rather than extreme or corner solutions (Debreu 1959).\nProducers are often assumed to face diminishing marginal returns, reflected in a convex production possibility set. Under such conditions, output expansions follow a predictable pattern, and average costs rise eventually.\n\n2.2 Existence and Efficiency of Equilibrium\n\nWith convexity, free market entry, symmetric information, and price-taking behavior, perfectly competitive markets are shown to possess a stable equilibrium that is Pareto efficient (Arrow and Debreu 1954).\nThese results typically rely on fixed-point theorems and the properties of convex sets, ensuring both the existence of equilibrium prices and (in many cases) uniqueness or stability (Debreu 1959).\n\n2.3 Normative Implications\n\nConvexity underpins the normative stance that, absent significant market failures, competitive markets gravitate toward Pareto-efficient resource allocations.\nConsequently, government interventions usually aim to correct externalities, public goods issues, or information asymmetries within a broader context of largely convex preferences and production sets.\n\n\n\n3. Non-Convexities in Reality\n3.1 Network Externalities and Increasing Returns to Scale\n\nIn contrast to diminishing returns, many digital or platform-based markets exhibit network externalities, or increasing returns to scale (Katz and Shapiro 1985; Rochet and Tirole 2003). As additional users join a platform, its value to each user grows, often driving corner solutions in both production and consumption.\nInstead of smoothly concave utility or production functions, certain markets feature segments of increasing marginal returns, leading to “winner-takes-all” or “winner-take-most” dynamics.\n\n3.2 Coordination Games and Multiple Equilibria\n\nNetwork externalities commonly create coordination games, where each agent’s optimal choice depends on the choices of others. Small initial advantages or random shocks may tip the market toward a specific product or standard, resulting in lock-in (Arthur 1994).\nSuch scenarios can produce multiple Nash equilibria, for instance everyone choosing Product A or everyone choosing Product B, with potentially large welfare differences between them.\n\n3.3 Extreme or Corner Solutions in Consumption and Production\n\nWith robust network effects, consumers or producers may converge on a single brand, platform, or location, effectively marginalizing other options—even if those alternatives might have been preferred under purely convex preferences.\nThese corner solutions deviate from the classical idea that diversification in consumption and moderate scales in production yield optimal outcomes.\n\n3.4 Rent-Seeking and Incumbent Power\n\nDominant firms or groups can exploit political influence—through lobbying or regulatory capture—to fortify their positions, reinforcing non-convex outcomes by stifling competition (Tirole 1988; Shleifer and Vishny 1993).\nRent-seeking intensifies the misallocation of resources, as efforts are diverted to defending or reinforcing incumbents’ power, often via barriers to entry, reduced competition, and growing inequalities.\n\n\n\n4. Government Interventions\n4.1 Theoretical View: Correcting Market Imperfections\n\nTraditionally, policy interventions focus on addressing market failures, assuming that preferences and technologies remain fundamentally convex and that interventions are limited and transparent.\n\n4.2 Empirical Evidence: Policy Amplifies Non-Convexities\n\nIn reality, incumbents can wield outsized influence through lobbying and political capture, prompting policies that strengthen market concentration (Tirole 1988).\nInstead of fostering genuinely competitive markets, such policies may lock in non-convex outcomes, creating a vicious cycle of entrenched monopolistic power and limited competition.\n\n4.3 Lock-in and Path Dependence\n\nWhen policy-making aligns with incumbent interests, even minor advantages can become self-reinforcing (Arthur 1994).\nConsequently, once a market tips toward a specific firm, region, or product, effective competition may prove infeasible without sweeping policy reforms or disruptive innovation.\n\n\n\n5. Conclusion\nAlthough classical economic models lean on convex preferences and technologies to assert the existence of unique, efficient equilibria, real-world dynamics often revolve around non-convex phenomena. Network externalities, coordination failures, and rent-seeking can drive corner solutions, multiple equilibria, and lock-in that preserve incumbent advantages. Far from mitigating these issues, government policies sometimes exacerbate them through preferential treatment of dominant actors. Recognizing these non-convex realities is crucial for crafting policy frameworks that transcend purely theoretical assumptions of convexity and address the path-dependent complexity characterizing modern markets.\n\n\nAppendix: Utilitarian Objective function\n\n\nCode\n#@title Utilitarian objective function\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom scipy.optimize import minimize\n\n# 함수 정의\ndef z_function(x, y, a, b):\n  return y * (x**a) + (1 - y) * ((b - x)**a)\n\n# x, y 범위 및 매개변수 설정\na = 0.3  # 매개변수 a 값 (0과 1 사이)\nb = 20  # 매개변수 b 값\n\nx = np.linspace(0, b, 100)  # x 범위: 0부터 20까지 100개의 점\ny = np.linspace(0, 1, 100)  # y 범위: 0부터 1까지 100개의 점\nX, Y = np.meshgrid(x, y)  # x, y 좌표 격자 생성\n\n\n# Z 값 계산\nZ = z_function(X, Y, a, b)\n\n\ndef negative_z_function(params):\n    x, y = params\n    return -z_function(x, y, a, b)  # 최솟값을 찾기 위해 음수 값 반환\n\n# 초기값 설정 (interior 범위 내)\ninitial_guess = [b / 2, 0.5]\n\n# 경계 조건 설정\nbounds = [(0, b), (0, 1)]\n\n# 최적화 실행\nresult = minimize(negative_z_function, initial_guess, bounds=bounds)\n\n# 결과 추출\nextreme_point_x, extreme_point_y = result.x\nextreme_point_z = z_function(extreme_point_x, extreme_point_y, a, b)\n\nprint(\"Extreme Point (x, y, z):\", extreme_point_x, extreme_point_y, extreme_point_z)\n\n# Calculate Hessian matrix\ndef hessian_matrix(x, y, a, b):\n  \"\"\"Calculates the Hessian matrix of the z_function.\"\"\"\n  d2z_dx2 = a * (a - 1) * (y * (x**(a - 2)) + (1 - y) * ((b - x)**(a - 2)))\n  d2z_dy2 = 0  # Second derivative with respect to y is 0\n  d2z_dxdy = a * (x**(a - 1) - (b - x)**(a - 1))\n  d2z_dydx = d2z_dxdy  # Mixed partial derivatives are equal\n\n  return [[d2z_dx2, d2z_dxdy], [d2z_dydx, d2z_dy2]]\n\n# Determine the type of extreme point\nhessian = hessian_matrix(extreme_point_x, extreme_point_y, a, b)\ndeterminant = np.linalg.det(hessian)\n\nif determinant &gt; 0 and hessian[0][0] &gt; 0:\n  extreme_type = \"Minimum\"\nelif determinant &gt; 0 and hessian[0][0] &lt; 0:\n  extreme_type = \"Maximum\"\nelse:\n  extreme_type = \"Saddle\"\n\nprint(\"Extreme Point Type:\", extreme_type)\n\n\n# 3D 그래프 그리기\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(X, Y, Z)\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('z')\nplt.title('3D Graph of z = y*x^a + (1-y)(b-x)^a')\n\n\n# global interior extreme point 표시\nax.scatter(extreme_point_x, extreme_point_y, extreme_point_z, color='red', marker='o', s=100)\nax.text(extreme_point_x, extreme_point_y, extreme_point_z, f'Extreme Point ({extreme_type})', color='red')\n\nplt.show()\n\n# Contour Plot 그리기\nfig, ax = plt.subplots()\ncontour = ax.contour(X, Y, Z)\nax.set_xlabel('x')\nax.set_ylabel('y')\nplt.title('Contour Plot of z = y*x^a + (1-y)(b-x)^a')\nplt.clabel(contour, inline=1, fontsize=10)\n\n# global interior extreme point 표시\nax.scatter(extreme_point_x, extreme_point_y, color='red', marker='o', s=100)\nax.text(extreme_point_x, extreme_point_y, 'Extreme Point', color='red')\n\nplt.show()\n\n\nExtreme Point (x, y, z): 10.0 0.5 1.9952623149688795\nExtreme Point Type: Saddle\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAppendix: Homogeneous function of degree 1\n\n\nCode\n# Define a return to scale\nscale = 1 # Constant return to scale, i.e. Homogeneous function of degree 1\n\n# Define parameter a\na = 1/4\n\n# total wealth of x\nk_x = 2\n# total wealth of y\nk_y = 2\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 경고 메시지 숨기기\nnp.seterr(invalid='ignore')\n\ndef numerical_derivative(f, X, Y, h=1e-5):\n    \"\"\" Compute numerical partial derivatives using central difference method.\"\"\"\n    dfdx = (f(X + h, Y) - f(X - h, Y)) / (2 * h)  # ∂f/∂x\n    dfdy = (f(X, Y + h) - f(X, Y - h)) / (2 * h)  # ∂f/∂y\n    return dfdx, dfdy\n\n# Define functions u_1(x,y) = x^a * y^(1-a) and u_2(x,y) = (2-x)(2-y)\ndef u1(x, y):\n    return x**(scale*a) * y**(scale*(1-a))\n\ndef u2(x, y):\n    return (k_x - x)**(scale*a) * (k_y - y)**(scale*(1-a))\n\n# Define the grid\nx = np.linspace(0, k_x, 15)\ny = np.linspace(0, k_y, 15)\nX, Y = np.meshgrid(x, y)\n\n# Compute the numerical derivatives (vector field components)\nU1, V1 = numerical_derivative(u1, X, Y)\nU2, V2 = numerical_derivative(u2, X, Y)\n\n# Reduce the density of vectors for better visualization\nx_sparse = np.linspace(0, k_x, 8)\ny_sparse = np.linspace(0, k_y, 8)\nX_sparse, Y_sparse = np.meshgrid(x_sparse, y_sparse)\nU1_sparse, V1_sparse = numerical_derivative(u1, X_sparse, Y_sparse)\nU2_sparse, V2_sparse = numerical_derivative(u2, X_sparse, Y_sparse)\n\n# Plot the combined vector fields and contour plots\n#plt.figure(figsize=(8, 8))\n\n# Contour plots of u_1 and u_2 (level curves only)\ncontour1 = plt.contour(X, Y, u1(X, Y), colors='blue', linestyles='solid', linewidths=1)\ncontour2 = plt.contour(X, Y, u2(X, Y), colors='red', linestyles='dashed', linewidths=1)\n\n# Overlay vector fields\nplt.quiver(X_sparse, Y_sparse, U1_sparse, V1_sparse, color='b', angles='xy', label='∇$u_1$')\nplt.quiver(X_sparse, Y_sparse, U2_sparse, V2_sparse, color='r', angles='xy', label='∇$u_2$')\n\n# Labels and grid\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Gradient Vector Fields & Level Curves of $u_1$ and $u_2$')\nplt.legend()\nplt.grid('scaled')\nplt.axis('square')\n\nplt.tight_layout()\n# Show the plot\nplt.show()\n\n# Compute the sum of gradients\nU_sum = U1 + U2\nV_sum = V1 + V2\n\n# Reduce the density of vectors for better visualization\nU_sum_sparse, V_sum_sparse = numerical_derivative(lambda x, y: u1(x, y) + u2(x, y), X_sparse, Y_sparse)\n\n# Plot the combined vector fields and contour plots\n#plt.figure(figsize=(8, 8))\n\n# Contour plots of u_1 and u_2 (level curves only)\ncontour1 = plt.contour(X, Y, u1(X, Y), colors='blue', linestyles='solid', linewidths=1)\ncontour2 = plt.contour(X, Y, u2(X, Y), colors='red', linestyles='dashed', linewidths=1)\n\n# Overlay sum of gradient vector fields\nplt.quiver(X_sparse, Y_sparse, U_sum_sparse, V_sum_sparse, color='g', angles='xy', label='∇($u_1 + u_2$)')\n\n# Labels and grid\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Sum of Gradient Vector Fields & Level Curves of $u_1$ and $u_2$')\nplt.legend()\n\nplt.grid('scaled')\nplt.axis('square')\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAppendix: Sigmoid utility function\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Define constants\nkx = (np.pi**3 / 2) ** (1/3)\nky = (2**(1/2)) * ((np.pi**3 / 2) ** (1/3))\n\n# Define the grid\nx = np.linspace(0, kx, 1000)\ny = np.linspace(0, ky, 1000)\nX, Y = np.meshgrid(x, y)\n\n# Define the functions\nu1 = 1 - np.cos(X**(1/3) * Y**(2/3))\nu2 = 1 - np.cos((kx - X)**(1/3) * (ky - Y)**(2/3))\n\n# Find intersection points where u1 == u2\nthreshold = 1e-3  # Numerical tolerance for equality\nintersection_mask = np.abs(u1 - u2) &lt; threshold\nX_intersect = X[intersection_mask]\nY_intersect = Y[intersection_mask]\nZ_intersect = u1[intersection_mask]  # u1 and u2 are nearly equal\n\n# Create 3D plot\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection='3d')\n\n# Plot intersection line\nax.scatter(X_intersect, Y_intersect, Z_intersect, color='black', s=10, label='Intersection Line')\n\n# Surface plots for reference\nax.plot_surface(X, Y, u1, cmap='Blues', alpha=0.5)\nax.plot_surface(X, Y, u2, cmap='Reds', alpha=0.5)\n\n# Labels and title\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\nax.set_title('3D Intersection of $u_1$ and $u_2$')\nax.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nArrow, Kenneth J., and Gerard Debreu. 1954. “Existence of an Equilibrium for a Competitive Economy.” Econometrica 22 (3): 265–90.\n\n\nArthur, W. Brian. 1994. Increasing Returns and Path Dependence in the Economy. Ann Arbor, MI: University of Michigan Press.\n\n\nDebreu, Gerard. 1959. Theory of Value: An Axiomatic Analysis of Economic Equilibrium. New Haven, CT: Yale University Press.\n\n\nKatz, Michael L., and Carl Shapiro. 1985. “Network Externalities, Competition, and Compatibility.” The American Economic Review 75 (3): 424–40.\n\n\nRochet, Jean-Charles, and Jean Tirole. 2003. “Platform Competition in Two-Sided Markets.” Journal of the European Economic Association 1 (4): 990–1029.\n\n\nShleifer, Andrei, and Robert W. Vishny. 1993. “Corruption.” The Quarterly Journal of Economics 108 (3): 599–617.\n\n\nTirole, Jean. 1988. The Theory of Industrial Organization. Cambridge, MA: MIT Press."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GitSAM",
    "section": "",
    "text": "비판:상속세 제도 개편\n내가 관심있는 것들…왜? 1."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "GitSAM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n그냥요.↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "깃샘학원\n“We Build Leaders, Not Just Winners.”\n\n깃샘학원의 목표는 성공한 엘리트 양성이 아니다: 단순한 시험 점수나 경제적 성공을 목표로 하지 않습니다.\n깃샘학원의 목표는 훌륭한 리더 양성이다: 공감력, 윤리적 책임감, 논리적 사고력, 실질적 문제 해결 능력을 갖추어 사회적으로 선한 영향력을 발휘할 수 있는 진정한 리더 양성이 목표이다.\n\n\n\n문제: 고위직 엘리트들의 항진명제식 발언 분석\n\n1. 서론\n고위직 엘리트들은 정치, 법조, 경제, 경영 등 다양한 분야에서 영향력을 행사하며, 공적인 자리에서의 발언이 중요한 의미를 갖는다. 그러나 이들의 발언 중 상당수는 논리적으로 항진명제(tautology)에 가까우며, 이는 정보 비대칭을 활용한 책임 회피 전략으로 사용되는 경우가 많다. 여기 분석에서는 몇가지 대표적 사례들을 분석하고, 이러한 발언 방식을 만들어 내는 구조적 원인을 탐구하며, 개선 방안을 제안하고자 한다.\n\n\n2. 사례 분석\n\n법조 분야\n\n“최근 고위 공직자 비리 의혹 수사에 대한 검찰의 입장은?”\n\n검찰총장: “모든 수사는 법과 절차에 따라 공정하게 진행됩니다.”\n논리적 문제점: 법과 절차에 따른다는 것은 당연한 원칙이며, 구체적인 수사 진행 상황을 설명하지 않음.\n개선된 답변: “해당 의혹과 관련해 △△ 증거를 확보했으며, 3월 말까지 관계자 소환 조사를 진행할 예정입니다. 또한, 수사 결과는 국민에게 투명하게 공개할 것입니다.”\n\n\n\n\n경제 분야\n\n“금리 인상 가능성에 대해 어떻게 전망하나요?”\n\n한국은행 총재: “통화 정책은 물가 안정을 우선시할 것입니다.”\n논리적 문제점: 물가 안정을 목표로 한다는 것은 이미 알려진 사실로, 구체적 조건이 제시되지 않음.\n개선된 답변: “실업률은 5% 미만으로 유지되지만 연내 물가 상승률이 3%를 넘을 경우, 4분기 중 금리를 0.25%p 추가 인상할 계획입니다. 기준 금리 인상을 통해 물가의 안정성을 확보하고자 합니다.”\n\n\n\n\n정치 분야\n\n“차기 총선에서 여당의 승리 가능성은 어떻게 보는가?”\n\n여당 대변인: “선거 결과는 유권자의 선택에 달려 있습니다.”\n논리적 문제점: 선거 결과가 유권자의 선택에 따른다는 것은 자명한 사실이며, 선거 전략이나 예측을 제공하지 않음.\n개선된 답변: “현 지지율(45%)과 △△ 정책 공약을 바탕으로 20석 이상 확대를 목표로 합니다.”\n\n\n\n\n경영 분야\n\n“C사 제품의 안전성 논란에 대한 대응은?”\n\nC사 홍보팀장: “고객 안전을 최우선으로 삼고 있습니다.”\n논리적 문제점: 고객 안전을 최우선으로 한다는 것은 원칙적인 선언일 뿐, 실제 조치에 대한 설명이 없음.\n개선된 답변: “문제 제품 5만 개 전량 리콜하며, 피해 보상금을 제품 가격의 120%로 책정했습니다. 또한, 향후 안전성 강화를 위한 추가 조치를 마련할 것입니다.”\n\n\n\n\n\n3. 발언 방식 분석 및 원인\n\n(1) 정보 비대칭 활용 극대화\n고위직 인사들은 상대적으로 많은 정보를 보유하고 있으나, 이를 공개하지 않음으로써 해석의 여지를 남긴다. 이는 논란을 회피하고 의도적으로 불확실성을 유지하는 전략이다.\n\n\n(2) 항진명제 활용\n애매모호한 표현을 사용하여 논란을 피하는 동시에, 어느 쪽으로든 해석이 가능하도록 발언함으로써 책임을 최소화한다.\n\n\n(3) 자격 논란과 인신공격 전략\n논리적으로 반박하기 어려운 경우, 상대방의 자격을 문제 삼아 논의 자체를 흐리는 방식이 자주 활용된다.\n\n\n(4) 사회적·제도적 보상 구조\n책임을 명확히 지는 것보다 애매한 발언을 통해 책임을 회피하는 것이 생존 전략으로 작용하는 구조가 자리 잡고 있다. 이러한 환경에서는 모호한 표현이 오히려 유리하게 작용할 가능성이 크다.\n\n\n\n4. 해결 방안\n\n(1) 발언의 책임 명확화\n\n공적인 자리에서의 발언이 결과에 미치는 영향을 추적하는 피드백 시스템을 도입하여, 모호한 발언이 아니라 구체적인 답변을 유도해야 한다.\n“중대성이 낮다”고 발언한 경우, 이후 정책·법적 판단에서 이에 대한 책임을 질 수 있도록 제도적 장치를 마련해야 한다.\n\n\n\n(2) 논의 구조 개선\n\n논리적 토론을 강화하고, 인신공격을 차단하는 규칙을 도입해야 한다.\n예를 들어, 토론 중 상대의 과거나 자격을 논하는 발언이 나오면 자동으로 발언권을 제한하는 시스템을 고려할 수 있다.\n\n\n\n(3) 교육 및 사고방식 변화\n\n기계식 시험 중심이 아닌, 논리적 사고 및 창의적 문제 해결을 평가하는 교육 방식으로 변화해야 한다.\n특히, AI와 블록체인을 활용한 교육 평가 시스템을 통해 논리적 사고 과정을 명확하게 평가하는 방법이 필요하다.\n\n\n\n\n5. 결론\n고위직 엘리트들의 항진명제식 발언은 정보 비대칭을 활용한 책임 회피 전략의 일부로 작용하며, 이는 사회적·제도적 보상 구조와 결합하여 지속적으로 강화된다. 이러한 문제를 해결하기 위해서는 발언의 책임을 명확히 하고, 논의 구조를 개선하며, 교육 시스템의 변화를 통해 논리적 사고력을 강화하는 접근이 필요하다. 특히, AI와 블록체인을 활용한 평가 방식은 발언의 모호성을 줄이고 구체적인 논리적 사고를 촉진하는 데 기여할 수 있을 것이다.\n\n\n\n\n해결책: 사고력과 리더십을 키우는 혁신적 교육 프로그램\n\n혁신적 교육 및 평가 시스템 (AI & 블록체인 기반)\n\nPrompt-based Evaluation: AI를 활용하여 학생의 사고 과정 자체를 블록체인에 기록하고, 창의성·논리성·윤리적 책임감까지 정량적·객관적으로 평가\nProof of Thinking: 학습 과정을 블록체인에 저장하여 자기성찰 능력을 강화하고, 위변조 방지\n토론·윤리적 판단·전략적 사고 중심 교육: 동료와의 토론을 통해 사고력과 문제 해결력을 강화\n\n\n\n혁신적 강의 방식\n\n개념 시각화: python, desmos 등 기술을 활용하여 수학과 과학의 추상적 개념을 구체적으로 시각화\n실전 응용 모형 중심: 실전에 활용되고 있는 과학 모형들을 구체적으로 이해하고 적용하는 방식\n\n\n\n자연과학 모형 이해\n\n대수학, 기하학, 미적분학, 확률통계학\n고전역학, 통계물리학, 상대성이론, 양자역학, 유기화학, 분자생물학\n국제 경시대회(AMC 등) 및 입시 대비 문제풀이\nPython을 활용한 데이터 계산 및 시각화\n\n\n\nAI 및 기술 도구 이해와 활용법\n\nAI 활용 리서치 및 정보 검색 방법\n금융·투자 전략을 위한 선형대수학 (Google Colab 활용)\n창업·재테크를 위한 통계학 (Google Sheets 활용)\nLLM(대규모 언어 모델) 개발을 위한 확률 최적화 이론 및 미적분 해석학"
  },
  {
    "objectID": "projects/asset_puzzle/asset_puzzle.html",
    "href": "projects/asset_puzzle/asset_puzzle.html",
    "title": "Asset Premium Puzzles",
    "section": "",
    "text": "Since the 1980s, financial economists have grappled with two so-called “puzzles” in asset pricing theory: the Equity Premium Puzzle (EPP) and the Risk-Free Rate Puzzle (RFRP). These puzzles refer to the pronounced gap between theoretical predictions derived from standard rational-expectations models with representative-agent utility maximization and the empirical observations on asset returns and risk-free rates. In particular, Mehra and Prescott (1985) and Weil (1989) spotlighted the disparity between observed excess returns on equities and the relatively tame variability of aggregate consumption, labeling the phenomenon a “puzzle” under the canonical equilibrium framework.\nBeneath both puzzles lies a single “master equation,” namely the Euler equation, which in its most general form states:\n\\[\n\\mathbb{E}_t \\big[m_t R_{t+1}\\big] = 1,\n\\]\nwhere \\(m_t\\) (the Stochastic Discount Factor) is closely tied to the marginal utility of consumption, and \\(R_{t+1}\\) represents the return on various assets. The equity premium and risk-free rate each follow from this broad formulation, yet empirical magnitudes diverge significantly from conventional theoretical predictions."
  },
  {
    "objectID": "projects/asset_puzzle/asset_puzzle.html#introduction",
    "href": "projects/asset_puzzle/asset_puzzle.html#introduction",
    "title": "Asset Premium Puzzles",
    "section": "",
    "text": "Since the 1980s, financial economists have grappled with two so-called “puzzles” in asset pricing theory: the Equity Premium Puzzle (EPP) and the Risk-Free Rate Puzzle (RFRP). These puzzles refer to the pronounced gap between theoretical predictions derived from standard rational-expectations models with representative-agent utility maximization and the empirical observations on asset returns and risk-free rates. In particular, Mehra and Prescott (1985) and Weil (1989) spotlighted the disparity between observed excess returns on equities and the relatively tame variability of aggregate consumption, labeling the phenomenon a “puzzle” under the canonical equilibrium framework.\nBeneath both puzzles lies a single “master equation,” namely the Euler equation, which in its most general form states:\n\\[\n\\mathbb{E}_t \\big[m_t R_{t+1}\\big] = 1,\n\\]\nwhere \\(m_t\\) (the Stochastic Discount Factor) is closely tied to the marginal utility of consumption, and \\(R_{t+1}\\) represents the return on various assets. The equity premium and risk-free rate each follow from this broad formulation, yet empirical magnitudes diverge significantly from conventional theoretical predictions."
  },
  {
    "objectID": "projects/asset_puzzle/asset_puzzle.html#theoretical-framework",
    "href": "projects/asset_puzzle/asset_puzzle.html#theoretical-framework",
    "title": "Asset Premium Puzzles",
    "section": "Theoretical Framework",
    "text": "Theoretical Framework\n\nEquity Premium Puzzle\nThe seminal work of Lucas (1978) and later Mehra and Prescott (1985) established the Equity Premium Puzzle within a representative-agent, general-equilibrium asset pricing model. The typical consumer (or representative agent) maximizes expected utility over time, commonly assumed to be CRRA (Constant Relative Risk Aversion):\n\\[\nU(C_t) = \\frac{C_t^{1-\\gamma}}{1-\\gamma}, \\quad \\gamma &gt; 0, \\,\\gamma \\neq 1,\n\\]\nwhere \\(\\gamma\\) is the coefficient of relative risk aversion (RRA). From the consumer’s intertemporal optimization, an Euler equation emerges:\n\\[\n\\mathbb{E}_t \\Big[\\beta \\Big(\\frac{C_{t+1}}{C_t}\\Big)^{-\\gamma} \\big(R_{e,t+1} - R_{f,t+1}\\big)\\Big] = 0,\n\\]\nwhere:\n\n\\(\\beta\\): subjective time discount factor (\\(0 &lt; \\beta &lt; 1\\)),\n\n\\(C_{t+1} / C_t\\): consumption growth rate,\n\n\\(R_{e,t+1}\\): equity return,\n\n\\(R_{f,t+1}\\): risk-free asset return.\n\nUnder some approximations (e.g., log-linearization of consumption growth), Mehra and Prescott (1985) famously derived:\n\\[\n\\mathbb{E}[R_e - R_f] \\approx \\gamma \\,\\sigma_c^2,\n\\]\nwhere \\(\\sigma_c^2\\) is the variance of consumption growth. If the variance of consumption growth is small (on the order of 1%–2% annually), the observed 6%–8% annual equity premium can only be reconciled by positing implausibly high risk aversion coefficients (often above 20). Since typical estimates of \\(\\gamma\\) in microeconomic or macroeconomic studies hover around 1–5, this gap forms the crux of the EPP.\n\n\nRisk-Free Rate Puzzle\nA closely related conundrum is the Risk-Free Rate Puzzle, initially highlighted by Weil (1989). Under the same CRRA framework and rational expectations, the Euler equation for the risk-free asset implies:\n\\[\n1 = \\mathbb{E}_t \\Big[\\beta \\Big(\\frac{C_{t+1}}{C_t}\\Big)^{-\\gamma} R_{f,t+1}\\Big].\n\\]\nApproximating in logs,\n\\[\nr_f \\approx \\delta + \\gamma \\,g_c - \\frac{1}{2}\\,\\gamma\\,(\\gamma + 1)\\,\\sigma_c^2,\n\\]\nwhere:\n\n\\(r_f = \\ln(R_f)\\): the log of the risk-free rate,\n\n\\(\\delta = -\\ln(\\beta)\\): time preference rate,\n\n\\(g_c = \\mathbb{E}[\\ln(C_{t+1}/C_t)]\\): average consumption growth rate.\n\nEmpirically, long-run real risk-free rates are typically in the 1%–3% range, whereas the above equation might predict rates of 4%–8% given plausible values for \\(\\gamma\\), \\(\\delta\\), and \\(g_c\\). Again, the severe mismatch between model forecasts and observed data has led researchers to classify it as a “puzzle.”\n\n\nCritical Assumptions\nTo preserve tractability, the standard model assumes:\n\nA representative agent — but who truly “represents” the market?\n\nTime-separability of the utility function — ensuring that period utilities add linearly over time.\n\nGlobal concavity of CRRA utility — guaranteeing diminishing marginal utility at all consumption levels.\n\nWhile these assumptions yield elegant closed-form solutions, they may excessively simplify real-world heterogeneity. Crucially, when the economy scales up over time, CRRA utility remains well-defined, but in practice this might obscure the role of vastly different consumption paths across distinct wealth brackets."
  },
  {
    "objectID": "projects/asset_puzzle/asset_puzzle.html#critical-perspective",
    "href": "projects/asset_puzzle/asset_puzzle.html#critical-perspective",
    "title": "Asset Premium Puzzles",
    "section": "Critical Perspective",
    "text": "Critical Perspective\n\nRethinking ‘Rational Expectations’\nTraditionally, Rational Expectations is seen as a condition that agents use all available information efficiently, forming unbiased forecasts. However, from a purely mathematical standpoint, “expectations” and “covariances” are simply operators for dealing with means and correlations of random variables. Such operators—particularly bilinear forms and inner products—require specific algebraic properties (linearity, symmetry, Cauchy–Schwarz inequality, etc.). While these simplifications can be powerful in physics or engineering, in economics they might be overly restrictive when applied to highly heterogeneous populations and institutions.\n\n\nThe Euler Equation\nBy construction, the Euler condition \\(\\mathbb{E}_t [m_{t+1} R_{t+1}] = 1\\) is mathematically akin to an inner product on a probability space. This yields a focus on second moments (variance, covariance) and often leads to elliptical distribution assumptions (e.g., normal, Student-\\(t\\)). Real-world wealth distributions and market participation, however, may be far from elliptical in their statistical properties—especially when only a small fraction of the population holds the majority of risky assets.\n\n\nImplications of Globally Concave CRRA Utility\nCRRA utility, with its global concavity, implies a declining marginal utility as consumption grows. If aggregate consumption (\\(C_t\\)) trends upward over time, the ratio of marginal utilities \\(\\bigl[u'(C_{t+1}) / u'(C_t)\\bigr]\\) naturally declines, ensuring an inverse relationship between the SDF (\\(m_{t+1}\\)) and any asset (or variable) with a long-term growth trend. In equity markets, returns \\(R_{t+1}\\) also tend to grow over time, so \\(m_t\\) and \\(R_{t+1}\\) end up negatively correlated by construction.\nIf one then chooses a suitably volatile variable (with sufficient high variance) to stand in for \\(m_{t+1}\\), one can reconcile observed excess returns with the theoretical predictions—effectively defusing the puzzle. In that sense, the puzzle may be an artifact of incomplete modeling of real-world heterogeneity."
  },
  {
    "objectID": "projects/asset_puzzle/asset_puzzle.html#explaining-asset-premiums",
    "href": "projects/asset_puzzle/asset_puzzle.html#explaining-asset-premiums",
    "title": "Asset Premium Puzzles",
    "section": "Explaining Asset Premiums",
    "text": "Explaining Asset Premiums\n\nThrough Market Heterogeneity\nRather than focusing exclusively on heterogeneous preferences (e.g., habit formation, behavioral biases), the perspective here is that heterogeneous economic environments—in particular, vast differences in wealth and market participation—play a decisive role:\n\nConcentrated Stock Market Participation\n\nA small fraction of wealthy households hold the majority of equity. Their risk attitudes and consumption patterns have disproportionate influence on asset prices.\n\nWealth and Consumption Inequality\n\nHigh-net-worth individuals exhibit markedly different consumption patterns from the average household, more closely aligning with equity market fluctuations.\n\n\nFrom this viewpoint, the high observed equity premium is not a puzzle at all once one acknowledges that a very small sub-population—namely, the extremely wealthy—holds large, volatile wealth positions that effectively determine marginal prices in the stock market. According to Federal Reserve data (Board 2025), 93% of households’ stock market wealth (though not 93% of total market capitalization) belongs to the wealthiest 10%. This implies that most aggregate equity risk and returns accrue to a relatively narrow stratum. For the “representative household,” which holds only about 7% of equity, stock price movements have minimal impact on its marginal utility of consumption. Hence, standard representative-agent formulations fail to capture the truly relevant marginal investor.\nEmpirical evidence from the following studies supports this line of reasoning:\n\nBasak and Cuoco (1998): Demonstrates that limited stock market participation elevates risk premia and depresses risk-free rates.\n\nYogo (2006): Shows that consumption by wealthy households closely tracks equity returns, reinforcing the link between high-end consumption dynamics and asset prices.\n\nGomes and Michaelides (2008): Connects growing income inequality with stock market participation patterns, resulting in rising equity premia.\n\nLettau, Ludvigson, and Ma (2019): Shows that a single macroeconomic factor tied to capital share (reflecting wealthy shareholders’ consumption) can explain a broad range of cross-sectional stock return premia.\n\n\n\nLimitations\nDespite offering a plausible explanation, this heterogeneity-based view faces practical hurdles:\n\nLow frequency data: Due to infrequent reporting on high-net-worth wealth, applying short-term no-arbitrage principles in cross-sectional asset pricing is problematic.\n\nDifficulty of reconciling EMH: Efficient Markets Hypothesis (EMH) posits that arbitrage opportunities vanish quickly, but wealth data often lack the granularity or frequency to confirm this.\n\nLong-run identification: Changes in upper-tier wealth or consumption may be valid for a long-run SDF (\\(m\\)), yet verifying this for short-run asset pricing remains challenging."
  },
  {
    "objectID": "projects/asset_puzzle/asset_puzzle.html#conclusion",
    "href": "projects/asset_puzzle/asset_puzzle.html#conclusion",
    "title": "Asset Premium Puzzles",
    "section": "Conclusion",
    "text": "Conclusion\nThe Equity Premium Puzzle and Risk-Free Rate Puzzle have dominated discussions in asset pricing for decades. However, labeling them as genuine “puzzles” may reflect an artifact of restrictive models that hinge on a single representative agent, uniform preferences, and high-level assumptions about consumption growth. By introducing heterogeneous market participation, particularly the reality that a small fraction of wealthy agents holds the lion’s share of risky assets, one finds that what appears to be a puzzle for the average consumer is, in fact, quite explicable among those who actually drive stock prices.\nIn short, when empirical ownership and wealth concentration data are properly accounted for, the puzzling gaps between theory and observation can diminish or disappear. The challenge remains to integrate heterogeneous agent frameworks with accurate micro-level data on wealth and consumption in order to provide a more comprehensive understanding of asset prices—an endeavor that holds promise for reconciling the so-called “puzzles” with empirical reality."
  },
  {
    "objectID": "projects/correlation_crypto/correlation_crypto.html",
    "href": "projects/correlation_crypto/correlation_crypto.html",
    "title": "Correlation within Crypto-currencies",
    "section": "",
    "text": "Abstract: 2025년 3월 현재, 시가총액이 크거나 투자자들에게 인기가 많은 주요 암호화폐(popular cryptocurrencies)를 선정하여 지난 1년간의 상관관계를 분석하였다. 대부분의 암호화폐 투자자들은 이러한 주요 암호화폐에 집중적으로 투자하는 경향이 있다. 한편, 암호화폐 자산에 대한 투자자의 평균 투자 기간은 단기(short-term)로, 일반적으로 1개월에서 3개월 사이에 해당한다. 이에 따라 본 연구에서는 데이터의 관측 빈도(observation frequency)를 일간(daily) 단위로 설정하고, 30일, 60일, 90일의 롤링 윈도우(rolling window)를 적용하여 주요 암호화폐 수익률의 선형 상관계수(Pearson’s coefficient)를 분석하였다. 이러한 분석은 변동성 헤징(volatility hedging)을 고려한 분산 투자(diversified investment) 전략 수립에 도움이 될 수 있다. 예를 들어, 일정한 투자 금액(예: 1억 원)을 주요 암호화폐 자산군 내에서 어떻게 배분할지 결정하는 데 있어, 상관계수 분석 결과가 투자 비중 조정에 유용한 정보를 제공할 것으로 기대된다."
  },
  {
    "objectID": "projects/correlation_crypto/correlation_crypto.html#서론",
    "href": "projects/correlation_crypto/correlation_crypto.html#서론",
    "title": "Correlation within Crypto-currencies",
    "section": "서론",
    "text": "서론\n비트코인(BTC)의 가격 및 수익률은, 단기적으로 다음과 같은 관계를 보여왔다.\n\nNDX (나스닥 100 지수)와 강한 양의 상관관계 (Nasdaq 2024, 2023),\nDXY (미국 달러 지수)와 강한 음의 상관관계 (Coindesk 2023; Coinglass 2023). 만약 비트코인 가격이 달러 가격과 장기적으로도 반대 방향으로 움직인다면, 이는 비트코인이 인플레이션 헤지 자산으로 간주될 수도 있을 가능성을 나타낸다 (Dyhrberg 2021).\n금 가격 (GOLD), 국내 실질 총생산량 (GDP) 과의 상관관계는 불명확하거나 간접적인 것으로 알려져 있음 (Cointelegraph 2023; Cryptoslate 2022).\n\n역사적 사례\n\n2020년 COVID-19 위기 이후 BTC와 NDX의 상관관계가 강화됨 (Nasdaq 2020),\n2022년 5월 연방준비제도(Fed)의 금리 인상 발표 당시, BTC와 NDX 모두 하락.\n2023년 비트코인 빙하기 기간 동안 BTC와 NDX의 상관관계 변화 분석 필요.\n2024년 3월 비트코인 ETF가 출시하여 기관 투자자 참여가 증가와 함께께 BTC과 NDX의 coupling이 심해짐.\n\n\n주요 암호화폐 목록 및 카테고리\n\n\n\n\n\n\n\n\n암호화폐 (Cryptocurrency)\n심볼 (Ticker)\n카테고리 (Category)\n\n\n\n\n비트코인 (Bitcoin)\nBTC/USD\nLayer 1\n\n\n이더리움 (Ethereum)\nETH/USD\nLayer 1, Smart Contract\n\n\n테더 (Tether)\nUSDT/USD\nStablecoin\n\n\n리플 (XRP)\nXRP/USD\nPayment Network\n\n\n솔라나 (Solana)\nSOL/USD\nLayer 1\n\n\n체인링크 (Chainlink)\nLINK/USD\nOracle\n\n\n온도 (Ondo)\nONDO/USD\nReal-World Asset (RWA)\n\n\n카르다노 (Cardano)\nADA/USD\nLayer 1\n\n\n트론 (Tron)\nTRX/USD\nLayer 1\n\n\n도지코인 (Dogecoin)\nDOGE/USD\nMeme Coin\n\n\n\n\n\n암호화폐 관련 정보 제공 매체 리뷰\n\n시세 데이터 (Price Data): 실시간 및 과거 가격 변동, 거래량(volume) 등\n\nCoinMarketCap\nCoinGecko\n\n온체인 데이터 (On-Chain Data): 거래량, 지갑 주소 변화, 네트워크 활성도 등\n\nGlassnode\nIntoTheBlock\n\n시장 분석 (Market Analysis): 전문가 및 AI 기반 분석 리포트\n\nMessari\nCryptoQuant\n\n뉴스 및 이벤트 (News & Events): 프로젝트 업데이트, 규제 변화 등\n\nCoinDesk\nThe Block\n\n소셜 미디어 분석 (Social Media Analysis): 트위터(X), 레딧(Reddit) 등에서의 커뮤니티 반응\n\nLunarCrush\nSantiment"
  },
  {
    "objectID": "projects/correlation_crypto/correlation_crypto.html#데이터-분석",
    "href": "projects/correlation_crypto/correlation_crypto.html#데이터-분석",
    "title": "Correlation within Crypto-currencies",
    "section": "데이터 분석",
    "text": "데이터 분석\n\n데이터\n\n데이터 소스: CCXT\n데이터 기간: 2024년 3월 1일 - 2025년 2월 28일\n데이터 빈도 (Data Frequency): 일간(Daily)\n분석 대상 암호화폐:\n\nBTC/USD, ETH/USD, USDT/USD, XRP/USD, SOL/USD, LINK/USD, ONDO/USD, ADA/USD, TRX/USD, DOGE/USD\n\n롤링 윈도우 크기 (Rolling Window Size): 30일, 60일, 90일\n\n\n\n분석 방법\n\n암호화폐의 일간 수익률(daily return)을 계산.\n각 롤링 윈도우 크기(30, 60, 90일)에 대해 롤링 상관 행렬(rolling correlation matrix)을 계산.\n평균 상관계수(mean of rolling correlation matrix)를 도출하여 암호화폐 간의 관계를 분석.\n\n\n\nCode\n# 분석 결과 (Results)\n\n# 여러 거래소에서 지원하는 거래쌍을 확인\n\nimport ccxt\nimport pandas as pd\n\n# 주요 암호화폐 목록\nTICKER_COIN = ['BTC/USDT', 'ETH/USDT', 'USDT/USD', 'XRP/USDT', 'SOL/USDT', 'LINK/USDT', 'ONDO/USDT', 'ADA/USDT', 'TRX/USDT', 'DOGE/USDT']\n\n# 지원하는 거래소 목록\nexchanges = ['binance', 'kraken', 'bitfinex', 'poloniex']\n\n# 각 거래소에서 지원하는 거래쌍 확인\nfor exchange_id in exchanges:\n    exchange = getattr(ccxt, exchange_id)()\n    markets = exchange.load_markets()\n    supported_pairs = [pair for pair in TICKER_COIN if pair in markets]\n    print(f\"{exchange_id} supports: {supported_pairs}\")\n\n# 주요 암호화폐 목록\nbinance_tickers = ['BTC/USDT', 'ETH/USDT', 'XRP/USDT', 'SOL/USDT', 'LINK/USDT', 'ADA/USDT', 'TRX/USDT', 'DOGE/USDT']\nkraken_tickers = ['USDT/USD']\npoloniex_tickers = ['ONDO/USDT']\n\n# 데이터 기간 설정\nSTART_DATE = '2024-03-01'\nEND_DATE = '2025-02-28'\n\n# 거래소 설정\nbinance = ccxt.binance()\nkraken = ccxt.kraken()\npoloniex = ccxt.poloniex()\n\n# 데이터 불러오기 함수\ndef fetch_crypto_data(exchange, tickers, start, end):\n    data = {}\n    start_timestamp = exchange.parse8601(f'{start}T00:00:00Z')\n    end_timestamp = exchange.parse8601(f'{end}T00:00:00Z')\n    for ticker in tickers:\n        try:\n            ohlcv = exchange.fetch_ohlcv(ticker, '1d', since=start_timestamp, limit=1000)\n            df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n            df.set_index('timestamp', inplace=True)\n            data[ticker] = df['close']\n        except Exception as e:\n            print(f\"Error fetching {ticker} from {exchange.id}: {e}\")\n    return pd.DataFrame(data)\n\n# 데이터 불러오기\nbinance_data = fetch_crypto_data(binance, binance_tickers, START_DATE, END_DATE)\nkraken_data = fetch_crypto_data(kraken, kraken_tickers, START_DATE, END_DATE)\npoloniex_data = fetch_crypto_data(poloniex, poloniex_tickers, START_DATE, END_DATE)\n\n# 모든 데이터를 하나의 DataFrame으로 병합\ncrypto_prices = pd.concat([binance_data, kraken_data, poloniex_data], axis=1)\n\n# 1) 일간 수익률 계산\ndef compute_returns(price_data: pd.DataFrame) -&gt; pd.DataFrame:\n    return price_data.pct_change().dropna(how='all')\n\ncrypto_returns = compute_returns(crypto_prices)\n\n# 2) 롤링 상관계수 계산\ndef rolling_correlation(returns: pd.DataFrame, window: int) -&gt; pd.DataFrame:\n    \"\"\"\n    returns: (date x tickers) DataFrame\n    window:  rolling window size (days)\n    \n    returns.rolling(window).corr() 결과는\n      - MultiIndex (date, ticker1)\n      - columns = ticker2\n    형태를 가집니다.\n    \"\"\"\n    corr_rolling = returns.rolling(window).corr()\n    return corr_rolling\n\n# 3) 날짜별 상관행렬을 모아서 평균 상관행렬을 산출\ndef average_correlation_matrix(returns: pd.DataFrame, window: int) -&gt; pd.DataFrame:\n    \"\"\"\n    - returns.rolling(window).corr() 결과를 사용\n    - 각 날짜별 (티커 x 티커) 상관행렬을 합산 후, 날짜 개수로 나누어 평균\n    \"\"\"\n    corr_rolling = rolling_correlation(returns, window)\n    \n    # MultiIndex에서 날짜(level=0) 목록을 추출\n    unique_dates = corr_rolling.index.get_level_values(0).unique()\n    tickers = returns.columns\n    \n    # 상관행렬 누적 합을 위한 (티커 x 티커) 형태의 빈 DataFrame\n    sum_matrix = pd.DataFrame(0.0, index=tickers, columns=tickers)\n    count = 0\n    \n    for date in unique_dates:\n        # (ticker1 x ticker2) 형태를 얻기 위해 xs(date, level=0)\n        date_corr = corr_rolling.xs(date, level=0)\n        # date_corr.index = ticker1, date_corr.columns = ticker2\n        \n        # 혹시 일부 티커에 대한 데이터가 누락되었을 경우를 대비하여 reindex\n        date_corr = date_corr.reindex(index=tickers, columns=tickers)\n        \n        # 날짜별 상관행렬(N x N)을 모두 누적\n        if date_corr.notna().all().all():\n            sum_matrix += date_corr.fillna(0.0)\n            count += 1\n    \n    # 평균 계산 (count가 0이 되지 않는다고 가정)\n    mean_matrix = sum_matrix / count\n    \n    return mean_matrix\n\n# 4) 롤링 상관계수 평균 계산\nrolling_corr_results = {}\nfor window in [30, 60, 90]:\n    mean_corr_matrix = average_correlation_matrix(crypto_returns, window)\n    rolling_corr_results[window] = mean_corr_matrix\n\n\nbinance supports: ['BTC/USDT', 'ETH/USDT', 'XRP/USDT', 'SOL/USDT', 'LINK/USDT', 'ADA/USDT', 'TRX/USDT', 'DOGE/USDT']\nkraken supports: ['BTC/USDT', 'ETH/USDT', 'USDT/USD', 'XRP/USDT', 'SOL/USDT', 'LINK/USDT', 'ADA/USDT', 'DOGE/USDT']\nbitfinex supports: ['BTC/USDT', 'ETH/USDT', 'USDT/USD', 'XRP/USDT', 'SOL/USDT', 'LINK/USDT', 'ADA/USDT', 'TRX/USDT', 'DOGE/USDT']\npoloniex supports: ['BTC/USDT', 'ETH/USDT', 'XRP/USDT', 'SOL/USDT', 'LINK/USDT', 'ONDO/USDT', 'ADA/USDT', 'TRX/USDT', 'DOGE/USDT']\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# 모든 행과 열이 출력되도록 설정\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.expand_frame_repr', False)\n\n# 결과 출력 및 시각화\nfor window, result in rolling_corr_results.items():\n    # 상관 행렬을 DataFrame으로 변환\n    result_df = result.dropna(how='all')\n    print(f\"\\n[Window = {window} days] Mean Correlation Matrix\\n\", result_df)\n    \n    # 히트맵 시각화\n    plt.figure(figsize=(10, 8))\n    \n    # 대각선 요소를 마스킹\n    mask = np.triu(np.ones(result_df.shape, dtype=bool))\n    \n    sns.heatmap(result_df, annot=True, cmap='coolwarm', center=0, mask=mask)\n    plt.title(f'Mean Rolling Correlation Matrix (Window Size: {window} days)')\n    plt.show()\n\n\n\n[Window = 30 days] Mean Correlation Matrix\n            BTC/USDT  ETH/USDT  XRP/USDT  SOL/USDT  LINK/USDT  ADA/USDT  TRX/USDT  DOGE/USDT  USDT/USD  ONDO/USDT\nBTC/USDT   1.000000  0.797860  0.567901  0.741174   0.683407  0.704172  0.382569   0.812311  0.450488   0.348317\nETH/USDT   0.797860  1.000000  0.557302  0.701898   0.736282  0.718387  0.379209   0.745424  0.284259   0.378396\nXRP/USDT   0.567901  0.557302  1.000000  0.551380   0.567762  0.651334  0.316163   0.590110  0.165080   0.228550\nSOL/USDT   0.741174  0.701898  0.551380  1.000000   0.672486  0.683087  0.318726   0.693132  0.252265   0.300724\nLINK/USDT  0.683407  0.736282  0.567762  0.672486   1.000000  0.757956  0.311395   0.667528  0.231900   0.414424\nADA/USDT   0.704172  0.718387  0.651334  0.683087   0.757956  1.000000  0.414284   0.724381  0.256822   0.326920\nTRX/USDT   0.382569  0.379209  0.316163  0.318726   0.311395  0.414284  1.000000   0.367542  0.183200   0.113355\nDOGE/USDT  0.812311  0.745424  0.590110  0.693132   0.667528  0.724381  0.367542   1.000000  0.313370   0.347007\nUSDT/USD   0.450488  0.284259  0.165080  0.252265   0.231900  0.256822  0.183200   0.313370  1.000000   0.239516\nONDO/USDT  0.348317  0.378396  0.228550  0.300724   0.414424  0.326920  0.113355   0.347007  0.239516   1.000000\n\n\n\n\n\n\n\n\n\n\n[Window = 60 days] Mean Correlation Matrix\n            BTC/USDT  ETH/USDT  XRP/USDT  SOL/USDT  LINK/USDT  ADA/USDT  TRX/USDT  DOGE/USDT  USDT/USD  ONDO/USDT\nBTC/USDT   1.000000  0.794499  0.515181  0.739416   0.663307  0.687373  0.316881   0.813995  0.441244   0.342867\nETH/USDT   0.794499  1.000000  0.496328  0.686704   0.709256  0.690786  0.305216   0.725333  0.290231   0.368211\nXRP/USDT   0.515181  0.496328  1.000000  0.501653   0.535512  0.629728  0.269128   0.538347  0.127987   0.201797\nSOL/USDT   0.739416  0.686704  0.501653  1.000000   0.650504  0.666240  0.271134   0.684931  0.235206   0.281334\nLINK/USDT  0.663307  0.709256  0.535512  0.650504   1.000000  0.742312  0.255740   0.638219  0.215399   0.396690\nADA/USDT   0.687373  0.690786  0.629728  0.666240   0.742312  1.000000  0.367683   0.707737  0.238349   0.308895\nTRX/USDT   0.316881  0.305216  0.269128  0.271134   0.255740  0.367683  1.000000   0.304689  0.153140   0.087188\nDOGE/USDT  0.813995  0.725333  0.538347  0.684931   0.638219  0.707737  0.304689   1.000000  0.310558   0.328836\nUSDT/USD   0.441244  0.290231  0.127987  0.235206   0.215399  0.238349  0.153140   0.310558  1.000000   0.239446\nONDO/USDT  0.342867  0.368211  0.201797  0.281334   0.396690  0.308895  0.087188   0.328836  0.239446   1.000000\n\n\n\n\n\n\n\n\n\n\n[Window = 90 days] Mean Correlation Matrix\n            BTC/USDT  ETH/USDT  XRP/USDT  SOL/USDT  LINK/USDT  ADA/USDT  TRX/USDT  DOGE/USDT  USDT/USD  ONDO/USDT\nBTC/USDT   1.000000  0.793041  0.472848  0.741661   0.645544  0.678996  0.268781   0.810553  0.427806   0.341224\nETH/USDT   0.793041  1.000000  0.449356  0.688664   0.695999  0.672070  0.241350   0.707762  0.283345   0.366039\nXRP/USDT   0.472848  0.449356  1.000000  0.455283   0.503215  0.608591  0.231506   0.493813  0.100239   0.190861\nSOL/USDT   0.741661  0.688664  0.455283  1.000000   0.634605  0.653246  0.249218   0.683791  0.235912   0.277560\nLINK/USDT  0.645544  0.695999  0.503215  0.634605   1.000000  0.727156  0.213635   0.610947  0.189270   0.385690\nADA/USDT   0.678996  0.672070  0.608591  0.653246   0.727156  1.000000  0.329834   0.693549  0.223739   0.299241\nTRX/USDT   0.268781  0.241350  0.231506  0.249218   0.213635  0.329834  1.000000   0.245551  0.126440   0.079691\nDOGE/USDT  0.810553  0.707762  0.493813  0.683791   0.610947  0.693549  0.245551   1.000000  0.298659   0.321072\nUSDT/USD   0.427806  0.283345  0.100239  0.235912   0.189270  0.223739  0.126440   0.298659  1.000000   0.242537\nONDO/USDT  0.341224  0.366039  0.190861  0.277560   0.385690  0.299241  0.079691   0.321072  0.242537   1.000000"
  },
  {
    "objectID": "projects/correlation_crypto/correlation_crypto.html#토론-discussions",
    "href": "projects/correlation_crypto/correlation_crypto.html#토론-discussions",
    "title": "Correlation within Crypto-currencies",
    "section": "토론 (Discussions)",
    "text": "토론 (Discussions)\n\n\n\n\n\n\nImportant\n\n\n\n변수들의 관찰 주기 (단기? 장기?)에 따라 또는 관찰 시기 (10년전? 지금?)에 따라 변수들 간의 선형관계는 유지되지 않을 수 있습니다. 2025년 현재 비트코인 (BTC) 가격은 시장 심리, 규제 변화, 기술적 요인 등에 크게 영향을 받고 있습니다.\n\n\n\n상관계수가 낮은 암호화폐 자산 조합을 식별하고, 헤지 투자 전략을 논의.\n특정 암호화폐 간의 높은 상관관계가 나타나는 이유 및 그에 따른 리스크 분석.\n\n2024년 3월부터 2025년 2월까지 암호화폐 시장에 큰 영향을 미친 주요 변화 시기와 원인:\n\n비트코인 반감기 (2024년 4월 20일): 비트코인 채굴 보상이 6.25 BTC에서 3.125 BTC로 절반으로 감소. 이는 비트코인의 공급 감소로 이어져 가격에 상승 압력을 가함.\n비트코인 ETF 자금 유입 증가 (2024년 10월): 비트코인 ETF로의 지속적인 자금 유입이 관찰됨. 10월까지 ETF 투자자들이 총 345,200 BTC(200억 달러 이상의 가치)를 매입.\n트럼프의 대통령 당선 (2024년 11월): 도널드 트럼프가 “암호화폐 대통령”이 되겠다는 공약을 내세우며 당선됨. 이는 암호화폐 시장에 대한 긍정적인 기대감을 불러일으킴.\nEU의 암호화폐 시장 규제(MiCA) 전면 시행 (2024년 12월 30일): 유럽연합에서 암호화폐 시장 규제(MiCA)가 전면 시행됨. 이로 인해 EU 전역에서 암호화폐 서비스 제공업체들에 대한 통일된 규제 프레임워크가 적용되기 시작.\n트럼프의 암호화폐 정책 발표 (2025년 1월): 트럼프 대통령이 취임 후 미국을 “암호화폐의 수도”로 만들겠다는 계획을 발표함. 여기에는 비트코인 전략적 비축 등의 아이디어가 포함됨.\n\nstars and bins에서 위의 변화 시기가 bins 역할을 한다는 가정하여, 기간 stars을 다음과 같이 나누어 상관관계를 conditional 해 본다.\n\n20224년 3월 1일 (관측기간 시작일) - 2024년 4월 20일\n2024년 4월 21일 - 2024년 9월 30일\n2024년 10월 1일 - 2024년 11월 5일\n2024년 11월 6일 - 2024년 12월 31일\n2025년 1월 1일 - 2025년 2월 28일 (관측기간 종료일)\n\n변화를 \\(Z\\)로 표기했다면, covariance decomposition formula에 의해,\n(추후 계속)"
  },
  {
    "objectID": "projects/correlation_crypto/correlation_crypto.html#부록-conditioning-theorems-in-probability-theory",
    "href": "projects/correlation_crypto/correlation_crypto.html#부록-conditioning-theorems-in-probability-theory",
    "title": "Correlation within Crypto-currencies",
    "section": "부록: Conditioning Theorems in Probability Theory",
    "text": "부록: Conditioning Theorems in Probability Theory\n\nAdam’s Law: Smoothing Property of Conditional Expectation\nAlso known as the Law of Total Expectation or Law of Iterated Expectations.\nIf \\(X\\) is a random variable whose expectation \\(E(X)\\) is defined and \\(Z\\) is any random variable defined on the same probability space, then: \\[E(X) = E(E(X|Z)).\\]\nA conditional expectation can be viewed as a Radon–Nikodym derivative, making the tower property a direct consequence of the chain rule for conditional expectations.\nA special discrete case: If \\(\\{Z_{i}\\}\\) is a finite or countable partition of the sample space, then: \\[E(X) = \\sum_{i} E(X \\mid Z_i)P(Z_i).\\]\n\n\nEve’s Law: Variance Decomposition Formula\nKnown as the Conditional Variance Formula or the Law of Iterated Variances.\nIf \\(X\\) and \\(Y\\) are random variables defined on the same probability space, and if \\(Y\\) has finite variance, then:\n\\[\\operatorname{Var}(Y)=E[\\operatorname{Var}(Y\\mid X)]+\\operatorname{Var}(E[Y\\mid X]).\\]\nThis is a special case of the covariance decomposition formula.\nApplications: Explained by X, on average - 분산 = “(조건부 분산)의 평균” + “(조건부 평균)의 분산”\n\nAnalysis of Variance (ANOVA): Variability in \\(Y\\) splits into an “unexplained” within-group variance and an “explained” between-group variance. The F-test examines if the explained variance is significantly large, indicating a meaningful effect of \\(X\\) on \\(Y\\).\nLinear Regression Models: The proportion of explained variance is measured as \\(R^2\\). For simple linear regression (single predictor), \\(R^2\\) equals the squared Pearson correlation coefficient between \\(X\\) and \\(Y\\).\nMachine Learning and Bayesian Inference: In many Bayesian and ensemble methods, one decomposes prediction uncertainty via the law of total variance. For a Bayesian neural network with random parameters \\(\\theta\\): \\(\\operatorname {Var} (Y)=\\operatorname {E} {\\bigl [}\\operatorname {Var} (Y\\mid \\theta ){\\bigr ]}+\\operatorname {Var} {\\bigl (}\\operatorname {E} [Y\\mid \\theta ]{\\bigr )}\\) often referred to as “aleatoric” (within-model) vs. “epistemic” (between-model) uncertainty\nInformation Theory: For jointly Gaussian \\((X,Y)\\), the fraction \\(\\operatorname {Var} (\\operatorname {E} [Y\\mid X])/\\operatorname {Var} (Y)\\) relates directly to the mutual information \\(I(Y;X)\\). In non-Gaussian settings, a high explained-variance ratio still indicates significant information about Y contained in X\n\nExample 1 (Exam Scores): Suppose students’ exam scores vary between two classrooms. The variance of all scores (\\(Y\\)) can be decomposed into the variance within classrooms (unexplained) and the variance between classroom averages (explained), reflecting differences in teaching quality or resources.\nExample 2 (Mixture of Two Gaussians): Consider \\(Y\\) as a mixture of two normal distributions, where the mixing distribution is Bernoulli with parameter \\(p\\). Suppose:\n\\[Y \\mid (X=0) \\sim N(\\mu_0, \\sigma_0^2), \\quad Y \\mid (X=1) \\sim N(\\mu_1, \\sigma_1^2).\\]\nThen the law of total variance gives:\n\\[\\operatorname{Var}(Y) = p\\sigma_1^2 + (1-p)\\sigma_0^2 + p(1-p)(\\mu_1 - \\mu_0)^2.\\]\n\n\nCovariance Decomposition Formula\nKnown as the Law of Total Covariance or Conditional Covariance Formula.\nIf \\(X\\), \\(Y\\), and \\(Z\\) are random variables defined on the same probability space, with finite covariance between \\(X\\) and \\(Y\\), then:\n\\[\\operatorname{cov}(X,Y)=E[\\operatorname{cov}(X,Y\\mid Z)]+\\operatorname{cov}(E[X\\mid Z],E[Y\\mid Z]).\\]\nThis relationship is a particular instance of the general Law of Total Cumulance and is crucial for analyzing dependencies among variables conditioned on a third variable or groupings.\n\n\nBias-Variance Decomposition of MSE\nKey: The Bias-Variance Decomposition emphasizes the trade-off between making \\(\\hat{Y}\\) reliably close to its own expected value (low variance) and aligning that expected value with the true target \\(\\mathbb{E}[Y]\\) (low bias).\nIn many estimation or prediction settings, we have a random outcome \\(Y\\) and an estimator (or model prediction) \\(\\hat{Y}\\). The Mean Squared Error (MSE) of \\(\\hat{Y}\\) is:\n\\[\n\\mathrm{MSE}(\\hat{Y})\n= \\mathbb{E}\\bigl[(Y - \\hat{Y})^2\\bigr].\n\\]\nIf we decompose \\(\\hat{Y}\\) around its expected value, we can split MSE into a bias term and a variance term (plus any irreducible noise in certain contexts). Formally, assume\n\\[\n\\hat{Y} = f(X) + \\text{estimation noise},\n\\quad\nY = f(X) + \\varepsilon,\n\\]\nwhere \\(\\varepsilon\\) is an irreducible error term with mean zero (e.g., observational or inherent noise). Then:\n\\[\n\\begin{aligned}\n\\mathrm{MSE}(\\hat{Y})\n&= \\mathbb{E}\\bigl[(Y - \\hat{Y})^2\\bigr]\\\\\n&= \\underbrace{\\mathbb{E}\\Bigl[\\bigl(\\hat{Y} - \\mathbb{E}[\\hat{Y}]\\bigr)^2\\Bigr]}_{\\text{Variance}(\\hat{Y})}\n\\;+\\;\n\\underbrace{\\Bigl(\\mathbb{E}[\\hat{Y}] \\;-\\; \\mathbb{E}[Y]\\Bigr)^2}_{\\text{Bias}(\\hat{Y})^2}\n\\;+\\;\n\\underbrace{\\mathbb{E}\\bigl[\\varepsilon^2\\bigr]}_{\\text{Irreducible noise}},\n\\end{aligned}\n\\]\nwhere: 1. Variance: \\(\\text{Var}(\\hat{Y})\\) represents how much \\(\\hat{Y}\\) fluctuates around its own mean. 2. Bias: \\(\\text{Bias}(\\hat{Y})^2\\) measures how far \\(\\hat{Y}\\) (on average) deviates from the true mean \\(\\mathbb{E}[Y]\\). 3. Irreducible noise: \\(\\mathbb{E}[\\varepsilon^2]\\) is the part of the error that cannot be reduced by any estimator.\nIn a strictly theoretical sense (when \\(\\varepsilon\\) is embedded in \\(Y\\)), one often writes the Bias-Variance decomposition as:\n\\[\n\\mathrm{MSE}(\\hat{Y})\n= \\underbrace{\\mathrm{Var}(\\hat{Y})}_{\\text{Variance term}}\n\\;+\\;\n\\underbrace{\\mathrm{Bias}(\\hat{Y})^2}_{\\text{Bias term}}.\n\\]\nHere, if there is additional irreducible noise, it appears as a separate constant term. This decomposition closely aligns with the Law of Total Variance (Eve’s Law) in the sense that the total mean squared difference can be split into a “spread around the estimator’s mean” plus the “squared difference of that mean from the true value,” mirroring how variance itself decomposes into conditional components."
  },
  {
    "objectID": "projects/gilded_age/gilded_age.html",
    "href": "projects/gilded_age/gilded_age.html",
    "title": "The New Gilded Age",
    "section": "",
    "text": "The term ‘Gilded Age’ was originally coined by Mark Twain in his novel The Gilded Age: A Tale of Today (Twain and Warner 1873), describing an era characterized by rapid economic expansion, extreme wealth concentration, and political corruption. A similar dynamic is emerging today, where financial and technological elites dominate economic output while wealth inequality reaches historic highs (Piketty 2014). The late 19th century saw industrial monopolies like Standard Oil and U.S. Steel controlling markets; today, tech giants such as Amazon, Apple, and Google exhibit similar dominance (Zucman 2019).\nSimultaneously, the Federal Reserve’s response to financial instability, particularly through excessive monetary expansion, contrasts with past policy mistakes that led to severe economic contractions due to monetary shrinkage (Bernanke 2000). If current economic trends persist—marked by the increasing concentration of wealth, hyperinflation risks, and geopolitical tensions—then the U.S. may be heading toward another crisis akin to the 1929 stock market collapse (Kindleberger 1978)."
  },
  {
    "objectID": "projects/gilded_age/gilded_age.html#extreme-wealth-concentration-and-economic-disparities",
    "href": "projects/gilded_age/gilded_age.html#extreme-wealth-concentration-and-economic-disparities",
    "title": "The New Gilded Age",
    "section": "Extreme Wealth Concentration and Economic Disparities",
    "text": "Extreme Wealth Concentration and Economic Disparities\nIn the late 19th century, “Robber Barons” controlled vast industrial empires while working-class Americans suffered under exploitative labor conditions (Irwin 2017). Today, the economic landscape reflects a similar dynamic: the top 1% of Americans hold over 30% of total U.S. wealth, and financial markets remain dominated by a handful of institutional investors and corporations (Saez and Zucman 2020). If historical trends hold, wealth concentration at this level often precedes financial and political crises."
  },
  {
    "objectID": "projects/gilded_age/gilded_age.html#financial-market-distortions-due-to-federal-reserve-policies",
    "href": "projects/gilded_age/gilded_age.html#financial-market-distortions-due-to-federal-reserve-policies",
    "title": "The New Gilded Age",
    "section": "Financial Market Distortions Due to Federal Reserve Policies",
    "text": "Financial Market Distortions Due to Federal Reserve Policies\nHistorically, the Federal Reserve’s failure to manage monetary policy effectively has exacerbated financial downturns. During the Great Depression, the Fed allowed the money supply to contract, worsening deflation (Friedman and Schwartz 1993). Conversely, in the 2008 financial crisis, the Fed implemented massive QE programs to avoid liquidity shortages (Gopinath and Gourinchas 2020). If the Fed continues expanding the money supply unchecked while maintaining low interest rates, it could trigger runaway inflation or asset bubbles (Reinhart and Rogoff 2010)."
  },
  {
    "objectID": "projects/gilded_age/gilded_age.html#protectionist-policies-and-global-trade-disruptions",
    "href": "projects/gilded_age/gilded_age.html#protectionist-policies-and-global-trade-disruptions",
    "title": "The New Gilded Age",
    "section": "Protectionist Policies and Global Trade Disruptions",
    "text": "Protectionist Policies and Global Trade Disruptions\nIn response to financial instability, the U.S. may turn to protectionist measures similar to those seen in the early 20th century, such as the Smoot-Hawley Tariff Act (Irwin 2017). If the U.S. imposes broad tariffs on allies like Canada, Mexico, and the EU (excluding the UK), retaliatory tariffs could significantly reduce global trade, accelerating economic fragmentation (Acker 2020)."
  },
  {
    "objectID": "projects/gilded_age/gilded_age.html#capital-controls",
    "href": "projects/gilded_age/gilded_age.html#capital-controls",
    "title": "The New Gilded Age",
    "section": "Capital Controls",
    "text": "Capital Controls\nTo prevent capital flight, the U.S. government might implement capital controls, restricting the movement of funds outside the country (Dornbusch 1996). Such policies could initially stabilize domestic financial markets by preventing liquidity outflows, but they would ultimately deter foreign investment and reduce the credibility of the U.S. dollar (Prasad 2021). If capital controls are implemented alongside protectionist trade policies, the global financial system could realign, reducing reliance on the dollar (Eichengreen 2019)."
  },
  {
    "objectID": "projects/gilded_age/gilded_age.html#internal-conflict",
    "href": "projects/gilded_age/gilded_age.html#internal-conflict",
    "title": "The New Gilded Age",
    "section": "Internal Conflict",
    "text": "Internal Conflict\nWith increasing partisan division, the U.S. could experience state-led resistance against federal economic policies. Democratic-led states might oppose Republican federal mandates, leading to legal disputes over taxation, social policies, and trade regulations (Levitsky and Ziblatt 2018). In extreme cases, states like California could advocate for economic or political autonomy, mirroring secessionist movements of the 19th century, while Texas, despite its strong Republican leanings, might push for greater state sovereignty in response to federal overreach or shifting national policies (Acker 2020)."
  },
  {
    "objectID": "projects/gilded_age/gilded_age.html#u.s.-dollar-as-the-global-reserve-currency",
    "href": "projects/gilded_age/gilded_age.html#u.s.-dollar-as-the-global-reserve-currency",
    "title": "The New Gilded Age",
    "section": "U.S. Dollar as the Global Reserve Currency",
    "text": "U.S. Dollar as the Global Reserve Currency\nThe decline of the British pound post-World War II illustrates how global reserve currencies can lose dominance due to internal and external economic shifts (Eichengreen 2019). If U.S. political instability continues, central banks worldwide may accelerate diversification away from dollar holdings, increasing reliance on alternative financial networks such as BRICS payment systems, Bitcoin, and other emerging digital currencies. (Prasad 2021)."
  },
  {
    "objectID": "projects/pareto_index/pareto_index.html",
    "href": "projects/pareto_index/pareto_index.html",
    "title": "Approximating Wealth Distribution",
    "section": "",
    "text": "The Pareto distribution has long been used to model wealth or income distributions, starting from the seminal work of Vilfredo Pareto in the late 19th century (Pareto 1964). One of the most intuitive ways to visualize the inequality within such a distribution is through the Lorenz curve, introduced by Lorenz in 1905 (Lorenz 1905). The Lorenz curve describes the cumulative proportion of total wealth (or resources) held by the bottom fraction of the population. Closely related to the Lorenz curve is the Gini coefficient, which provides a scalar measure of inequality (Gini 1912).\nThis document first derives the Lorenz curve for a Pareto distribution, then demonstrates how to estimate the Pareto index (shape parameter) from Lorenz curve observations. We additionally relate the Pareto distribution to Constant Relative Risk Aversion (CRRA) utility function, highlighting the same shared structure in a differential equation that governs both."
  },
  {
    "objectID": "projects/pareto_index/pareto_index.html#introduction",
    "href": "projects/pareto_index/pareto_index.html#introduction",
    "title": "Approximating Wealth Distribution",
    "section": "",
    "text": "The Pareto distribution has long been used to model wealth or income distributions, starting from the seminal work of Vilfredo Pareto in the late 19th century (Pareto 1964). One of the most intuitive ways to visualize the inequality within such a distribution is through the Lorenz curve, introduced by Lorenz in 1905 (Lorenz 1905). The Lorenz curve describes the cumulative proportion of total wealth (or resources) held by the bottom fraction of the population. Closely related to the Lorenz curve is the Gini coefficient, which provides a scalar measure of inequality (Gini 1912).\nThis document first derives the Lorenz curve for a Pareto distribution, then demonstrates how to estimate the Pareto index (shape parameter) from Lorenz curve observations. We additionally relate the Pareto distribution to Constant Relative Risk Aversion (CRRA) utility function, highlighting the same shared structure in a differential equation that governs both."
  },
  {
    "objectID": "projects/pareto_index/pareto_index.html#deriving-the-lorenz-curve-of-a-pareto-distribution",
    "href": "projects/pareto_index/pareto_index.html#deriving-the-lorenz-curve-of-a-pareto-distribution",
    "title": "Approximating Wealth Distribution",
    "section": "2. Deriving the Lorenz Curve of a Pareto Distribution",
    "text": "2. Deriving the Lorenz Curve of a Pareto Distribution\n\n2.1. Definition of the Lorenz Curve\nFor a continuous distribution of wealth (X), the Lorenz curve \\(L(p)\\) is defined as the fraction of total wealth owned by the bottom \\(p\\) fraction of the population:\n\\[\nL(p) \\;=\\; \\frac{\\int_{x_m}^{x(p)} x\\,f(x)\\,dx}{\\int_{x_m}^{\\infty} x\\,f(x)\\,dx},\n\\]\nwhere\n\n\\(p = F\\bigl(x(p)\\bigr)\\) is the cumulative proportion of individuals with wealth below \\(x(p)\\),\nThe numerator represents the cumulative wealth of the bottom \\(p\\) fraction,\nThe denominator represents the total wealth in the system, given by the expected value of \\(X\\) over its support.\n\n\n\n2.2. Pareto Distribution\nA Pareto distribution with shape parameter \\(\\alpha&gt;0\\) and scale parameter \\(x_m&gt;0\\) is defined by the PDF\n\\[\nf(x) \\;=\\; \\alpha\\,\\frac{x_m^\\alpha}{x^{\\alpha+1}},\n\\quad x \\ge x_m,\n\\]\nand the corresponding CDF\n\\[\nF(x) \\;=\\; 1 \\;-\\;\\Bigl(\\tfrac{x_m}{x}\\Bigr)^\\alpha,\n\\quad x \\ge x_m.\n\\]\nEquivalently, for \\(0 &lt; p &lt; 1\\), the quantile \\(x(p)\\) satisfying \\(F\\bigl(x(p)\\bigr)=p\\) is\n\\[\np \\;=\\; 1 \\;-\\;\\Bigl(\\tfrac{x_m}{x(p)}\\Bigr)^\\alpha\n\\;\\;\\Longleftrightarrow\\;\\;\nx(p) \\;=\\; \\frac{x_m}{\\bigl(1 - p\\bigr)^{1/\\alpha}}.\n\\]\n\n\n2.3. Total Wealth\nLet the total wealth be \\(W_{\\text{total}} = E[X]\\), the expected value of \\(X\\). Substituting the PDF of the Pareto distribution into the definition of expectation,\n\\[\nE[X]\n\\;=\\; \\int_{x_m}^{\\infty} x\\,f(x)\\,dx\n\\;=\\; \\int_{x_m}^{\\infty} x \\,\\alpha\\,\\frac{x_m^\\alpha}{x^{\\alpha+1}}\\,dx\n\\;=\\; \\alpha\\,x_m^\\alpha \\int_{x_m}^{\\infty} x^{-\\alpha}\\,dx.\n\\]\nFor \\(\\alpha&gt;1\\), the improper integral converges and we obtain\n\\[\nW_{\\text{total}}\n\\;=\\; E[X]\n\\;=\\; \\frac{\\alpha\\,x_m}{\\alpha - 1}.\n\\]\n\n\n2.4. Cumulative Wealth for the Bottom \\(p\\) Fraction\nThe cumulative wealth held by the bottom \\(p\\) fraction is\n\\[\nW(p)\n\\;=\\;\\int_{x_m}^{x(p)} x\\,f(x)\\,dx\n\\;=\\;\\alpha\\,x_m^\\alpha \\int_{x_m}^{x(p)} x^{-\\alpha}\\,dx\n\\;=\\;\\alpha\\,x_m^\\alpha\n\\Bigl[\\frac{x^{-\\alpha+1}}{-\\alpha+1}\\Bigr]_{x_m}^{x(p)}.\n\\]\nSimplifying,\n\\[\nW(p)\n\\;=\\; \\frac{\\alpha\\,x_m}{\\alpha - 1}\n\\;\\Bigl(1 - \\bigl(\\tfrac{x_m}{x(p)}\\bigr)^{\\alpha - 1}\\Bigr).\n\\]\n\n\n2.5. Lorenz Curve for the Pareto Distribution\nBy definition,\n\\[\nL(p)\n\\;=\\; \\frac{W(p)}{W_{\\text{total}}}\n\\;=\\; \\frac{\\frac{\\alpha\\,x_m}{\\alpha - 1}\\,\\Bigl(1 - \\bigl(\\tfrac{x_m}{x(p)}\\bigr)^{\\alpha - 1}\\Bigr)}\n            {\\frac{\\alpha\\,x_m}{\\alpha - 1}}\n\\;=\\; 1 - \\bigl(1 - p\\bigr)^{\\frac{\\alpha - 1}{\\alpha}}.\n\\]\nHence, for a Pareto distribution, the Lorenz curve is\n\\[\nL(p) \\;=\\; 1 \\;-\\; \\bigl(1 - p\\bigr)^{\\tfrac{\\alpha - 1}{\\alpha}}.\n\\]\n\nIf \\(\\alpha \\gg 1\\), the distribution is more equal, and \\(L(p)\\) is closer to the 45-degree line of perfect equality.\n\nIf \\(\\alpha\\) is only slightly larger than 1, the distribution is more unequal, with significant concentration of wealth in the upper tail."
  },
  {
    "objectID": "projects/pareto_index/pareto_index.html#estimating-the-pareto-index-from-the-lorenz-curve",
    "href": "projects/pareto_index/pareto_index.html#estimating-the-pareto-index-from-the-lorenz-curve",
    "title": "Approximating Wealth Distribution",
    "section": "3. Estimating the Pareto Index from the Lorenz Curve",
    "text": "3. Estimating the Pareto Index from the Lorenz Curve\nSuppose empirical data or external studies indicate specific points \\((p, L(p))\\) on the Lorenz curve. We can use\n\\[\nL(p) \\;=\\; 1 - (1 - p)^{\\frac{\\alpha - 1}{\\alpha}}\n\\]\nto solve numerically for \\(\\alpha\\). Commonly cited examples:\nSeveral Empirical Illustrations\n\nPareto 80:20 Rule: \\(p=0.80\\), \\(L(p)=0.20\\)\n\\(\\Rightarrow\\) \\(\\alpha \\approx \\frac{\\ln(4)}{\\ln(5)} \\approx 1.16\\).\nPareto 90:10 Rule: \\(p=0.90\\), \\(L(p)=0.10\\)\n\\(\\Rightarrow\\) \\(\\alpha \\approx 1.05\\).\nU.S. Stock Market: According to a report by Axios (2024), the top 10% own about 93% of total equity wealth, implying \\(p=0.90\\) and \\(L(p)=0.07\\). Solving yields \\(\\alpha \\approx 1.03\\).\n\nCredit Suisse Global Wealth Report: In 2013, it was reported that the top 1% control about 50% of global wealth, which implies \\(p=0.99\\) and \\(L(p)=0.50\\). Solving gives \\(\\alpha \\approx 1.18\\). Additionally, the top 10% were said to own about 85% of global wealth (\\(p=0.90\\), \\(L(p)=0.15\\)), giving \\(\\alpha \\approx 1.08\\). Comparing such estimates across years (e.g., 2013 vs. 2020) can reveal the time dynamics of the global wealth distribution (Credit Suisse 2013, 2020)."
  },
  {
    "objectID": "projects/pareto_index/pareto_index.html#the-gini-coefficient",
    "href": "projects/pareto_index/pareto_index.html#the-gini-coefficient",
    "title": "Approximating Wealth Distribution",
    "section": "4. The Gini Coefficient",
    "text": "4. The Gini Coefficient\nThe Gini coefficient is a measure of wealth or income inequality that is closely related to the Lorenz curve. The Gini coefficient is defined as the ratio of the area between the Lorenz curve and the 45-degree equality line to the total area under the 45-degree line. Mathematically, the Gini coefficient ( G ) is given by:\n\\[\nG = 1 - 2 \\int_0^1 L(p) \\, dp.\n\\]\nSubstituting the Lorenz curve for a Pareto distribution:\n\\[\nL(p) = 1 - (1 - p)^{\\frac{\\alpha - 1}{\\alpha}},\n\\]\nwe obtain:\n\\[\nG = 1 - 2 \\int_0^1 \\big[1 - (1 - p)^{\\frac{\\alpha - 1}{\\alpha}}\\big] \\, dp = \\frac{1}{2\\alpha - 1}.\n\\]\nThus, for a Pareto distribution, the Gini coefficient is:\n\\[\nG = \\frac{1}{2\\alpha - 1}, \\quad \\text{for } \\alpha &gt; \\frac{1}{2}.\n\\]\nSome features of the Gini coefficient:\n\nAs \\(\\alpha \\to 1^+\\), the Gini coefficient approaches 1, indicating extreme inequality (a few individuals hold nearly all the wealth).\nAs \\(\\alpha \\to \\infty\\), the Gini coefficient approaches 0, indicating perfect equality.\nFor typical empirical values of \\(\\alpha\\) in wealth distributions (e.g., 1.1 to 1.8), the Gini coefficient ranges from 0.83 to 0.38, reflecting significant inequality\nRelative measure: \\(G\\) compares the distribution to perfect equality, but does not capture absolute differences.\n\nNon‐additivity: One cannot simply average the Gini coefficients of subpopulations to obtain an overall Gini coefficient.\n\nSensitivity: The Gini coefficient is sensitive to changes in the middle of the distribution, but less so at the tails."
  },
  {
    "objectID": "projects/pareto_index/pareto_index.html#pareto-distribution-and-crra-utility",
    "href": "projects/pareto_index/pareto_index.html#pareto-distribution-and-crra-utility",
    "title": "Approximating Wealth Distribution",
    "section": "5. Pareto Distribution and CRRA Utility",
    "text": "5. Pareto Distribution and CRRA Utility\nIn microeconomic theory, a widely used utility specification is the Constant Relative Risk Aversion (CRRA) form (Arrow et al. 1974; Pratt 1978). The CRRA utility function \\(u(x)\\) satisfies\n\\[\n-\\frac{x\\,u''(x)}{u'(x)} \\;=\\; \\gamma,\n\\]\nwhere \\(\\gamma&gt;0\\) is the coefficient of relative risk aversion. Solving for \\(u(x)\\) under boundary conditions such as \\(u(1)=0\\) yields:\n\nIf \\(\\gamma=1\\), \\(u(x)=\\ln x\\). (using the L’hopital’s Rule)\nIf \\(\\gamma\\neq 1\\), \\(u(x)=\\frac{x^{\\,1-\\gamma}-1}{\\,1-\\gamma\\,}\\).\n\nLarger \\(\\gamma\\) indicates higher risk aversion, while \\(\\gamma=0\\) corresponds to risk neutrality (\\(u(x)=x\\)).\nA Pareto PDF can also be derived from a differential equation with similar form. If \\(f(x)\\) is the PDF of a Pareto random variable on \\(x\\ge x_m&gt;0\\), one can write:\n\\[\n-\\frac{x\\,f'(x)}{\\,f(x)\\!}\\;=\\;(1+\\alpha)\\,x_m^{\\alpha},\n\\]\nwhich likewise has a “power‐law” solution structure. Thus, Pareto distributions and CRRA utilities each emerge from a linear differential equation of analogous form, underscoring a conceptual parallel in how “power‐type” functional solutions can appear in both economic choice models (through marginal utility) and in heavy‐tailed probability distributions.\nFurthermore, in mainstream economic theory, marginal utility \\(u'(x)\\) is assumed to be strictly positive, and \\(u''(x)\\) typically negative (diminishing marginal utility). In probability theory, any valid PDF \\(f(x)\\) must be positive, and for heavy‐tailed distributions like Pareto, \\(f(x)\\) decreases for large \\(x\\). These parallels lead to a one‐to‐one analogy between certain types of declining utilities and distributions whose density functions also decline in \\(x\\).\n\nRemark: There is a well‐known relationship via logarithmic transforms: if \\(X\\) is Pareto(\\(x_m,\\alpha\\)), then \\(Y=\\ln(X/x_m)\\) is exponentially distributed with rate \\(\\alpha\\). This exponential distribution also arises from a first‐order linear differential equation, reinforcing these structural similarities."
  },
  {
    "objectID": "projects/pareto_index/pareto_index.html#conclusion",
    "href": "projects/pareto_index/pareto_index.html#conclusion",
    "title": "Approximating Wealth Distribution",
    "section": "6. Conclusion",
    "text": "6. Conclusion\nBecause the Pareto distribution has only two parameters (\\(x_m\\) and \\(\\alpha\\)), even minimal distributional data—such as “the bottom 80% own 20% of total wealth”—enables one to solve directly for the Pareto index \\(\\alpha\\). This simplicity makes the Pareto distribution a convenient model or approximation for global wealth distribution, although in practice the estimated \\(\\alpha\\) can vary greatly depending on the dataset, sampling, and specific segment of the population observed.\nIn wealth and income distribution analysis, pairing empirical Lorenz curves with Pareto modeling remains a powerful—if simplified—approach to gauging inequality. For both theoretical and practical reasons, it continues to be integral in economic research, policy discussions, and broader studies of social welfare. Meanwhile, connections to CRRA utility function illustrate that core economic principles and certain types of heavy‐tailed probabilistic behavior can share similar mathematical underpinnings."
  },
  {
    "objectID": "projects/tfp.html",
    "href": "projects/tfp.html",
    "title": "TFP Growth, Stationarity, and Structural Change",
    "section": "",
    "text": "The growth rate of Total Factor Productivity (TFP) is not a fixed constant; rather, it evolves over time and may be subject to long-term decline. While mainstream macroeconomic models frequently assume a constant TFP growth rate, this assumption is increasingly being questioned. Several structural factors—diminishing returns to R&D, demographic constraints, and sectoral shifts towards lower-productivity industries—suggest that aggregate TFP growth may continue to slow. However, TFP growth is not uniform across sectors. Certain industries, such as AI, biotechnology, and advanced energy technologies, exhibit significantly higher productivity gains, leading to growing sectoral disparities in innovation.\nFrom an econometric perspective, a decline in TFP growth could alter its stationarity properties, weakening its stochastic trend and making it more likely to transition from an \\(I(1)\\) (non-stationary) stochastic process to an \\(I(0)\\) (stationary) stochastic process. If structural breaks exist in TFP growth, traditional time-series models based on constant-trend cointegration assumptions may fail. Thus, accurately modeling TFP trends requires dynamic econometric techniques that account for time-varying structural changes.\n\n\n\n\n\nTFP represents technological progress that enhances economic output beyond capital and labor inputs. In the Solow-Swan growth model, TFP follows an exponential trend, expressed as:\n\\[\nA_t = A_0 e^{gt} \\epsilon_t\n\\]\nwhere \\(g\\) is the growth rate and \\(\\epsilon_t\\) captures short-term fluctuations. This implies that TFP is not strictly stationary; rather, its log form follows a unit root process with a deterministic trend:\n\\[\n\\log A_t = g t + u_t, \\quad u_t \\sim I(0)\n\\]\nThus, mainstream macroeconomic models do not assume TFP is stationary in levels, but often assume its deviation from trend is stationary.\n\n\n\nEmpirical studies show:\n\nLong-Run TFP behaves as a non-stationary process (\\(I(1)\\)), meaning that it exhibits persistent growth and shocks that do not revert over time.\nShort-Run TFP Shocks (deviations from trend) are often stationary (\\(I(0)\\)) process.\n\nIf GDP and capital stock are also \\(I(1)\\), Solow’s production function suggests they should be cointegrated:\n\\[\n\\log Y_t - \\alpha \\log K_t - (1-\\alpha) \\log L_t - g t = u_t, \\quad u_t \\sim I(0)\n\\]\nThus, GDP, capital, and labor may be cointegrated due to the common trend driven by TFP.\n\n\n\nMany studies suggest that TFP growth rates have declined since the mid-20th century.\nU.S. TFP Growth Trends (Annual Growth Rates):\n\n1950s–1970s: ~2.5% per year\n\n1980s–1990s: ~1.5% per year\n\n2000s–2020s: ~0.5%-1.0% per year\n\nThe productivity slowdown hypothesis suggests that long-term economic growth potential is becoming weaker than before.\n\n\n\nSeveral key factors contribute to the observed decline in TFP growth:\n\nThe Low-Hanging Fruit Hypothesis (Gordon, 2016)\n\nPast technological revolutions (electricity, automobiles, antibiotics) were one-time events that provided massive boosts.\n\nRecent innovations (social media, fintech, AI) may not have the same economy-wide productivity effects.\n\nDeclining R&D Efficiency (Bloom et al., 2020)\n\n“Ideas are getting harder to find.”\n\nR&D investment has increased, but the number of researchers needed to sustain TFP growth has risen exponentially.\n\nDemographic Constraints and Human Capital Decay\n\nAging populations in advanced economies reduce labor force dynamism and innovation.\n\nEducation quality improvements may have plateaued, reducing skilled labor supply.\n\nSectoral Shift to Low-Productivity Industries\n\nAdvanced economies have shifted from manufacturing to services (e.g., healthcare, education, public sector).\n\nServices typically have lower TFP growth, as they rely heavily on human labor and are difficult to automate.\n\nIncreased Regulation and Market Distortions\n\nGrowing regulations, tax policies, and political uncertainty increase inefficiencies, reducing overall productivity.\n\n\n\n\n\nWhile aggregate TFP growth may appear to be declining, there are huge disparities across sectors. Just as wealth concentration is increasing, TFP growth could also be highly concentrated within specific industries:\n\nAI and Automation\n\nAI-driven automation could drastically improve productivity, particularly in certain white-collar jobs.\n\nHowever, productivity gains may be highly concentrated in tech-driven industries, rather than the economy as a whole.\n\nSectoral Disparities in TFP Growth\n\nWithin-market concentration: A small number of firms (e.g., winners in the winner-takes-all structure) are experiencing high TFP growth, while others stagnate.\n\nCross-market concentration: Some sectors (AI, biotech) may experience massive productivity boosts, while others (traditional services, public sector) may stagnate.\n\nEnergy Revolution and New Technological Frontiers\n\nAdvances in nuclear fusion, renewables, and space-based energy production could dramatically shift TFP growth.\n\nHowever, large-scale adoption is slow due to political, regulatory, and financial constraints.\n\n\n\n\n\n\n\n\nIf the long-run TFP growth rate is declining, its stochastic trend may weaken, making it more likely to transition from a unit root (\\(I(1)\\)) process to a stationary (\\(I(0)\\)) process over time. This has important implications for empirical modeling and forecasting, particularly in cases where researchers assume a constant growth trend in TFP-driven models. To properly account for these shifts, researchers should:\n\nUse structural break tests (e.g., Bai-Perron) to detect shifts in TFP growth trends and assess whether different growth regimes exist.\n\nApply rolling unit root tests to examine whether TFP has moved from an \\(I(1)\\) process toward an \\(I(0)\\) process over time.\n\nIncorporate time-varying cointegration approaches to avoid biased estimations that assume stable relationships in economic growth models.\n\n\n\n\nTFP growth is not uniform across industries; while aggregate TFP growth may be declining, specific sectors are experiencing rapid technological advancements. Just as wealth and market concentration have increased, TFP gains are increasingly concentrated within high-tech and innovation-driven industries. The result is a growing disparity between leading-edge industries (AI, biotech, energy) and traditional sectors (public services, healthcare, education), which exhibit slower productivity improvements.\nAs sectoral divergence in productivity widens, traditional macroeconomic models that assume homogeneous TFP growth may require revision. Future research should focus on developing heterogeneous growth models that account for cross-sectoral differences, while also employing more dynamic econometric techniques to capture the evolving nature of TFP trends."
  },
  {
    "objectID": "projects/tfp.html#introduction",
    "href": "projects/tfp.html#introduction",
    "title": "TFP Growth, Stationarity, and Structural Change",
    "section": "",
    "text": "The growth rate of Total Factor Productivity (TFP) is not a fixed constant; rather, it evolves over time and may be subject to long-term decline. While mainstream macroeconomic models frequently assume a constant TFP growth rate, this assumption is increasingly being questioned. Several structural factors—diminishing returns to R&D, demographic constraints, and sectoral shifts towards lower-productivity industries—suggest that aggregate TFP growth may continue to slow. However, TFP growth is not uniform across sectors. Certain industries, such as AI, biotechnology, and advanced energy technologies, exhibit significantly higher productivity gains, leading to growing sectoral disparities in innovation.\nFrom an econometric perspective, a decline in TFP growth could alter its stationarity properties, weakening its stochastic trend and making it more likely to transition from an \\(I(1)\\) (non-stationary) stochastic process to an \\(I(0)\\) (stationary) stochastic process. If structural breaks exist in TFP growth, traditional time-series models based on constant-trend cointegration assumptions may fail. Thus, accurately modeling TFP trends requires dynamic econometric techniques that account for time-varying structural changes."
  },
  {
    "objectID": "projects/tfp.html#main",
    "href": "projects/tfp.html#main",
    "title": "TFP Growth, Stationarity, and Structural Change",
    "section": "",
    "text": "TFP represents technological progress that enhances economic output beyond capital and labor inputs. In the Solow-Swan growth model, TFP follows an exponential trend, expressed as:\n\\[\nA_t = A_0 e^{gt} \\epsilon_t\n\\]\nwhere \\(g\\) is the growth rate and \\(\\epsilon_t\\) captures short-term fluctuations. This implies that TFP is not strictly stationary; rather, its log form follows a unit root process with a deterministic trend:\n\\[\n\\log A_t = g t + u_t, \\quad u_t \\sim I(0)\n\\]\nThus, mainstream macroeconomic models do not assume TFP is stationary in levels, but often assume its deviation from trend is stationary.\n\n\n\nEmpirical studies show:\n\nLong-Run TFP behaves as a non-stationary process (\\(I(1)\\)), meaning that it exhibits persistent growth and shocks that do not revert over time.\nShort-Run TFP Shocks (deviations from trend) are often stationary (\\(I(0)\\)) process.\n\nIf GDP and capital stock are also \\(I(1)\\), Solow’s production function suggests they should be cointegrated:\n\\[\n\\log Y_t - \\alpha \\log K_t - (1-\\alpha) \\log L_t - g t = u_t, \\quad u_t \\sim I(0)\n\\]\nThus, GDP, capital, and labor may be cointegrated due to the common trend driven by TFP.\n\n\n\nMany studies suggest that TFP growth rates have declined since the mid-20th century.\nU.S. TFP Growth Trends (Annual Growth Rates):\n\n1950s–1970s: ~2.5% per year\n\n1980s–1990s: ~1.5% per year\n\n2000s–2020s: ~0.5%-1.0% per year\n\nThe productivity slowdown hypothesis suggests that long-term economic growth potential is becoming weaker than before.\n\n\n\nSeveral key factors contribute to the observed decline in TFP growth:\n\nThe Low-Hanging Fruit Hypothesis (Gordon, 2016)\n\nPast technological revolutions (electricity, automobiles, antibiotics) were one-time events that provided massive boosts.\n\nRecent innovations (social media, fintech, AI) may not have the same economy-wide productivity effects.\n\nDeclining R&D Efficiency (Bloom et al., 2020)\n\n“Ideas are getting harder to find.”\n\nR&D investment has increased, but the number of researchers needed to sustain TFP growth has risen exponentially.\n\nDemographic Constraints and Human Capital Decay\n\nAging populations in advanced economies reduce labor force dynamism and innovation.\n\nEducation quality improvements may have plateaued, reducing skilled labor supply.\n\nSectoral Shift to Low-Productivity Industries\n\nAdvanced economies have shifted from manufacturing to services (e.g., healthcare, education, public sector).\n\nServices typically have lower TFP growth, as they rely heavily on human labor and are difficult to automate.\n\nIncreased Regulation and Market Distortions\n\nGrowing regulations, tax policies, and political uncertainty increase inefficiencies, reducing overall productivity.\n\n\n\n\n\nWhile aggregate TFP growth may appear to be declining, there are huge disparities across sectors. Just as wealth concentration is increasing, TFP growth could also be highly concentrated within specific industries:\n\nAI and Automation\n\nAI-driven automation could drastically improve productivity, particularly in certain white-collar jobs.\n\nHowever, productivity gains may be highly concentrated in tech-driven industries, rather than the economy as a whole.\n\nSectoral Disparities in TFP Growth\n\nWithin-market concentration: A small number of firms (e.g., winners in the winner-takes-all structure) are experiencing high TFP growth, while others stagnate.\n\nCross-market concentration: Some sectors (AI, biotech) may experience massive productivity boosts, while others (traditional services, public sector) may stagnate.\n\nEnergy Revolution and New Technological Frontiers\n\nAdvances in nuclear fusion, renewables, and space-based energy production could dramatically shift TFP growth.\n\nHowever, large-scale adoption is slow due to political, regulatory, and financial constraints."
  },
  {
    "objectID": "projects/tfp.html#conclusion",
    "href": "projects/tfp.html#conclusion",
    "title": "TFP Growth, Stationarity, and Structural Change",
    "section": "",
    "text": "If the long-run TFP growth rate is declining, its stochastic trend may weaken, making it more likely to transition from a unit root (\\(I(1)\\)) process to a stationary (\\(I(0)\\)) process over time. This has important implications for empirical modeling and forecasting, particularly in cases where researchers assume a constant growth trend in TFP-driven models. To properly account for these shifts, researchers should:\n\nUse structural break tests (e.g., Bai-Perron) to detect shifts in TFP growth trends and assess whether different growth regimes exist.\n\nApply rolling unit root tests to examine whether TFP has moved from an \\(I(1)\\) process toward an \\(I(0)\\) process over time.\n\nIncorporate time-varying cointegration approaches to avoid biased estimations that assume stable relationships in economic growth models.\n\n\n\n\nTFP growth is not uniform across industries; while aggregate TFP growth may be declining, specific sectors are experiencing rapid technological advancements. Just as wealth and market concentration have increased, TFP gains are increasingly concentrated within high-tech and innovation-driven industries. The result is a growing disparity between leading-edge industries (AI, biotech, energy) and traditional sectors (public services, healthcare, education), which exhibit slower productivity improvements.\nAs sectoral divergence in productivity widens, traditional macroeconomic models that assume homogeneous TFP growth may require revision. Future research should focus on developing heterogeneous growth models that account for cross-sectoral differences, while also employing more dynamic econometric techniques to capture the evolving nature of TFP trends."
  },
  {
    "objectID": "projects/tfp.html#integration-order",
    "href": "projects/tfp.html#integration-order",
    "title": "TFP Growth, Stationarity, and Structural Change",
    "section": "Integration Order",
    "text": "Integration Order\nThe integration order of a time series determines how many times it must be differenced to become stationary. A series is:\n\n\\(I(0)\\) (stationary) process if it has a constant mean, variance, and autocovariance.\n\\(I(1)\\) (unit root) process if it is non-stationary but becomes stationary after first differencing.\n\\(I(d)\\) process if it requires \\(d\\) differences to become stationary.\n\nEconomic time series such as GDP, money supply, and asset prices often exhibit \\(I(1)\\) behavior, meaning they contain stochastic trends and require differencing to achieve stationarity."
  },
  {
    "objectID": "projects/tfp.html#joint-covariance-stationarity-vs.-cointegration",
    "href": "projects/tfp.html#joint-covariance-stationarity-vs.-cointegration",
    "title": "TFP Growth, Stationarity, and Structural Change",
    "section": "Joint Covariance Stationarity vs. Cointegration",
    "text": "Joint Covariance Stationarity vs. Cointegration\nJoint covariance stationarity applies when each time-series maintains a constant mean, variance, and autocovariance over time. Cointegration, on the other hand, describes cases where two or more non-stationary \\(I(1)\\) time-series share a long-term equilibrium, forming a stationary linear combination.\n\nJoint Covariance Stationary Series (Weak-Sense Stationarity)\nA set of time series \\(X_t\\) and \\(Y_t\\) are jointly covariance stationary if they satisfy:\n\nConstant Mean: \\(E[X_t] = \\mu_X\\), \\(E[Y_t] = \\mu_Y\\) for all \\(t\\).\nConstant Variance: \\(Var(X_t)\\) and \\(Var(Y_t)\\) do not change over time.\nAutocovariance Depends Only on Lag: \\(Cov(X_t, X_{t-h})\\) and \\(Cov(Y_t, Y_{t-h})\\) depend only on the lag \\(h\\), not on \\(t\\).\n\nIf two time series are both weakly stationary, then any linear combination of them is also stationary.\n\n\nCointegrated Series\nA set of time series \\(X_t\\) and \\(Y_t\\) are cointegrated if:\n\nEach series is \\(I(1)\\) (non-stationary) process.\nA linear combination exists that is \\(I(0)\\) (stationary) process:\n\\[\n\\beta_1 X_t + \\beta_2 Y_t = u_t\n\\]\n\nwhere \\(u_t\\) is \\(I(0)\\) (stationary) process.\nThus, even though individual variables are non-stationary, their linear combination is stationary, implying a long-run equilibrium relationship.\n\n\nThe Relationship Between Covariance Stationarity and Cointegration\n\n\n\n\n\n\n\n\nProperty\nJoint Covariance Stationary Series\nCointegrated Series\n\n\n\n\nStationarity\nEach series is stationary (I(0))\nEach series is non-stationary (I(1)), but a linear combination is stationary\n\n\nUnit Root \\(I(d)\\)\nI(0) for each series\nI(1) for each series, but a specific linear combination is I(0)\n\n\nMean & Variance Stability\nMean & variance are constant over time\nIndividual series do not have stable mean & variance, but the combination does\n\n\nLong-run Relationship\nNo long-term relationship constraint\nA long-run equilibrium relationship exists\n\n\n\n\nCointegrated Series Can Be Transformed into Covariance Stationary Series\nIf \\(X_t\\) and \\(Y_t\\) are cointegrated, their first differences \\(\\Delta X_t\\), \\(\\Delta Y_t\\) (or the residual \\(u_t\\)) are stationary.\n\nThe error correction term \\(u_t\\) is stationary \\(I(0)\\) process, meaning it satisfies the covariance stationarity conditions.\n\n\n\nJoint Covariance Stationary Series Are Not Cointegrated\nIf \\(X_t\\) and \\(Y_t\\) are both already \\(I(0)\\) (stationary), then any linear combination of them is also stationary.\n\nThey cannot be cointegrated because cointegration only applies to non-stationary (\\(I(1)\\)) series.\nIf all series are already covariance stationary, testing for cointegration is unnecessary."
  },
  {
    "objectID": "projects/tfp.html#implications-for-empirical-analysis",
    "href": "projects/tfp.html#implications-for-empirical-analysis",
    "title": "TFP Growth, Stationarity, and Structural Change",
    "section": "Implications for Empirical Analysis",
    "text": "Implications for Empirical Analysis\n\nBefore testing for cointegration, check for stationarity. If all series are \\(I(0)\\), cointegration does not apply.\nIf series are cointegrated, their residuals (error correction term) should be covariance stationary.\nMany macroeconomic variables (e.g., GDP & consumption, money supply & inflation) are cointegrated rather than purely covariance stationary."
  },
  {
    "objectID": "time-series/labor_decoupling.html",
    "href": "time-series/labor_decoupling.html",
    "title": "노동: 기여한 만큼 분배가 되었나?",
    "section": "",
    "text": "Neoclassical Growth Model(신고전적 성장 모형)은 경제성장의 핵심 지표로 총생산량(Gross Domestic Product, GDP)을 중시한다. 이 모형에서는 생산의 핵심 요소를 다음과 같이 세 가지로 구분한다 (Solow 1956).\n\n총 노동량(Labor Input, \\(L\\))\n\n총 자본량(Capital Input, \\(K\\))\n\n노동증강기술(Labor-Augmenting Technology, \\(A\\))\n\n그러나 자본량(\\(K\\))은 직접적으로 측정하기 어려우며, 일반적으로 Perpetual Inventory Method(영속적 재고법)과 같은 방식을 활용하여 추정한다 (Hall and Jones 1999). 이에 더하여, 노동증강기술(\\(A\\))은 더욱 측정이 어려운 정성적 요소이기 때문에, 많은 연구에서는 Constant Returns to Scale(규모에 대한 수익 불변)을 가정한 Cobb-Douglas 생산함수를 이용하고, 추정 과정에서 발생하는 오차항(Error Term)을 기술진보(\\(A\\))로 대체하기도 한다 (Barro 1991). 이러한 방식이 현실을 얼마나 정확하게 반영하는지는 논란의 여지가 크다.\n모형(Model)은 본질적으로 이론적 가설이며, 경제성장에 대한 다양한 견해를 반영할 수 있는 도구이기 때문에, 각자의 가설과 주장을 펼치는 것은 학문적으로 허용될 수 있다. 그러나 문제의 본질은 해당 모형이 현실을 얼마나 정확하게 설명하고 예측할 수 있는지에 있다. 현실을 반영하지 못하고, 지속적으로 예측에 실패하는 경제성장 모형은 실질적으로 활용 가치가 떨어질 수밖에 없다 (Stiglitz 2017).\n경제성장과 관련하여 실용적인 측면에서 가장 중요한 변수는 총 노동량(\\(L\\))이라고 볼 수 있다. 이는 비교적 객관적인 측정이 가능하며, 경제의 실질적 변화를 분석하는 데 유용하다. 이러한 맥락에서 노동 생산성(Productivity)과 노동 분배(Labor Compensation)라는 두 가지 핵심 개념을 살펴볼 필요가 있다."
  },
  {
    "objectID": "time-series/labor_decoupling.html#서론",
    "href": "time-series/labor_decoupling.html#서론",
    "title": "노동: 기여한 만큼 분배가 되었나?",
    "section": "",
    "text": "Neoclassical Growth Model(신고전적 성장 모형)은 경제성장의 핵심 지표로 총생산량(Gross Domestic Product, GDP)을 중시한다. 이 모형에서는 생산의 핵심 요소를 다음과 같이 세 가지로 구분한다 (Solow 1956).\n\n총 노동량(Labor Input, \\(L\\))\n\n총 자본량(Capital Input, \\(K\\))\n\n노동증강기술(Labor-Augmenting Technology, \\(A\\))\n\n그러나 자본량(\\(K\\))은 직접적으로 측정하기 어려우며, 일반적으로 Perpetual Inventory Method(영속적 재고법)과 같은 방식을 활용하여 추정한다 (Hall and Jones 1999). 이에 더하여, 노동증강기술(\\(A\\))은 더욱 측정이 어려운 정성적 요소이기 때문에, 많은 연구에서는 Constant Returns to Scale(규모에 대한 수익 불변)을 가정한 Cobb-Douglas 생산함수를 이용하고, 추정 과정에서 발생하는 오차항(Error Term)을 기술진보(\\(A\\))로 대체하기도 한다 (Barro 1991). 이러한 방식이 현실을 얼마나 정확하게 반영하는지는 논란의 여지가 크다.\n모형(Model)은 본질적으로 이론적 가설이며, 경제성장에 대한 다양한 견해를 반영할 수 있는 도구이기 때문에, 각자의 가설과 주장을 펼치는 것은 학문적으로 허용될 수 있다. 그러나 문제의 본질은 해당 모형이 현실을 얼마나 정확하게 설명하고 예측할 수 있는지에 있다. 현실을 반영하지 못하고, 지속적으로 예측에 실패하는 경제성장 모형은 실질적으로 활용 가치가 떨어질 수밖에 없다 (Stiglitz 2017).\n경제성장과 관련하여 실용적인 측면에서 가장 중요한 변수는 총 노동량(\\(L\\))이라고 볼 수 있다. 이는 비교적 객관적인 측정이 가능하며, 경제의 실질적 변화를 분석하는 데 유용하다. 이러한 맥락에서 노동 생산성(Productivity)과 노동 분배(Labor Compensation)라는 두 가지 핵심 개념을 살펴볼 필요가 있다."
  },
  {
    "objectID": "time-series/labor_decoupling.html#본론",
    "href": "time-series/labor_decoupling.html#본론",
    "title": "노동: 기여한 만큼 분배가 되었나?",
    "section": "본론",
    "text": "본론\n\n1. 노동생산성(Labor Productivity)의 변화\n노동생산성은 단위 노동량당 산출량을 의미하며, 다음과 같이 정의할 수 있다 (Acemoglu et al. 2014).\n\\[\n\\text{노동생산성} = \\frac{Y_t}{L_t}\n\\]\n여기서 \\(Y_t\\)는 총생산량(GDP)의 대표적 대리 변수(proxy)이며, \\(L_t\\)는 총 노동량을 나타낸다. 각각의 변수는 다음과 같이 측정하였다.\n\n총생산량 Proxy (\\(Y_t\\)): Real Gross Domestic Product per Capita\n\n장기적으로 볼 때, 총생산량은 시간에 따라 증가하는 추세를 보였다.\n\n총노동량 Proxy (\\(L_t\\)): Hours Worked by Full-Time and Part-Time Employees\n\n이 역시 시간에 따라 증가하는 경향을 보였다.\n\n\n이로부터 계산한 노동생산성(\\(Y_t/L_t\\)) 역시 시간에 따라 증가하는 함수임을 확인할 수 있다. 즉, 경제가 성장함에 따라 노동자 1인당 산출하는 생산량은 꾸준히 증가해 왔다.\n\n\n2. 노동분배량(Labor Compensation)의 변화\n노동생산성이 증가하면, 일반적으로 노동자에게 돌아가는 보상 역시 증가해야 한다는 것이 경제적 정의(economic fairness)와 균형적 성장(balanced growth)의 핵심 원칙이다 (Piketty 2014). 그렇다면 노동 분배(Labor Compensation) 역시 시간에 따라 증가하였을까?\n이를 확인하기 위해, 노동자의 평균 실질 소득을 대리 변수로 활용하였다.\n\n노동에 분배된 총량 Proxy (\\(X_t\\)): Employed Full-Time: Median Usual Weekly Real Earnings\n\n장기적으로 증가하는 경향을 보였으나, 변동성이 존재하였다.\n\n노동분배량 (\\(\\frac{X_t}{L_t}\\))\n\n노동생산성이 증가하는 것과는 대조적으로, 노동자에게 분배된 소득의 비율(\\(X_t/L_t\\))은 오히려 시간에 따라 감소하는 경향을 보였다.\n\n\n이를 시각적으로 명확히 비교하기 위해, 노동분배량을 \\(\\frac{X_t}{L_t} \\times 100\\)으로 스케일링하여 그래프로 나타냈다.\nFRED Graph (1980년 이후 노동생산성과 노동분배량 비교)\n그래프를 살펴보면, 1980년대 이후 노동생산성과 노동분배량 사이의 격차가 점점 더 커지고 있음을 확인할 수 있다. 이는 무엇을 의미하는가? 노동생산성이 증가함에도 불구하고, 노동자에게 돌아가는 보상의 증가 속도가 이에 미치지 못하고 있다는 점을 시사한다. 다시 말해, “노동생산성과 노동자 보상의 분리 현상(decoupling)”이 지속적으로 심화되고 있음을 보여준다 (Stansbury and Summers 2020). 이러한 현상은 장기적으로 경제적 불평등(economic inequality)을 악화시키는 주요 원인 중 하나로 작용할 수 있다 (Stiglitz 2012).\n\n\n3. 노동 소득 분배율 감소의 원인\n노동소득 분배율(Labor Share)의 감소에 대한 경제학적 분석은 다양한 요인을 고려해야 하지만, 주요한 원인으로 다음 두 가지를 지적할 수 있다.\n\n기술 진보(Technological Progress)\n\nOECD의 분석에 따르면, 인공지능(AI)과 같은 첨단 기술의 발전은 특정 고숙련 노동자(high-skilled workers)에게는 유리하게 작용하지만, 그렇지 않은 노동자들에게는 불리한 영향을 미칠 수 있다 (OECD 2018). 이는 노동시장 내 임금 불평등(wage inequality)을 심화시키는 요인으로 작용한다.\n\n세계화(Globalization)\n\n생산 공정의 해외 이전(offshoring)과 국제 무역의 확대는 저임금 노동력을 활용한 생산 방식을 증가시켜, 선진국 내 노동자의 소득 증가율을 둔화시키는 결과를 초래할 수 있다 (Autor, Dorn, and Hanson 2013).\n\n\n\n\n4. 노동참여율(Labor Force Participation Rate)의 변화\n또한, 노동소득 분배율 감소와 노동시장 변화를 분석하기 위해, 노동참여율(Labor Force Participation Rate)을 함께 고려할 필요가 있다. 이를 위해 Current Population Survey (CPS)에서 조사한 Labor Force Participation Rate를 분석하였다.\n\nCOVID-19 팬데믹이 발생한 2020년에는 노동참여율이 일시적으로 급락했으나, 이후 빠르게 정상 수준으로 회복되었다 (Coibion, Gorodnichenko, and Weber 2020).\n\n장기적으로는 완만한 하락세를 보이고 있으며, 이는 인구 고령화(demographic aging) 등의 요인과도 관련이 있을 것으로 추정된다 (Krueger 2017)."
  },
  {
    "objectID": "time-series/labor_decoupling.html#결론",
    "href": "time-series/labor_decoupling.html#결론",
    "title": "노동: 기여한 만큼 분배가 되었나?",
    "section": "결론",
    "text": "결론\n노동생산성과 노동자 보상의 분리 현상(decoupling)은 실증적 데이터에 의해 명확히 확인된다. 노동생산성이 지속적으로 증가함에도 불구하고, 노동자에게 분배되는 소득의 비율은 점진적으로 감소해 왔다 (Karabarbounis and Neiman 2014). 이러한 현상의 주요 원인으로 기술 진보와 세계화가 지목되며, 장기적으로는 노동시장 내 불평등 심화 및 경제적 불안정성을 초래할 가능성이 크다 (Milanovic 2016). 향후 연구에서는 정책적 대응 방안과 노동소득 분배율 회복을 위한 제도적 개선책을 추가적으로 검토할 필요가 있다."
  }
]