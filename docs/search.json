[
  {
    "objectID": "thesis/index.html",
    "href": "thesis/index.html",
    "title": "Too Big to Fail?",
    "section": "",
    "text": "1 Summary\nKey highlights\n\nMarket Distortions and Asymmetric Returns: This study provides empirical evidence that post-2008 QE policies, ETF-driven passive flows, and market concentration have contributed to more asymmetric stock return distributions, challenging the conventional no-arbitrage principle in asset pricing.\nThe Paradox of the “Too Big to Fail” (TBTF) Portfolio: A simple strategy selecting the top 10 largest market-cap stocks has outperformed traditional benchmarks, not due to superior fundamentals, but as a byproduct of systemic inefficiencies, wealth concentration, and policy-driven capital allocation.\nImplications for Market Efficiency and Competition: The persistent success of the TBTF strategy suggests deepening inefficiencies in financial markets. If the strategy fails, it may indicate a shift toward more competitive and efficient value discovery, highlighting the paradox of profiting from distorted markets while awaiting their correction.\n\nThis study empirically investigates whether stock return distributions in the U.S. stock market have become more asymmetric, potentially resembling a mixture of heterogeneous assets under an arbitrage-limited structure rather than a mixture of homogeneous assets under the no-arbitrage principle. This perspective challenges mainstream economic theories that rely on efficient market hypothesis (EMH), the no-arbitrage assumption, and global convexity assumption in a consumer preference or production opportunity model.\nThe study further explores distortions in free-market competition within the U.S. financial markets following the Federal Reserve’s post-2008 quantitative easing (QE) policies and the rise of ETFs as dominant investment vehicles. It hypothesizes that these structural shifts have exacerbated mispricing, increased asymmetric wealth concentration, and intensified extreme wealth polarization, ultimately reducing overall market efficiency. By fueling passive capital flows and reinforcing momentum-driven investment behavior, ETFs have amplified the self-reinforcing cycle of market concentration, where a small number of dominant firms continue to absorb disproportionate market share and capital inflows, further entrenching their position.\nThese distortions provide a structural backdrop for the success of the “Too Big to Fail” (TBTF) portfolio strategy, which consists of the 10 largest market-cap stocks from the U.S. stock market (Nasdaq, NYSE, AMEX) with a monthly rebalancing approach and an internally competitive weighting scheme. Empirical findings indicate that the TBTF portfolio has demonstrated superior risk-adjusted performance across multiple measures, including the Sharpe ratio, Sortino ratio, and Omega ratio. The strategy has been particularly effective in the post-2010 period, where its outperformance has been both substantial and persistent. Additionally, the strategy benefits from low turnover, reducing transaction costs and increasing practical feasibility.\nThis study ultimately highlights a paradox: the TBTF strategy remains “sadly optimal,” as it provides easy profits when it succeeds and signals broader market improvements when it does not. If the strategy continues to thrive, it suggests that market inefficiencies are deepening, whereas its failure could indicate a return to more competitive and efficient financial markets. However, the strategy’s success, particularly in the post-2010 period, may not necessarily reflect superior firm fundamentals. Instead, it may be a direct consequence of structural market imbalances driven by central bank interventions, passive capital flows from ETFs, and the network effects of technological monopolization. These dynamics challenge the traditional view that excess returns reflect rational compensation for risk; rather, they suggest that persistent outperformance may stem from artificial distortions in capital allocation and value discovery mechanisms. This raises concerns about the long-term implications of central bank intervention, wealth concentration, and the erosion of competitive market dynamics."
  },
  {
    "objectID": "thesis/03_data.html",
    "href": "thesis/03_data.html",
    "title": "Too Big to Fail?",
    "section": "",
    "text": "Pooled Panel data\nPanel data\nCRSP from WRDS\nprice data from Yahoo Finance\nFama-French data library\nFRED database\n\n\n\n\n\nIn CRSP monthly stock data, dates like “2010-04-01” typically represent the end-of-month data, such as the market price on 2010-04-30, not the start of the month.\nAdjust CRSP dates (e.g., 2020-01-01) to month-end (e.g., 2020-01-31) to match regular (e.g. Yahoo Finance) resampled dates.\ndf[‘date’] = pd.to_datetime(df[‘date’])+ pd.offsets.MonthEnd(0)\n\n\n\n이전 해 12월 말(예: 2000년 12월) 시점에서 NYSE의 median 시가총액을 기준으로 Small과 Big의 경계선이 결정. 실제로 포트폴리오를 구성하는 시점은 그 다음 해 6월 말 (예: 2001년 6월 말).\n\n6개월의 시간 차이를 둔 것은 실무적으로 연말 결산 데이터와 회계자료(book equity 등)의 발표와 확인, 데이터베이스화 과정이 약 4~6개월 정도 소요되기 때문.\nFama-French의 초기 연구(1992, 1993, 1996년 등)에 따르면, 시가총액(market equity) 기준의 기업 순위(percentile rank)는 대체로 상당히 지속적(persistent)이며, 6개월 이내에 급격하게 변동하는 기업은 상대적으로 적었다고 함.\n\n예시:\n\n이전 해 12월 말에 10 percentile 이상이었고 (당시 큰 편에 속했지만),\n그 다음 해 6월 말에 측정된 시가총액이 NYSE median market equity보다 작다면(즉 50 percentile 이하라면),\n실제로 포트폴리오에 편입될 때는 Small 그룹으로 분류됩니다.\n\n\n\n\nThe segmentation of financial time-series data at 2010-01-01 enables a clear distinction between two fundamentally different economic regimes. The pre-2010 period is characterized by financial deregulation, crises, and heightened volatility, while the post-2010 period reflects policy-driven recovery, evolving geopolitical risks, technological transformations, and the popularization of Exchange-Traded Funds (ETFs). This division enhances the robustness of empirical analysis, allowing for a comparative assessment of portfolio performance across distinct macroeconomic environments.\n\n\nThe dataset analyzed in this study spans 1996-01-01 to 2023-12-31 with a monthly frequency. To assess portfolio performance across different economic conditions, the time series is divided at 2010-01-01, creating two equal sub-periods: - 1996-01-01 to 2009-12-31 (14 years) - 2010-01-01 to 2023-12-31 (14 years)\nThis segmentation aligns with major financial and macroeconomic shifts, particularly the aftermath of the 2008-2009 Global Financial Crisis, which ushered in a new era of monetary policy and market behavior, as well as the rise in popularity and widespread adoption of ETFs.\n\n\n\nThe pre-2010 period was marked by high financial volatility, including the Asian Financial Crisis (1997), Dot-com Bubble (2000-2001), 9/11 attacks (2001), and Global Financial Crisis (2008-2009). These events significantly influenced market structures and investment dynamics.\nConversely, the post-2010 period saw a shift towards policy-driven markets, with extensive Quantitative Easing (QE) programs, U.S.-China trade tensions (2018), the COVID-19 pandemic (2020), the rise of AI-driven market trends (2023-present), and the proliferation of ETFs as a mainstream investment vehicle. This transition underscores the fundamental differences between the two periods in terms of risk exposure, liquidity conditions, technological advancements, and investment strategies.\nBy segmenting the data at 2010-01-01, this study enables a structured analysis of how portfolio performance responds to structurally distinct economic environments, enhancing the robustness of financial research.\n\n\n\n\n\nThe following table outlines major macroeconomic and financial events that influenced global markets during each period. The relative importance of each event is indicated in parentheses (e.g., Very High, High).\n\n\n\n\n\n\n\n\n\n\nDate\nEvent & Description\nImportance\n\n\n\n\n1997-07-02\nAsian Financial Crisis begins: The collapse of the Thai baht triggered widespread currency devaluations and IMF interventions.\nVery High\n\n\n1999-01-01\nIntroduction of the Euro: The launch of the euro as a unified currency, reshaping global trade and financial markets.\nHigh\n\n\n1999-11-12\nGramm-Leach-Bliley Act: Repeal of the Glass-Steagall Act, increasing risk-taking in banking, later contributing to the 2008 financial crisis.\nHigh\n\n\n2000-03-10\nDot-com Bubble Peak: The NASDAQ index reaches its highest point before the bubble bursts.\nVery High\n\n\n2000-04 ~ 2001-Q1\nDot-com Crash: Market correction results in widespread collapse of internet-based companies.\nVery High\n\n\n2001-09-11\n9/11 Terrorist Attacks: Immediate shock to financial markets, heightened global uncertainty.\nVery High\n\n\n2008-09-15\nLehman Brothers Collapse: Key event marking the onset of the Global Financial Crisis.\nVery High\n\n\n2008-10-03\nTARP Bailout Program: U.S. government enacts a $700 billion rescue package to stabilize financial institutions.\nVery High\n\n\n2009-Q2\nEnd of the Great Recession: The NBER officially marks the end of the economic downturn.\nHigh\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nEvent & Description\nImportance\n\n\n\n\n2009-Q3 ~ 2010\nPost-crisis economic recovery: The Federal Reserve initiates QE programs, boosting liquidity and asset prices.\nHigh\n\n\n2012-09-13\nQE3 Announcement: The Fed announces an open-ended bond-buying program.\nHigh\n\n\n2013-12-18\nTapering Policy Begins: The Fed begins scaling back QE measures, causing increased market volatility.\nHigh\n\n\n2015-10-28\nEnd of QE3: The Fed concludes its final round of quantitative easing, signaling policy normalization.\nHigh\n\n\n2018-01-01\nU.S.-China Trade War Escalates: Tariff impositions disrupt global trade, affecting equity and commodity markets.\nVery High\n\n\n2020-03-11\nCOVID-19 Pandemic Declared: The stock market crashes, prompting historic monetary and fiscal stimulus measures.\nVery High\n\n\n2023-Present\nRise of AI in Financial Markets: The mainstream adoption of AI technologies, such as ChatGPT, influences market sentiment and productivity outlook.\nHigh\n\n\n2000s-Present\nProliferation of ETFs: ETFs have grown significantly, offering investors diversified, low-cost investment options, transforming investment strategies.\nHigh\n\n\n\nNote: The Proliferation of ETFs spans both periods but has been particularly impactful in the post-2010 period, reflecting their integration into mainstream investment strategies."
  },
  {
    "objectID": "thesis/03_data.html#appendix-top-10-permno-시트-샘플",
    "href": "thesis/03_data.html#appendix-top-10-permno-시트-샘플",
    "title": "Too Big to Fail?",
    "section": "",
    "text": "In CRSP monthly stock data, dates like “2010-04-01” typically represent the end-of-month data, such as the market price on 2010-04-30, not the start of the month.\nAdjust CRSP dates (e.g., 2020-01-01) to month-end (e.g., 2020-01-31) to match regular (e.g. Yahoo Finance) resampled dates.\ndf[‘date’] = pd.to_datetime(df[‘date’])+ pd.offsets.MonthEnd(0)\n\n\n\n이전 해 12월 말(예: 2000년 12월) 시점에서 NYSE의 median 시가총액을 기준으로 Small과 Big의 경계선이 결정. 실제로 포트폴리오를 구성하는 시점은 그 다음 해 6월 말 (예: 2001년 6월 말).\n\n6개월의 시간 차이를 둔 것은 실무적으로 연말 결산 데이터와 회계자료(book equity 등)의 발표와 확인, 데이터베이스화 과정이 약 4~6개월 정도 소요되기 때문.\nFama-French의 초기 연구(1992, 1993, 1996년 등)에 따르면, 시가총액(market equity) 기준의 기업 순위(percentile rank)는 대체로 상당히 지속적(persistent)이며, 6개월 이내에 급격하게 변동하는 기업은 상대적으로 적었다고 함.\n\n예시:\n\n이전 해 12월 말에 10 percentile 이상이었고 (당시 큰 편에 속했지만),\n그 다음 해 6월 말에 측정된 시가총액이 NYSE median market equity보다 작다면(즉 50 percentile 이하라면),\n실제로 포트폴리오에 편입될 때는 Small 그룹으로 분류됩니다.\n\n\n\n\nThe segmentation of financial time-series data at 2010-01-01 enables a clear distinction between two fundamentally different economic regimes. The pre-2010 period is characterized by financial deregulation, crises, and heightened volatility, while the post-2010 period reflects policy-driven recovery, evolving geopolitical risks, technological transformations, and the popularization of Exchange-Traded Funds (ETFs). This division enhances the robustness of empirical analysis, allowing for a comparative assessment of portfolio performance across distinct macroeconomic environments.\n\n\nThe dataset analyzed in this study spans 1996-01-01 to 2023-12-31 with a monthly frequency. To assess portfolio performance across different economic conditions, the time series is divided at 2010-01-01, creating two equal sub-periods: - 1996-01-01 to 2009-12-31 (14 years) - 2010-01-01 to 2023-12-31 (14 years)\nThis segmentation aligns with major financial and macroeconomic shifts, particularly the aftermath of the 2008-2009 Global Financial Crisis, which ushered in a new era of monetary policy and market behavior, as well as the rise in popularity and widespread adoption of ETFs.\n\n\n\nThe pre-2010 period was marked by high financial volatility, including the Asian Financial Crisis (1997), Dot-com Bubble (2000-2001), 9/11 attacks (2001), and Global Financial Crisis (2008-2009). These events significantly influenced market structures and investment dynamics.\nConversely, the post-2010 period saw a shift towards policy-driven markets, with extensive Quantitative Easing (QE) programs, U.S.-China trade tensions (2018), the COVID-19 pandemic (2020), the rise of AI-driven market trends (2023-present), and the proliferation of ETFs as a mainstream investment vehicle. This transition underscores the fundamental differences between the two periods in terms of risk exposure, liquidity conditions, technological advancements, and investment strategies.\nBy segmenting the data at 2010-01-01, this study enables a structured analysis of how portfolio performance responds to structurally distinct economic environments, enhancing the robustness of financial research."
  },
  {
    "objectID": "thesis/03_data.html#appendix-key-events-in-each-period",
    "href": "thesis/03_data.html#appendix-key-events-in-each-period",
    "title": "Too Big to Fail?",
    "section": "",
    "text": "The following table outlines major macroeconomic and financial events that influenced global markets during each period. The relative importance of each event is indicated in parentheses (e.g., Very High, High).\n\n\n\n\n\n\n\n\n\n\nDate\nEvent & Description\nImportance\n\n\n\n\n1997-07-02\nAsian Financial Crisis begins: The collapse of the Thai baht triggered widespread currency devaluations and IMF interventions.\nVery High\n\n\n1999-01-01\nIntroduction of the Euro: The launch of the euro as a unified currency, reshaping global trade and financial markets.\nHigh\n\n\n1999-11-12\nGramm-Leach-Bliley Act: Repeal of the Glass-Steagall Act, increasing risk-taking in banking, later contributing to the 2008 financial crisis.\nHigh\n\n\n2000-03-10\nDot-com Bubble Peak: The NASDAQ index reaches its highest point before the bubble bursts.\nVery High\n\n\n2000-04 ~ 2001-Q1\nDot-com Crash: Market correction results in widespread collapse of internet-based companies.\nVery High\n\n\n2001-09-11\n9/11 Terrorist Attacks: Immediate shock to financial markets, heightened global uncertainty.\nVery High\n\n\n2008-09-15\nLehman Brothers Collapse: Key event marking the onset of the Global Financial Crisis.\nVery High\n\n\n2008-10-03\nTARP Bailout Program: U.S. government enacts a $700 billion rescue package to stabilize financial institutions.\nVery High\n\n\n2009-Q2\nEnd of the Great Recession: The NBER officially marks the end of the economic downturn.\nHigh\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nEvent & Description\nImportance\n\n\n\n\n2009-Q3 ~ 2010\nPost-crisis economic recovery: The Federal Reserve initiates QE programs, boosting liquidity and asset prices.\nHigh\n\n\n2012-09-13\nQE3 Announcement: The Fed announces an open-ended bond-buying program.\nHigh\n\n\n2013-12-18\nTapering Policy Begins: The Fed begins scaling back QE measures, causing increased market volatility.\nHigh\n\n\n2015-10-28\nEnd of QE3: The Fed concludes its final round of quantitative easing, signaling policy normalization.\nHigh\n\n\n2018-01-01\nU.S.-China Trade War Escalates: Tariff impositions disrupt global trade, affecting equity and commodity markets.\nVery High\n\n\n2020-03-11\nCOVID-19 Pandemic Declared: The stock market crashes, prompting historic monetary and fiscal stimulus measures.\nVery High\n\n\n2023-Present\nRise of AI in Financial Markets: The mainstream adoption of AI technologies, such as ChatGPT, influences market sentiment and productivity outlook.\nHigh\n\n\n2000s-Present\nProliferation of ETFs: ETFs have grown significantly, offering investors diversified, low-cost investment options, transforming investment strategies.\nHigh\n\n\n\nNote: The Proliferation of ETFs spans both periods but has been particularly impactful in the post-2010 period, reflecting their integration into mainstream investment strategies."
  },
  {
    "objectID": "thesis/01_intro.html",
    "href": "thesis/01_intro.html",
    "title": "Too Big to Fail?",
    "section": "",
    "text": "1 Introduction\n현실: CAPM과 MPT는 대표성과 대체성, 유동성을 전제했으나 현실은 다름 - 시장이 systematic risk pricing → structural rent allocation으로 변화 - 시장은 위험 기반 수익률이 아니라 구조적 rent에 대해 보상 중 미래 연구: 구조적 비효율성을 반영한 새로운 자산 가격 모형 설계\n\nStylized Facts (실증 패턴 요약)\nMixture Structure of Market Returns, Return Distribution Decomposition\n\ntime-varying mixture of returns\nPercentile Transition Marix and Stationary Structure\n\nMobility Inequality\nEntropy / Gini index on stationary distribution\nQuantile persistence score, upward/downward mobility imbalance\n\n\nFrom Risk-Based to Rent-Based Pricing\n\nWelfare Implications and Capital Allocation\n\nThe Sadly Optimal TBTF Strategy\n\nTBTF 포트폴리오: 상위 10개 시총 종목 동일 비중 또는 내부 경쟁 가중치\n수익률, 변동성, 샤프비율, Sortino, Omega 비교\n“Win if persists, optimal if fails” – 사회 전체로는 전략의 붕괴가 바람직하나 개인에겐 최적\n\n\nAppendix - Key Events in Each Period - Market concentration - Regular Big vs. Small - Top 10 membership and their average rank, Membership Transition - Stationary distribution within the membership\npercentile 기준. 순위 기반 (order-statistics 기반) - capital concentration on extreme (not middle) - level concentration: - mobility lock-in: 계층 이동성 감소. upward mobility가 단절 (구조적 원인: education, labor market, inheritance 등) - capital polarization: 계층 간 격차 확대 = + divergence - level polarization: 자본주의에서 자본량의 증가비율이 현재 자본량에 비례한다면, 자본량은 기하급수적으로 증가하는 게 당연. (geometrically series, y’=y given y(0)) - share polarization: 총자본량의 증가비율이 현재 총자본량에 비례하여 증가한다고 가정. 각 계층별 capital share가 일정하지 않고 한 곳으로 몰린다면, 몰린 계층의 자본본\nWithin a Market\nAcross Markets"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Research Projects\nThis section collects active research inquiries under development…"
  },
  {
    "objectID": "projects/dichotomy.html",
    "href": "projects/dichotomy.html",
    "title": "Identifying the Most Significant Dichotomy",
    "section": "",
    "text": "Wealth concentration and inequality have long been central issues in economics, attracting renewed attention as wealth disparities continue to widen. The well-documented Pareto Principle suggests that the top 20% holds approximately 80% of total wealth, but empirical studies indicate that this ratio has become even more skewed over time (Piketty and Saez 2003; Alvaredo et al. 2013; Saez and Zucman 2016). Such trends raise fundamental questions about the underlying dynamics of wealth distribution: Is wealth accumulation a zero-sum game? If so, what is the most evident dichotomy that best exposes this phenomenon?\nAt every moment, newly created capital is introduced into the economic system through production, subsequently distributed among economic agents, and then either dissipated through consumption or retained through investment. Unlike a static framework where total wealth is fixed, real-world economic systems operate as dynamic entities where total wealth evolves over time. The interplay between production, distribution, consumption, and investment determines how wealth is allocated and reallocated across different socioeconomic groups. If the system maintains a stable and proportional allocation of new wealth over time, the relative shares of net worth for different groups remain unchanged. However, if allocation mechanisms fluctuate, then the wealth share of one group necessarily increases at the expense of another—reinforcing a zero-sum dynamic.\nThe key question, therefore, is: How should we partition the population to most clearly expose this zero-sum characteristic? In other words, at which wealth cutoff percentile do we observe the highest absolute correlation between two complementary wealth groups? Should we divide the population into the top 10% versus the bottom 90%? Or is a more extreme division, such as top 0.1% versus bottom 99.9%, more effective in revealing these dynamics?"
  },
  {
    "objectID": "projects/dichotomy.html#introduction",
    "href": "projects/dichotomy.html#introduction",
    "title": "Identifying the Most Significant Dichotomy",
    "section": "",
    "text": "Wealth concentration and inequality have long been central issues in economics, attracting renewed attention as wealth disparities continue to widen. The well-documented Pareto Principle suggests that the top 20% holds approximately 80% of total wealth, but empirical studies indicate that this ratio has become even more skewed over time (Piketty and Saez 2003; Alvaredo et al. 2013; Saez and Zucman 2016). Such trends raise fundamental questions about the underlying dynamics of wealth distribution: Is wealth accumulation a zero-sum game? If so, what is the most evident dichotomy that best exposes this phenomenon?\nAt every moment, newly created capital is introduced into the economic system through production, subsequently distributed among economic agents, and then either dissipated through consumption or retained through investment. Unlike a static framework where total wealth is fixed, real-world economic systems operate as dynamic entities where total wealth evolves over time. The interplay between production, distribution, consumption, and investment determines how wealth is allocated and reallocated across different socioeconomic groups. If the system maintains a stable and proportional allocation of new wealth over time, the relative shares of net worth for different groups remain unchanged. However, if allocation mechanisms fluctuate, then the wealth share of one group necessarily increases at the expense of another—reinforcing a zero-sum dynamic.\nThe key question, therefore, is: How should we partition the population to most clearly expose this zero-sum characteristic? In other words, at which wealth cutoff percentile do we observe the highest absolute correlation between two complementary wealth groups? Should we divide the population into the top 10% versus the bottom 90%? Or is a more extreme division, such as top 0.1% versus bottom 99.9%, more effective in revealing these dynamics?"
  },
  {
    "objectID": "projects/dichotomy.html#data",
    "href": "projects/dichotomy.html#data",
    "title": "Identifying the Most Significant Dichotomy",
    "section": "Data",
    "text": "Data\nOur empirical analysis is based on FRED (Federal Reserve Economic Data), covering quarterly observations from 1989 to 2024. The dataset is structured into wealth brackets representing net wealth shares at different percentile levels:\nGroups (stars)\n\n\\(X_4(t)\\): Share of Net Worth Held by the Top 0.1% (99.9th to 100th Wealth Percentiles) (WFRBSTP1300)\n\\(X_3(t)\\): Share of Net Worth Held by the 99th to 99.9th Wealth Percentiles (WFRBS99T999273)\n\nc.f. Share of Net Worth Held by the Top 1% (99th to 100th Wealth Percentiles) (WFRBST01134)\n\n\\(X_2(t)\\): Share of Net Worth Held by the 90th to 99th Wealth Percentiles (WFRBSN09161)\n\\(X_1(t)\\): Share of Net Worth Held by the 50th to 90th Wealth Percentiles (WFRBSN40188)\n\\(X_0(t)\\): Share of Net Worth Held by the Bottom 50% (1st to 50th Wealth Percentiles) (WFRBSB50215)\n\nAdditionally, we reference wealth cutoff amount to identify the minimum level of wealth required to belong to specific top percentile groups:\nCutoff Levels (bins)\n\n\\(p_4\\) or 99.9th: Minimum Wealth Cutoff for the Top 0.1% (99.9th to 100th Wealth Percentiles) (WFRBLTP1311)\n\\(p_3\\) or 99th: Minimum Wealth Cutoff for the 99th to 99.9th Wealth Percentiles (WFRBL99T999309)\n\\(p_2\\) or 90th: Minimum Wealth Cutoff for the 90th to 99th Wealth Percentiles (WFRBLN09304)\n\\(p_1\\) or 50th: Minimum Wealth Cutoff for the 50th to 90th Wealth Percentiles (WFRBLN40302)"
  },
  {
    "objectID": "projects/dichotomy.html#methodology",
    "href": "projects/dichotomy.html#methodology",
    "title": "Identifying the Most Significant Dichotomy",
    "section": "Methodology",
    "text": "Methodology\nGiven that total share of net wealth must always sum to one, any partition of the population into two groups remains complementary: \\[\nX_0 + X_1 + X_2 + X_3 + X_4 = 1.\n\\]\nTo quantify the most evident dichotomy, we define two complementary wealth groups for different percentile cutoffs:\n\nWhen \\(p = p_1\\):\n\\[\n  a(t) = X_4(t)+X_3(t)+X_2(t)+X_1(t), \\quad b(t) = X_0(t).\n\\]\nWhen \\(p = p_2\\):\n\\[\n  a(t) = X_4(t)+X_3(t)+X_2(t), \\quad b(t) = X_1(t)+X_0(t).\n\\]\nWhen \\(p = p_3\\):\n\\[\n  a(t) = X_4(t)+X_3(t), \\quad b(t) = X_2(t)+X_1(t)+X_0(t).\n\\]\nWhen \\(p = p_4\\):\n\\[\n  a(t) = X_4(t), \\quad b(t) = X_3(t)+X_2(t)+X_1(t)+X_0(t).\n\\]\n\nFor each cutoff \\(p\\), we compute the correlation:\n\\[\n   y(p) = \\mathrm{corr}\\bigl(a(t),\\,b(t)\\bigr).\n\\]\nWe seek the wealth cutoff \\(p\\) that maximizes the absolute correlation \\(|y(p)|\\), revealing the strongest inverse relationship between the two resulting wealth groups. A high absolute correlation suggests that fluctuations in one group’s net worth share are systematically mirrored by the other, reinforcing the zero-sum nature of wealth accumulation. This dichotomy provides insight into how different capital accumulation mechanisms—through labor or capital investment—shape long-term wealth distribution.\nStrong inverse correlations at certain percentiles may indicate critical thresholds where redistribution policies—such as capital taxation or inheritance taxation—could have the most pronounced effects (Piketty 2011; Piketty, Saez, and Stantcheva 2014; Saez and Zucman 2016)."
  },
  {
    "objectID": "projects/dichotomy.html#empirical-results",
    "href": "projects/dichotomy.html#empirical-results",
    "title": "Identifying the Most Significant Dichotomy",
    "section": "Empirical Results",
    "text": "Empirical Results\nAfter processing the quarterly dataset (excluding missing values), we find that the 90th percentile cutoff (\\(p_2\\)) exhibits the highest absolute correlation between the two complementary wealth groups. Specifically, as of 2022-07-01, the minimum wealth required to be in the top 10% was approximately $2,152,788.\nThis suggests that dividing the population into top 10% vs. bottom 90% most effectively reveals the zero-sum nature of wealth redistribution, compared to other partitions such as top 50% vs. bottom 50% or top 0.1% vs. bottom 99.9%.\nThese findings imply that the most structurally significant wealth division occurs between the top 10% and the rest, rather than between the ultra-rich and lower percentiles. This observation aligns with broader discussions on wealth polarization, where the top 10% increasingly dominates capital ownership while the bottom 90% exhibits a more recessive trajectory."
  },
  {
    "objectID": "projects/dichotomy.html#conclusions",
    "href": "projects/dichotomy.html#conclusions",
    "title": "Identifying the Most Significant Dichotomy",
    "section": "Conclusions",
    "text": "Conclusions\nThis study demonstrates that partitioning the population at the 90th wealth percentile provides the most evident dichotomy in revealing the zero-sum nature of capital accumulation. Over time, wealth redistribution mechanisms result in strong inverse correlations between net worth shares of different groups, underscoring the struggling aspects of capital accumulation. The analysis suggests that the structural division between the top 10% and the bottom 90% is more significant than commonly assumed top 1% vs. bottom 99% splits, reinforcing the notion that wealth concentration extends beyond the ultra-rich and affects broader socioeconomic strata.\nThese findings hold important implications for public policy, particularly in debates surrounding progressive taxation, capital gains policies, and inheritance tax structures. A strong inverse correlation at the 90th percentile threshold suggests that redistributive policies targeted at this level could have significant implications for long-term wealth dynamics. This aligns with prior research emphasizing the role of tax policy in shaping wealth accumulation patterns and mitigating excessive concentration of economic power (Piketty, Saez, and Stantcheva 2014; Saez and Zucman 2016).\nWhile this study primarily focuses on empirical correlation analysis, future research should explore additional macroeconomic variables to refine our understanding of wealth distribution dynamics. Incorporating GDP growth rates, investment patterns, labor market structures, and monetary policy changes may provide further insights into how systemic wealth flows evolve in response to economic shocks and policy interventions. Additionally, extending the dataset to include international comparisons could offer a broader perspective on whether the 90th percentile threshold serves as a critical inflection point for wealth inequality across different economies. Further research integrating both empirical and theoretical approaches will be essential in developing more effective strategies for addressing wealth concentration and economic mobility.\n\n\nCode\nimport pandas as pd\nimport pandas_datareader.data as web\nimport matplotlib.pyplot as plt\n\n# 데이터 기간 설정\nstart_date = '1989-07-01'\nend_date = '2024-07-01'\n\n# FRED 데이터 가져오기\nseries_ids = {\n    'X_4': 'WFRBSTP1300',\n    'X_3': 'WFRBS99T999273',\n    'X_2': 'WFRBSN09161',\n    'X_1': 'WFRBSN40188',\n    'X_0': 'WFRBSB50215'\n}\n\ndata = pd.DataFrame()\nfor name, series_id in series_ids.items():\n    data[name] = web.DataReader(series_id, 'fred', start_date, end_date)\n\n# 결측값 제거\ndata.dropna(inplace=True)\n\n\n# FRED에서 wealth level 데이터 가져오기\nwealth_level_ids = {\n    1: 'WFRBLN40302',\n    2: 'WFRBLN09304',\n    3: 'WFRBL99T999309',\n    4: 'WFRBLTP1311'\n}\n\nwealth_levels = {}\nfor p, series_id in wealth_level_ids.items():\n    latest_data = web.DataReader(series_id, 'fred', start_date, end_date).dropna().iloc[-1]\n    wealth_levels[p] = (latest_data.name, latest_data.iloc[0])  # 날짜와 값을 함께 저장\n    \n# 총 관측치 수 출력\nprint(f\"Total number of observations after removing NaN values: {len(data)}\")\n\n\nTotal number of observations after removing NaN values: 141\n\n\n\n\nCode\n# 상관관계 계산 함수\ndef calculate_correlation(a, b):\n    return a.corr(b)\n\n# 상관관계 계산\ncorrelations = {\n    1: calculate_correlation(data['X_4'] + data['X_3'] + data['X_2'] + data['X_1'], data['X_0']),\n    2: calculate_correlation(data['X_4'] + data['X_3'] + data['X_2'], data['X_1'] + data['X_0']),\n    3: calculate_correlation(data['X_4'] + data['X_3'], data['X_2'] + data['X_1'] + data['X_0']),\n    4: calculate_correlation(data['X_4'], data['X_3'] + data['X_2'] + data['X_1'] + data['X_0'])\n}\n\n# 결과 출력\nfor p, y in correlations.items():\n    print(f\"Correlation for p_{p}: {y:.4f}\")\n\n# 최소 상관관계를 가지는 wealth percentile 찾기\nmin_corr_p = min(correlations, key=correlations.get)\npercentile_map = {1: \"50th\", 2: \"90th\", 3: \"99th\", 4: \"99.9th\"}\nprint(f\"Minimum correlation is at p_{min_corr_p}: {percentile_map[min_corr_p]}\")\nwealth_date, wealth_level = wealth_levels[min_corr_p]\nprint(f\"Wealth level for {percentile_map[min_corr_p]} percentile on {wealth_date.date()}: ${wealth_level:,.0f}\")\n\n\n# 그래프 표현\np_values = list(correlations.keys())\ny_values = list(correlations.values())\n\nplt.plot(p_values, y_values, marker='o')\nplt.xlabel('Wealth Cutoff (p)')\nplt.ylabel('Correlation (y(p))')\nplt.title('Wealth Cutoff vs Correlation')\nplt.grid(True)\nplt.show()\n\n\nCorrelation for p_1: -0.9983\nCorrelation for p_2: -0.9998\nCorrelation for p_3: -0.9996\nCorrelation for p_4: -0.9989\nMinimum correlation is at p_2: 90th\nWealth level for 90th percentile on 2022-07-01: $2,153,046"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GitSAM",
    "section": "",
    "text": "비판:상속세 제도 개편\n내가 관심있는 것들…왜? 1."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "GitSAM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n그냥요.↩︎"
  },
  {
    "objectID": "guides/regressions.html",
    "href": "guides/regressions.html",
    "title": "GMM and OLS as Geometry of error space",
    "section": "",
    "text": "This short study is designed to explain the structural difference between Ordinary Least Squares (OLS) and the Generalized Method of Moments (GMM) from a geometric and mathematical standpoint, emphasizing their foundations in inner product spaces, with an analogy to Einstein’s Field Equations in general relativity. This analogy would be powerful not only for deep mathematical understanding but also for teaching estimation theory with geometric and physical intuition."
  },
  {
    "objectID": "guides/regressions.html#ols",
    "href": "guides/regressions.html#ols",
    "title": "GMM and OLS as Geometry of error space",
    "section": "1. OLS",
    "text": "1. OLS\n\nOrthogonal Projection in Euclidean Space\nOLS solves the following problem:\n\\[\n\\hat{\\beta}_{OLS} = \\arg\\min_\\beta \\| y - X\\beta \\|_2^2 = (X^\\top X)^{-1} X^\\top y\n\\]\n\nThis corresponds to projecting \\(y\\) orthogonally onto the column space of \\(X\\).\nThe residual \\(\\varepsilon = y - X\\hat{\\beta}\\) satisfies:\n\n\\[\nX^\\top \\varepsilon = 0\n\\]\n\n\nInner Product and Geometry\n\nThe \\(L^2\\) norm used in OLS is induced by the standard Euclidean inner product:\n\n\\[\n\\langle u, v \\rangle = u^\\top v\n\\]\n\nThe distance function becomes:\n\n\\[\n\\| u \\|_2 = \\sqrt{u^\\top u}\n\\]\n\nThe set of parameter values yielding equal error defines a level set (isocurve):\n\n\\[\n\\{ \\beta \\mid \\| y - X\\beta \\|_2^2 = c \\} \\Rightarrow \\text{spheres in parameter space}\n\\]\n\nThis reflects Pythagorean geometry — the isocurves are circles (in 2D), spheres (in 3D), or hyperspheres in higher dimensions."
  },
  {
    "objectID": "guides/regressions.html#gmm",
    "href": "guides/regressions.html#gmm",
    "title": "GMM and OLS as Geometry of error space",
    "section": "2. GMM",
    "text": "2. GMM\n\nWeighted Projection via Positive Definite Kernel\nGMM generalizes the idea by allowing estimation over a broader space defined by arbitrary moment conditions:\n\\[\n\\hat{\\theta}_{GMM} = \\arg\\min_\\theta \\left[ \\bar{g}_n(\\theta)^\\top W \\bar{g}_n(\\theta) \\right]\n\\]\nwhere:\n\n\\(\\bar{g}_n(\\theta) = \\frac{1}{n} \\sum_{i=1}^n g(Z_i, \\theta)\\)\n\\(W\\) is a positive definite weighting matrix, often an estimate of the optimal variance-covariance structure\n\n\n\nGeneralized Inner Product and Geometry\n\nGMM defines a new inner product:\n\n\\[\n\\langle u, v \\rangle_W = u^\\top W v \\quad \\text{with } W \\succ 0\n\\]\n\nThe corresponding norm is:\n\n\\[\n\\| u \\|_W = \\sqrt{u^\\top W u}\n\\]\n\nThe isocurves of the GMM objective:\n\n\\[\n\\{ \\theta \\mid \\bar{g}_n(\\theta)^\\top W \\bar{g}_n(\\theta) = c \\} \\Rightarrow \\text{ellipsoids in parameter space}\n\\]\nThus, unlike OLS, GMM does not treat all directions equally; the weighting matrix \\(W\\) skews the geometry so that errors in some directions are penalized more."
  },
  {
    "objectID": "guides/regressions.html#visual-and-geometric-summary",
    "href": "guides/regressions.html#visual-and-geometric-summary",
    "title": "GMM and OLS as Geometry of error space",
    "section": "3. Visual and Geometric Summary",
    "text": "3. Visual and Geometric Summary\n\n\n\n\n\n\n\n\nConcept\nOLS\nGMM\n\n\n\n\nInner product\n\\(\\langle u, v \\rangle\\)\n\\(\\langle u, v \\rangle_W = u^\\top W v\\)\n\n\nNorm\nEuclidean (\\(L^2\\))\nMahalanobis-like (\\(W\\)-norm)\n\n\nIsocurve shape\nCircle / Sphere\nEllipse / Ellipsoid\n\n\nGeometry\nUniform in all directions\nAnisotropic (weighted directions)\n\n\n\n\nOLS minimizes error under the geometry of circles.\nGMM minimizes error under the geometry of ellipses."
  },
  {
    "objectID": "guides/regressions.html#einstein-field-equations-and-gmm-structural-analogy",
    "href": "guides/regressions.html#einstein-field-equations-and-gmm-structural-analogy",
    "title": "GMM and OLS as Geometry of error space",
    "section": "4. Einstein Field Equations and GMM: Structural Analogy",
    "text": "4. Einstein Field Equations and GMM: Structural Analogy\nEinstein’s Field Equations (EFE) in general relativity:\n\\[\nG_{\\mu\\nu} = R_{\\mu\\nu} - \\frac{1}{2} R g_{\\mu\\nu} = 8\\pi T_{\\mu\\nu}\n\\]\nwhere:\n\n\\(T_{\\mu\\nu}\\): Stress-energy tensor, representing energy and matter (matter-energy distribution)\n\\(G_{\\mu\\nu}\\): Einstein tensor, encoding the curvature of spacetime (curvature)\n\n\\(R_{\\mu\\nu}\\): Ricci curvature tensor\n\\(R\\): Scalar curvature\n\\(g_{\\mu\\nu}\\): Metric tensor, defining the inner product in spacetime and governing geodesics\n\n\n\nAnalogy to Estimation Frameworks\n\n\n\n\n\n\n\n\n\nFeature\nOLS\nGMM\nEFE (Physics)\n\n\n\n\nSpace\nEuclidean\nKernel-defined\nCurved spacetime\n\n\nInner product\n\\(I\\)\n\\(W\\) (kernel)\n\\(g_{\\mu\\nu}\\) (metric)\n\n\nProjection\nOrthogonal\nWeighted / Generalized\nEnergy-curvature balance\n\n\nLevel sets\nCircles\nEllipses\nLightcones / geodesics\n\n\n\n\n\nAdditional Explanation\nIn EFE, the metric tensor \\(g_{\\mu\\nu}\\) defines how distances and angles are measured — it is the analogue of a positive definite kernel in GMM. The field equations determine how the geometry (curvature) of spacetime reacts to matter and energy. In this sense, spacetime is optimized or shaped in response to external inputs, just as GMM shapes its estimation space based on the kernel \\(W\\) and moment functions.\nThe level sets in general relativity are often visualized as lightcones — the surface separating causal influence from spacelike separation. Geometrically, a lightcone can be interpreted as the degenerate case of a conic section, where the quadric form\n\\[\nQ(x) = x^\\top g_{\\mu\\nu} x = 0\n\\]\nresults in a pair of intersecting lines: this represents all null (light-like) directions emanating from a point. These are the boundary cases between time-like and space-like intervals, analogous to the way ellipsoids in GMM collapse into degenerate forms under singular kernel matrices.\nThus, in both GMM and EFE, the shape and degeneracy of level sets encode deep information about the underlying structure — whether it is a statistical model or the geometry of spacetime.\n\n\nAnalogy to Estimation Frameworks\n\n\n\n\n\n\n\n\n\nFeature\nOLS\nGMM\nEFE (Physics)\n\n\n\n\nSpace\nEuclidean\nKernel-defined\nCurved spacetime\n\n\nInner product\n\\(I\\)\n\\(W\\) (kernel)\n\\(g_{\\mu\\nu}\\) (metric)\n\n\nProjection\nOrthogonal\nWeighted / Generalized\nEnergy-curvature balance\n\n\nLevel sets\nCircles\nEllipses\nLightcones / geodesics\n\n\n\nIn all three cases, the key structure-defining object (\\(I\\), \\(W\\), or \\(g_{\\mu\\nu}\\)) defines how vectors are compared, how error or curvature is measured, and how optimization or balance occurs."
  },
  {
    "objectID": "guides/regressions.html#conclusion",
    "href": "guides/regressions.html#conclusion",
    "title": "GMM and OLS as Geometry of error space",
    "section": "5. Conclusion",
    "text": "5. Conclusion\n\nOLS and GMM are not just estimation techniques, but geometric procedures grounded in inner product space theory.\nOLS relies on Euclidean projection, yielding circular/spherical symmetry.\nGMM generalizes the space through a positive definite kernel, yielding elliptical contours and emphasizing certain directions.\nThis parallels how general relativity defines geometry via the metric tensor, adapting the very notion of measurement to the structure of the system."
  },
  {
    "objectID": "guides/regressions.html#visuals",
    "href": "guides/regressions.html#visuals",
    "title": "GMM and OLS as Geometry of error space",
    "section": "Visuals",
    "text": "Visuals\n\nOLS vs GMM : 동일한 선형 회귀 구조에서도 서로 다른 projection\n설정 요약:\n\n데이터 생성:\n\n\\(y = \\beta_0 + \\beta_1 x + \\varepsilon\\)\n잡음 \\(\\varepsilon\\)는 \\(x\\)의 값이 커질수록 분산이 커지는 이질적(heteroskedastic) 형태로 생성\n\nOLS:\n\n모든 관측값을 동일한 중요도로 간주하여 잔차 제곱합을 최소화. 즉, 평균적인 방향으로 선을 맞춤.\nOLS는 전체 데이터를 균등하게 반영하며, 잡음이 큰 부분에서도 기울기가 영향을 받음.\n\nGMM:\n\n잔차의 분산(또는 신뢰도)에 따라 가중치 positive definite weighting를 달리 부여. 이 경우 분산의 역수를 사용하여 노이즈가 적은 관측값을 더 신뢰하도록 추정.\nGMM은 잡음이 작은 구간(왼쪽)의 패턴에 더 많은 가중치를 부여하여 기울기가 더 가파르게 추정\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulate data with heteroskedastic noise (to favor GMM adjustment)\nnp.random.seed(0)\nn = 100\nx = np.linspace(0, 10, n)\nX = np.vstack([np.ones(n), x]).T\n\n# True model\nbeta_true = np.array([1, 2])\n# Heteroskedastic noise: variance increases with x\nnoise_std = 0.5 + 1.5 * (x / x.max())  # ranges from 0.5 to 2.0\ny = X @ beta_true + np.random.normal(0, noise_std)\n\n# OLS estimation\nbeta_ols = np.linalg.inv(X.T @ X) @ X.T @ y\ny_hat_ols = X @ beta_ols\n\n# GMM weighting: inverse of variance (precision weighting)\nW = np.diag(1 / noise_std**2)\n\n# GMM estimation (optimal weighting under heteroskedasticity)\nXTWX = X.T @ W @ X\nXTWy = X.T @ W @ y\nbeta_gmm = np.linalg.inv(XTWX) @ XTWy\ny_hat_gmm = X @ beta_gmm\n\n# Plot with aspect ratio 1:1\nplt.figure(figsize=(8, 8))\nplt.scatter(x, y, color='lightgray', label='Observed data')\nplt.plot(x, y_hat_ols, label='OLS projection', color='blue', linewidth=2)\nplt.plot(x, y_hat_gmm, label='GMM projection (precision-weighted)', color='red', linestyle='--', linewidth=2)\nplt.title(\"OLS vs GMM Projection with Heteroskedastic Data\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.axis('equal')  # Set equal aspect ratio\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nobjective function (quadratic form)의 level set\n참고: GMM의 목적함수는 원래부터 정규화(normalized)되어 있는 반면, OLS의 목적함수는 정규화 없이 나타나는 일반적 2차형식이므로, 두 방법을 엄밀히 비교하려면 OLS도 동일한 형태로 정규화해 주어야 함.\n\n\n\n\n\n\n\n\n\n방법\n목적함수 형태\n거리해석\nLevel set 형태\n\n\n\n\nOLS (일반형)\n\\((y - X\\beta)^\\top (y - X\\beta)\\)\nEuclidean norm\n타원 (elliptical)\n\n\nOLS (정규화형)\n\\((\\beta - \\hat{\\beta})^\\top (X^\\top X)^{-1} (\\beta - \\hat{\\beta})\\)\nMahalanobis norm\n구형 (spherical)\n\n\nGMM\n\\(\\bar{g}(\\theta)^\\top W \\bar{g}(\\theta)\\)\nMahalanobis norm\n타원 또는 구형\n\n\n\n\n\nCode\n# Z-score normalization of x to improve XtX condition\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nx_normalized = scaler.fit_transform(x.reshape(-1, 1)).flatten()\nX_normalized = np.vstack([np.ones(n), x_normalized]).T\n\n# Recalculate OLS and GMM using normalized X\nbeta_ols_norm = np.linalg.inv(X_normalized.T @ X_normalized) @ X_normalized.T @ y\ny_hat_ols_norm = X_normalized @ beta_ols_norm\n\n# GMM estimation with same W\nXTWX_norm = X_normalized.T @ W @ X_normalized\nXTWy_norm = X_normalized.T @ W @ y\nbeta_gmm_norm = np.linalg.inv(XTWX_norm) @ XTWy_norm\ny_hat_gmm_norm = X_normalized @ beta_gmm_norm\n\n# New grid around beta_ols_norm\nb0_vals = np.linspace(beta_ols_norm[0] - 1, beta_ols_norm[0] + 1, 100)\nb1_vals = np.linspace(beta_ols_norm[1] - 1, beta_ols_norm[1] + 1, 100)\nB0, B1 = np.meshgrid(b0_vals, b1_vals)\nB_flat = np.vstack([B0.ravel(), B1.ravel()])\n\n# Normalized OLS objective (Mahalanobis)\nXtX_inv_norm = np.linalg.inv(X_normalized.T @ X_normalized)\ndelta_norm = B_flat - beta_ols_norm[:, None]\nJ_ols_normalized = np.einsum('ji,jk,ki-&gt;i', delta_norm, XtX_inv_norm, delta_norm).reshape(B0.shape)\n\n# GMM objective with normalized X\nJ_gmm_norm = []\nfor i in range(B_flat.shape[1]):\n    r = y - X_normalized @ B_flat[:, i]\n    obj = r.T @ W @ r\n    J_gmm_norm.append(obj)\nJ_gmm_norm = np.array(J_gmm_norm).reshape(B0.shape)\n\n# Plotting\nfig, axs = plt.subplots(1, 2, figsize=(14, 6))\n\n# Normalized OLS (should be spherical)\ncs1 = axs[0].contour(B0, B1, J_ols_normalized, levels=20, cmap='Blues')\naxs[0].plot(beta_ols_norm[0], beta_ols_norm[1], 'bo', label='OLS solution')\naxs[0].set_title(\"OLS (Normalized X): Spherical Level Sets\")\naxs[0].set_xlabel(r\"$\\beta_0$\")\naxs[0].set_ylabel(r\"$\\beta_1$\")\naxs[0].axis('equal')\naxs[0].legend()\naxs[0].grid(True)\n\n# GMM with normalized X\ncs2 = axs[1].contour(B0, B1, J_gmm_norm, levels=20, cmap='Reds')\naxs[1].plot(beta_gmm_norm[0], beta_gmm_norm[1], 'ro', label='GMM solution')\naxs[1].set_title(\"GMM (Normalized X): Ellipsoidal Level Sets\")\naxs[1].set_xlabel(r\"$\\beta_0$\")\naxs[1].set_ylabel(r\"$\\beta_1$\")\naxs[1].axis('equal')\naxs[1].legend()\naxs[1].grid(True)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "guides/labor_decoupling.html",
    "href": "guides/labor_decoupling.html",
    "title": "노동: 기여한 만큼 분배가 되었나?",
    "section": "",
    "text": "Neoclassical Growth Model(신고전적 성장 모형)은 경제성장의 핵심 지표로 총생산량(Gross Domestic Product, GDP)을 중시한다. 이 모형에서는 생산의 핵심 요소를 다음과 같이 세 가지로 구분한다 (Solow 1956).\n\n총 노동량(Labor Input, \\(L\\))\n\n총 자본량(Capital Input, \\(K\\))\n\n노동증강기술(Labor-Augmenting Technology, \\(A\\))\n\n그러나 자본량(\\(K\\))은 직접적으로 측정하기 어려우며, 일반적으로 Perpetual Inventory Method(영속적 재고법)과 같은 방식을 활용하여 추정한다 (Hall and Jones 1999). 이에 더하여, 노동증강기술(\\(A\\))은 더욱 측정이 어려운 정성적 요소이기 때문에, 많은 연구에서는 Constant Returns to Scale(규모에 대한 수익 불변)을 가정한 Cobb-Douglas 생산함수를 이용하고, 추정 과정에서 발생하는 오차항(Error Term)을 기술진보(\\(A\\))로 대체하기도 한다 (Barro 1991). 이러한 방식이 현실을 얼마나 정확하게 반영하는지는 논란의 여지가 크다.\n모형(Model)은 본질적으로 이론적 가설이며, 경제성장에 대한 다양한 견해를 반영할 수 있는 도구이기 때문에, 각자의 가설과 주장을 펼치는 것은 학문적으로 허용될 수 있다. 그러나 문제의 본질은 해당 모형이 현실을 얼마나 정확하게 설명하고 예측할 수 있는지에 있다. 현실을 반영하지 못하고, 지속적으로 예측에 실패하는 경제성장 모형은 실질적으로 활용 가치가 떨어질 수밖에 없다 (Stiglitz 2017).\n경제성장과 관련하여 실용적인 측면에서 가장 중요한 변수는 총 노동량(\\(L\\))이라고 볼 수 있다. 이는 비교적 객관적인 측정이 가능하며, 경제의 실질적 변화를 분석하는 데 유용하다. 이러한 맥락에서 노동 생산성(Productivity)과 노동 분배(Labor Compensation)라는 두 가지 핵심 개념을 살펴볼 필요가 있다."
  },
  {
    "objectID": "guides/labor_decoupling.html#서론",
    "href": "guides/labor_decoupling.html#서론",
    "title": "노동: 기여한 만큼 분배가 되었나?",
    "section": "",
    "text": "Neoclassical Growth Model(신고전적 성장 모형)은 경제성장의 핵심 지표로 총생산량(Gross Domestic Product, GDP)을 중시한다. 이 모형에서는 생산의 핵심 요소를 다음과 같이 세 가지로 구분한다 (Solow 1956).\n\n총 노동량(Labor Input, \\(L\\))\n\n총 자본량(Capital Input, \\(K\\))\n\n노동증강기술(Labor-Augmenting Technology, \\(A\\))\n\n그러나 자본량(\\(K\\))은 직접적으로 측정하기 어려우며, 일반적으로 Perpetual Inventory Method(영속적 재고법)과 같은 방식을 활용하여 추정한다 (Hall and Jones 1999). 이에 더하여, 노동증강기술(\\(A\\))은 더욱 측정이 어려운 정성적 요소이기 때문에, 많은 연구에서는 Constant Returns to Scale(규모에 대한 수익 불변)을 가정한 Cobb-Douglas 생산함수를 이용하고, 추정 과정에서 발생하는 오차항(Error Term)을 기술진보(\\(A\\))로 대체하기도 한다 (Barro 1991). 이러한 방식이 현실을 얼마나 정확하게 반영하는지는 논란의 여지가 크다.\n모형(Model)은 본질적으로 이론적 가설이며, 경제성장에 대한 다양한 견해를 반영할 수 있는 도구이기 때문에, 각자의 가설과 주장을 펼치는 것은 학문적으로 허용될 수 있다. 그러나 문제의 본질은 해당 모형이 현실을 얼마나 정확하게 설명하고 예측할 수 있는지에 있다. 현실을 반영하지 못하고, 지속적으로 예측에 실패하는 경제성장 모형은 실질적으로 활용 가치가 떨어질 수밖에 없다 (Stiglitz 2017).\n경제성장과 관련하여 실용적인 측면에서 가장 중요한 변수는 총 노동량(\\(L\\))이라고 볼 수 있다. 이는 비교적 객관적인 측정이 가능하며, 경제의 실질적 변화를 분석하는 데 유용하다. 이러한 맥락에서 노동 생산성(Productivity)과 노동 분배(Labor Compensation)라는 두 가지 핵심 개념을 살펴볼 필요가 있다."
  },
  {
    "objectID": "guides/labor_decoupling.html#본론",
    "href": "guides/labor_decoupling.html#본론",
    "title": "노동: 기여한 만큼 분배가 되었나?",
    "section": "본론",
    "text": "본론\n\n1. 노동생산성(Labor Productivity)의 변화\n노동생산성은 단위 노동량당 산출량을 의미하며, 다음과 같이 정의할 수 있다 (Acemoglu et al. 2014).\n\\[\n\\text{노동생산성} = \\frac{Y_t}{L_t}\n\\]\n여기서 \\(Y_t\\)는 총생산량(GDP)의 대표적 대리 변수(proxy)이며, \\(L_t\\)는 총 노동량을 나타낸다. 각각의 변수는 다음과 같이 측정하였다.\n\n총생산량 Proxy (\\(Y_t\\)): Real Gross Domestic Product per Capita\n\n장기적으로 볼 때, 총생산량은 시간에 따라 증가하는 추세를 보였다.\n\n총노동량 Proxy (\\(L_t\\)): Hours Worked by Full-Time and Part-Time Employees\n\n이 역시 시간에 따라 증가하는 경향을 보였다.\n\n\n이로부터 계산한 노동생산성(\\(Y_t/L_t\\)) 역시 시간에 따라 증가하는 함수임을 확인할 수 있다. 즉, 경제가 성장함에 따라 노동자 1인당 산출하는 생산량은 꾸준히 증가해 왔다.\n\n\n2. 노동분배량(Labor Compensation)의 변화\n노동생산성이 증가하면, 일반적으로 노동자에게 돌아가는 보상 역시 증가해야 한다는 것이 경제적 정의(economic fairness)와 균형적 성장(balanced growth)의 핵심 원칙이다 (Piketty 2014). 그렇다면 노동 분배(Labor Compensation) 역시 시간에 따라 증가하였을까?\n이를 확인하기 위해, 노동자의 평균 실질 소득을 대리 변수로 활용하였다.\n\n노동에 분배된 총량 Proxy (\\(X_t\\)): Employed Full-Time: Median Usual Weekly Real Earnings\n\n장기적으로 증가하는 경향을 보였으나, 변동성이 존재하였다.\n\n노동분배량 (\\(\\frac{X_t}{L_t}\\))\n\n노동생산성이 증가하는 것과는 대조적으로, 노동자에게 분배된 소득의 비율(\\(X_t/L_t\\))은 오히려 시간에 따라 감소하는 경향을 보였다.\n\n\n이를 시각적으로 명확히 비교하기 위해, 노동분배량을 \\(\\frac{X_t}{L_t} \\times 100\\)으로 스케일링하여 그래프로 나타냈다.\nFRED Graph (1980년 이후 노동생산성과 노동분배량 비교)\n그래프를 살펴보면, 1980년대 이후 노동생산성과 노동분배량 사이의 격차가 점점 더 커지고 있음을 확인할 수 있다. 이는 무엇을 의미하는가? 노동생산성이 증가함에도 불구하고, 노동자에게 돌아가는 보상의 증가 속도가 이에 미치지 못하고 있다는 점을 시사한다. 다시 말해, “노동생산성과 노동자 보상의 분리 현상(decoupling)”이 지속적으로 심화되고 있음을 보여준다 (Stansbury and Summers 2020). 이러한 현상은 장기적으로 경제적 불평등(economic inequality)을 악화시키는 주요 원인 중 하나로 작용할 수 있다 (Stiglitz 2012).\n\n\n3. 노동 소득 분배율 감소의 원인\n노동소득 분배율(Labor Share)의 감소에 대한 경제학적 분석은 다양한 요인을 고려해야 하지만, 주요한 원인으로 다음 두 가지를 지적할 수 있다.\n\n기술 진보(Technological Progress)\n\nOECD의 분석에 따르면, 인공지능(AI)과 같은 첨단 기술의 발전은 특정 고숙련 노동자(high-skilled workers)에게는 유리하게 작용하지만, 그렇지 않은 노동자들에게는 불리한 영향을 미칠 수 있다 (OECD 2018). 이는 노동시장 내 임금 불평등(wage inequality)을 심화시키는 요인으로 작용한다.\n\n세계화(Globalization)\n\n생산 공정의 해외 이전(offshoring)과 국제 무역의 확대는 저임금 노동력을 활용한 생산 방식을 증가시켜, 선진국 내 노동자의 소득 증가율을 둔화시키는 결과를 초래할 수 있다 (Autor, Dorn, and Hanson 2013).\n\n\n\n\n4. 노동참여율(Labor Force Participation Rate)의 변화\n또한, 노동소득 분배율 감소와 노동시장 변화를 분석하기 위해, 노동참여율(Labor Force Participation Rate)을 함께 고려할 필요가 있다. 이를 위해 Current Population Survey (CPS)에서 조사한 Labor Force Participation Rate를 분석하였다.\n\nCOVID-19 팬데믹이 발생한 2020년에는 노동참여율이 일시적으로 급락했으나, 이후 빠르게 정상 수준으로 회복되었다 (Coibion, Gorodnichenko, and Weber 2020).\n\n장기적으로는 완만한 하락세를 보이고 있으며, 이는 인구 고령화(demographic aging) 등의 요인과도 관련이 있을 것으로 추정된다 (Krueger 2017)."
  },
  {
    "objectID": "guides/labor_decoupling.html#결론",
    "href": "guides/labor_decoupling.html#결론",
    "title": "노동: 기여한 만큼 분배가 되었나?",
    "section": "결론",
    "text": "결론\n노동생산성과 노동자 보상의 분리 현상(decoupling)은 실증적 데이터에 의해 명확히 확인된다. 노동생산성이 지속적으로 증가함에도 불구하고, 노동자에게 분배되는 소득의 비율은 점진적으로 감소해 왔다 (Karabarbounis and Neiman 2014). 이러한 현상의 주요 원인으로 기술 진보와 세계화가 지목되며, 장기적으로는 노동시장 내 불평등 심화 및 경제적 불안정성을 초래할 가능성이 크다 (Milanovic 2016). 향후 연구에서는 정책적 대응 방안과 노동소득 분배율 회복을 위한 제도적 개선책을 추가적으로 검토할 필요가 있다."
  },
  {
    "objectID": "guides/index.html",
    "href": "guides/index.html",
    "title": "Guides",
    "section": "",
    "text": "Educational Guides\nThis section provides basic tutorials and conceptual guides in economics, finance, and data analysis."
  },
  {
    "objectID": "guides/correlation_crypto.html",
    "href": "guides/correlation_crypto.html",
    "title": "Correlation within Crypto-currencies",
    "section": "",
    "text": "Abstract: 2025년 3월 현재, 시가총액이 크거나 투자자들에게 인기가 많은 주요 암호화폐(popular cryptocurrencies)를 선정하여 지난 1년간의 상관관계를 분석하였다. 대부분의 암호화폐 투자자들은 이러한 주요 암호화폐에 집중적으로 투자하는 경향이 있다. 한편, 암호화폐 자산에 대한 투자자의 평균 투자 기간은 단기(short-term)로, 일반적으로 1개월에서 3개월 사이에 해당한다. 이에 따라 본 연구에서는 데이터의 관측 빈도(observation frequency)를 일간(daily) 단위로 설정하고, 30일, 60일, 90일의 롤링 윈도우(rolling window)를 적용하여 주요 암호화폐 수익률의 선형 상관계수(Pearson’s coefficient)를 분석하였다. 이러한 분석은 변동성 헤징(volatility hedging)을 고려한 분산 투자(diversified investment) 전략 수립에 도움이 될 수 있다. 예를 들어, 일정한 투자 금액(예: 1억 원)을 주요 암호화폐 자산군 내에서 어떻게 배분할지 결정하는 데 있어, 상관계수 분석 결과가 투자 비중 조정에 유용한 정보를 제공할 것으로 기대된다."
  },
  {
    "objectID": "guides/correlation_crypto.html#서론",
    "href": "guides/correlation_crypto.html#서론",
    "title": "Correlation within Crypto-currencies",
    "section": "서론",
    "text": "서론\n비트코인(BTC)의 가격 및 수익률은, 단기적으로 다음과 같은 관계를 보여왔다.\n\nNDX (나스닥 100 지수)와 강한 양의 상관관계 (Nasdaq 2024, 2023),\nDXY (미국 달러 지수)와 강한 음의 상관관계 (Coindesk 2023; Coinglass 2023). 만약 비트코인 가격이 달러 가격과 장기적으로도 반대 방향으로 움직인다면, 이는 비트코인이 인플레이션 헤지 자산으로 간주될 수도 있을 가능성을 나타낸다 (Dyhrberg 2021).\n금 가격 (GOLD), 국내 실질 총생산량 (GDP) 과의 상관관계는 불명확하거나 간접적인 것으로 알려져 있음 (Cointelegraph 2023; Cryptoslate 2022).\n\n역사적 사례\n\n2020년 COVID-19 위기 이후 BTC와 NDX의 상관관계가 강화됨 (Nasdaq 2020),\n2022년 5월 연방준비제도(Fed)의 금리 인상 발표 당시, BTC와 NDX 모두 하락.\n2023년 비트코인 빙하기 기간 동안 BTC와 NDX의 상관관계 변화 분석 필요.\n2024년 3월 비트코인 ETF가 출시하여 기관 투자자 참여가 증가와 함께께 BTC과 NDX의 coupling이 심해짐.\n\n\n주요 암호화폐 목록 및 카테고리\n\n\n\n\n\n\n\n\n암호화폐 (Cryptocurrency)\n심볼 (Ticker)\n카테고리 (Category)\n\n\n\n\n비트코인 (Bitcoin)\nBTC/USD\nLayer 1\n\n\n이더리움 (Ethereum)\nETH/USD\nLayer 1, Smart Contract\n\n\n테더 (Tether)\nUSDT/USD\nStablecoin\n\n\n리플 (XRP)\nXRP/USD\nPayment Network\n\n\n솔라나 (Solana)\nSOL/USD\nLayer 1\n\n\n체인링크 (Chainlink)\nLINK/USD\nOracle\n\n\n온도 (Ondo)\nONDO/USD\nReal-World Asset (RWA)\n\n\n카르다노 (Cardano)\nADA/USD\nLayer 1\n\n\n트론 (Tron)\nTRX/USD\nLayer 1\n\n\n도지코인 (Dogecoin)\nDOGE/USD\nMeme Coin\n\n\n\n\n\n암호화폐 관련 정보 제공 매체 리뷰\n\n시세 데이터 (Price Data): 실시간 및 과거 가격 변동, 거래량(volume) 등\n\nCoinMarketCap\nCoinGecko\n\n온체인 데이터 (On-Chain Data): 거래량, 지갑 주소 변화, 네트워크 활성도 등\n\nGlassnode\nIntoTheBlock\n\n시장 분석 (Market Analysis): 전문가 및 AI 기반 분석 리포트\n\nMessari\nCryptoQuant\n\n뉴스 및 이벤트 (News & Events): 프로젝트 업데이트, 규제 변화 등\n\nCoinDesk\nThe Block\n\n소셜 미디어 분석 (Social Media Analysis): 트위터(X), 레딧(Reddit) 등에서의 커뮤니티 반응\n\nLunarCrush\nSantiment"
  },
  {
    "objectID": "guides/correlation_crypto.html#데이터-분석",
    "href": "guides/correlation_crypto.html#데이터-분석",
    "title": "Correlation within Crypto-currencies",
    "section": "데이터 분석",
    "text": "데이터 분석\n\n데이터\n\n데이터 소스: CCXT\n데이터 기간: 2024년 3월 1일 - 2025년 2월 28일\n데이터 빈도 (Data Frequency): 일간(Daily)\n분석 대상 암호화폐:\n\nBTC/USD, ETH/USD, USDT/USD, XRP/USD, SOL/USD, LINK/USD, ONDO/USD, ADA/USD, TRX/USD, DOGE/USD\n\n롤링 윈도우 크기 (Rolling Window Size): 30일, 60일, 90일\n\n\n\n분석 방법\n\n암호화폐의 일간 수익률(daily return)을 계산.\n각 롤링 윈도우 크기(30, 60, 90일)에 대해 롤링 상관 행렬(rolling correlation matrix)을 계산.\n평균 상관계수(mean of rolling correlation matrix)를 도출하여 암호화폐 간의 관계를 분석.\n\n\n\nCode\n# 분석 결과 (Results)\n\n# 여러 거래소에서 지원하는 거래쌍을 확인\n\nimport ccxt\nimport pandas as pd\n\n# 주요 암호화폐 목록\nTICKER_COIN = ['BTC/USDT', 'ETH/USDT', 'USDT/USD', 'XRP/USDT', 'SOL/USDT', 'LINK/USDT', 'ONDO/USDT', 'ADA/USDT', 'TRX/USDT', 'DOGE/USDT']\n\n# 지원하는 거래소 목록\nexchanges = ['binance', 'kraken', 'bitfinex', 'poloniex']\n\n# 각 거래소에서 지원하는 거래쌍 확인\nfor exchange_id in exchanges:\n    exchange = getattr(ccxt, exchange_id)()\n    markets = exchange.load_markets()\n    supported_pairs = [pair for pair in TICKER_COIN if pair in markets]\n    print(f\"{exchange_id} supports: {supported_pairs}\")\n\n# 주요 암호화폐 목록\nbinance_tickers = ['BTC/USDT', 'ETH/USDT', 'XRP/USDT', 'SOL/USDT', 'LINK/USDT', 'ADA/USDT', 'TRX/USDT', 'DOGE/USDT']\nkraken_tickers = ['USDT/USD']\npoloniex_tickers = ['ONDO/USDT']\n\n# 데이터 기간 설정\nSTART_DATE = '2024-03-01'\nEND_DATE = '2025-02-28'\n\n# 거래소 설정\nbinance = ccxt.binance()\nkraken = ccxt.kraken()\npoloniex = ccxt.poloniex()\n\n# 데이터 불러오기 함수\ndef fetch_crypto_data(exchange, tickers, start, end):\n    data = {}\n    start_timestamp = exchange.parse8601(f'{start}T00:00:00Z')\n    end_timestamp = exchange.parse8601(f'{end}T00:00:00Z')\n    for ticker in tickers:\n        try:\n            ohlcv = exchange.fetch_ohlcv(ticker, '1d', since=start_timestamp, limit=1000)\n            df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n            df.set_index('timestamp', inplace=True)\n            data[ticker] = df['close']\n        except Exception as e:\n            print(f\"Error fetching {ticker} from {exchange.id}: {e}\")\n    return pd.DataFrame(data)\n\n# 데이터 불러오기\nbinance_data = fetch_crypto_data(binance, binance_tickers, START_DATE, END_DATE)\nkraken_data = fetch_crypto_data(kraken, kraken_tickers, START_DATE, END_DATE)\npoloniex_data = fetch_crypto_data(poloniex, poloniex_tickers, START_DATE, END_DATE)\n\n# 모든 데이터를 하나의 DataFrame으로 병합\ncrypto_prices = pd.concat([binance_data, kraken_data, poloniex_data], axis=1)\n\n# 1) 일간 수익률 계산\ndef compute_returns(price_data: pd.DataFrame) -&gt; pd.DataFrame:\n    return price_data.pct_change().dropna(how='all')\n\ncrypto_returns = compute_returns(crypto_prices)\n\n# 2) 롤링 상관계수 계산\ndef rolling_correlation(returns: pd.DataFrame, window: int) -&gt; pd.DataFrame:\n    \"\"\"\n    returns: (date x tickers) DataFrame\n    window:  rolling window size (days)\n    \n    returns.rolling(window).corr() 결과는\n      - MultiIndex (date, ticker1)\n      - columns = ticker2\n    형태를 가집니다.\n    \"\"\"\n    corr_rolling = returns.rolling(window).corr()\n    return corr_rolling\n\n# 3) 날짜별 상관행렬을 모아서 평균 상관행렬을 산출\ndef average_correlation_matrix(returns: pd.DataFrame, window: int) -&gt; pd.DataFrame:\n    \"\"\"\n    - returns.rolling(window).corr() 결과를 사용\n    - 각 날짜별 (티커 x 티커) 상관행렬을 합산 후, 날짜 개수로 나누어 평균\n    \"\"\"\n    corr_rolling = rolling_correlation(returns, window)\n    \n    # MultiIndex에서 날짜(level=0) 목록을 추출\n    unique_dates = corr_rolling.index.get_level_values(0).unique()\n    tickers = returns.columns\n    \n    # 상관행렬 누적 합을 위한 (티커 x 티커) 형태의 빈 DataFrame\n    sum_matrix = pd.DataFrame(0.0, index=tickers, columns=tickers)\n    count = 0\n    \n    for date in unique_dates:\n        # (ticker1 x ticker2) 형태를 얻기 위해 xs(date, level=0)\n        date_corr = corr_rolling.xs(date, level=0)\n        # date_corr.index = ticker1, date_corr.columns = ticker2\n        \n        # 혹시 일부 티커에 대한 데이터가 누락되었을 경우를 대비하여 reindex\n        date_corr = date_corr.reindex(index=tickers, columns=tickers)\n        \n        # 날짜별 상관행렬(N x N)을 모두 누적\n        if date_corr.notna().all().all():\n            sum_matrix += date_corr.fillna(0.0)\n            count += 1\n    \n    # 평균 계산 (count가 0이 되지 않는다고 가정)\n    mean_matrix = sum_matrix / count\n    \n    return mean_matrix\n\n# 4) 롤링 상관계수 평균 계산\nrolling_corr_results = {}\nfor window in [30, 60, 90]:\n    mean_corr_matrix = average_correlation_matrix(crypto_returns, window)\n    rolling_corr_results[window] = mean_corr_matrix\n\n\nbinance supports: ['BTC/USDT', 'ETH/USDT', 'XRP/USDT', 'SOL/USDT', 'LINK/USDT', 'ADA/USDT', 'TRX/USDT', 'DOGE/USDT']\nkraken supports: ['BTC/USDT', 'ETH/USDT', 'USDT/USD', 'XRP/USDT', 'SOL/USDT', 'LINK/USDT', 'ADA/USDT', 'DOGE/USDT']\nbitfinex supports: ['BTC/USDT', 'ETH/USDT', 'USDT/USD', 'XRP/USDT', 'SOL/USDT', 'LINK/USDT', 'ADA/USDT', 'TRX/USDT', 'DOGE/USDT']\npoloniex supports: ['BTC/USDT', 'ETH/USDT', 'XRP/USDT', 'SOL/USDT', 'LINK/USDT', 'ONDO/USDT', 'ADA/USDT', 'TRX/USDT', 'DOGE/USDT']\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# 모든 행과 열이 출력되도록 설정\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.expand_frame_repr', False)\n\n# 결과 출력 및 시각화\nfor window, result in rolling_corr_results.items():\n    # 상관 행렬을 DataFrame으로 변환\n    result_df = result.dropna(how='all')\n    print(f\"\\n[Window = {window} days] Mean Correlation Matrix\\n\", result_df)\n    \n    # 히트맵 시각화\n    plt.figure(figsize=(10, 8))\n    \n    # 대각선 요소를 마스킹\n    mask = np.triu(np.ones(result_df.shape, dtype=bool))\n    \n    sns.heatmap(result_df, annot=True, cmap='coolwarm', center=0, mask=mask)\n    plt.title(f'Mean Rolling Correlation Matrix (Window Size: {window} days)')\n    plt.show()\n\n\n\n[Window = 30 days] Mean Correlation Matrix\n            BTC/USDT  ETH/USDT  XRP/USDT  SOL/USDT  LINK/USDT  ADA/USDT  TRX/USDT  DOGE/USDT  USDT/USD  ONDO/USDT\nBTC/USDT   1.000000  0.799023  0.571409  0.742674   0.685759  0.705127  0.382673   0.813506  0.453075   0.349189\nETH/USDT   0.799023  1.000000  0.560924  0.704025   0.738146  0.719168  0.380368   0.747405  0.287549   0.379388\nXRP/USDT   0.571409  0.560924  1.000000  0.555324   0.571432  0.654387  0.316596   0.593454  0.168924   0.231347\nSOL/USDT   0.742674  0.704025  0.555324  1.000000   0.674793  0.684855  0.319976   0.695519  0.256270   0.302724\nLINK/USDT  0.685759  0.738146  0.571432  0.674793   1.000000  0.758199  0.312546   0.670449  0.235565   0.416409\nADA/USDT   0.705127  0.719168  0.654387  0.684855   0.758199  1.000000  0.414425   0.725036  0.259016   0.326216\nTRX/USDT   0.382673  0.380368  0.316596  0.319976   0.312546  0.414425  1.000000   0.368036  0.185100   0.113918\nDOGE/USDT  0.813506  0.747405  0.593454  0.695519   0.670449  0.725036  0.368036   1.000000  0.316106   0.348871\nUSDT/USD   0.453075  0.287549  0.168924  0.256270   0.235565  0.259016  0.185100   0.316106  1.000000   0.239804\nONDO/USDT  0.349189  0.379388  0.231347  0.302724   0.416409  0.326216  0.113918   0.348871  0.239804   1.000000\n\n\n\n\n\n\n\n\n\n\n[Window = 60 days] Mean Correlation Matrix\n            BTC/USDT  ETH/USDT  XRP/USDT  SOL/USDT  LINK/USDT  ADA/USDT  TRX/USDT  DOGE/USDT  USDT/USD  ONDO/USDT\nBTC/USDT   1.000000  0.795082  0.519163  0.740573   0.665564  0.688032  0.317942   0.814901  0.443909   0.343185\nETH/USDT   0.795082  1.000000  0.500093  0.688049   0.711332  0.691410  0.307392   0.727154  0.292628   0.368087\nXRP/USDT   0.519163  0.500093  1.000000  0.506109   0.539206  0.632774  0.270203   0.542379  0.132056   0.203707\nSOL/USDT   0.740573  0.688049  0.506109  1.000000   0.652499  0.667697  0.272907   0.687128  0.238826   0.282378\nLINK/USDT  0.665564  0.711332  0.539206  0.652499   1.000000  0.742219  0.258096   0.641447  0.219595   0.397008\nADA/USDT   0.688032  0.691410  0.632774  0.667697   0.742219  1.000000  0.367572   0.708013  0.239894   0.307548\nTRX/USDT   0.317942  0.307392  0.270203  0.272907   0.258096  0.367572  1.000000   0.306818  0.154949   0.087524\nDOGE/USDT  0.814901  0.727154  0.542379  0.687128   0.641447  0.708013  0.306818   1.000000  0.313387   0.329378\nUSDT/USD   0.443909  0.292628  0.132056  0.238826   0.219595  0.239894  0.154949   0.313387  1.000000   0.239725\nONDO/USDT  0.343185  0.368087  0.203707  0.282378   0.397008  0.307548  0.087524   0.329378  0.239725   1.000000\n\n\n\n\n\n\n\n\n\n\n[Window = 90 days] Mean Correlation Matrix\n            BTC/USDT  ETH/USDT  XRP/USDT  SOL/USDT  LINK/USDT  ADA/USDT  TRX/USDT  DOGE/USDT  USDT/USD  ONDO/USDT\nBTC/USDT   1.000000  0.793497  0.477055  0.742079   0.647762  0.679410  0.271722   0.811224  0.430286   0.340974\nETH/USDT   0.793497  1.000000  0.453535  0.688855   0.698123  0.672899  0.245750   0.709918  0.286294   0.366227\nXRP/USDT   0.477055  0.453535  1.000000  0.459386   0.506985  0.611727  0.234199   0.497991  0.104104   0.192323\nSOL/USDT   0.742079  0.688855  0.459386  1.000000   0.635790  0.654002  0.251438   0.684728  0.236922   0.277509\nLINK/USDT  0.647762  0.698123  0.506985  0.635790   1.000000  0.726980  0.217462   0.614228  0.193645   0.386708\nADA/USDT   0.679410  0.672899  0.611727  0.654002   0.726980  1.000000  0.330679   0.693853  0.225267   0.298087\nTRX/USDT   0.271722  0.245750  0.234199  0.251438   0.217462  0.330679  1.000000   0.249756  0.129628   0.080934\nDOGE/USDT  0.811224  0.709918  0.497991  0.684728   0.614228  0.693853  0.249756   1.000000  0.301844   0.321654\nUSDT/USD   0.430286  0.286294  0.104104  0.236922   0.193645  0.225267  0.129628   0.301844  1.000000   0.242525\nONDO/USDT  0.340974  0.366227  0.192323  0.277509   0.386708  0.298087  0.080934   0.321654  0.242525   1.000000"
  },
  {
    "objectID": "guides/correlation_crypto.html#토론-discussions",
    "href": "guides/correlation_crypto.html#토론-discussions",
    "title": "Correlation within Crypto-currencies",
    "section": "토론 (Discussions)",
    "text": "토론 (Discussions)\n\n\n\n\n\n\nImportant\n\n\n\n변수들의 관찰 주기 (단기? 장기?)에 따라 또는 관찰 시기 (10년전? 지금?)에 따라 변수들 간의 선형관계는 유지되지 않을 수 있습니다. 2025년 현재 비트코인 (BTC) 가격은 시장 심리, 규제 변화, 기술적 요인 등에 크게 영향을 받고 있습니다.\n\n\n\n상관계수가 낮은 암호화폐 자산 조합을 식별하고, 헤지 투자 전략을 논의.\n특정 암호화폐 간의 높은 상관관계가 나타나는 이유 및 그에 따른 리스크 분석.\n\n2024년 3월부터 2025년 2월까지 암호화폐 시장에 큰 영향을 미친 주요 변화 시기와 원인:\n\n비트코인 반감기 (2024년 4월 20일): 비트코인 채굴 보상이 6.25 BTC에서 3.125 BTC로 절반으로 감소. 이는 비트코인의 공급 감소로 이어져 가격에 상승 압력을 가함.\n비트코인 ETF 자금 유입 증가 (2024년 10월): 비트코인 ETF로의 지속적인 자금 유입이 관찰됨. 10월까지 ETF 투자자들이 총 345,200 BTC(200억 달러 이상의 가치)를 매입.\n트럼프의 대통령 당선 (2024년 11월): 도널드 트럼프가 “암호화폐 대통령”이 되겠다는 공약을 내세우며 당선됨. 이는 암호화폐 시장에 대한 긍정적인 기대감을 불러일으킴.\nEU의 암호화폐 시장 규제(MiCA) 전면 시행 (2024년 12월 30일): 유럽연합에서 암호화폐 시장 규제(MiCA)가 전면 시행됨. 이로 인해 EU 전역에서 암호화폐 서비스 제공업체들에 대한 통일된 규제 프레임워크가 적용되기 시작.\n트럼프의 암호화폐 정책 발표 (2025년 1월): 트럼프 대통령이 취임 후 미국을 “암호화폐의 수도”로 만들겠다는 계획을 발표함. 여기에는 비트코인 전략적 비축 등의 아이디어가 포함됨.\n\nstars and bins에서 위의 변화 시기가 bins 역할을 한다는 가정하여, 기간 stars을 다음과 같이 나누어 상관관계를 conditional 해 본다.\n\n20224년 3월 1일 (관측기간 시작일) - 2024년 4월 20일\n2024년 4월 21일 - 2024년 9월 30일\n2024년 10월 1일 - 2024년 11월 5일\n2024년 11월 6일 - 2024년 12월 31일\n2025년 1월 1일 - 2025년 2월 28일 (관측기간 종료일)\n\n변화를 \\(Z\\)로 표기했다면, covariance decomposition formula에 의해,\n(추후 계속)"
  },
  {
    "objectID": "guides/correlation_crypto.html#부록-conditioning-theorems-in-probability-theory",
    "href": "guides/correlation_crypto.html#부록-conditioning-theorems-in-probability-theory",
    "title": "Correlation within Crypto-currencies",
    "section": "부록: Conditioning Theorems in Probability Theory",
    "text": "부록: Conditioning Theorems in Probability Theory\n\nAdam’s Law: Smoothing Property of Conditional Expectation\nAlso known as the Law of Total Expectation or Law of Iterated Expectations.\nIf \\(X\\) is a random variable whose expectation \\(E(X)\\) is defined and \\(Z\\) is any random variable defined on the same probability space, then: \\[E(X) = E(E(X|Z)).\\]\nA conditional expectation can be viewed as a Radon–Nikodym derivative, making the tower property a direct consequence of the chain rule for conditional expectations.\nA special discrete case: If \\(\\{Z_{i}\\}\\) is a finite or countable partition of the sample space, then: \\[E(X) = \\sum_{i} E(X \\mid Z_i)P(Z_i).\\]\n\n\nEve’s Law: Variance Decomposition Formula\nKnown as the Conditional Variance Formula or the Law of Iterated Variances.\nIf \\(X\\) and \\(Y\\) are random variables defined on the same probability space, and if \\(Y\\) has finite variance, then:\n\\[\\operatorname{Var}(Y)=E[\\operatorname{Var}(Y\\mid X)]+\\operatorname{Var}(E[Y\\mid X]).\\]\nThis is a special case of the covariance decomposition formula.\nApplications\n\n분산 = “(조건부 분산)의 평균” + “(조건부 평균)의 분산”\nAnalysis of Variance (ANOVA): Variability in \\(Y\\) splits into an “unexplained” within-group variance and an “explained” between-group variance. The F-test examines if the explained variance is significantly large, indicating a meaningful effect of \\(X\\) on \\(Y\\).\nLinear Regression Models: The proportion of explained variance is measured as \\(R^2\\). For simple linear regression (single predictor), \\(R^2\\) equals the squared Pearson correlation coefficient between \\(X\\) and \\(Y\\).\nMachine Learning and Bayesian Inference: In many Bayesian and ensemble methods, one decomposes prediction uncertainty via the law of total variance. For a Bayesian neural network with random parameters \\(\\theta\\): \\(\\operatorname {Var} (Y)=\\operatorname {E} {\\bigl [}\\operatorname {Var} (Y\\mid \\theta ){\\bigr ]}+\\operatorname {Var} {\\bigl (}\\operatorname {E} [Y\\mid \\theta ]{\\bigr )}\\) often referred to as “aleatoric” (within-model) vs. “epistemic” (between-model) uncertainty\nInformation Theory: For jointly Gaussian \\((X,Y)\\), the fraction \\(\\operatorname {Var} (\\operatorname {E} [Y\\mid X])/\\operatorname {Var} (Y)\\) relates directly to the mutual information \\(I(Y;X)\\). In non-Gaussian settings, a high explained-variance ratio still indicates significant information about Y contained in X\n\nExample 1 (Exam Scores): Suppose students’ exam scores vary between two classrooms. The variance of all scores (\\(Y\\)) can be decomposed into the variance within classrooms (unexplained) and the variance between classroom averages (explained), reflecting differences in teaching quality or resources.\nExample 2 (Mixture of Two Gaussians): Consider \\(Y\\) as a mixture of two normal distributions, where the mixing distribution is Bernoulli with parameter \\(p\\). Suppose:\n\\[Y \\mid (X=0) \\sim N(\\mu_0, \\sigma_0^2), \\quad Y \\mid (X=1) \\sim N(\\mu_1, \\sigma_1^2).\\]\nThen the law of total variance gives:\n\\[\\operatorname{Var}(Y) = p\\sigma_1^2 + (1-p)\\sigma_0^2 + p(1-p)(\\mu_1 - \\mu_0)^2.\\]\n\n\nCovariance Decomposition Formula\nKnown as the Law of Total Covariance or Conditional Covariance Formula.\nIf \\(X\\), \\(Y\\), and \\(Z\\) are random variables defined on the same probability space, with finite covariance between \\(X\\) and \\(Y\\), then:\n\\[\\operatorname{cov}(X,Y)=E[\\operatorname{cov}(X,Y\\mid Z)]+\\operatorname{cov}(E[X\\mid Z],E[Y\\mid Z]).\\]\nThis relationship is a particular instance of the general Law of Total Cumulance and is crucial for analyzing dependencies among variables conditioned on a third variable or groupings.\n\n\nBias-Variance Decomposition of MSE\nKey: The Bias-Variance Decomposition emphasizes the trade-off between making \\(\\hat{Y}\\) reliably close to its own expected value (low variance) and aligning that expected value with the true target \\(\\mathbb{E}[Y]\\) (low bias).\nIn many estimation or prediction settings, we have a random outcome \\(Y\\) and an estimator (or model prediction) \\(\\hat{Y}\\). The Mean Squared Error (MSE) of \\(\\hat{Y}\\) is:\n\\[\n\\mathrm{MSE}(\\hat{Y})\n= \\mathbb{E}\\bigl[(Y - \\hat{Y})^2\\bigr].\n\\]\nIf we decompose \\(\\hat{Y}\\) around its expected value, we can split MSE into a bias term and a variance term (plus any irreducible noise in certain contexts). Formally, assume\n\\[\n\\hat{Y} = f(X) + \\text{estimation noise},\n\\quad\nY = f(X) + \\varepsilon,\n\\]\nwhere \\(\\varepsilon\\) is an irreducible error term with mean zero (e.g., observational or inherent noise). Then:\n\\[\n\\begin{aligned}\n\\mathrm{MSE}(\\hat{Y})\n&= \\mathbb{E}\\bigl[(Y - \\hat{Y})^2\\bigr]\\\\\n&= \\underbrace{\\mathbb{E}\\Bigl[\\bigl(\\hat{Y} - \\mathbb{E}[\\hat{Y}]\\bigr)^2\\Bigr]}_{\\text{Variance}(\\hat{Y})}\n\\;+\\;\n\\underbrace{\\Bigl(\\mathbb{E}[\\hat{Y}] \\;-\\; \\mathbb{E}[Y]\\Bigr)^2}_{\\text{Bias}(\\hat{Y})^2}\n\\;+\\;\n\\underbrace{\\mathbb{E}\\bigl[\\varepsilon^2\\bigr]}_{\\text{Irreducible noise}},\n\\end{aligned}\n\\]\nwhere:\n\nVariance: \\(\\text{Var}(\\hat{Y})\\) represents how much \\(\\hat{Y}\\) fluctuates around its own mean.\nBias: \\(\\text{Bias}(\\hat{Y})^2\\) measures how far \\(\\hat{Y}\\) (on average) deviates from the true mean \\(\\mathbb{E}[Y]\\).\nIrreducible noise: \\(\\mathbb{E}[\\varepsilon^2]\\) is the part of the error that cannot be reduced by any estimator.\n\nIn a strictly theoretical sense (when \\(\\varepsilon\\) is embedded in \\(Y\\)), one often writes the Bias-Variance decomposition as:\n\\[\n\\mathrm{MSE}(\\hat{Y})\n= \\underbrace{\\mathrm{Var}(\\hat{Y})}_{\\text{Variance term}}\n\\;+\\;\n\\underbrace{\\mathrm{Bias}(\\hat{Y})^2}_{\\text{Bias term}}.\n\\]\nHere, if there is additional irreducible noise, it appears as a separate constant term. This decomposition closely aligns with the Law of Total Variance (Eve’s Law) in the sense that the total mean squared difference can be split into a “spread around the estimator’s mean” plus the “squared difference of that mean from the true value,” mirroring how variance itself decomposes into conditional components."
  },
  {
    "objectID": "drafts/structure_cointegration.html",
    "href": "drafts/structure_cointegration.html",
    "title": "Time-Varying Cointegration",
    "section": "",
    "text": "This study investigates whether key U.S. economic indicators exhibit time-varying cointegration and how structural breaks affect their long-run relationships. The analysis aims to:\n\nIdentify which variables are cointegrated in the long term.\nTrack how these relationships evolve over time.\nDetect and interpret structural breaks in equilibrium relationships.\nExtend beyond traditional models by using fractional and threshold cointegration frameworks."
  },
  {
    "objectID": "drafts/structure_cointegration.html#research-overview",
    "href": "drafts/structure_cointegration.html#research-overview",
    "title": "Time-Varying Cointegration",
    "section": "",
    "text": "This study investigates whether key U.S. economic indicators exhibit time-varying cointegration and how structural breaks affect their long-run relationships. The analysis aims to:\n\nIdentify which variables are cointegrated in the long term.\nTrack how these relationships evolve over time.\nDetect and interpret structural breaks in equilibrium relationships.\nExtend beyond traditional models by using fractional and threshold cointegration frameworks."
  },
  {
    "objectID": "drafts/structure_cointegration.html#preliminaries",
    "href": "drafts/structure_cointegration.html#preliminaries",
    "title": "Time-Varying Cointegration",
    "section": "2. Preliminaries",
    "text": "2. Preliminaries\n\nSpurious correlation vs. Cointegration: High correlation between non-stationary variables may be misleading unless the variables are cointegrated.\nCointegration: A linear combination of I(1) variables that is stationary (I(0)) implies a stable long-term equilibrium.\nInterpretation: Variables sharing a cointegration relationship tend to move together over time, despite individual stochastic trends.\nEmpirical Rule: In long-term data (e.g., &gt;10 years), a correlation coefficient &gt; 0.7 between I(1) series may suggest stable equilibrium.\nNelson & Plosser (1982): U.S. macroeconomic series often follow stochastic trends, cautioning against naive regression without testing for cointegration."
  },
  {
    "objectID": "drafts/structure_cointegration.html#research-questions",
    "href": "drafts/structure_cointegration.html#research-questions",
    "title": "Time-Varying Cointegration",
    "section": "3. Research Questions",
    "text": "3. Research Questions\n\nWhich economic indicators form long-term relationships?\nHow have these relationships changed over time?\nDo identified structural breaks correspond to major economic shocks (e.g., 2008, COVID-19, inflation)?"
  },
  {
    "objectID": "drafts/structure_cointegration.html#data-and-preprocessing",
    "href": "drafts/structure_cointegration.html#data-and-preprocessing",
    "title": "Time-Varying Cointegration",
    "section": "4. Data and Preprocessing",
    "text": "4. Data and Preprocessing\nPeriod: January 1990 – December 2024 (34 years)\nFrequency: Monthly\n\n\n\n\n\n\n\n\n\n\nCategory\nVariable\nSource\nStart Year\nNotes\n\n\n\n\nEquity\nSPY, NDX\nYahoo Finance\n1993, 1985\nDaily/Monthly\n\n\nCurrency\nDXY\nFRED/Yahoo\n1973\nDaily/Monthly\n\n\nBonds\nFed Funds Rate, 10Y Treasury Yield\nFRED\n1954, 1953\nMonthly\n\n\nMoney Supply\nM2\nFRED\n1959\nMonthly\n\n\nCommodities\nGold Price\nYahoo\n1975\nDaily/Monthly\n\n\nInflation\nCPI\nFRED\n1947\nMonthly\n\n\nConsumption\nConsumer Sentiment\nFRED\n1978\nMonthly\n\n\nInvestment\nReal GPDIC1\nFRED\n1960\nOriginally quarterly; interpolated monthly"
  },
  {
    "objectID": "drafts/structure_cointegration.html#methodology",
    "href": "drafts/structure_cointegration.html#methodology",
    "title": "Time-Varying Cointegration",
    "section": "5. Methodology",
    "text": "5. Methodology\n\nA. Testing for Cointegration\n\nStep 1: Unit Root Testing\n\nEnsure variables are I(1)\nMethods:\n\nADF Test\nPhillips-Perron Test\nADF-GLS (ERS)\nKPSS\n\n\n\n\nStep 2: Cointegration Existence\n\nApply only if variables are I(1)\nMethods:\n\nJohansen Test (multivariate)\nEngle-Granger Test (pairwise)\n\nIf cointegration fails: consider VAR or short-run models\n\n\n\n\nB. Detecting Structural Breaks\n\nStep 1: Break Detection in Traditional Cointegration\n\nMethods:\n\nBai-Perron Test\nQuandt-Andrews Test\nRolling Johansen Test\nCUSUM & CUSUMSQ Tests\n\n\n\n\nStep 2: Qualitative Mapping to Events\n\n\n\nBreakpoint\nLikely Cause\n\n\n\n\n2008-Q3\nGlobal Financial Crisis\n\n\n2011-Q3\nEuropean Debt Crisis\n\n\n2020-Q1\nCOVID-19 Shock\n\n\n2022-Q1\nInflation & Fed Rate Hikes\n\n\n\nOverlay structural breaks with macroeconomic shocks, policy shifts, and global market events.\n\n\n\nC. Fractional Cointegration Extension\n\nStep 1: Testing\n\nEstimate fractional differencing order (\\(d\\)) via:\n\nGPH Test\nRobinson Test\n\n\n\n\nStep 2: Detecting Breaks\n\nUse methods for long-memory models:\n\nRolling estimates of \\(d\\)\nWavelet-based structural break detection\nRolling Hurst exponent analysis\n\n\n\n\n\nD. Comparison of Breakpoints (Traditional vs. Fractional)\n\nCommon breakpoints strengthen the validity of structural shifts.\nTraditional: discrete shifts; Fractional: gradual long-memory transitions.\n\n\n\nE. Threshold Cointegration Models\n\nStep 1: Apply TECM\n\nEstimate threshold level (\\(\\gamma\\))\nModel: \\[\n\\Delta Y_t =\n\\begin{cases}\n\\alpha_1 (Y_{t-1} - \\beta X_{t-1}) + \\epsilon_t, & \\text{if } |Y_{t-1} - \\beta X_{t-1}| &gt; \\gamma \\\\\n\\alpha_2 (Y_{t-1} - \\beta X_{t-1}) + \\epsilon_t, & \\text{otherwise}\n\\end{cases}\n\\]\n\n\n\nStep 2: Interpret Regime-dependent Adjustments\n\nUse Sup-Wald test for significance\nEvaluate asymmetry in adjustment speeds (\\(\\alpha_1 \\ne \\alpha_2\\))\n\n\n\n\nF. Threshold Fractional Cointegration (TFECM)\n\nStep 1: Estimation\n\nCombine fractional differencing with threshold effects: \\[\n\\Delta Y_t =\n\\begin{cases}\n(1 - L)^{d_1} X_t + \\epsilon_t, & \\text{if } |X_t - \\beta Y_t| &gt; \\gamma \\\\\n(1 - L)^{d_2} Y_t + \\eta_t, & \\text{otherwise}\n\\end{cases}\n\\]\n\n\n\nStep 2: Interpretation\n\nCaptures memory-driven and threshold-based nonlinearity\nUse Sup LM test for threshold significance"
  },
  {
    "objectID": "drafts/structure_cointegration.html#summary-evaluation",
    "href": "drafts/structure_cointegration.html#summary-evaluation",
    "title": "Time-Varying Cointegration",
    "section": "6. Summary Evaluation",
    "text": "6. Summary Evaluation\nStrengths: - Systematic, step-by-step progression from standard to advanced models - Combination of linear and nonlinear, short-memory and long-memory models - Identifies persistent shifts and gradual regime changes\nChallenges: - Data-driven thresholds may introduce bias - Advanced methods require substantial computational resources - Fractional and nonlinear models need theoretical grounding for interpretation\nConclusion: This framework offers a rigorous, flexible, and empirically grounded approach to studying evolving long-run relationships in macroeconomic data, with wide applications in investment strategy, economic forecasting, and policy evaluation."
  },
  {
    "objectID": "drafts/market_conditions.html",
    "href": "drafts/market_conditions.html",
    "title": "Free Market Conditions",
    "section": "",
    "text": "This appendix examines the interdependence of key theoretical conditions in economics and finance: Perfectly Competitive Markets (PCM), the Capital Asset Pricing Model (CAPM), the Efficient Market Hypothesis (EMH), and the No-Arbitrage (NA) condition. While these concepts share common foundations in equilibrium and efficiency, they serve distinct purposes. Their structural assumptions determine how prices are formed, how risk is allocated, and whether arbitrage opportunities persist in the long run.\n\nPerfectly Competitive Market (PCM) and Its Implications for Financial Markets\nPCM represents an idealized economic structure where prices are determined by market forces under strict competition. The defining characteristics include:\n\nMany buyers and sellers, ensuring a high degree of competition.\n\nPrice-taking behavior, where no single participant influences market prices.\n\nHomogeneous goods, meaning that products are perfect substitutes.\n\nInformation symmetry, ensuring that all participants have equal access to relevant data.\n\nNo friction on entry and exit, allowing participants to freely enter or exit without cost.\n\nZero long-run economic profits, as competition eliminates excess returns in equilibrium.\n\nThe welfare theorem states that if all traders have convex preferences in PCM, then the equilibrium allocation of limited resources is Pareto efficient. However, PCM does not explicitly incorporate risk, distinguishing it from financial models such as CAPM. Although PCM shares price-taking behavior and information symmetry with CAPM, it does not inherently account for risk-return trade-offs, which are central to financial market equilibrium.\n\n\nThe Capital Asset Pricing Model (CAPM) and Systematic Risk Pricing\nCAPM is a single-factor asset pricing model that determines equilibrium asset prices based on systematic risk exposure. It extends the competitive market framework to financial markets under the following assumptions:\n\nMean-variance optimization, where investors maximize expected utility based on risk-return trade-offs.\n\nRisk characterization through two moments, assuming that asset returns are fully described by their mean and variance.\n\nHomogeneous assets, meaning financial assets can be interpreted as contingent claims (i.e., claims on future payoffs that can be normalized using a risk-free currency), ensuring that the law of one price holds.\n\nInformation symmetry, allowing all investors to have identical expectations about risk and returns.\n\nFrictionless markets, with no transaction costs, taxes, or constraints on borrowing and lending.\n\nMarket portfolio as the tangency portfolio, implying that all investors hold a combination of the risk-free asset and the market portfolio.\n\nWhile CAPM shares PCM’s assumption of competitive equilibrium, it explicitly incorporates systematic risk pricing, which is absent in traditional PCM models.\n\n\nNo-Arbitrage (NA) and the Role of Market Structures\nThe NA condition is fundamental to financial market sustainability. Unlike PCM and CAPM, which describe equilibrium-based price formation, NA ensures that risk-free arbitrage does not persist indefinitely.\nPersistent arbitrage leads to:\n\nCapital concentration, where wealth accumulates disproportionately among arbitrageurs, reducing market competitiveness.\n\nInefficient risk-sharing, as distortions in capital allocation prevent the effective pricing of idiosyncratic risk.\n\nMarket failure, where financial markets lose their role as efficient allocators of capital.\n\nNA is not a direct consequence of PCM or CAPM but rather a structural requirement for a well-functioning financial system. The absence of arbitrage opportunities is necessary for risk-adjusted returns to reflect systematic risk rather than mispricing.\n\n\nThe Efficient Market Hypothesis (EMH) as an Informational Condition for NA\nEMH provides an informational framework under which NA can hold. NA requires that market participants act on available information to eliminate arbitrage. This is feasible if:\n\nInvestors are rational, meaning they respond optimally to profit opportunities.\n\nInformation is symmetric, ensuring that arbitrage opportunities do not persist due to asymmetric knowledge.\n\nHowever, empirical evidence suggests that NA does not always hold due to:\n\nBehavioral biases, where sentiment-driven trading creates arbitrage opportunities.\n\nMarket microstructure effects, where asymmetric information and liquidity constraints delay arbitrage elimination.\n\nWhile EMH supports NA in principle, it is not sufficient for ensuring arbitrage-free markets in the presence of bounded rationality and institutional frictions.\n\n\nEmpirical Implications of Arbitrage Persistence\nEmpirical deviations from NA and EMH do not necessarily invalidate these theories but highlight structural inefficiencies in financial markets. Persistent arbitrage opportunities suggest:\n\nInvestor irrationality, supporting behavioral finance explanations.\n\nInformation asymmetry, consistent with market microstructure theories.\n\nRegulatory distortions, such as too-big-to-fail policies, short-selling bans, and liquidity constraints.\n\nIf arbitrage persists over long time horizons, this may signal failures in market structure that undermine competition and risk allocation. The consequences include:\n\nExcessive capital concentration, reinforcing financial monopolies.\n\nDeterioration in risk-sharing mechanisms, reducing market efficiency.\n\nNA is not just an empirical observation but a necessary structural condition for financial market stability.\n\n\nThe Role of Institutional Frameworks\nThe interplay between NA, EMH, and CAPM depends on regulatory and institutional safeguards. While theoretical models assume that arbitrage disappears naturally, real-world markets require institutional mechanisms to enforce NA and maintain EMH. Key mechanisms include:\n\nFinancial disclosure requirements, ensuring that relevant information is accessible to all participants.\n\nMarket integrity measures, such as circuit breakers and trade halts, preventing extreme mispricings.\n\nRegulations on leverage and short-selling, reducing excessive arbitrage concentration.\n\nLiquidity provisions and market-making incentives, ensuring that mispricings do not persist due to temporary liquidity shortages.\n\nWithout these structural conditions, arbitrage can persist, distorting the intended function of financial markets. The ability of CAPM to predict asset prices, the validity of EMH, and the enforcement of NA all depend on institutional frameworks that mitigate market failures.\n\n\nConceptual Summary\n\n\n\n\n\n\n\n\n\n\nFeature\nPCM\nCAPM\nNA\nEMH\n\n\n\n\nMarket Type\nCompetitive goods market\nCompetitive capital markets\nStructural equilibrium condition\nInformational efficiency condition\n\n\nPrice Mechanism\nSupply and demand equilibrium\nRisk-return equilibrium (beta-based)\nElimination of risk-free arbitrage\nPrices fully reflect information\n\n\nInformation Symmetry\nYes\nYes\nNot required, but supports NA\nNecessary for strong-form EMH\n\n\nMarket Frictions\nNo friction on entry & exit\nUnlimited borrowing/lending assumption\nPrevents arbitrage persistence\nCan be affected by liquidity constraints\n\n\n\nNA, EMH, and CAPM all rely on market structures to function effectively. While PCM provides the foundation for competitive equilibrium, it lacks a risk framework, which CAPM introduces. NA serves as a constraint that ensures risk-free arbitrage does not disrupt financial markets, while EMH posits that prices reflect available information, a condition necessary but not sufficient for NA. These theories, though distinct, are interconnected through institutional safeguards and regulatory mechanisms that sustain financial market efficiency."
  },
  {
    "objectID": "drafts/capm_industry.html",
    "href": "drafts/capm_industry.html",
    "title": "Industry CAPM",
    "section": "",
    "text": "This study investigates the empirical validity of the single-factor Capital Asset Pricing Model (CAPM) when applied to value-weighted industry portfolios over a multi-decade horizon (1999–2023). While the CAPM remains a foundational model in asset pricing theory, its assumptions—such as static equilibrium and constant risk loadings (betas)—may be unrealistic in dynamic, evolving markets. In this context, we assess whether the model adequately explains cross-sectional return differences at the industry level.\nWe focus on three core empirical challenges: 1. The time-variation in industry-specific market betas, which undermines the model’s assumption of stable covariance structures. 2. The uncertainty in estimating the market risk premium, which can distort expected returns and cost-of-capital estimations. 3. The presence of persistent pricing errors (alphas) that suggest structural mispricing and potentially omitted risk factors.\nOur approach draws on the empirical framework pioneered by Fama and French (1997), extended with contemporary data and methods. Using firm-level CRSP data and SIC-based industry classification, we construct monthly value-weighted portfolios and implement rolling beta estimations, cross-sectional regressions, and alpha decomposition to evaluate the model’s performance.\nReferences:\n\nFama, Eugene F., and Kenneth R. French. “Industry costs of equity.” Journal of Financial Economics 43.2 (1997): 153–193.\n\nFama-French Data Library. CRSP and SIC Classification Methodologies."
  },
  {
    "objectID": "drafts/capm_industry.html#introduction",
    "href": "drafts/capm_industry.html#introduction",
    "title": "Industry CAPM",
    "section": "",
    "text": "This study investigates the empirical validity of the single-factor Capital Asset Pricing Model (CAPM) when applied to value-weighted industry portfolios over a multi-decade horizon (1999–2023). While the CAPM remains a foundational model in asset pricing theory, its assumptions—such as static equilibrium and constant risk loadings (betas)—may be unrealistic in dynamic, evolving markets. In this context, we assess whether the model adequately explains cross-sectional return differences at the industry level.\nWe focus on three core empirical challenges: 1. The time-variation in industry-specific market betas, which undermines the model’s assumption of stable covariance structures. 2. The uncertainty in estimating the market risk premium, which can distort expected returns and cost-of-capital estimations. 3. The presence of persistent pricing errors (alphas) that suggest structural mispricing and potentially omitted risk factors.\nOur approach draws on the empirical framework pioneered by Fama and French (1997), extended with contemporary data and methods. Using firm-level CRSP data and SIC-based industry classification, we construct monthly value-weighted portfolios and implement rolling beta estimations, cross-sectional regressions, and alpha decomposition to evaluate the model’s performance.\nReferences:\n\nFama, Eugene F., and Kenneth R. French. “Industry costs of equity.” Journal of Financial Economics 43.2 (1997): 153–193.\n\nFama-French Data Library. CRSP and SIC Classification Methodologies."
  },
  {
    "objectID": "drafts/capm_industry.html#data-and-methodology",
    "href": "drafts/capm_industry.html#data-and-methodology",
    "title": "Industry CAPM",
    "section": "Data and Methodology",
    "text": "Data and Methodology\n\nData Source:\n\nCRSP: Monthly time-series market capitalization data for public stocks listed on Nasdaq, NYSE, and AMEX from 1994-01 to 2023-12.\nFama-French Data Library: Market excess returns (Rm-Rf) and one-month Treasury bill rates as the proxy for the risk-free rate.\n\nTime Frequency and Period: Monthly, covering 30 years (1994-01 to 2023-12), utilizing 5-year rolling windows to estimate monthly industry-specific betas (initial estimation starts from 1999-02).\nIndustry Classification: Ten major industries defined based on SIC codes:\n\nAgriculture, Mining, Construction, Manufacturing, Transportation, Utilities, Wholesale, Retail, Finance, and Services.\nAdjustment1: Reclassified ‘Public’ to ‘Service’, excluded ‘Missing’."
  },
  {
    "objectID": "drafts/capm_industry.html#model-specification",
    "href": "drafts/capm_industry.html#model-specification",
    "title": "Industry CAPM",
    "section": "Model Specification",
    "text": "Model Specification\nThe single-factor CAPM model used:\n\\[\n(E[r_i] - r_f)=  \\alpha_i + \\beta_i (E[r_m] - r_f)\n\\]\nWhere:\n\n\\(E[r_i]\\): Time-averaged return (i.e., realized net growth rate) of industry portfolio ( i )\n\\(r_f\\): Time-averaged return of the risk-free asset (e.g., one-month T-bill)\n\\(E[r_m]\\): Time-averaged return of the market portfolio, defined as a value-weighted convex combination of all industry portfolios\n\\(\\beta_i\\): Industry-specific market beta, representing the linear projection coefficient onto the market excess return under a single-factor model. It is traditionally assumed to be constant under static equilibrium conditions.\n\\(\\alpha_i\\): Orthogonal component of the industry portfolio’s return relative to the market factor; equivalently, the mean residual from an orthogonal projection onto the market return. This term captures either pricing errors under CAPM or the effects of omitted risk factors.\n\n\nThe Alpha\nThe interpretation of \\(\\alpha_i\\) aligns with the residual term in the linear projection:\n\\[\nr_i - r_f = \\alpha_i + \\beta_i (r_m - r_f) + \\varepsilon_i\n\\]\nwhere \\(\\alpha_i\\) is the average component of \\(\\varepsilon_i\\), and \\(\\varepsilon_i\\) is orthogonal to the regressor. Under ideal CAPM assumptions, \\(\\alpha_i = 0\\), but empirically it often deviates from zero due to model misspecification or omitted risk factors.\n\n\nSimple Logical Analysis\nTo better understand the structural dynamics behind industry-driven market behavior, we consider two stylized scenarios:\n\nFirst, imagine a situation where a single dominant industry—such as Services—accounts for a disproportionately large share of total market capitalization (e.g., 70%). In this case, the market portfolio, defined as a value-weighted aggregate, would exhibit a very high linear correlation with the dominant industry’s return. This undermines diversification and implies that market-wide movements are largely driven by a single sector.\nSecond, consider a scenario where two large industries, each accounting for ~45% of the market, are perfectly negatively correlated. Such a structure would lead to very low overall market volatility, as gains in one sector would offset losses in the other, thus creating strong hedging opportunities and enhancing the benefits of diversification.\n\nIn reality, however, the two industries that collectively dominate the market—Services and Manufacturing—exhibit strong positive correlation. As a result, market volatility is amplified, and hedging opportunities are limited, especially in passive value-weighted portfolios. This concentration and correlation structure challenge one of CAPM’s implicit assumptions: that the market portfolio is a well-diversified proxy for systematic risk."
  },
  {
    "objectID": "drafts/capm_industry.html#from-1999-to-2023",
    "href": "drafts/capm_industry.html#from-1999-to-2023",
    "title": "Industry CAPM",
    "section": "From 1999 to 2023",
    "text": "From 1999 to 2023\n\nHistorical excess risk premiums of the US stock market\n\n\nCode\n# 30 years of crsp_monthly\n# start_date = \"1994-01-31\" # i.e. '1994-02-01'\n# end_date = \"2023-12-31\"\n\n# Because of 5 year rolling estimation of monthly beta\nstart_date = \"1999-01-31\"\nend_date = \"2023-12-31\"\n\nprint(f\"Start Date: {start_date}\")\nprint(f\"End Date: {end_date}\")\n\n\nStart Date: 1999-01-31\nEnd Date: 2023-12-31\n\n\n\n\nCode\n#@title Libraries and Time-window\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.formula.api as smf\n\nimport sqlite3\n\ntidy_finance = sqlite3.connect(\"../../colab/tidy_finance_python.sqlite\")\n\nfactors_ff3_monthly = pd.read_sql_query(\n  sql=\"SELECT month, mkt_excess, rf FROM factors_ff3_monthly\",\n  con=tidy_finance,\n  parse_dates={\"month\"}\n)\n\n# 1994-01-01 indicates mktcap at 1994-01-31 which is the start date\n# the first return is calculated\ncrsp_monthly = pd.read_sql_query(\n  sql=\"SELECT permno, month, ret, ret_excess, mktcap, mktcap_lag, siccd, industry, exchange FROM crsp_monthly\",\n  con=tidy_finance,\n  parse_dates={\"month\"}\n)\n\n# 5 year rolling estimated beta is available from 1999-01-01\nbeta = (pd.read_sql_query(\n    sql=\"SELECT permno, month, beta_monthly FROM beta\",\n    con=tidy_finance,\n    parse_dates={\"month\"})\n)\n\nbeta_lag = (beta\n  .assign(month = lambda x: x[\"month\"] + pd.DateOffset(months=1))\n  .get([\"permno\", \"month\", \"beta_monthly\"])\n  .rename(columns={\"beta_monthly\": \"beta_lag\"})\n  .dropna()\n)\n\n# Calculate 12-month moving average\nfactors_ff3_monthly['mkt_excess_ma12'] = factors_ff3_monthly['mkt_excess'].rolling(window=12).mean()\n\n# Plot: Market Excess Return with 12-month Moving Average\nplt.figure(figsize=(12, 5))\nplt.plot(factors_ff3_monthly['month'], factors_ff3_monthly['mkt_excess'], label='Monthly Excess Return', color='lightsteelblue')\nplt.plot(factors_ff3_monthly['month'], factors_ff3_monthly['mkt_excess_ma12'], label='12-Month Moving Average', color='darkblue')\nplt.axhline(0, color='gray', linestyle='--', linewidth=1)\nplt.title('Monthly Market Excess Return with 12-Month Moving Average', fontsize=14)\nplt.xlabel('Date')\nplt.ylabel('Excess Return')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nStructural Shifts in Industry Concentration\n\nInter-Industry Divergence: Growing disparity in concentration levels across industries\nIntra-Industry Consolidation: Increasing dominance of top firms within each industry\n\n\n\nCode\n#@title Number of Firms in Industry Portfolios\n# First, create a dummy column for counting\ncrsp_monthly['count'] = 1\n\n# Create the pivot table\npfo_number = crsp_monthly.pivot_table(\n    values='count',  # The column to aggregate (count in this case)\n    index='month',    # The column to use as index\n    columns='industry', # The column to use as columns\n    aggfunc='sum',    # The aggregation function to use (sum in this case)\n    fill_value=0      # Fill NaN values with 0\n)\n\nsorted_columns = pfo_number.mean().sort_values(ascending=False).index\npfo_number[sorted_columns].plot(\n    kind='line',\n    xlabel='month',\n    ylabel='number of firms',\n    title='Number of Firms in Industry'\n)\nplt.legend(bbox_to_anchor=(1.0, 1.0)) # legend outside\nplt.show()\n\npfo_number[['Missing','Public']].plot(\n    kind='line',\n    xlabel='month',\n    ylabel='number of firms',\n    title='Number of Firms in Industry'\n)\nplt.show()\n\nprint('The average number of firms in Missing industry is', pfo_number['Missing'].mean().round(2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe average number of firms in Missing industry is 2.82\n\n\n\n\nCode\n#@title Industry Concentration Dynamics\n\n# Average Firm Size in Industry Portfolios (Public in Black)\n\npfo_size = crsp_monthly.pivot_table(\n    index='month',\n    columns='industry',\n    values='mktcap',\n    aggfunc='mean'\n)\n\nsorted_columns = pfo_size.mean().sort_values(ascending=False).index\n\nax = pfo_size[sorted_columns].plot(\n    kind='line',\n    xlabel='month',\n    ylabel='mktcap',\n    title='Average Firm Size in Industry Portfolios',\n    linewidth=1.5\n)\n\n# Set Public line to black\nfor line, col in zip(ax.get_lines(), sorted_columns):\n    if col == \"Public\":\n        line.set_color('black')\n        line.set_linewidth(2.0)\n\nplt.legend(bbox_to_anchor=(1.0, 1.0))  # legend outside\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n#@title 산업 내 HHI (Herfindahl-Hirschman Index)\n\n# Step 1: 각 월, 각 산업 내 기업별 시가총액 비중 계산\ncrsp_monthly['mktcap_share'] = (\n    crsp_monthly\n    .groupby(['month', 'industry'], group_keys=False)['mktcap']\n    .transform(lambda x: x / x.sum())\n)\n\n# Step 2: HHI 계산 (각 산업의 각 월에 대해)\nindustry_hhi = (\n    crsp_monthly\n    .assign(mktcap_share_sq=lambda x: x['mktcap_share'] ** 2)\n    .groupby(['month', 'industry'], group_keys=False)['mktcap_share_sq']\n    .sum()\n    .unstack()\n    .sort_index()\n)\n\n# 산업 내 Top-5 Market Cap Share 계산 \ndef top5_share_func(df):\n    # group에는 'month', 'industry'가 포함되므로 사용하지 않음\n    top5_sum = df.nlargest(5, 'mktcap')['mktcap'].sum()\n    total = df['mktcap'].sum()\n    return top5_sum / total if total != 0 else np.nan\n\n# Step: 그룹핑 컬럼을 index로 빼서 apply의 group에서 제거\ntop5_share = (\n    crsp_monthly\n    .sort_values(['month', 'industry', 'mktcap'], ascending=[True, True, False])\n    .set_index(['month', 'industry'])  # &lt;-- group에 포함되지 않게 index로 설정\n    .groupby(['month', 'industry'], group_keys=False)\n    .apply(top5_share_func)  # group에 month/industry 포함되지 않음\n    .unstack()  # 산업별 column\n    .sort_index()\n)\n\nselected_industries = ['Transportation', 'Utilities', 'Retail', 'Manufacturing', 'Services']\n\n# HHI plot\nindustry_hhi[selected_industries].plot(\n    figsize=(12, 5),\n    title='HHI: Industry Concentration Over Time',\n    ylabel='Herfindahl Index',\n    xlabel='Month'\n)\nplt.legend(bbox_to_anchor=(1.0, 1.0))\nplt.show()\n\n# Top-5 Share plot\ntop5_share[selected_industries].plot(\n    figsize=(12, 5),\n    title='Top-5 Market Cap Share in Each Industry Over Time',\n    ylabel='Top-5 Share',\n    xlabel='Month'\n)\nplt.legend(bbox_to_anchor=(1.0, 1.0))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe sharp increase in Top-5 market capitalization share since the post-2009 period highlights a structural shift toward greater industry concentration—particularly within the transportation, utilities, retail, manufacturing, and services sectors. This trend indicates that a small number of dominant firms increasingly account for a disproportionate share of total industry market value.\nWhile average firm size already suggested this pattern, the Top-5 share offers a direct and quantifiable measure. Notably, the Services sector has experienced a persistent rise in concentration since 2011, likely driven by digital transformation, platform-based business models, and network effects. The Manufacturing sector, meanwhile, remained relatively stable until 2019 before undergoing a rapid increase in dominance, possibly due to technology-driven scale economies.\nThese developments coincide with macroeconomic shifts following the 2008 financial crisis, including accommodative policies like quantitative easing (QE), which may have reinforced the “winner-takes-most” dynamics. Importantly, this rising concentration may partially explain observed deviations from CAPM predictions, as industry-level returns become increasingly shaped by a few large-cap firms with idiosyncratic risk-return profiles.\n\n\nEvolution of Industry Market Cap Shares (1999–2023)\n\n\nCode\n#@title df: Drop industry 'Missing' and Re-classify industry 'Public' to 'Services'\n\n# Copy original\ndf = crsp_monthly.copy()\n\n# Drop Missing\ndf = df[df['industry'] != 'Missing']\n\n# Reclassify Public → Services\ndf.loc[df['industry'] == 'Public', 'industry'] = 'Services'\n\n# Merge with factor data and beta\ndf = (df\n  .merge(beta, how=\"inner\", on=[\"permno\", \"month\"])\n  .merge(beta_lag, how=\"inner\", on=[\"permno\", \"month\"])\n  .merge(factors_ff3_monthly, how=\"inner\", on=[\"month\"])\n)\n\n\n\n\nCode\n#@title Market Cap Share of industry portfolios\npfo_share = df.pivot_table(index='month', columns='industry', values='mktcap', aggfunc='sum')\n\n# Normalize pfo_share to sum to 1 for each row\npfo_share[:] = pfo_share.div(pfo_share.sum(axis=1), axis=0)\n\nsorted_columns = pfo_share.mean().sort_values(ascending=False).index\npfo_share[sorted_columns].plot(\n    kind='line',\n    xlabel='month',\n    ylabel='mktcap',\n    title='Market Cap Share of industry portfolios'\n)\nplt.legend(bbox_to_anchor=(1.0, 1.0)) # legend outside\nplt.show()\n\n\n\n\n\n\n\n\n\nThe market portfolio’s composition—measured by the value-weighted market capitalization share of each industry—has experienced notable structural changes since 1999. While the majority of industries have remained relatively small in terms of aggregate market weight, three sectors—Manufacturing, Services, and Finance—have consistently dominated.\nIn particular, the Manufacturing sector’s dominance has gradually declined from nearly 50% in 1999 to below 40% in 2023. Conversely, the Services sector, especially after absorbing “Public” firms, has expanded significantly, rising from under 20% to over 30% in the same period. The Finance sector saw a sharp decline following the 2008 financial crisis and has since stabilized at a lower level.\nThese compositional shifts reflect evolving patterns in industrial dominance and have direct implications for the systematic risk profile of the aggregate market portfolio. As sectoral weights change, so too does the market beta composition underlying the CAPM framework.\n\n\nTime-Varying Systematic Risk by Industry\n\n\nCode\n#@title Time-varying industry Market Betas\n\n# ===============================================\n# 1. Market Cap-weighted Industry Beta (Value-Weighted Beta)\n# ===============================================\n# CAPM의 factor loading인 beta는 산업 내 대형 기업일수록 시장과의 공분산에 더 큰 영향을 미치므로,\n# 산업별 단순 평균 beta는 산업의 실제 systematic risk를 과소/과대평가할 수 있습니다.\n# 따라서 각 기업의 시가총액으로 가중평균한 value-weighted beta를 계산합니다.\n\n# Step 1: Beta weighted by market cap\ndf['beta_weighted'] = df['beta_monthly'] * df['mktcap']\n\n# Step 2: Group by month and industry to compute weighted beta\npfo_beta_weighted = (\n    df.groupby(['month', 'industry'])[['beta_weighted', 'mktcap']]\n      .sum()\n      .assign(beta_vw=lambda x: x['beta_weighted'] / x['mktcap'])\n      .reset_index()\n      .pivot(index='month', columns='industry', values='beta_vw')\n)\n\n# ===============================================\n# 2. Time-Series Plot of Value-Weighted Industry Betas\n# ===============================================\nsorted_columns = pfo_beta_weighted.mean().sort_values(ascending=False).index\n\npfo_beta_weighted[sorted_columns].plot(\n    kind='line',\n    figsize=(12, 6),\n    xlabel='Month',\n    ylabel='Value-weighted Beta',\n    title='Time-varying Value-weighted Industry Beta'\n)\nplt.legend(bbox_to_anchor=(1.0, 1.0))\nplt.show()\n\n# ===============================================\n# 3. Boxplot of Value-Weighted Industry Betas\n# ===============================================\n# Melt for seaborn\npfo_beta_weighted_melted = pd.melt(\n    pfo_beta_weighted.reset_index(),\n    id_vars=['month'],\n    value_vars=pfo_beta_weighted.columns\n)\npfo_beta_weighted_melted.columns = ['month', 'industry', 'beta']\n\n# Sort industries by average beta\nmean_beta_vw = pfo_beta_weighted_melted.groupby('industry')['beta'].mean().sort_values(ascending=False)\n\n# Boxplot\nplt.figure(figsize=(10, 6))\nsns.boxplot(\n    y='industry',\n    x='beta',\n    data=pfo_beta_weighted_melted,\n    order=mean_beta_vw.index,\n    orient='h'\n)\nplt.title('Boxplot of Value-weighted Beta for Each Industry (Sorted by Mean)')\nplt.show()\n\n# ===============================================\n# 4. Scatter Plot: Mean Beta vs. Mean Market Cap Share\n# ===============================================\n# Mean industry beta (value-weighted)\nbeta_mean = pfo_beta_weighted.mean()\n\n# Mean market cap share (already normalized)\nmktcap_share_mean = pfo_share.mean()\n\n# Scatter Plot\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=beta_mean, y=mktcap_share_mean)\nfor industry in beta_mean.index:\n    plt.text(beta_mean[industry], mktcap_share_mean[industry], industry, fontsize=9)\nplt.xlabel('Mean Industry Beta (Value-weighted)')\nplt.ylabel('Mean Market Cap Share')\nplt.title('Mean Industry Beta vs. Mean Market Cap Share')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of value-weighted industry betas uncovers several important dynamics in the evolution of systematic risk exposures across sectors.\nFirst, the time-series plot shows a random-walk-like behavior in beta trajectories, suggesting that industry-level risk exposure is far from stable and must be modeled as time-varying. The boxplot reinforces this heterogeneity:\n\nIndustries such as Manufacturing and Retail exhibit narrow beta distributions, indicating consistent risk exposure among firms in these sectors.\n\nIn contrast, Mining and Construction show wide dispersion, pointing to greater intra-industry variability in systematic risk.\n\nA comparison of beta levels also reveals structural asymmetries:\n\nRetail, Utilities, and Agriculture maintain beta values consistently below one, aligning with their roles as defensive sectors.\n\nConversely, the Services sector displays beta values above one, along with a rising market cap share—suggesting it has become a key driver of market returns.\n\nThis imbalance implies that a value-weighted market portfolio—heavily exposed to high-beta sectors—offers limited hedging potential, especially in downturns.\nThe scatter plot of mean beta vs. mean market cap share further illustrates this, with Manufacturing standing out as a structural outlier: it holds an average beta near 1 but dominates in market share. These findings support a more strategic allocation to low-beta sectors, particularly in anticipation of macroeconomic risks. This aligns with recent investment behavior by Buffett, who has increased exposure to retail-sector firms like Ulta Beauty, likely as a hedge against cyclical downturns.\n\n\nEmpirical Testing of CAPM Using Fama-MacBeth Regressions\n\n\nCode\n#@title 10 Value-Weighted industry pfos\n\ndef weighted_avg(x, weights):\n    \"\"\"Calculates the weighted average of a series.\"\"\"\n    return np.average(x, weights=weights)\n\n# Apply weighted_avg function to pivot_table\npfo_vw_ret_excess = df.pivot_table(\n    index='month',\n    columns='industry',\n    values='ret_excess',\n    aggfunc=lambda x: weighted_avg(x, df.loc[x.index, 'mktcap'])\n)\n\npfo_vw_beta_lag = df.pivot_table(\n    index='month',\n    columns='industry',\n    values='beta_lag',\n    aggfunc=lambda x: weighted_avg(x, df.loc[x.index, 'mktcap'])\n)\n\nmean_vw_beta_lag = pfo_vw_beta_lag.mean().rename('mean_beta_lag')\nmean_vw_ret_excess = pfo_vw_ret_excess.mean().rename('mean_ret_excess')\n\nmkt_excess = factors_ff3_monthly['mkt_excess'].mean()\nrf = factors_ff3_monthly['rf'].mean()\n\n\n\n\nCode\n#@title Cross-sectional regressions for each month\n\n# Fama-MacBeth (1973) two-pass procedure \n\nrisk_premiums = (df\n  .groupby(\"month\")[['ret_excess', 'beta_lag']]\n  .apply(lambda x: smf.ols(formula=\"ret_excess ~ beta_lag\", data=x).fit().params)\n  .reset_index()\n)\n\n# Time-series Aggregation (i.e. average)\n# average across the time-series dimension to get the mean risk premium for each characteristic\n# calculate t-test statistics for each regressor,\n# critical values of 1.96 (at 5% significance) or 2.576 (at 1% significance) for two-tailed significance tests\n\nmean_premiums = (risk_premiums\n  .melt(id_vars=\"month\", var_name=\"factor\", value_name=\"estimate\")\n  .groupby(\"factor\")[\"estimate\"]\n  .apply(lambda x: pd.Series({\n      \"mean_premium\": 100*x.mean(),\n      \"t_statistic\": x.mean()/x.std()*np.sqrt(len(x))\n    })\n  )\n  .reset_index()\n  .pivot(index=\"factor\", columns=\"level_1\", values=\"estimate\")\n  .reset_index()\n)\n\n# reporting standard errors of risk premiums, after adjusting for autocorrelation (Newey and West (1987) standard errors)\n\nmean_premiums_newey_west = (risk_premiums\n  .melt(id_vars=\"month\", var_name=\"factor\", value_name=\"estimate\")\n  .groupby(\"factor\")\n  .apply(lambda x: (\n      x[\"estimate\"].mean()/\n        smf.ols(\"estimate ~ 1\", x)\n        .fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 6}).bse\n    ), include_groups=False\n  )\n  .reset_index()\n  .rename(columns={\"Intercept\": \"t_statistic_newey_west\"})\n)\n\nfm_reg = (mean_premiums\n  .merge(mean_premiums_newey_west, on=\"factor\")\n  .round(3)\n)\nfm_reg['mean_premium'] = fm_reg['mean_premium']*12\n\nprint('Annual Risk Premium of Market Beta')\nfm_reg\n\n\nAnnual Risk Premium of Market Beta\n\n\n\n\n\n\n\n\n\nfactor\nmean_premium\nt_statistic\nt_statistic_newey_west\n\n\n\n\n0\nIntercept\n9.432\n3.891\n3.046\n\n\n1\nbeta_lag\n1.860\n0.766\n0.725\n\n\n\n\n\n\n\nWe employ the Fama-MacBeth (1973) two-pass regression method to estimate the annualized market risk premium under the single-factor CAPM framework. The first pass consists of estimating rolling betas for each firm, which are then aggregated into value-weighted industry betas. The second pass involves running monthly cross-sectional regressions of industry excess returns on lagged betas from 1999 to 2023. To account for possible serial correlation, we report both naïve t-statistics and Newey-West (1987) adjusted statistics.\nThe results show a striking pattern:\n\nThe estimated intercept (alpha) averages 9.43% annually, and this deviation from the theoretical risk-free rate is statistically significant (t = 3.89; NW-adjusted t = 3.05).\n\nThe estimated beta risk premium, on the other hand, is only 1.86% annually, with no statistical significance (t = 0.77; NW-adjusted t = 0.73).\n\nThis finding leads to two critical implications:\n\nThe CAPM fails to explain a substantial portion of the cross-sectional variation in industry returns.\nThere is likely a mispricing component or omitted factor structure that the single-factor model cannot capture.\n\nFrom a modeling perspective, the coexistence of a strong alpha and weak beta suggests that estimation errors are compounding: both the time-varying nature of betas and the instability of risk premia contribute to the overall model misspecification. These results are consistent with the view that industry-specific risk profiles may involve multiple dimensions of risk, and that static CAPM assumptions are empirically untenable over long horizons.\n\n\nSecurity Market Line and Conditional Alpha\n\n\nCode\n#@title CAPM SML prediction plot\n\nimport matplotlib.ticker as mtick\n\n# Combine beta and return\npfo_sml = pd.concat([mean_vw_beta_lag, mean_vw_ret_excess], axis=1)\npfo_sml = pfo_sml.reset_index().rename(columns={'index': 'industry'})\n\n# CAPM Regression Line (fitted to 10 points)\nmodel = smf.ols('mean_ret_excess ~ mean_beta_lag', data=pfo_sml).fit()\nintercept_capm_fit = model.params['Intercept']\n\n# SML: CAPM predicted line (Rf intercept)\nintercept_capm_theory = rf\n\n# SML: Fama-MacBeth implied line (intercept from fm_reg table)\nintercept_fm = fm_reg.loc[fm_reg['factor'] == 'Intercept', 'mean_premium'].values[0] / 100 / 12  # monthly rate\n\n# Start plot\nplt.figure(figsize=(8, 6))\n\n# Scatter plot of 10 industries\nfor _, row in pfo_sml.iterrows():\n    plt.scatter(row['mean_beta_lag'], row['mean_ret_excess'], color='black')\n    plt.annotate(row['industry'], (row['mean_beta_lag'] + 0.01, row['mean_ret_excess']), fontsize=9)\n\n# Draw SMLs\n# Theoretical CAPM SML (Rf, slope = E[Rm - Rf])\nplt.axline((0, intercept_capm_theory), slope=mkt_excess, linestyle='dashed', color='black', label='CAPM (Rf Intercept)')\n\n# Regression fit line (OLS over 10 industry points)\nplt.axline((0, intercept_capm_fit), slope=mkt_excess, linestyle='dashed', color='red', label='OLS Fit on 10 Points')\n\n# Fama-MacBeth implied line (Intercept from FM regression)\nplt.axline((0, intercept_fm), slope=mkt_excess, linestyle='dashed', color='blue', label='Fama-MacBeth Intercept')\n\n# Format\nplt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\nplt.xlabel('Mean Beta (Lagged)')\nplt.ylabel('Mean Excess Return (Monthly)')\nplt.title('Unconditional Security Market Line (Industry-Level CAPM)')\nplt.legend(loc='lower right')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n#@title 개별 산업의 mispricing 정도를 파악\n\n# 1. Calculate conditional alpha\npfo_sml['capm_pred'] = pfo_sml['mean_beta_lag'] * mkt_excess\npfo_sml['alpha'] = pfo_sml['mean_ret_excess'] - pfo_sml['capm_pred']\n\n# 2. Sort industries by alpha\npfo_sml_sorted = pfo_sml.sort_values(by='alpha', ascending=False)\n\n# 3. Barplot of alpha\nimport seaborn as sns\nplt.figure(figsize=(10, 6))\nsns.barplot(data=pfo_sml_sorted, x='alpha', y='industry', hue='industry', palette='coolwarm', dodge=False)\nplt.axvline(0, color='black', linestyle='--')\nplt.gca().xaxis.set_major_formatter(mtick.PercentFormatter(1.0))\nplt.xlabel('Conditional Alpha (Monthly)')\nplt.title('Industry-level Conditional Alpha under CAPM')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nWe visualize the unconditional Security Market Line (SML) implied by the single-factor CAPM across 10 value-weighted industry portfolios. Each point on the plot corresponds to an industry, placed according to its average market beta (horizontal axis) and realized average excess return (vertical axis) over the sample period.\nThree lines are shown for comparison:\n\nThe black dashed line represents the theoretical CAPM SML, where the intercept equals the average risk-free rate and the slope equals the average market risk premium.\nThe red dashed line is a simple OLS fit across the 10 industry points, which minimizes cross-sectional error.\nThe blue dashed line uses the Fama-MacBeth estimated intercept, incorporating pricing errors in the CAPM framework.\n\nMost industry points lie between the CAPM-predicted line and the Fama-MacBeth-adjusted line. This suggests that while the risk-return relationship remains approximately linear, the CAPM fails to account for substantial pricing errors, as reflected in large intercept terms.\nTo quantify these deviations more precisely, we calculate conditional alphas for each industry. These alphas represent the difference between realized and CAPM-predicted returns, conditional on the industry’s average beta.\n\nPositive alpha implies the industry earned more than predicted by its systematic risk exposure.\nNegative alpha suggests overvaluation relative to CAPM expectations.\n\nThe results reveal persistent mispricing across several sectors, reinforcing earlier conclusions about the model’s empirical inadequacy. The CAPM may still serve as a baseline pricing model, but the presence of large unexplained returns calls for either a multi-factor extension or a fundamental rethinking of the linear risk-return paradigm."
  },
  {
    "objectID": "drafts/capm_industry.html#conclusions",
    "href": "drafts/capm_industry.html#conclusions",
    "title": "Industry CAPM",
    "section": "Conclusions",
    "text": "Conclusions\nThis short study evaluates the empirical validity of the single-factor CAPM using value-weighted industry portfolios over a 25-year period (1999–2023). Several key findings emerge:\n\nIndustry-specific market betas exhibit substantial time variation, contradicting the CAPM’s assumption of stable factor loadings. This instability weakens the model’s explanatory power over long horizons and complicates its use in asset pricing and cost-of-capital estimation.\nThe market risk premium, when estimated empirically, shows large uncertainty and wide confidence bounds, limiting its practical usefulness in capital budgeting and valuation decisions.\nFama-MacBeth regressions reveal economically large and statistically significant intercepts (alphas), while the estimated risk premium on beta is both small and statistically insignificant. This suggests that the single-factor CAPM omits important pricing components or fails to capture cross-sectional return dynamics.\nIndustry-level CAPM predictions show structural deviations from theoretical SML predictions. Finance and Transportation exhibit relatively low pricing errors, potentially due to regulatory distortions (e.g., “Too Big to Fail”) or reduced market responsiveness.\nLastly, the increasing dominance of a few large-cap firms—particularly in Services and Manufacturing—implies that market-wide returns are increasingly shaped by concentrated industry dynamics. This structural concentration further limits the diversification benefits assumed under standard portfolio theory.\n\nOverall, while CAPM remains a foundational framework in asset pricing, this analysis highlights its limitations in capturing the complexities of modern equity markets—particularly when applied at the industry level over long horizons."
  },
  {
    "objectID": "drafts/capm_industry.html#footnotes",
    "href": "drafts/capm_industry.html#footnotes",
    "title": "Industry CAPM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn this study, firms originally categorized under the “Public” industry in the CRSP database are reassigned to the “Services” industry group. This decision is based on a review of the largest firms within the Public category—such as Tesla, Zoom, Airbnb, PayPal, and Coinbase—which predominantly operate in service-driven, software-based, or platform-oriented business models. Although a few firms like Tesla or Kraft Heinz engage in manufacturing, the overall structure and revenue sources of the Public group are better aligned with the characteristics of modern service industries. This reassignment enhances interpretability in CAPM-based industry comparisons while maintaining consistency in economic logic.↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "깃샘학원\n“We Build Leaders, Not Just Winners.”\n\n깃샘학원의 목표는 성공한 엘리트 양성이 아니다: 단순한 시험 점수나 경제적 성공을 목표로 하지 않습니다.\n깃샘학원의 목표는 훌륭한 리더 양성이다: 공감력, 윤리적 책임감, 논리적 사고력, 실질적 문제 해결 능력을 갖추어 사회적으로 선한 영향력을 발휘할 수 있는 진정한 리더 양성이 목표이다.\n\n\n\n문제: 고위직 엘리트들의 항진명제식 발언 분석\n\n1. 서론\n고위직 엘리트들은 정치, 법조, 경제, 경영 등 다양한 분야에서 영향력을 행사하며, 공적인 자리에서의 발언이 중요한 의미를 갖는다. 그러나 이들의 발언 중 상당수는 논리적으로 항진명제(tautology)에 가까우며, 이는 정보 비대칭을 활용한 책임 회피 전략으로 사용되는 경우가 많다. 여기 분석에서는 몇가지 대표적 사례들을 분석하고, 이러한 발언 방식을 만들어 내는 구조적 원인을 탐구하며, 개선 방안을 제안하고자 한다.\n\n\n2. 사례 분석\n\n법조 분야\n\n“최근 고위 공직자 비리 의혹 수사에 대한 검찰의 입장은?”\n\n검찰총장: “모든 수사는 법과 절차에 따라 공정하게 진행됩니다.”\n논리적 문제점: 법과 절차에 따른다는 것은 당연한 원칙이며, 구체적인 수사 진행 상황을 설명하지 않음.\n개선된 답변: “해당 의혹과 관련해 △△ 증거를 확보했으며, 3월 말까지 관계자 소환 조사를 진행할 예정입니다. 또한, 수사 결과는 국민에게 투명하게 공개할 것입니다.”\n\n\n\n\n경제 분야\n\n“금리 인상 가능성에 대해 어떻게 전망하나요?”\n\n한국은행 총재: “통화 정책은 물가 안정을 우선시할 것입니다.”\n논리적 문제점: 물가 안정을 목표로 한다는 것은 이미 알려진 사실로, 구체적 조건이 제시되지 않음.\n개선된 답변: “실업률은 5% 미만으로 유지되지만 연내 물가 상승률이 3%를 넘을 경우, 4분기 중 금리를 0.25%p 추가 인상할 계획입니다. 기준 금리 인상을 통해 물가의 안정성을 확보하고자 합니다.”\n\n\n\n\n정치 분야\n\n“차기 총선에서 여당의 승리 가능성은 어떻게 보는가?”\n\n여당 대변인: “선거 결과는 유권자의 선택에 달려 있습니다.”\n논리적 문제점: 선거 결과가 유권자의 선택에 따른다는 것은 자명한 사실이며, 선거 전략이나 예측을 제공하지 않음.\n개선된 답변: “현 지지율(45%)과 △△ 정책 공약을 바탕으로 20석 이상 확대를 목표로 합니다.”\n\n\n\n\n경영 분야\n\n“C사 제품의 안전성 논란에 대한 대응은?”\n\nC사 홍보팀장: “고객 안전을 최우선으로 삼고 있습니다.”\n논리적 문제점: 고객 안전을 최우선으로 한다는 것은 원칙적인 선언일 뿐, 실제 조치에 대한 설명이 없음.\n개선된 답변: “문제 제품 5만 개 전량 리콜하며, 피해 보상금을 제품 가격의 120%로 책정했습니다. 또한, 향후 안전성 강화를 위한 추가 조치를 마련할 것입니다.”\n\n\n\n\n\n3. 발언 방식 분석 및 원인\n\n(1) 정보 비대칭 활용 극대화\n고위직 인사들은 상대적으로 많은 정보를 보유하고 있으나, 이를 공개하지 않음으로써 해석의 여지를 남긴다. 이는 논란을 회피하고 의도적으로 불확실성을 유지하는 전략이다.\n\n\n(2) 항진명제 활용\n애매모호한 표현을 사용하여 논란을 피하는 동시에, 어느 쪽으로든 해석이 가능하도록 발언함으로써 책임을 최소화한다.\n\n\n(3) 자격 논란과 인신공격 전략\n논리적으로 반박하기 어려운 경우, 상대방의 자격을 문제 삼아 논의 자체를 흐리는 방식이 자주 활용된다.\n\n\n(4) 사회적·제도적 보상 구조\n책임을 명확히 지는 것보다 애매한 발언을 통해 책임을 회피하는 것이 생존 전략으로 작용하는 구조가 자리 잡고 있다. 이러한 환경에서는 모호한 표현이 오히려 유리하게 작용할 가능성이 크다.\n\n\n\n4. 해결 방안\n\n(1) 발언의 책임 명확화\n\n공적인 자리에서의 발언이 결과에 미치는 영향을 추적하는 피드백 시스템을 도입하여, 모호한 발언이 아니라 구체적인 답변을 유도해야 한다.\n“중대성이 낮다”고 발언한 경우, 이후 정책·법적 판단에서 이에 대한 책임을 질 수 있도록 제도적 장치를 마련해야 한다.\n\n\n\n(2) 논의 구조 개선\n\n논리적 토론을 강화하고, 인신공격을 차단하는 규칙을 도입해야 한다.\n예를 들어, 토론 중 상대의 과거나 자격을 논하는 발언이 나오면 자동으로 발언권을 제한하는 시스템을 고려할 수 있다.\n\n\n\n(3) 교육 및 사고방식 변화\n\n기계식 시험 중심이 아닌, 논리적 사고 및 창의적 문제 해결을 평가하는 교육 방식으로 변화해야 한다.\n특히, AI와 블록체인을 활용한 교육 평가 시스템을 통해 논리적 사고 과정을 명확하게 평가하는 방법이 필요하다.\n\n\n\n\n5. 결론\n고위직 엘리트들의 항진명제식 발언은 정보 비대칭을 활용한 책임 회피 전략의 일부로 작용하며, 이는 사회적·제도적 보상 구조와 결합하여 지속적으로 강화된다. 이러한 문제를 해결하기 위해서는 발언의 책임을 명확히 하고, 논의 구조를 개선하며, 교육 시스템의 변화를 통해 논리적 사고력을 강화하는 접근이 필요하다. 특히, AI와 블록체인을 활용한 평가 방식은 발언의 모호성을 줄이고 구체적인 논리적 사고를 촉진하는 데 기여할 수 있을 것이다.\n\n\n\n\n해결책: 사고력과 리더십을 키우는 혁신적 교육 프로그램\n\n혁신적 교육 및 평가 시스템 (AI & 블록체인 기반)\n\nPrompt-based Evaluation: AI를 활용하여 학생의 사고 과정 자체를 블록체인에 기록하고, 창의성·논리성·윤리적 책임감까지 정량적·객관적으로 평가\nProof of Thinking: 학습 과정을 블록체인에 저장하여 자기성찰 능력을 강화하고, 위변조 방지\n토론·윤리적 판단·전략적 사고 중심 교육: 동료와의 토론을 통해 사고력과 문제 해결력을 강화\n\n\n\n혁신적 강의 방식\n\n개념 시각화: python, desmos 등 기술을 활용하여 수학과 과학의 추상적 개념을 구체적으로 시각화\n실전 응용 모형 중심: 실전에 활용되고 있는 과학 모형들을 구체적으로 이해하고 적용하는 방식\n\n\n\n자연과학 모형 이해\n\n대수학, 기하학, 미적분학, 확률통계학\n고전역학, 통계물리학, 상대성이론, 양자역학, 유기화학, 분자생물학\n국제 경시대회(AMC 등) 및 입시 대비 문제풀이\nPython을 활용한 데이터 계산 및 시각화\n\n\n\nAI 및 기술 도구 이해와 활용법\n\nAI 활용 리서치 및 정보 검색 방법\n금융·투자 전략을 위한 선형대수학 (Google Colab 활용)\n창업·재테크를 위한 통계학 (Google Sheets 활용)\nLLM(대규모 언어 모델) 개발을 위한 확률 최적화 이론 및 미적분 해석학"
  },
  {
    "objectID": "drafts/capm.html",
    "href": "drafts/capm.html",
    "title": "Revisiting the CAPM and Diversification",
    "section": "",
    "text": "Introduction\nThe Capital Asset Pricing Model (CAPM), developed by Sharpe (1964), Lintner (1965), and Mossin (1966), remains a cornerstone of modern finance, linking expected returns to risk. It classifies risk into two categories: systematic risk, which stems from market-wide factors and cannot be diversified away, and unsystematic risk, which is asset-specific and can be mitigated through diversification. The CAPM posits that only systematic risk, measured by beta, justifies a return premium, as investors can eliminate unsystematic risk by holding a diversified portfolio, ideally approximating the market portfolio. This principle aligns with broader linear factor models like the Arbitrage Pricing Theory (APT) (Ross 1976), which extends the CAPM by incorporating multiple systematic risk factors while similarly dismissing diversifiable risk under no-arbitrage conditions.\nHowever, the CAPM’s empirical validity has been contested. Banz (1981) documented the size effect, where small-cap stocks outperform CAPM predictions, while Basu (1977) identified the value effect, showing excess returns for stocks with high earnings-to-price ratios. These anomalies spurred the development of multifactor models, such as the Fama-French three-factor model (Fama and French 1993), which augment beta with size and value factors. Beyond these, market concentration has emerged as a critical lens for understanding asset pricing deviations. Hou and Robinson (2006) found that firms in concentrated industries earn higher returns, attributing this to economic rents from market power. Edmans (2009) linked ownership concentration to superior performance, while Choi et al. (2017) showed that institutional investors with concentrated portfolios outperform diversified ones. Neuhann and Sockin (2024) explored how financial market concentration distorts capital allocation, and Bustamante and Donangelo (2017) tied product market concentration to industry returns.\nFrom a theoretical perspective, Magill and Quinzii (1996) argued that incomplete markets—lacking sufficient contingent claims—prevent full risk hedging, challenging CAPM assumptions. Cochrane (1996) emphasized the role of investment-based pricing, while Campbell (1992) critiqued volatility as an incomplete risk measure. Socioeconomic analyses, such as Saez and Zucman (2016), further connect market concentration to wealth inequality, highlighting broader implications. This rich body of literature suggests that market structure and concentration significantly complicate the CAPM’s risk-return framework, necessitating a deeper examination.\n\n\nMain\n\nEmpirical and Mathematical Foundations of Diversification\nDiversification’s risk-reducing power is well-established empirically and mathematically. Elton and Gruber (1977) analyzed 3,290 securities, demonstrating that a portfolio of just four stocks markedly reduces variance compared to a single stock, underscoring diversification’s practical utility.\nMathematically, consider an equally weighted portfolio of \\(n\\) securities. The portfolio variance \\(\\sigma_p^2\\) is given by:\n\\[\n\\sigma_p^2 = \\frac{1}{n} \\bar{\\sigma}^2 + \\frac{n-1}{n} \\bar{\\rho} \\bar{\\sigma}^2\n\\]\nwhere:\n\n\\(\\bar{\\sigma}^2\\) = average variance of individual securities\n\n\\(\\bar{\\rho}\\) = average correlation between securities\n\nAs \\(n\\) increases, \\(\\sigma_p^2\\) converges to \\(\\bar{\\rho} \\bar{\\sigma}^2\\), indicating that covariance, not individual variance, dominates portfolio risk. When \\(\\bar{\\rho} &lt; 1\\), diversification lowers volatility, resembling how higher-order terms in Taylor polynomials diminish to smooth a function or how fractal geometry simplifies irregularities with scale.\n\n\nA Critique of Risk as Volatility\nThe CAPM equates risk with volatility, but this assumption is narrow. Long-term investors may prioritize structural risks—e.g., economic shifts or sector obsolescence—over short-term price swings. A volatile growth stock might be less “risky” to them than a stable but declining asset. Campbell (1992) supports this critique, arguing that volatility oversimplifies the multifaceted nature of risk, a view echoed by behavioral finance perspectives (Shiller 2003).\n\n\nDiversification and the Risk-Return Trade-Off\nThe CAPM ties diversification to the risk-return trade-off, suggesting investors can eliminate unsystematic risk while earning returns proportional to systematic risk exposure. In a stochastic setting, diverse agents (e.g., farmers, energy producers) share idiosyncratic risks, enhancing welfare (Cochrane 2009). Yet, this assumes a broad, competitive market. When the market portfolio—say, the S&P 500—is dominated by a few highly correlated stocks (e.g., tech giants), diversification falters. High \\(\\bar{\\rho}\\) reduces variance’s sensitivity to \\(n\\), undermining the CAPM’s benefits (Grullon, Larkin, and Michaely 2019).\n\n\nMarket Concentration and the Upper Frontier\nIn the standard arbitrage-free asset pricing framework, the upper mean-variance frontier assets correlate perfectly (negatively) with the stochastic discount factor (SDF). In concentrated markets, dominant firms with economic moats (Bustamante and Donangelo 2017) act as principal components, compressing the payoff space. For example, if the Herfindahl-Hirschman Index (HHI) measures this dominance or concentration, then a high HHI signals reliance on few assets, limiting diversification. Investors may then seek arbitrage in these stocks, amplifying concentration (Valta 2012).\n\n\nImplications for Investors and Market Stability\nIn concentrated markets, diversification yields to capturing rents from dominant firms. These firms use primary-market capital to reinforce moats, distributing profits rather than fostering competition (Hou and Robinson 2006). Secondary-market trading becomes zero-sum, redistributing wealth without value creation. Early investors in concentrated stocks gain disproportionately, widening inequality (Saez and Zucman 2016) and challenging the CAPM’s traditional data-driven risk-return logic.\n\n\nBroader Socioeconomic Consequences\nConcentration reduces contingent claim diversity, impairing hedging capacity (Magill and Quinzii 1996). A shock to a dominant sector triggers systemic ripples, increasing instability. This homogeneity shifts markets from managing uncertainty to rewarding market power, exacerbating wealth gaps and contradicting the CAPM’s egalitarian risk-sharing ideal.\n\n\n\nConclusion\nThe CAPM’s diversification-driven risk-return framework faces significant challenges from market concentration. As diversification weakens, investors prioritize rents over risk reduction, simplifying markets into systems dominated by a few firms. This shift threatens stability, equity, and hedging capacity, urging a rethinking of the CAPM and policies to enhance market diversity.\n\n\n\n\n\nReferences\n\nBanz, Rolf W. 1981. “The Relationship Between Return and Market Value of Common Stocks.” Journal of Financial Economics 9 (1): 3–18.\n\n\nBasu, Sanjoy. 1977. “Investment Performance of Common Stocks in Relation to Their Price‐earnings Ratios: A Test of the Efficient Market Hypothesis.” The Journal of Finance 32 (3): 663–82.\n\n\nBustamante, M Cecilia, and Andres Donangelo. 2017. “Product Market Competition and Industry Returns.” The Review of Financial Studies 30 (12): 4216–66.\n\n\nCampbell, John Y. 1992. “Intertemporal Asset Pricing Without Consumption Data.” National Bureau of Economic Research Cambridge, Mass., USA.\n\n\nChoi, Nicole, Mark Fedenia, Hilla Skiba, and Tatyana Sokolyk. 2017. “Portfolio Concentration and Performance of Institutional Investors Worldwide.” Journal of Financial Economics 123 (1): 189–208.\n\n\nCochrane, John H. 1996. “A Cross-Sectional Test of an Investment-Based Asset Pricing Model.” Journal of Political Economy 104 (3): 572–621.\n\n\n———. 2009. Asset Pricing: Revised Edition. Princeton university press.\n\n\nEdmans, Alex. 2009. “Blockholder Trading, Market Efficiency, and Managerial Myopia.” The Journal of Finance 64 (6): 2481–2513.\n\n\nElton, Edwin J, and Martin J Gruber. 1977. “Risk Reduction and Portfolio Size: An Analytical Solution.” The Journal of Business 50 (4): 415–37.\n\n\nFama, Eugene F, and Kenneth R French. 1993. “Common Risk Factors in the Returns on Stocks and Bonds.” Journal of Financial Economics 33 (1): 3–56.\n\n\nGrullon, Gustavo, Yelena Larkin, and Roni Michaely. 2019. “Are US Industries Becoming More Concentrated?” Review of Finance 23 (4): 697–743.\n\n\nHou, Kewei, and David T Robinson. 2006. “Industry Concentration and Average Stock Returns.” The Journal of Finance 61 (4): 1927–56.\n\n\nLintner, John. 1965. “The Valuation of Risk Assets and the Selection of Risky Investments in Stock Portfolios and Capital Budgets.” The Review of Economics and Statistics 47 (1): 13–37.\n\n\nMagill, Michael, and Martine Quinzii. 1996. “Incomplete Markets over an Infinite Horizon: Long-Lived Securities and Speculative Bubbles.” Journal of Mathematical Economics 26 (1): 133–70.\n\n\nMossin, Jan. 1966. “Equilibrium in a Capital Asset Market.” Econometrica 34 (4): 768–83.\n\n\nNeuhann, Daniel, and Michael Sockin. 2024. “Financial Market Concentration and Misallocation.” Journal of Financial Economics 159: 103875.\n\n\nRoss, Stephen A. 1976. “The Arbitrage Theory of Capital Asset Pricing.” Journal of Economic Theory 13 (3): 341–60.\n\n\nSaez, Emmanuel, and Gabriel Zucman. 2016. “Wealth Inequality in the United States Since 1913: Evidence from Capitalized Income Tax Data.” The Quarterly Journal of Economics 131 (2): 519–78.\n\n\nSharpe, William F. 1964. “Capital Asset Prices: A Theory of Market Equilibrium Under Conditions of Risk.” The Journal of Finance 19 (3): 425–42.\n\n\nShiller, Robert J. 2003. “From Efficient Markets Theory to Behavioral Finance.” Journal of Economic Perspectives 17 (1): 83–104.\n\n\nValta, Philip. 2012. “Competition and the Cost of Debt.” Journal of Financial Economics 105 (3): 661–82."
  },
  {
    "objectID": "drafts/index.html",
    "href": "drafts/index.html",
    "title": "Drafts",
    "section": "",
    "text": "Working Drafts\nThese are conceptual and exploratory pieces that may evolve into papers or teaching materials."
  },
  {
    "objectID": "drafts/market_incomplete.html",
    "href": "drafts/market_incomplete.html",
    "title": "Monetary Policy and Market Imperfections",
    "section": "",
    "text": "Neoclassical economics emphasizes that rational individuals base decisions primarily on long-term trends rather than short-term fluctuations. In this view, rational agents with forward-looking expectations base decisions on an infinite-horizon optimization, meaning they will not invest in assets or projects that have a declining long-term trajectory or negative net present value. Market prices, in turn, are thought to reflect these rational expectations about future fundamentals. This philosophy is exemplified in models of Milton Friedman and his successors, where individuals smooth consumption over time and focus on permanent income, and in modern macroeconomic models that assume agents plan for the long run. The neoclassical mindset links naturally with Friedman’s monetarism, which asserts that steady, rules-based growth of the money supply yields better outcomes than reactive fine-tuning. Both frameworks assume that people anticipate the future well, so erratic policy shifts mainly lead to price changes rather than lasting gains in output or employment. Representative models include Friedman’s Permanent Income Hypothesis, Bewley’s incomplete-market model, Aiyagari’s general equilibrium model with heterogeneous agents, and the speculative asset-pricing model by Harrison & Kreps (Friedman 1957; Bewley 1986; Aiyagari 1994; Harrison and Kreps 1978).\nFriedman’s monetarism closely aligns with neoclassical thinking, promoting predictable, rules-based monetary policy to ensure market stability and long-term neutrality of money. In contrast, Keynesian and New Keynesian frameworks highlight the importance of short-term interventions due to market frictions such as price stickiness. New Keynesians agree that people are forward-looking, but they highlight that wages and prices can be “sticky” (slow to adjust), which prevents markets from clearing quickly. As a result, even rational agents may be temporarily unable to reach optimal outcomes, and monetary policy can have powerful real effects in the short term. For example, if prices cannot adjust immediately, an unexpected cut in interest rates may boost real spending and output rather than just raising prices. New Keynesian models therefore rationalize active stabilization policy – they contend that without it, the economy can suffer prolonged recessions or unemployment that a well-timed policy intervention could ameliorate. This stands in contrast to the monetarist/neoclassical view that markets self-correct relatively quickly. Despite these differences, both schools provide valuable insight: neoclassical and monetarist models offer clarity about long-run tendencies and private sector behavior under rational expectations, while Keynesian models stress the importance of short-run dynamics and coordination failures. The analysis in this paper bridges these perspectives by using theoretical models to examine how market imperfections – such as incomplete markets and heterogeneous expectations – modify the standard predictions. Although the models are abstract, each provides important long-term policy guidance. Understanding their lessons can help policymakers design monetary strategies that foster stability without creating new problems in the process."
  },
  {
    "objectID": "drafts/market_incomplete.html#introduction",
    "href": "drafts/market_incomplete.html#introduction",
    "title": "Monetary Policy and Market Imperfections",
    "section": "",
    "text": "Neoclassical economics emphasizes that rational individuals base decisions primarily on long-term trends rather than short-term fluctuations. In this view, rational agents with forward-looking expectations base decisions on an infinite-horizon optimization, meaning they will not invest in assets or projects that have a declining long-term trajectory or negative net present value. Market prices, in turn, are thought to reflect these rational expectations about future fundamentals. This philosophy is exemplified in models of Milton Friedman and his successors, where individuals smooth consumption over time and focus on permanent income, and in modern macroeconomic models that assume agents plan for the long run. The neoclassical mindset links naturally with Friedman’s monetarism, which asserts that steady, rules-based growth of the money supply yields better outcomes than reactive fine-tuning. Both frameworks assume that people anticipate the future well, so erratic policy shifts mainly lead to price changes rather than lasting gains in output or employment. Representative models include Friedman’s Permanent Income Hypothesis, Bewley’s incomplete-market model, Aiyagari’s general equilibrium model with heterogeneous agents, and the speculative asset-pricing model by Harrison & Kreps (Friedman 1957; Bewley 1986; Aiyagari 1994; Harrison and Kreps 1978).\nFriedman’s monetarism closely aligns with neoclassical thinking, promoting predictable, rules-based monetary policy to ensure market stability and long-term neutrality of money. In contrast, Keynesian and New Keynesian frameworks highlight the importance of short-term interventions due to market frictions such as price stickiness. New Keynesians agree that people are forward-looking, but they highlight that wages and prices can be “sticky” (slow to adjust), which prevents markets from clearing quickly. As a result, even rational agents may be temporarily unable to reach optimal outcomes, and monetary policy can have powerful real effects in the short term. For example, if prices cannot adjust immediately, an unexpected cut in interest rates may boost real spending and output rather than just raising prices. New Keynesian models therefore rationalize active stabilization policy – they contend that without it, the economy can suffer prolonged recessions or unemployment that a well-timed policy intervention could ameliorate. This stands in contrast to the monetarist/neoclassical view that markets self-correct relatively quickly. Despite these differences, both schools provide valuable insight: neoclassical and monetarist models offer clarity about long-run tendencies and private sector behavior under rational expectations, while Keynesian models stress the importance of short-run dynamics and coordination failures. The analysis in this paper bridges these perspectives by using theoretical models to examine how market imperfections – such as incomplete markets and heterogeneous expectations – modify the standard predictions. Although the models are abstract, each provides important long-term policy guidance. Understanding their lessons can help policymakers design monetary strategies that foster stability without creating new problems in the process."
  },
  {
    "objectID": "drafts/market_incomplete.html#core-assumptions-of-neoclassical-economics-and-monetarism",
    "href": "drafts/market_incomplete.html#core-assumptions-of-neoclassical-economics-and-monetarism",
    "title": "Monetary Policy and Market Imperfections",
    "section": "Core Assumptions of Neoclassical Economics and Monetarism",
    "text": "Core Assumptions of Neoclassical Economics and Monetarism\nThree core assumptions underpin the neoclassical and monetarist perspective in macroeconomics:\n\nRational expectations and infinite-horizon optimization: Individuals and firms are assumed to form expectations about the future in a rational way (using all available information) and to optimize their decisions over an infinite time horizon. In practice, this means economic agents base current consumption, saving, and investment on the expected present value of long-term outcomes, rather than overreacting to short-lived changes. They anticipate the future effects of policies, so systematic policy actions are largely already “priced in” to decisions. This assumption was emphasized by the new classical economists who followed Friedman, such as Robert Lucas, in arguing that only unexpected policy moves affect real behavior in the short run. Under rational expectations, people won’t persistently spend windfalls or chase assets whose fundamentals don’t justify their price, since they foresee the eventual reversion to fundamental value.\nMarket clearing in the long run (flexible prices): Neoclassical models typically assume that prices of goods, services, and factors adjust to equilibrate supply and demand, at least in the long run. While short-term frictions can occur, the long-run default is an economy at full employment with resources fully utilized. Any deviations (recessions or booms) are seen as temporary, provided policy does not introduce long-term distortions. This view contrasts with Keynesian models where wages or prices might remain out of equilibrium for an extended period. The neoclassical stance is that given enough time, economic forces will push the economy back to its potential output with stable growth. Monetarists, too, believed that “markets naturally move toward a stable center” in the absence of big shocks. Thus, they argue against aggressive intervention that attempts to exploit short-run trade-offs (like pumping up output at the cost of higher inflation), because eventually prices adjust and only inflation remains.\nNeutrality of money regarding real economic outcomes in the long run:(Lucas Jr 1972; Friedman 1968). A cornerstone of monetarism and neoclassical thought is that changes in the money supply only have transient effects on real variables (output, employment) and no effect in the long run. In the long run, an exogenous increase in the money stock is reflected in higher nominal prices and wages, but real consumption, investment, and output return to their original path. In other words, money is “neutral” with respect to real economic activity once prices have fully adjusted. Most economists agree that this long-run neutrality holds approximately true in practice – doubling the money supply eventually doubles the price level – and monetarists place great importance on it. This assumption underlies the monetarist recommendation to avoid monetary surprises: any attempt to permanently boost employment by printing money will just create inflation once people’s expectations catch up. Rational agents, thinking in an infinite-horizon framework, will not be tricked for long; they come to expect higher inflation, negating any output gains. Monetary policy, therefore, is seen primarily as a tool for controlling inflation and nominal variables, not as a way to engineer long-term higher growth.\n\nThese core assumptions shape the policy mindset in the neoclassical/monetarist framework. If agents are highly forward-looking and markets tend to clear, discretionary stabilization policy has limited power – it might only cause short-term blips or even destabilize expectations. Instead, maintaining credible, consistent policy (such as a steady money growth rule or inflation target) is viewed as the optimal approach for long-run welfare. In the next sections, we examine how relaxing some of these assumptions – by introducing incomplete markets, borrowing constraints, or heterogeneous beliefs – changes the conclusions and policy implications."
  },
  {
    "objectID": "drafts/market_incomplete.html#complete-vs.-incomplete-markets",
    "href": "drafts/market_incomplete.html#complete-vs.-incomplete-markets",
    "title": "Monetary Policy and Market Imperfections",
    "section": "Complete vs. Incomplete Markets",
    "text": "Complete vs. Incomplete Markets\nComplete markets allow full insurance against risks and unlimited borrowing, resulting in smooth consumption aligned with permanent income. Incomplete markets, however, feature borrowing constraints and uninsurable shocks, creating heterogeneity and precautionary savings, leading to wealth disparities and more volatile consumption patterns (Aiyagari 1994; Bewley 1986). Thus, one fundamental way real economies depart from the idealized benchmark is that markets are incomplete. In a complete market environment, individuals can fully insure against uncertainties and can borrow or save freely at a given interest rate. In that ideal case, people smooth their consumption over time nearly perfectly. Consumption ends up being much less volatile than income, because during bad times individuals can borrow or draw on savings, and during good times they can save extra income for the future. In fact, theory predicts that with complete markets, consumption at any point reflects an individual’s permanent income (the expected long-term average income), not the transitory ups and downs of current income. A direct implication is that temporary policy measures (like one-time stimulus checks or short-lived tax cuts) would have only a small effect on consumption—because rational consumers know such windfalls are transitory, they prefer to save a large portion of them, aiming to maintain a stable consumption path. In a complete market, households effectively pool risks and smooth out idiosyncratic shocks; as a result, their spending is steady and mainly influenced by changes in expected lifetime resources rather than short-term liquidity fluctuations.\nBy contrast, in incomplete market settings, individuals do not have access to perfect insurance or unlimited borrowing. They face idiosyncratic income shocks (e.g., job loss, illness) that they must largely bear on their own. Additionally, they may encounter liquidity constraints or borrowing limits that prevent them from smoothing consumption fully. The models developed by Truman Bewley, S. Rao Aiyagari, and others formalize this situation. In these models, all agents are ex ante identical (they have the same preferences and potential income distribution), but they become ex post heterogeneous because each experiences different income shocks over time and they cannot completely insure against these shocks (Bewley 1986; Aiyagari 1994). Households thus engage in precautionary saving—they tend to save more when they have income, building a buffer of assets to self-insure against future bad draws. Consumption is no longer completely smooth; when a negative shock hits a liquidity-constrained household, it may have to cut consumption sharply because it cannot borrow easily. Conversely, a positive shock to a hand-to-mouth household leads to a spike in consumption if they were previously constrained.\nIncomplete markets therefore produce higher marginal propensities to consume out of transitory income changes—in other words, constrained people would spend a larger fraction of any temporary income windfall than they would under complete markets (Aiyagari 1994). This is consistent with empirical data showing many households, especially those with low wealth, quickly spend stimulus payments or bonuses, as they have unmet needs or debts to pay.\nAnother key difference is that incomplete markets generate a non-trivial distribution of wealth. Since each individual’s asset accumulation depends on their history of shocks and their precautionary saving motive, over time the economy develops inequality in wealth and consumption. Some agents will build up sizable precautionary balances (if they experience good income luck or have frugal preferences), while others might remain near the borrowing constraint with minimal savings. The wealth distribution in such models is typically highly skewed, capturing the fact that a small fraction of people may hold a large share of total assets – a feature very much in line with real-world data. By contrast, in a representative agent or complete markets model, distributional issues are either absent or of no consequence, since everyone effectively pools risks together. Incomplete markets thus bring distributional considerations to the forefront of macroeconomic analysis.\nFor policymakers, these differences mean that monetary and fiscal policy can have uneven effects across the population and can influence aggregate demand through channels that are muted in complete-market models. For example, an interest rate cut in an incomplete market setting might stimulate borrowing and spending for some agents, but for others it mainly reduces their interest income (if they are savers), potentially widening inequality. Likewise, a government stimulus targeted at liquidity-constrained households could yield a relatively large boost to consumption (due to their high propensity to consume out of additional income), whereas the same payment to a wealthier, fully insured household might just be saved. In summary, incomplete markets make the macroeconomy less “frictionless” and more sensitive to distribution and credit conditions. We next examine specific models that incorporate these features, to draw out their policy insights.\n\nFriedman Consumption Smoothing Model (Complete Markets)\nMilton Friedman’s model of consumption – known as the Permanent Income Hypothesis (PIH) – suggests consumption depends primarily on permanent rather than temporary income changes. Hence, monetary policy must focus on long-term credibility rather than short-run stimulus (Friedman 1957; Hall 1978). This is a cornerstone of the neoclassical view on consumption behavior. Friedman proposed that an individual’s consumption at any given time is determined not by current income alone, but by their permanent income, which is the expected long-term average income. Temporary fluctuations in income, according to this theory, have only a small effect on consumption because people use saving and borrowing to smooth out those fluctuations. In other words, households act like long-term planners: if they receive an unexpectedly high income this year, they will not dramatically raise their spending, understanding that the extra income may not last. Instead, they will save most of it (or pay down debt), spreading the benefit over future years. Conversely, if income dips briefly, they can draw on past savings or borrow to maintain their usual consumption level, expecting to repay when income recovers. This behavior leads to relatively stable consumption paths, as illustrated by Friedman’s famous observation that consumption is much smoother than the often volatile income streams that individuals experience year to year.\nFriedman’s model assumes that credit markets function well (people can borrow against future income) and that consumers are forward-looking and rational. Under these conditions, monetary and fiscal policy have limited ability to alter consumption unless they affect expected long-term income. For example, a one-time tax rebate or a temporarily lower interest rate might not stimulate much extra spending – consumers recognize that this is a short-term change. Indeed, a key takeaway of the permanent income theory is that policies which only increase current income without raising expected future income will mostly lead to higher saving rather than higher spending. Friedman contrasted this with the Keynesian view in which consumers have a high marginal propensity to consume out of current income (perhaps because they are myopic or liquidity-constrained). He argued that the Keynesian assumption was flawed in ignoring forward-looking behavior. Empirical puzzles of the mid-20th century (such as why consumption didn’t rise one-for-one with income gains from, say, war-time fiscal expansions) could be explained by PIH: people understood those income gains were temporary and saved much of them.\nIn policy terms, the Friedman consumption model supports a rather conservative use of demand management. A central bank that rapidly expands money or lowers interest rates might not trigger a large consumption boom unless people believe those actions will persist and raise their permanent income or wealth. Similarly, a government stimulus check will be partly saved if households treat it as a transitory windfall. An important implication is that discretionary policy “surprises” are not a reliable way to boost aggregate demand – rational agents will react mainly to the expected persistent components of policy. Monetarists like Friedman instead advocated rule-based policies (such as steadily growing the money supply at a fixed rate) to provide a stable environment for consumers and investors to plan. If policy is erratic, it could even be counterproductive: for instance, trying to exploit a short-run trade-off by pushing unemployment lower than its natural rate would just raise inflation expectations, with little lasting benefit to output (this is essentially Friedman’s adaptive expectations version of the Phillips Curve argument). In summary, Friedman’s complete-market consumption model underscores the importance of expectations and permanent income. It suggests that monetary policy should focus on the long-term nominal stability (controlling inflation) and avoid frequent discretionary shifts, because people will see through those shifts and adjust their saving behavior accordingly. It also implies that fiscal stabilization (e.g. stimulus payments) will be most effective when aimed at households likely to be liquidity-constrained, a point that becomes clearer once we consider incomplete market models.\n\n\nConsumption-Investment Trade-off under Liquidity Constraints\nLiquidity constraints disrupt optimal consumption-investment trade-offs. Constrained agents cannot invest sufficiently during downturns, weakening monetary policy’s effectiveness, particularly in stimulating investment or consumption among constrained households. For example, consider a household facing liquidity constraints and uncertain future income. An interest rate cut reduces the returns on their precautionary savings, forcing the household to reduce the buffer stock meant to protect against income fluctuations. Consequently, this household may have limited resources available for productive investments such as education or small business expansion. Younger or non-saver households facing liquidity constraints prioritize current consumption over future consumption due to diminishing marginal returns of future utility. Thus, when interest rates decrease, these households are more likely to immediately spend additional available funds rather than accumulate precautionary savings or invest in long-term productive assets. In contrast, a wealthier, unconstrained household may use lower interest rates to cheaply finance additional investment opportunities, potentially increasing their wealth relative to constrained households.\nA central theme in a standard intertemporal choice problem is the trade-off between consuming today and investing for tomorrow. In a frictionless world, consumers equate the marginal benefit of spending an extra dollar today with the marginal benefit of saving that dollar (investing it to spend later). This optimality condition (often called the Euler equation in macroeconomics) ensures that resources are allocated to their most valued use over time. However, in reality many households and firms face liquidity constraints or borrowing limits that prevent them from freely making this trade-off. Such constraints are a key imperfection that alters the impact of monetary policy and other shocks.\nWhen agents are liquidity-constrained, they cannot borrow as much as they would like against future income. This means in bad times they might want to maintain consumption or invest in opportunities (human capital, business expansion, etc.), but they simply lack the funds or credit access to do so. Consequently, current consumption may fall below the level that would be chosen under complete markets, and valuable investments might be foregone. For instance, a skilled worker who becomes unemployed may cut back sharply on consumption – not because their lifetime income prospects are shattered, but because in that moment they don’t have liquid assets or credit to smooth over the gap. Likewise, a small business might pass up a profitable investment because banks refuse credit due to the firm’s lack of collateral. These scenarios lead to a suboptimal allocation of resources over time, amplifying short-run fluctuations and causing longer-run consequences (lost growth from underinvestment, etc.).\nFrom a policy perspective, liquidity constraints mean that monetary policy may have an asymmetric effect. If the central bank raises interest rates, it generally cools off borrowing and spending – both unconstrained and constrained agents will cut back (the former by choice, the latter perhaps by necessity as credit becomes more expensive or scarce). But if the central bank lowers interest rates to stimulate the economy, those who are constrained might still be unable to borrow (banks may not lend to them even at low rates, if their balance sheet is weak or job uncertain) and thus cannot increase consumption or investment. In other words, there is a segment of the population for whom monetary easing doesn’t translate into more spending because they were not borrowing in the first place (they were at their borrowing limit). Instead, the stimulus might mainly induce already well-capitalized agents to borrow or invest more – which can have distributional effects.\nOn the other hand, consider fiscal policy: a transfer (like a stimulus check or unemployment benefit extension) given to a liquidity-constrained household is likely to be spent in large part, precisely because that household’s consumption was suppressed by lack of funds. Empirical evidence and incomplete-market models both find that households with little liquid wealth have high marginal propensities to consume (MPCs) out of such transfers. This contrasts with the near-zero MPC out of a transitory income increase for a fully smoothed consumer in Friedman’s framework. Therefore, liquidity constraints reconcile why Keynesian-style demand stimulus can work in practice (many people do spend most of an extra dollar if they were cash-strapped), even though Friedman’s theory might suggest they shouldn’t. Modern heterogeneous agent models incorporate this insight by showing that when a large fraction of consumers are hand-to-mouth or buffer-stock savers, aggregate consumption is sensitive to the distribution of income and cash-on-hand.\nFor investment, liquidity constraints imply that not all investment opportunities are realized, especially among smaller firms or entrepreneurs, if external finance is costly or unavailable. In a recession, even if the central bank slashes interest rates, banks may be risk-averse and tighten lending standards, so only the safest borrowers benefit from low rates. This can lead to a situation often described as “pushing on a string,” where monetary policy loses traction in stimulating additional private investment or consumption because the bottleneck is in credit access, not the cost of credit per se.\nIn summary, the consumption-investment trade-off under liquidity constraints highlights that market imperfections can dampen or distort the transmission of monetary policy. A perfectly rational, unconstrained agent might respond to lower interest rates by optimally borrowing and spending more (since the opportunity cost of funds is lower). But a constrained agent does nothing (they can’t borrow anyway), and an unconstrained wealthy agent might already be satiated in consumption and only shift their portfolio. These dynamics mean that in downturns, monetary policy might need support from fiscal measures that target constrained agents to be fully effective. It also means that policymakers should be aware of credit conditions and possibly use regulatory tools to ensure that rate cuts get passed through to borrowers. The general principle is that in the presence of liquidity constraints, short-run fluctuations can have long-run costs (foregone investment, lower human capital accumulation) and policies should aim to alleviate these constraints during bad times.\n\n\nBewley Model: Precautionary Savings in an Incomplete Market\n\nAssumes heterogeneous agents face idiosyncratic income shocks and borrowing limits.\nExogenous interest rates.\nGenerates wealth inequality through precautionary savings.\nHighlights the importance of social safety nets and targeted fiscal policies for macroeconomic stability (Bewley 1986).\n\nThe Bewley model (named after economist Truman Bewley) is a foundational framework for analyzing incomplete markets with heterogeneous agents. In Bewley’s setup, we consider a large number of infinitely-lived consumers who face idiosyncratic income shocks in each period. These shocks are uninsurable – there is no complete set of insurance markets for them – and consumers can only trade a single risk-free asset (such as a bond or money) to self-insure. Moreover, consumers face a borrowing limit (they cannot have debt beyond a certain level, often this ad hoc level is set to zero for simplicity). Despite all consumers having the same preferences and income process ex ante, the randomness of shocks makes them heterogeneous ex post in terms of their asset holdings and current income. This type of model is often called a heterogeneous agent incomplete-markets model, or simply a Bewley model, after the seminal work in (Bewley 1986). It has become a workhorse for understanding consumption, saving, and wealth distribution under uncertainty.\nIn the Bewley model, each consumer solves a consumption-saving problem: how much to consume today versus save as a buffer for future uncertainty. A typical finding is the emergence of a precautionary saving motive – people save not just for lifecycle reasons (retirement, etc.) but also to buffer against income risk. Those who experience good shocks build up assets, while those hit by bad shocks draw down assets or if they have none, they hit the borrowing constraint and their consumption drops. Over time, the model reaches an equilibrium where the cross-sectional distribution of wealth is stationary (in a statistical sense): some fraction of the population has high wealth, some has low wealth, with persistent inequality generated purely from idiosyncratic risk and saving behavior. This equilibrium typically features a fat-tailed distribution, meaning there are some very high-wealth individuals (who had a run of good shocks or especially strong saving discipline) and a significant mass of low-wealth individuals who might be frequently at the edge of the borrowing constraint. Quantitatively, such models can generate wealth concentration that qualitatively resembles that observed in real economies (though matching the extreme concentration in actual data often requires adding other elements like heterogeneity in earnings ability or rates of return).\nOne key aspect of the Bewley model is that the interest rate is treated as exogenous (or determined outside the model, say by a central bank or a global capital market). In other words, Bewley’s original formulation is a partial equilibrium analysis: it looks at an individual’s optimal saving given an interest rate, but does not necessarily determine that interest rate from within the model. This is akin to studying a small open economy where people can save or borrow at a fixed world interest rate, or a situation with a perfectly elastic supply of funds. Under this fixed interest rate, not everyone can dissave indefinitely because of the borrowing constraint, so in aggregate there will typically be positive net saving (since precautionary motives induce people to hold assets). If the interest rate is high relative to people’s time preference and risk, the low-wealth agents will borrow up to the limit and the high-wealth will save a lot, and an equilibrium wealth distribution forms. If the interest rate is too high, precautionary saving might not be enough to sustain it (people try to borrow too much); if it’s too low, people accumulate assets and the economy might reach a point where the lowest wealth is at the borrowing limit and highest is still saving – typically there is some interest rate that balances asset demand and the “excess” of precautionary saving.\nWhile the technical details can be involved, the intuition gleaned from the Bewley model is powerful for policy. It shows how incomplete markets alone (without any price rigidity or aggregate shocks) can lead to under-consumption by some and the accumulation of large buffers by others. This has implications for long-run growth and inequality. If many people are constrained and cannot invest in their education or businesses, the economy might underperform its potential. It also implies that policies like social insurance (unemployment insurance, social security, etc.) can affect aggregate outcomes: for example, providing more generous unemployment benefits might reduce the need for precautionary saving, which could actually stimulate consumption among lower-wealth households and reduce inequality. On the flip side, too generous a safety net could reduce the incentive to save at all. Bewley-type models have been used to examine optimal policy in this context, such as what level of unemployment insurance optimally trades off providing insurance versus maintaining incentives.\nAn important extension of the Bewley model is to use it for wealth distribution insights. The model clarifies that even if everyone has identical earning potential, incomplete markets will generate inequality simply due to luck and precautionary behavior. This suggests that some observed inequality is not due to differences in skill or hard work, but due to insufficient insurance against life’s risks. Policymakers concerned with excessive inequality might draw on this insight to justify progressive taxation or public insurance programs that effectively do what missing markets would have – help smooth incomes and consumption across states of the world. Indeed, one policy implication highlighted in such models is that improving access to credit for credit-worthy but constrained households, or providing more public insurance, could make the overall economy better off by allowing more efficient consumption and investment choices (though there are always trade-offs and moral hazard issues to consider).\nIn summary, the Bewley model provides a micro-founded explanation for why some people end up liquidity-constrained and how that influences their behavior. For monetary policy, it warns that aggregate demand may be more sensitive to the distribution of wealth and income than traditional models would suggest – if a recession hits the lower-wealth population hard, their consumption will contract strongly (since they can’t borrow), potentially deepening the downturn. Purely focusing on interest rates as a lever might be insufficient; fiscal redistributive tools or direct transfers could be more potent in such scenarios. The model’s relevance has grown as economists recognize the limitations of the representative-agent paradigm and seek to incorporate heterogeneous agent effects into macroeconomic policy analysis.\n\n\nAiyagari Model: General Equilibrium with Incomplete Markets\n\nIncorporates endogenous determination of interest rates through production equilibrium.\nDemonstrates “excess capital accumulation” due to precautionary motives.\nAdvocates capital income taxation for improved welfare and highlights the distributional consequences of monetary policy changes (Aiyagari 1994).\n\nS. Rao Aiyagari’s model builds directly on the Bewley framework but adds an important layer: a production economy that yields a general equilibrium determination of prices (interest rate and possibly wages). In the Aiyagari (1994) model, we still have infinitely-lived agents with idiosyncratic income shocks and borrowing constraints (precisely the Bewley setup on the household side), but now those households supply savings to, and borrow from, a productive sector with capital. In essence, Aiyagari embeds the precautionary savings behavior into a full macroeconomic model with capital accumulation. The result is a self-contained macroeconomic equilibrium where the interest rate is endogenously determined by the supply and demand for capital, rather than being fixed externally. Households’ collective saving (driven by precautionary motives) feeds into the capital stock, and firms’ demand for capital (based on productivity and diminishing returns) determines the equilibrium interest rate that clears the capital market.\nOne of Aiyagari’s key findings is that in an economy with uninsurable income risk, the equilibrium interest rate will generally be lower than it would be in a comparable complete-markets economy. Intuitively, because households value holding assets as a buffer (beyond what they would in a no-risk scenario), they tend to save more, which pushes down the return to capital. In other words, there is excess aggregate saving due to precautionary motives, leading to a larger capital stock and lower interest rate than the classical model without income risk would predict. This is sometimes referred to as the “Aiyagari excess capital result.” It implies that the laissez-faire outcome might not be socially optimal – there could be “too much” capital from a certain perspective, because individuals don’t internalize that by saving so much for themselves, they depress the return for everyone. One practical implication Aiyagari pointed out is that a government could improve welfare by taxing capital income and redistributing it (or using it to fund social insurance) in such an economy. By doing so, it reduces the need for individuals to self-insure via excessive capital accumulation, potentially moving the economy closer to the golden-rule level of capital (where consumption is maximized). This was a striking result since in a standard frictionless model, capital taxation is often detrimental in the long run – but here, moderate capital taxation can correct an inefficiency arising from incomplete markets.\nThe Aiyagari model also provides insight into the interplay between inequality and aggregate production. Unlike the Bewley model, which was partial equilibrium, here the distribution of wealth affects aggregate supply (through capital accumulation). If the wealth is concentrated in fewer hands, the aggregate consumption could be lower (since wealthy individuals have lower MPCs, they might save a lot of their income), and the aggregate capital might be higher (since those with excess wealth invest it). This has led to extensive research on the quantitative impact of redistributive policies on growth and output. For instance, if you redistribute wealth from the rich (low MPC) to the poor (high MPC), you might raise current consumption but reduce saving and thus future capital – whether that is good or bad for long-run output depends on parameters, but in some cases it can actually increase output if the economy was above the golden rule level of capital to start with (Marcet, Obiols-Homs, and Weil 2007).\nAnother aspect is the feedback of interest rates on inequality. In Aiyagari’s equilibrium, the interest rate settles at a level where households are indifferent between saving and not saving (on the margin). If interest rates are very low, borrowing is cheap, but also the reward for saving is low, which could discourage some saving. However, typically in these models many households still save because of risk aversion and precaution. The low interest rate also means that those who are borrowing-constrained are not paying a huge interest burden (assuming they can borrow at that rate), but many cannot borrow much anyway due to the constraint. Overall, compared to a representative-agent model, the Aiyagari model predicts different responses to monetary policy. For example, if the central bank lowers the interest rate (below the equilibrium that would prevail from just technology and time preference), it transfers resources from savers to borrowers. In an economy with inequality, this has non-neutral effects: borrowers (often poorer agents) gain relief and might consume more, while savers (wealthier agents) earn less on their assets and might consume less (or seek riskier investments). The distributional effects of monetary policy come into play. Recent research in heterogeneous agent New Keynesian (HANK) models builds on this by adding nominal rigidities, but even in the basic Aiyagari model, one can see that monetary policy is not just about one representative agent’s intertemporal choice – it will create winners and losers due to heterogeneity in assets and consumption propensities.\nFor policymakers, Aiyagari’s work underscores a few points: (1) Monetary neutrality may not hold cleanly in the short run even if prices are flexible, because redistributions caused by interest rate changes can affect aggregate demand; (2) there may be a role for permanent fiscal policy (like capital taxation or debt issuance) to influence the long-run capital stock and interest rate in a way that improves welfare, countering the incomplete-market externality; (3) evaluating monetary policy requires understanding the underlying wealth distribution – for instance, a low interest rate environment will tend to benefit borrowers and younger households (via cheaper credit, higher asset values) while hurting those who rely on interest income (like pensioners or wealthier rentiers). If mismanaged, prolonged ultra-low rates can contribute to asset price inflation (as savers seek returns in real estate or stocks), thereby widening wealth inequality if only the already-wealthy hold those appreciating assets. Indeed, some attribute the rise in asset valuations and wealth concentration in recent decades partly to very low global real interest rates and ample liquidity, consistent with the mechanisms in Aiyagari-type models.\nIn conclusion, the Aiyagari model enriches our understanding by marrying heterogeneity with production. It reminds us that macroeconomic policy cannot be divorced from distributional considerations. The long-term natural rate of interest, the effectiveness of fiscal redistribution, and the impact of monetary policy all look different once we acknowledge that not everyone is alike in the economy. By capturing how liquidity constraints and precautionary savings influence aggregate capital, this model provides guidance on questions like whether and how to tax wealth, and how aggressive monetary policy should be in, say, pushing interest rates to very low levels. Policymakers drawing on these insights might strive for a balance: ensuring there is enough aggregate saving for investment and growth, but not so much that it reflects unmet social insurance needs or creates financial imbalances.\n\n\nHarrison & Kreps (1978) Model: Asset Pricing with Heterogeneous Beliefs\n\nHeterogeneous investor beliefs combined with short-sale constraints can cause speculative bubbles, elevating asset prices above fundamental values.\nSuggests improving market completeness and transparency to curb speculation and volatility (Harrison and Kreps 1978).\n\nThe Harrison and Kreps (1978) model introduces a different kind of market imperfection into macro-finance: heterogeneous beliefs among investors, combined with constraints on short selling. Unlike the previous models (which focused on borrowing constraints and income risk), this model lives in the world of asset trading and speculation. Harrison and Kreps asked what happens in an asset market when investors have differing opinions about an asset’s value and they are not allowed to short sell the asset freely. Their answer was groundbreaking: even if all investors are rational (in that they update beliefs consistently with their own information), the mere diversity of opinion can lead to asset prices exceeding the valuation of even the most optimistic individual investor.\nHere’s the intuition: suppose some investors (“optimists”) believe a stock or house will be very valuable in the future, while others (“pessimists”) believe it will not. If short selling is constrained (pessimists cannot easily borrow the asset to sell it short), the market price will be determined largely by the optimists’ willingness to pay. Now add the element of speculation – investors may buy an asset not just for its fundamental value (like dividends or rent) but also for the option to resell it in the future. Harrison & Kreps showed that when beliefs differ, an investor might pay more than their own estimate of the asset’s fundamental value because they anticipate that someone even more optimistic might buy it at a higher price later. In effect, a resale option is priced in. This leads to what we might call a speculative premium on the asset price. The price can rise above the level that any single investor would pay if they had to hold the asset forever. In their words, the right to resell makes investors willing to pay more than the asset’s “hold-to-maturity” value. The inability of pessimists to short sell means nothing counteracts this upward pressure – the pessimists simply sit out of the market rather than actively pushing the price down by shorting. Thus, the market price reflects an over-optimistic valuation, driven by the most bullish views and the prospect of flipping the asset.\nThis mechanism helps explain phenomena like asset price bubbles or situations where market prices seem to detach from fundamental values. Real estate is a commonly cited example: investors might buy houses at high prices not only because they expect rising rents or income (fundamentals), but because they think they can later sell the house to someone else at an even higher price (speculation). If enough people believe housing prices will keep rising, and skeptics can’t effectively short the housing market, the result is a self-reinforcing price boom. The Harrison-Kreps model formalized how even fully rational agents with rational expectations (each given their own belief) can end up trading at prices that embed a speculative component. It doesn’t require irrational exuberance; it only requires disagreement and some friction (short-sale constraints) that prevents full arbitrage. In their equilibrium, everyone understands the price is above their own fundamental valuation, but they also know someone else might be willing to pay even more, so it can still be rational to buy now and plan to sell later – a clear parallel to the greater fool theory, but derived in a rigorous way.\nPolicy implications from the Harrison & Kreps model revolve around financial market regulation and information disclosure. One implication is that short-selling constraints can fuel overpricing. If regulators make short selling too restrictive (perhaps in an attempt to curb volatility or prevent speculative attacks), they might inadvertently remove a balancing force that keeps prices close to fundamentals. The model would suggest that allowing more short selling (with proper oversight to avoid abuse) could actually lead to more informative, less one-sided pricing. Another implication is the value of transparency and common information. In the model, beliefs are heterogeneous and “dogmatic” – each trader sticks to their prior and interprets signals in their own way. If public information can help align beliefs (or at least inform the pessimists and optimists of each other’s views), it might reduce the degree of disagreement. However, complete agreement is unrealistic; differences in models, data interpretation, or risk appetite will always create some dispersion of opinion.\nFrom a monetary policy perspective, one might not immediately see a connection, since H&K is about asset pricing in a frictional financial market. But there are subtle links. Central banks today pay close attention to asset markets – housing, equities, etc. – because large deviations of asset prices from fundamentals can pose risks to financial stability and the broader economy. For instance, if low interest rates contribute to a speculative housing boom (by making borrowing cheap and encouraging optimistic beliefs about ongoing price growth), a subsequent crash could harm banks and consumers, leading to a recession. The H&K model suggests that a booming asset market is not necessarily a sign of solid fundamentals; it could be a sign of constrained pessimism and resale-driven pricing. Policymakers, therefore, should be cautious in interpreting asset price signals. It also provides an argument for macroprudential policies: tools that directly address asset market excess (for example, tighter loan-to-value ratios in mortgage lending during a housing boom, or stricter margin requirements in stock trading). These can be seen as ways to mitigate the speculative dynamics – essentially pricking bubbles before they grow too large. By making it harder to purely speculate (through leverage restrictions) or by encouraging more two-sided markets (perhaps by permitting certain derivatives or short positions), regulators might reduce the likelihood of severe mispricings.\nIn summary, the Harrison & Kreps model adds another layer to our understanding: market outcomes can be inefficient not just because of real-side frictions (like incomplete insurance) but also because of financial-side frictions (like trading constraints and belief dispersion). It is a reminder that even with rational actors, markets may need regulatory oversight to ensure they reflect true economic value. For a policymaker, being aware of this mechanism is important. It cautions against assuming that all investors have the same expectations (they don’t), and it illustrates why asset price booms can develop even without obvious irrationality. Recognizing a speculative bubble early is notoriously difficult, but understanding models like this helps officials appreciate the warning signs (e.g., when asset prices only make sense under very optimistic scenarios and buyers cite the ability to resell as justification). It also supports measures to improve market completeness – such as permitting more sophisticated financial instruments – because a more complete market (ability to hedge, to short, etc.) ironically may prevent the wild swings that incomplete markets allow."
  },
  {
    "objectID": "drafts/market_incomplete.html#policy-implications-of-models",
    "href": "drafts/market_incomplete.html#policy-implications-of-models",
    "title": "Monetary Policy and Market Imperfections",
    "section": "Policy Implications of Models",
    "text": "Policy Implications of Models\n\nMonetarism emphasizes stable, predictable policy frameworks.\nBewley and Aiyagari models highlight social insurance and targeted redistribution.\nHarrison & Kreps stress regulation of speculative financial markets.\n\nEach theoretical framework provides distinct policy insights for designing monetary and economic strategies. Key implications can be summarized clearly:\n\nFriedman/Monetarist (Complete Markets)\nMonetarist models emphasize predictable and stable monetary policies. Temporary stimulus measures are ineffective since consumers respond mainly to permanent income changes. Monetary policy should thus follow clear, credible rules (e.g., a fixed money growth rate or inflation targeting), focusing on long-term price stability. Fiscal prudence is advised, as large deficits can be counterproductive due to anticipated future taxation (Ricardian equivalence). Ultimately, monetary policy should avoid attempts to permanently influence real variables like unemployment or income distribution directly, which are better addressed through structural and fiscal reforms.\nBewley (Incomplete Markets)\nThe Bewley model underscores the importance of social insurance and targeted redistribution. Because individuals face uninsurable risks and liquidity constraints, they maintain precautionary savings, limiting investment and consumption in downturns. Thus, policies providing social safety nets (unemployment insurance, healthcare, targeted stimulus payments) help stabilize aggregate demand and prevent severe economic downturns. Furthermore, targeted redistribution or improved credit access can mitigate inequality-driven demand shortfalls, addressing systemic underconsumption and secular stagnation risks. Effective policy involves balancing adequate insurance to stabilize demand without overly dampening incentives to work or save.\nAiyagari (Incomplete Markets & General Equilibrium)\nExtending Bewley’s framework, Aiyagari emphasizes that precautionary saving leads to excessive capital accumulation and lower equilibrium interest rates. A crucial policy implication is that moderate capital income taxation, combined with redistribution, can improve overall welfare. Monetary policy, particularly interest-rate adjustments, significantly affects wealth distribution—low rates benefit borrowers (often younger or lower-wealth groups) at the expense of savers. Policymakers should therefore coordinate monetary and fiscal policies to achieve optimal capital allocation, moderate inequality, and promote balanced, sustainable economic growth.\nHarrison & Kreps (Speculative Markets)\nThe Harrison & Kreps model highlights the necessity of financial market regulation and transparency to address speculative bubbles driven by heterogeneous beliefs and short-sale constraints. Policy interventions should facilitate market completeness (e.g., by allowing regulated short selling or derivatives trading) and enforce transparency through robust disclosure standards. During speculative booms, targeted macroprudential tools (loan-to-value ratios, capital buffers) and proactive central-bank communication can mitigate bubbles without overly aggressive monetary tightening, thus integrating financial stability concerns effectively into monetary policy.\n\nIn summary, a robust monetary policy approach integrates these perspectives, recognizing the importance of stable monetary frameworks, adequate social insurance, fiscal coordination, and prudent financial regulation. Successful policy practice involves using complementary tools (monetary, fiscal, regulatory) and adapting dynamically based on the interplay of inflation stability, wealth distribution, and financial stability. Historical experiences, such as the 2008 crisis and the COVID-19 response, illustrate the effectiveness of combining these insights into comprehensive, multidimensional policy strategies."
  },
  {
    "objectID": "drafts/market_incomplete.html#bridging-the-gap-between-theory-and-reality",
    "href": "drafts/market_incomplete.html#bridging-the-gap-between-theory-and-reality",
    "title": "Monetary Policy and Market Imperfections",
    "section": "Bridging the Gap Between Theory and Reality",
    "text": "Bridging the Gap Between Theory and Reality\nModels should guide, not dictate, policy. No single model fully captures the complexity of real economies; thus, policymakers must integrate insights across multiple frameworks—complete markets, incomplete markets, and speculative dynamics—to design effective policies.\nReal economies simultaneously involve heterogeneous agents, liquidity constraints, price rigidities, and speculative market behavior. Friedman’s monetarism emphasizes stable rules and credible expectations; Bewley and Aiyagari highlight liquidity constraints and precautionary savings; Harrison & Kreps emphasize speculation driven by belief heterogeneity. Modern macroeconomic approaches increasingly integrate these elements (e.g., Heterogeneous Agent New Keynesian models).\nHowever, real-world policymaking faces challenges not fully accounted for by theory:\n\nExpectations and Behavioral Factors: Individuals often deviate from rational expectations, resulting in behavioral biases and herd behaviors. Effective policy should therefore actively manage expectations through credible communication (forward guidance).\nPolitical Economy and Credibility: Optimal theoretical policies may not align with political realities. Policymakers thus rely on robust frameworks—such as inflation targeting and automatic stabilizers—that remain effective even under delayed or politically compromised responses.\nFinancial and Global Contexts: Traditional models underestimated financial sector impacts on policy effectiveness. Policymakers must explicitly account for banking sector stability and international capital flows in policy design.\n\nIn practice, policymakers should stress-test policies using multiple models, employ targeted empirical approaches informed by microdata, and regularly update strategies based on observed outcomes. For instance, contrary to initial views, the monetary, fiscal, and financial-market policies implemented during the COVID-19 pandemic—particularly in the U.S.—highlight a failure of effective coordination, as evidenced by subsequent extreme wealth inequality and persistent inflation. Many experts criticize the lack of coherent policy integration, arguing it exacerbated economic distortions and reduced overall policy effectiveness.\nIn summary, bridging theory and reality requires flexible and empirically informed policy strategies, recognizing theoretical insights and practical limitations while ensuring effective coordination to mitigate unintended adverse outcomes."
  },
  {
    "objectID": "drafts/market_incomplete.html#empirical-review-evaluating-policy-recommendations-in-practice",
    "href": "drafts/market_incomplete.html#empirical-review-evaluating-policy-recommendations-in-practice",
    "title": "Monetary Policy and Market Imperfections",
    "section": "Empirical Review: Evaluating Policy Recommendations in Practice",
    "text": "Empirical Review: Evaluating Policy Recommendations in Practice\nHistorical experience provides valuable tests for theoretical predictions, highlighting both successes and failures in policy implementation:\n\nGreat Depression: Friedman and Schwartz demonstrated that monetary contraction deepened the Depression. The period validated the importance of maintaining stable aggregate demand and money supply, while illustrating risks of debt-deflation spirals. It emphasized roles for both monetary stabilization and fiscal intervention (e.g., New Deal, WWII spending), leading to institutional innovations like deposit insurance and lender-of-last-resort roles for central banks.\nPostwar Keynesian-Monetarist Debate & Stagflation: The stagflation of the 1970s validated Friedman’s critique of simplistic Keynesian demand management and the natural rate hypothesis. Volcker’s aggressive monetary tightening demonstrated that inflation control requires credible commitment and acceptance of short-term economic pain. This period underscored the difficulty of targeting monetary aggregates directly, resulting in central banks shifting towards inflation targeting regimes with more flexible interest rate rules.\nGreat Moderation and 2008 Financial Crisis: The New Keynesian synthesis (price-stickiness, monetary rules, and representative agents) initially seemed successful during the Great Moderation, but failed to predict growing inequality and financial vulnerabilities that triggered the 2008 crisis. The crisis validated incomplete-market frameworks (Bewley/Aiyagari) and speculative asset-price models (Harrison-Kreps), highlighting the critical need to include financial frictions and heterogeneity in macroeconomic analysis. Quantitative easing stabilized financial markets, but disproportionately benefited wealthier asset holders, intensifying inequality.\nCOVID-19 Pandemic and Inflation Spike: The aggressive fiscal and monetary responses during COVID-19 prevented immediate economic collapse but led to persistent inflation and increased wealth inequality due to poor coordination and mis-targeted stimulus. In the U.S., particularly, stimulus measures lacked coherence, causing asset bubbles and skewed benefits to the wealthy. Current inflation challenges highlight monetarist concerns about excessive stimulus and underline the importance of coordinated fiscal-monetary policy frameworks.\n\nKey empirical lessons:\n\nCredible monetary policy frameworks that stabilize inflation are critical for long-run economic stability.\nMarket imperfections (liquidity constraints, incomplete markets, financial fragility) significantly affect policy effectiveness and must be explicitly considered.\nDistributional issues matter: Monetary policy can inadvertently exacerbate inequality, stressing the need for complementary fiscal redistributive policies.\nExpectations and clear communication are essential for policy effectiveness, reinforcing the rational expectations perspective on policy transparency.\n\nIn sum, successful policy in practice requires combining disciplined monetary frameworks, proactive management of market imperfections and inequalities, and flexibility to adapt based on evolving empirical evidence and economic conditions."
  },
  {
    "objectID": "drafts/market_incomplete.html#conclusion",
    "href": "drafts/market_incomplete.html#conclusion",
    "title": "Monetary Policy and Market Imperfections",
    "section": "Conclusion",
    "text": "Conclusion\nEffective monetary policy requires a balance between long-run credibility and stability (as emphasized by neoclassical and monetarist models) and short-run interventions addressing market imperfections and inequality. Stable monetary frameworks—such as credible inflation targets and rule-based policies—anchor expectations, reduce uncertainty, and support sustainable economic growth. However, real economies exhibit significant market frictions, requiring complementary fiscal and regulatory interventions.\nIncomplete-market models (Bewley, Aiyagari) highlight the necessity of policies that alleviate liquidity constraints and promote broader economic participation. Financial-market models (Harrison & Kreps) stress the importance of regulatory vigilance to prevent speculative excesses. Recent empirical experiences—particularly the uncoordinated COVID-19 policy response in the U.S., which contributed to extreme wealth inequality and persistent inflation—underscore the risks of fragmented policy approaches.\nThe long-term orientation of neoclassical models also reminds us that economic growth and efficiency are paramount for raising living standards broadly. Sustainable reductions in wealth inequality are best achieved through inclusive growth—ensuring more people can participate in economic advancement via skill-building, access to capital, and productive investments. Policies that enhance productivity—education, infrastructure, and innovation—are crucial complements to monetary policy. While these are typically fiscal or structural measures, monetary policy plays a role by maintaining a low-inflation, stable macroeconomic backdrop and preventing the misallocation of capital into unproductive speculative ventures.\nTo mitigate wealth inequality without sacrificing efficiency, policymakers should consider measures such as:\n\nProgressive taxation and wealth taxes to fund public goods and transfers efficiently (as Aiyagari’s framework suggests, moderate capital taxation can be welfare-enhancing).\n\nPublic investment in education, healthcare, and economic opportunity to foster broad-based growth and human capital development, reducing inequality over time.\n\nEncouraging broad-based asset ownership through mechanisms like employee stock ownership plans or tax incentives for retirement savings, ensuring more households benefit from asset appreciation.\n\nMaintaining low and stable inflation to protect real incomes, particularly for lower-income households who lack financial hedges against rising prices.\n\nUltimately, the most effective policy approach integrates monetary stability, fiscal coordination, and regulatory prudence to foster inclusive, sustainable economic prosperity while ensuring that short-term interventions do not undermine long-term growth and efficiency."
  },
  {
    "objectID": "drafts/market_incomplete.html#appendix",
    "href": "drafts/market_incomplete.html#appendix",
    "title": "Monetary Policy and Market Imperfections",
    "section": "Appendix",
    "text": "Appendix\nThe appendix provides technical details for the theoretical models referenced. Each section includes the formal derivation of key equations, calibration and simulation exercises, and case analyses illustrating the implications of each model.\n\nFriedman’s Permanent Income Model\n\n1. Theoretical Formulation\nFriedman’s Permanent Income Hypothesis (PIH) posits that an individual’s consumption at time \\(t\\) is based on expected lifetime income rather than current income. The standard model assumes perfect capital markets, quadratic utility, and rational expectations.\nLet the consumer’s lifetime utility function be: \\[\nU = \\sum_{t=0}^{\\infty} \\beta^t u(c_t),\n\\] where \\(\\beta\\) is the subjective discount factor and \\(u(c_t)\\) is quadratic utility: \\[\nu(c_t) = -\\frac{1}{2} (c_t - c^*)^2.\\]\nThe budget constraint is: \\[\nA_{t+1} = (1+r)(A_t + Y_t - C_t),\n\\] where \\(A_t\\) is assets, \\(r\\) is the interest rate, \\(Y_t\\) is income, and \\(C_t\\) is consumption. Given rational expectations, Hall (1978) derived that optimal consumption follows a martingale process: \\[\nE_t[c_{t+1}] = c_t.\n\\] This implies that changes in consumption are unpredictable: \\[\nE_t[c_{t+1} - c_t] = 0.\n\\] Thus, only permanent changes in income affect consumption significantly, while transitory changes are mostly saved.\n\n\n2. Calibration and Simulation\nWe calibrate the model using standard parameter values: - Discount factor: \\(\\beta = 0.96\\) - Interest rate: \\(r = 0.04\\) - Income process: \\(Y_t = Y_p + Y_t^T\\), where \\(Y_p\\) is permanent income and \\(Y_t^T\\) is a transitory shock. - Variance of transitory income shock: \\(\\sigma_T^2 = 0.02\\) - Variance of permanent income shock: \\(\\sigma_P^2 = 0.01\\)\nSimulation results illustrate consumption smoothing. Given a one-time positive income shock of \\(\\Delta Y_T\\), consumption increases only modestly: \\[\n\\Delta C_t \\approx \\frac{\\sigma_T^2}{\\sigma_P^2 + \\sigma_T^2} \\Delta Y_T.\n\\] This highlights that temporary income changes have minimal effects on consumption compared to permanent income changes.\n\n\n3. Case Analysis: Impact of Temporary vs. Permanent Income Changes\nTo empirically validate the model, we compare U.S. consumption responses to temporary stimulus payments vs. permanent tax cuts:\n\n2001 U.S. Tax Rebate (temporary): Studies found that only 20-40% of the rebate was spent immediately, consistent with PIH predictions.\n1980s Reagan Tax Cuts (permanent): Consumption increased significantly, aligning with the model’s implication that permanent income shifts drive behavior.\n\nThis supports the policy conclusion that temporary monetary stimulus has limited consumption effects, reinforcing the monetarist argument for stable, rules-based policy frameworks over discretionary interventions.\n\n\n\nBewley Model\n\n1. Theoretical Formulation\nThe Bewley model describes a heterogeneous agent economy with idiosyncratic income shocks and liquidity constraints. Agents optimize consumption and savings in response to uncertain income paths, leading to precautionary savings behavior.\nThe agent maximizes expected lifetime utility: \\[\nU = \\sum_{t=0}^{\\infty} \\beta^t u(c_t),\n\\] subject to the budget constraint: \\[\nA_{t+1} = (1+r)A_t + Y_t - C_t,\n\\] and a borrowing constraint: \\[\nA_t \\geq 0.\n\\] The income process follows a stochastic evolution: \\[\nY_t = Y_p + \\varepsilon_t,\n\\] where \\(Y_p\\) is the persistent component and \\(\\varepsilon_t\\) is a transitory shock.\nThe Euler equation governs optimal consumption: \\[\nE_t \\left[ u'(c_t) \\right] = \\beta (1+r) E_t \\left[ u'(c_{t+1}) \\right].\n\\] Binding borrowing constraints lead to higher marginal propensities to consume (MPC), distinguishing this model from the complete-market benchmark.\n\n\n2. Calibration and Simulation\nWe calibrate the model using empirically relevant parameters:\n\nRisk aversion: \\(\\sigma = 2\\)\nDiscount factor: \\(\\beta = 0.96\\)\nInterest rate: \\(r = 0.04\\)\nIncome process variance: \\(\\sigma_Y^2 = 0.02\\)\n\nSimulating the model shows the emergence of a stationary wealth distribution. Agents with lower wealth levels exhibit high MPCs, while wealthier agents accumulate savings to self-insure against income shocks.\n\n\n3. Case Analysis: Wealth Distribution and Policy Implications\nEmpirical evidence shows that wealth inequality observed in real economies is well captured by Bewley models. For example:\n\nU.S. Wealth Distribution: The model reproduces the fact that the top 10% of wealth holders own a disproportionately large share of total assets.\nImpact of Credit Market Access: Relaxing borrowing constraints leads to higher consumption smoothing, reducing excess precautionary savings.\nEffects of Monetary Policy: Interest rate cuts primarily benefit liquidity-constrained households, increasing consumption, while wealthier agents respond weakly due to already accumulated assets.\n\nThis model underscores the importance of credit access and social insurance policies to counteract excessive precautionary savings and stabilize aggregate demand.\n\n\n\nAiyagari Model\n\n1. Theoretical Formulation\nBuilding on the Bewley model, Aiyagari introduces production and general equilibrium. Households supply capital and labor, and firms use a Cobb-Douglas production function: \\[\nY = K^\\alpha L^{1-\\alpha}.\n\\] The interest rate \\(r\\) and wage \\(w\\) adjust to ensure market clearing: \\[\nK = \\sum A_i, \\quad L = \\sum l_i.\n\\] The stationary distribution of wealth results in an endogenous equilibrium interest rate lower than in the complete markets model due to precautionary savings.\n\n\n2. Calibration and Simulation\n\nCapital share: \\(\\alpha = 0.36\\)\nDiscount factor: \\(\\beta = 0.96\\)\nDepreciation: \\(\\delta = 0.08\\)\nRisk aversion: \\(\\sigma = 2\\)\n\nSimulations confirm excess capital accumulation, implying that moderate capital taxation can improve welfare by reallocating excess savings towards consumption.\n\n\n3. Case Analysis: Redistribution and Policy Implications\n\nEffect of Redistribution: Moderate taxation on capital income reallocates resources and increases aggregate welfare.\nImpact on Interest Rates: The equilibrium interest rate is lower than in the complete markets model, reflecting the precautionary savings motive.\n\nThis model highlights the distributional consequences of monetary and fiscal policy and suggests that targeted redistribution can enhance efficiency without major losses in output.\n\n\n\nHarrison & Kreps Model\n\n1. Theoretical Formulation\nThe Harrison & Kreps (1978) model explores how heterogeneous beliefs among investors can lead to speculative bubbles in asset pricing. The key insight is that optimistic investors dominate the pricing mechanism when short-selling constraints exist, leading to equilibrium prices that can exceed fundamental values.\nConsider a two-period model where investors have differing subjective probabilities about a risky asset’s payoff in period \\(t=2\\). The risky asset pays \\(X_H\\) with probability \\(p_H\\) (optimists’ belief) and \\(X_L\\) with probability \\(1 - p_H\\). Similarly, pessimists believe the probabilities are \\(p_L\\) and \\(1 - p_L\\), where \\(p_H &gt; p_L\\).\nThe price of the asset in period \\(t=1\\) is determined by the highest bidder, given short-sale constraints: \\[\nP_1 = \\max\\{ P_H, P_L \\},\n\\] where \\(P_H\\) and \\(P_L\\) are the optimists’ and pessimists’ valuation of the asset, respectively: \\[\nP_H = \\frac{p_H X_H + (1 - p_H) X_L}{1 + r}, \\quad P_L = \\frac{p_L X_H + (1 - p_L) X_L}{1 + r}.\n\\] If short selling is not allowed, the price is set by the optimists, even if pessimists believe it to be overvalued.\nIn period \\(t=2\\), the true payoff \\(X\\) is realized, and prices adjust accordingly: \\[\nP_2 = X.\n\\] If the optimists’ belief was overly optimistic, a crash occurs, showing that speculative bubbles arise due to heterogeneous expectations rather than fundamental mispricing alone.\n\n\n2. Calibration and Simulation\nTo simulate this model, we set:\n\nRisk-free rate: \\(r = 0.03\\)\nPayoffs: \\(X_H = 120\\), \\(X_L = 80\\)\nBeliefs: \\(p_H = 0.8\\), \\(p_L = 0.5\\)\n\nIf short selling is restricted, the period-1 price reflects the optimists’ valuation: \\[\nP_1 = \\frac{0.8(120) + 0.2(80)}{1.03} = 97.09.\n\\] If short selling is allowed, the pessimists’ valuation influences pricing: \\[\nP_1 = \\frac{0.5(120) + 0.5(80)}{1.03} = 97.09.\n\\]\nSimulated Results:\n\nIf \\(p_H\\) is too optimistic, asset prices rise above fundamentals, and when reality sets in at \\(t=2\\), prices drop, mimicking bubble dynamics.\nIf short-selling is unrestricted, prices better reflect fundamentals, reducing volatility.\n\n\n\n3. Case Analysis: Speculative Bubbles and Policy Implications\nEmpirical applications of the Harrison & Kreps model highlight key cases of speculative booms and busts:\n\nDot-com Bubble (1999–2000): Investor optimism, combined with limited mechanisms for short-selling, led to massive overvaluation.\nHousing Market Crash (2008): Housing assets were driven by speculative demand and optimistic credit assessments; when reality hit, prices crashed.\n\nPolicy takeaways:\n\nMarket Transparency: Improving information flow reduces belief dispersion, dampening bubbles.\nShort-Selling Mechanisms: Allowing short positions prevents excessive speculative premiums.\nMacroprudential Policies: Loan-to-value and capital requirements mitigate credit-fueled bubbles.\n\nThis model underscores how heterogeneous beliefs and market constraints drive speculative price deviations, necessitating a balanced approach to financial regulation."
  },
  {
    "objectID": "drafts/structure_tfp.html",
    "href": "drafts/structure_tfp.html",
    "title": "TFP Growth, Stationarity, and Structural Change",
    "section": "",
    "text": "The growth rate of Total Factor Productivity (TFP) is not a fixed constant; rather, it evolves over time and may be subject to long-term decline. While mainstream macroeconomic models frequently assume a constant TFP growth rate, this assumption is increasingly being questioned. Several structural factors—diminishing returns to R&D, demographic constraints, and sectoral shifts towards lower-productivity industries—suggest that aggregate TFP growth may continue to slow. However, TFP growth is not uniform across sectors. Certain industries, such as AI, biotechnology, and advanced energy technologies, exhibit significantly higher productivity gains, leading to growing sectoral disparities in innovation.\nFrom an econometric perspective, a decline in TFP growth could alter its stationarity properties, weakening its stochastic trend and making it more likely to transition from an \\(I(1)\\) (non-stationary) stochastic process to an \\(I(0)\\) (stationary) stochastic process. If structural breaks exist in TFP growth, traditional time-series models based on constant-trend cointegration assumptions may fail. Thus, accurately modeling TFP trends requires dynamic econometric techniques that account for time-varying structural changes.\n\n\n\n\n\nTFP represents technological progress that enhances economic output beyond capital and labor inputs. In the Solow-Swan growth model, TFP follows an exponential trend, expressed as:\n\\[\nA_t = A_0 e^{gt} \\epsilon_t\n\\]\nwhere \\(g\\) is the growth rate and \\(\\epsilon_t\\) captures short-term fluctuations. This implies that TFP is not strictly stationary; rather, its log form follows a unit root process with a deterministic trend:\n\\[\n\\log A_t = g t + u_t, \\quad u_t \\sim I(0)\n\\]\nThus, mainstream macroeconomic models do not assume TFP is stationary in levels, but often assume its deviation from trend is stationary.\n\n\n\nEmpirical studies show:\n\nLong-Run TFP behaves as a non-stationary process (\\(I(1)\\)), meaning that it exhibits persistent growth and shocks that do not revert over time.\nShort-Run TFP Shocks (deviations from trend) are often stationary (\\(I(0)\\)) process.\n\nIf GDP and capital stock are also \\(I(1)\\), Solow’s production function suggests they should be cointegrated:\n\\[\n\\log Y_t - \\alpha \\log K_t - (1-\\alpha) \\log L_t - g t = u_t, \\quad u_t \\sim I(0)\n\\]\nThus, GDP, capital, and labor may be cointegrated due to the common trend driven by TFP.\n\n\n\nMany studies suggest that TFP growth rates have declined since the mid-20th century.\nU.S. TFP Growth Trends (Annual Growth Rates):\n\n1950s–1970s: ~2.5% per year\n\n1980s–1990s: ~1.5% per year\n\n2000s–2020s: ~0.5%-1.0% per year\n\nThe productivity slowdown hypothesis suggests that long-term economic growth potential is becoming weaker than before.\n\n\n\nSeveral key factors contribute to the observed decline in TFP growth:\n\nThe Low-Hanging Fruit Hypothesis (Gordon, 2016)\n\nPast technological revolutions (electricity, automobiles, antibiotics) were one-time events that provided massive boosts.\n\nRecent innovations (social media, fintech, AI) may not have the same economy-wide productivity effects.\n\nDeclining R&D Efficiency (Bloom et al., 2020)\n\n“Ideas are getting harder to find.”\n\nR&D investment has increased, but the number of researchers needed to sustain TFP growth has risen exponentially.\n\nDemographic Constraints and Human Capital Decay\n\nAging populations in advanced economies reduce labor force dynamism and innovation.\n\nEducation quality improvements may have plateaued, reducing skilled labor supply.\n\nSectoral Shift to Low-Productivity Industries\n\nAdvanced economies have shifted from manufacturing to services (e.g., healthcare, education, public sector).\n\nServices typically have lower TFP growth, as they rely heavily on human labor and are difficult to automate.\n\nIncreased Regulation and Market Distortions\n\nGrowing regulations, tax policies, and political uncertainty increase inefficiencies, reducing overall productivity.\n\n\n\n\n\nWhile aggregate TFP growth may appear to be declining, there are huge disparities across sectors. Just as wealth concentration is increasing, TFP growth could also be highly concentrated within specific industries:\n\nAI and Automation\n\nAI-driven automation could drastically improve productivity, particularly in certain white-collar jobs.\n\nHowever, productivity gains may be highly concentrated in tech-driven industries, rather than the economy as a whole.\n\nSectoral Disparities in TFP Growth\n\nWithin-market concentration: A small number of firms (e.g., winners in the winner-takes-all structure) are experiencing high TFP growth, while others stagnate.\n\nCross-market concentration: Some sectors (AI, biotech) may experience massive productivity boosts, while others (traditional services, public sector) may stagnate.\n\nEnergy Revolution and New Technological Frontiers\n\nAdvances in nuclear fusion, renewables, and space-based energy production could dramatically shift TFP growth.\n\nHowever, large-scale adoption is slow due to political, regulatory, and financial constraints.\n\n\n\n\n\n\n\n\nIf the long-run TFP growth rate is declining, its stochastic trend may weaken, making it more likely to transition from a unit root (\\(I(1)\\)) process to a stationary (\\(I(0)\\)) process over time. This has important implications for empirical modeling and forecasting, particularly in cases where researchers assume a constant growth trend in TFP-driven models. To properly account for these shifts, researchers should:\n\nUse structural break tests (e.g., Bai-Perron) to detect shifts in TFP growth trends and assess whether different growth regimes exist.\n\nApply rolling unit root tests to examine whether TFP has moved from an \\(I(1)\\) process toward an \\(I(0)\\) process over time.\n\nIncorporate time-varying cointegration approaches to avoid biased estimations that assume stable relationships in economic growth models.\n\n\n\n\nTFP growth is not uniform across industries; while aggregate TFP growth may be declining, specific sectors are experiencing rapid technological advancements. Just as wealth and market concentration have increased, TFP gains are increasingly concentrated within high-tech and innovation-driven industries. The result is a growing disparity between leading-edge industries (AI, biotech, energy) and traditional sectors (public services, healthcare, education), which exhibit slower productivity improvements.\nAs sectoral divergence in productivity widens, traditional macroeconomic models that assume homogeneous TFP growth may require revision. Future research should focus on developing heterogeneous growth models that account for cross-sectoral differences, while also employing more dynamic econometric techniques to capture the evolving nature of TFP trends."
  },
  {
    "objectID": "drafts/structure_tfp.html#introduction",
    "href": "drafts/structure_tfp.html#introduction",
    "title": "TFP Growth, Stationarity, and Structural Change",
    "section": "",
    "text": "The growth rate of Total Factor Productivity (TFP) is not a fixed constant; rather, it evolves over time and may be subject to long-term decline. While mainstream macroeconomic models frequently assume a constant TFP growth rate, this assumption is increasingly being questioned. Several structural factors—diminishing returns to R&D, demographic constraints, and sectoral shifts towards lower-productivity industries—suggest that aggregate TFP growth may continue to slow. However, TFP growth is not uniform across sectors. Certain industries, such as AI, biotechnology, and advanced energy technologies, exhibit significantly higher productivity gains, leading to growing sectoral disparities in innovation.\nFrom an econometric perspective, a decline in TFP growth could alter its stationarity properties, weakening its stochastic trend and making it more likely to transition from an \\(I(1)\\) (non-stationary) stochastic process to an \\(I(0)\\) (stationary) stochastic process. If structural breaks exist in TFP growth, traditional time-series models based on constant-trend cointegration assumptions may fail. Thus, accurately modeling TFP trends requires dynamic econometric techniques that account for time-varying structural changes."
  },
  {
    "objectID": "drafts/structure_tfp.html#main",
    "href": "drafts/structure_tfp.html#main",
    "title": "TFP Growth, Stationarity, and Structural Change",
    "section": "",
    "text": "TFP represents technological progress that enhances economic output beyond capital and labor inputs. In the Solow-Swan growth model, TFP follows an exponential trend, expressed as:\n\\[\nA_t = A_0 e^{gt} \\epsilon_t\n\\]\nwhere \\(g\\) is the growth rate and \\(\\epsilon_t\\) captures short-term fluctuations. This implies that TFP is not strictly stationary; rather, its log form follows a unit root process with a deterministic trend:\n\\[\n\\log A_t = g t + u_t, \\quad u_t \\sim I(0)\n\\]\nThus, mainstream macroeconomic models do not assume TFP is stationary in levels, but often assume its deviation from trend is stationary.\n\n\n\nEmpirical studies show:\n\nLong-Run TFP behaves as a non-stationary process (\\(I(1)\\)), meaning that it exhibits persistent growth and shocks that do not revert over time.\nShort-Run TFP Shocks (deviations from trend) are often stationary (\\(I(0)\\)) process.\n\nIf GDP and capital stock are also \\(I(1)\\), Solow’s production function suggests they should be cointegrated:\n\\[\n\\log Y_t - \\alpha \\log K_t - (1-\\alpha) \\log L_t - g t = u_t, \\quad u_t \\sim I(0)\n\\]\nThus, GDP, capital, and labor may be cointegrated due to the common trend driven by TFP.\n\n\n\nMany studies suggest that TFP growth rates have declined since the mid-20th century.\nU.S. TFP Growth Trends (Annual Growth Rates):\n\n1950s–1970s: ~2.5% per year\n\n1980s–1990s: ~1.5% per year\n\n2000s–2020s: ~0.5%-1.0% per year\n\nThe productivity slowdown hypothesis suggests that long-term economic growth potential is becoming weaker than before.\n\n\n\nSeveral key factors contribute to the observed decline in TFP growth:\n\nThe Low-Hanging Fruit Hypothesis (Gordon, 2016)\n\nPast technological revolutions (electricity, automobiles, antibiotics) were one-time events that provided massive boosts.\n\nRecent innovations (social media, fintech, AI) may not have the same economy-wide productivity effects.\n\nDeclining R&D Efficiency (Bloom et al., 2020)\n\n“Ideas are getting harder to find.”\n\nR&D investment has increased, but the number of researchers needed to sustain TFP growth has risen exponentially.\n\nDemographic Constraints and Human Capital Decay\n\nAging populations in advanced economies reduce labor force dynamism and innovation.\n\nEducation quality improvements may have plateaued, reducing skilled labor supply.\n\nSectoral Shift to Low-Productivity Industries\n\nAdvanced economies have shifted from manufacturing to services (e.g., healthcare, education, public sector).\n\nServices typically have lower TFP growth, as they rely heavily on human labor and are difficult to automate.\n\nIncreased Regulation and Market Distortions\n\nGrowing regulations, tax policies, and political uncertainty increase inefficiencies, reducing overall productivity.\n\n\n\n\n\nWhile aggregate TFP growth may appear to be declining, there are huge disparities across sectors. Just as wealth concentration is increasing, TFP growth could also be highly concentrated within specific industries:\n\nAI and Automation\n\nAI-driven automation could drastically improve productivity, particularly in certain white-collar jobs.\n\nHowever, productivity gains may be highly concentrated in tech-driven industries, rather than the economy as a whole.\n\nSectoral Disparities in TFP Growth\n\nWithin-market concentration: A small number of firms (e.g., winners in the winner-takes-all structure) are experiencing high TFP growth, while others stagnate.\n\nCross-market concentration: Some sectors (AI, biotech) may experience massive productivity boosts, while others (traditional services, public sector) may stagnate.\n\nEnergy Revolution and New Technological Frontiers\n\nAdvances in nuclear fusion, renewables, and space-based energy production could dramatically shift TFP growth.\n\nHowever, large-scale adoption is slow due to political, regulatory, and financial constraints."
  },
  {
    "objectID": "drafts/structure_tfp.html#conclusion",
    "href": "drafts/structure_tfp.html#conclusion",
    "title": "TFP Growth, Stationarity, and Structural Change",
    "section": "",
    "text": "If the long-run TFP growth rate is declining, its stochastic trend may weaken, making it more likely to transition from a unit root (\\(I(1)\\)) process to a stationary (\\(I(0)\\)) process over time. This has important implications for empirical modeling and forecasting, particularly in cases where researchers assume a constant growth trend in TFP-driven models. To properly account for these shifts, researchers should:\n\nUse structural break tests (e.g., Bai-Perron) to detect shifts in TFP growth trends and assess whether different growth regimes exist.\n\nApply rolling unit root tests to examine whether TFP has moved from an \\(I(1)\\) process toward an \\(I(0)\\) process over time.\n\nIncorporate time-varying cointegration approaches to avoid biased estimations that assume stable relationships in economic growth models.\n\n\n\n\nTFP growth is not uniform across industries; while aggregate TFP growth may be declining, specific sectors are experiencing rapid technological advancements. Just as wealth and market concentration have increased, TFP gains are increasingly concentrated within high-tech and innovation-driven industries. The result is a growing disparity between leading-edge industries (AI, biotech, energy) and traditional sectors (public services, healthcare, education), which exhibit slower productivity improvements.\nAs sectoral divergence in productivity widens, traditional macroeconomic models that assume homogeneous TFP growth may require revision. Future research should focus on developing heterogeneous growth models that account for cross-sectoral differences, while also employing more dynamic econometric techniques to capture the evolving nature of TFP trends."
  },
  {
    "objectID": "drafts/structure_tfp.html#integration-order",
    "href": "drafts/structure_tfp.html#integration-order",
    "title": "TFP Growth, Stationarity, and Structural Change",
    "section": "Integration Order",
    "text": "Integration Order\nThe integration order of a time series determines how many times it must be differenced to become stationary. A series is:\n\n\\(I(0)\\) (stationary) process if it has a constant mean, variance, and autocovariance.\n\\(I(1)\\) (unit root) process if it is non-stationary but becomes stationary after first differencing.\n\\(I(d)\\) process if it requires \\(d\\) differences to become stationary.\n\nEconomic time series such as GDP, money supply, and asset prices often exhibit \\(I(1)\\) behavior, meaning they contain stochastic trends and require differencing to achieve stationarity."
  },
  {
    "objectID": "drafts/structure_tfp.html#joint-covariance-stationarity-vs.-cointegration",
    "href": "drafts/structure_tfp.html#joint-covariance-stationarity-vs.-cointegration",
    "title": "TFP Growth, Stationarity, and Structural Change",
    "section": "Joint Covariance Stationarity vs. Cointegration",
    "text": "Joint Covariance Stationarity vs. Cointegration\nJoint covariance stationarity applies when each time-series maintains a constant mean, variance, and autocovariance over time. Cointegration, on the other hand, describes cases where two or more non-stationary \\(I(1)\\) time-series share a long-term equilibrium, forming a stationary linear combination.\n\nJoint Covariance Stationary Series (Weak-Sense Stationarity)\nA set of time series \\(X_t\\) and \\(Y_t\\) are jointly covariance stationary if they satisfy:\n\nConstant Mean: \\(E[X_t] = \\mu_X\\), \\(E[Y_t] = \\mu_Y\\) for all \\(t\\).\nConstant Variance: \\(Var(X_t)\\) and \\(Var(Y_t)\\) do not change over time.\nAutocovariance Depends Only on Lag: \\(Cov(X_t, X_{t-h})\\) and \\(Cov(Y_t, Y_{t-h})\\) depend only on the lag \\(h\\), not on \\(t\\).\n\nIf two time series are both weakly stationary, then any linear combination of them is also stationary.\n\n\nCointegrated Series\nA set of time series \\(X_t\\) and \\(Y_t\\) are cointegrated if:\n\nEach series is \\(I(1)\\) (non-stationary) process.\nA linear combination exists that is \\(I(0)\\) (stationary) process:\n\\[\n\\beta_1 X_t + \\beta_2 Y_t = u_t\n\\]\n\nwhere \\(u_t\\) is \\(I(0)\\) (stationary) process.\nThus, even though individual variables are non-stationary, their linear combination is stationary, implying a long-run equilibrium relationship.\n\n\nThe Relationship Between Covariance Stationarity and Cointegration\n\n\n\n\n\n\n\n\nProperty\nJoint Covariance Stationary Series\nCointegrated Series\n\n\n\n\nStationarity\nEach series is stationary (I(0))\nEach series is non-stationary (I(1)), but a linear combination is stationary\n\n\nUnit Root \\(I(d)\\)\nI(0) for each series\nI(1) for each series, but a specific linear combination is I(0)\n\n\nMean & Variance Stability\nMean & variance are constant over time\nIndividual series do not have stable mean & variance, but the combination does\n\n\nLong-run Relationship\nNo long-term relationship constraint\nA long-run equilibrium relationship exists\n\n\n\n\nCointegrated Series Can Be Transformed into Covariance Stationary Series\nIf \\(X_t\\) and \\(Y_t\\) are cointegrated, their first differences \\(\\Delta X_t\\), \\(\\Delta Y_t\\) (or the residual \\(u_t\\)) are stationary.\n\nThe error correction term \\(u_t\\) is stationary \\(I(0)\\) process, meaning it satisfies the covariance stationarity conditions.\n\n\n\nJoint Covariance Stationary Series Are Not Cointegrated\nIf \\(X_t\\) and \\(Y_t\\) are both already \\(I(0)\\) (stationary), then any linear combination of them is also stationary.\n\nThey cannot be cointegrated because cointegration only applies to non-stationary (\\(I(1)\\)) series.\nIf all series are already covariance stationary, testing for cointegration is unnecessary."
  },
  {
    "objectID": "drafts/structure_tfp.html#implications-for-empirical-analysis",
    "href": "drafts/structure_tfp.html#implications-for-empirical-analysis",
    "title": "TFP Growth, Stationarity, and Structural Change",
    "section": "Implications for Empirical Analysis",
    "text": "Implications for Empirical Analysis\n\nBefore testing for cointegration, check for stationarity. If all series are \\(I(0)\\), cointegration does not apply.\nIf series are cointegrated, their residuals (error correction term) should be covariance stationary.\nMany macroeconomic variables (e.g., GDP & consumption, money supply & inflation) are cointegrated rather than purely covariance stationary."
  },
  {
    "objectID": "guides/finance_asset_puzzle.html",
    "href": "guides/finance_asset_puzzle.html",
    "title": "Asset Premium Puzzles",
    "section": "",
    "text": "Since the 1980s, financial economists have grappled with two so-called “puzzles” in asset pricing theory: the Equity Premium Puzzle (EPP) and the Risk-Free Rate Puzzle (RFRP). These puzzles refer to the pronounced gap between theoretical predictions derived from standard rational-expectations models with representative-agent utility maximization and the empirical observations on asset returns and risk-free rates. In particular, Mehra and Prescott (1985) and Weil (1989) spotlighted the disparity between observed excess returns on equities and the relatively tame variability of aggregate consumption, labeling the phenomenon a “puzzle” under the canonical equilibrium framework.\nBeneath both puzzles lies a single “master equation,” namely the Euler equation, which in its most general form states:\n\\[\n\\mathbb{E}_t \\big[m_t R_{t+1}\\big] = 1,\n\\]\nwhere \\(m_t\\) (the Stochastic Discount Factor) is closely tied to the marginal utility of consumption, and \\(R_{t+1}\\) represents the return on various assets. The equity premium and risk-free rate each follow from this broad formulation, yet empirical magnitudes diverge significantly from conventional theoretical predictions."
  },
  {
    "objectID": "guides/finance_asset_puzzle.html#introduction",
    "href": "guides/finance_asset_puzzle.html#introduction",
    "title": "Asset Premium Puzzles",
    "section": "",
    "text": "Since the 1980s, financial economists have grappled with two so-called “puzzles” in asset pricing theory: the Equity Premium Puzzle (EPP) and the Risk-Free Rate Puzzle (RFRP). These puzzles refer to the pronounced gap between theoretical predictions derived from standard rational-expectations models with representative-agent utility maximization and the empirical observations on asset returns and risk-free rates. In particular, Mehra and Prescott (1985) and Weil (1989) spotlighted the disparity between observed excess returns on equities and the relatively tame variability of aggregate consumption, labeling the phenomenon a “puzzle” under the canonical equilibrium framework.\nBeneath both puzzles lies a single “master equation,” namely the Euler equation, which in its most general form states:\n\\[\n\\mathbb{E}_t \\big[m_t R_{t+1}\\big] = 1,\n\\]\nwhere \\(m_t\\) (the Stochastic Discount Factor) is closely tied to the marginal utility of consumption, and \\(R_{t+1}\\) represents the return on various assets. The equity premium and risk-free rate each follow from this broad formulation, yet empirical magnitudes diverge significantly from conventional theoretical predictions."
  },
  {
    "objectID": "guides/finance_asset_puzzle.html#theoretical-framework",
    "href": "guides/finance_asset_puzzle.html#theoretical-framework",
    "title": "Asset Premium Puzzles",
    "section": "Theoretical Framework",
    "text": "Theoretical Framework\n\nEquity Premium Puzzle\nThe seminal work of Lucas (1978) and later Mehra and Prescott (1985) established the Equity Premium Puzzle within a representative-agent, general-equilibrium asset pricing model. The typical consumer (or representative agent) maximizes expected utility over time, commonly assumed to be CRRA (Constant Relative Risk Aversion):\n\\[\nU(C_t) = \\frac{C_t^{1-\\gamma}}{1-\\gamma}, \\quad \\gamma &gt; 0, \\,\\gamma \\neq 1,\n\\]\nwhere \\(\\gamma\\) is the coefficient of relative risk aversion (RRA). From the consumer’s intertemporal optimization, an Euler equation emerges:\n\\[\n\\mathbb{E}_t \\Big[\\beta \\Big(\\frac{C_{t+1}}{C_t}\\Big)^{-\\gamma} \\big(R_{e,t+1} - R_{f,t+1}\\big)\\Big] = 0,\n\\]\nwhere:\n\n\\(\\beta\\): subjective time discount factor (\\(0 &lt; \\beta &lt; 1\\)),\n\n\\(C_{t+1} / C_t\\): consumption growth rate,\n\n\\(R_{e,t+1}\\): equity return,\n\n\\(R_{f,t+1}\\): risk-free asset return.\n\nUnder some approximations (e.g., log-linearization of consumption growth), Mehra and Prescott (1985) famously derived:\n\\[\n\\mathbb{E}[R_e - R_f] \\approx \\gamma \\,\\sigma_c^2,\n\\]\nwhere \\(\\sigma_c^2\\) is the variance of consumption growth. If the variance of consumption growth is small (on the order of 1%–2% annually), the observed 6%–8% annual equity premium can only be reconciled by positing implausibly high risk aversion coefficients (often above 20). Since typical estimates of \\(\\gamma\\) in microeconomic or macroeconomic studies hover around 1–5, this gap forms the crux of the EPP.\n\n\nRisk-Free Rate Puzzle\nA closely related conundrum is the Risk-Free Rate Puzzle, initially highlighted by Weil (1989). Under the same CRRA framework and rational expectations, the Euler equation for the risk-free asset implies:\n\\[\n1 = \\mathbb{E}_t \\Big[\\beta \\Big(\\frac{C_{t+1}}{C_t}\\Big)^{-\\gamma} R_{f,t+1}\\Big].\n\\]\nApproximating in logs,\n\\[\nr_f \\approx \\delta + \\gamma \\,g_c - \\frac{1}{2}\\,\\gamma\\,(\\gamma + 1)\\,\\sigma_c^2,\n\\]\nwhere:\n\n\\(r_f = \\ln(R_f)\\): the log of the risk-free rate,\n\n\\(\\delta = -\\ln(\\beta)\\): time preference rate,\n\n\\(g_c = \\mathbb{E}[\\ln(C_{t+1}/C_t)]\\): average consumption growth rate.\n\nEmpirically, long-run real risk-free rates are typically in the 1%–3% range, whereas the above equation might predict rates of 4%–8% given plausible values for \\(\\gamma\\), \\(\\delta\\), and \\(g_c\\). Again, the severe mismatch between model forecasts and observed data has led researchers to classify it as a “puzzle.”\n\n\nCritical Assumptions\nTo preserve tractability, the standard model assumes:\n\nA representative agent — but who truly “represents” the market?\n\nTime-separability of the utility function — ensuring that period utilities add linearly over time.\n\nGlobal concavity of CRRA utility — guaranteeing diminishing marginal utility at all consumption levels.\n\nWhile these assumptions yield elegant closed-form solutions, they may excessively simplify real-world heterogeneity. Crucially, when the economy scales up over time, CRRA utility remains well-defined, but in practice this might obscure the role of vastly different consumption paths across distinct wealth brackets."
  },
  {
    "objectID": "guides/finance_asset_puzzle.html#critical-perspective",
    "href": "guides/finance_asset_puzzle.html#critical-perspective",
    "title": "Asset Premium Puzzles",
    "section": "Critical Perspective",
    "text": "Critical Perspective\n\nRethinking ‘Rational Expectations’\nTraditionally, Rational Expectations is seen as a condition that agents use all available information efficiently, forming unbiased forecasts. However, from a purely mathematical standpoint, “expectations” and “covariances” are simply operators for dealing with means and correlations of random variables. Such operators—particularly bilinear forms and inner products—require specific algebraic properties (linearity, symmetry, Cauchy–Schwarz inequality, etc.). While these simplifications can be powerful in physics or engineering, in economics they might be overly restrictive when applied to highly heterogeneous populations and institutions.\n\n\nThe Euler Equation\nBy construction, the Euler condition \\(\\mathbb{E}_t [m_{t+1} R_{t+1}] = 1\\) is mathematically akin to an inner product on a probability space. This yields a focus on second moments (variance, covariance) and often leads to elliptical distribution assumptions (e.g., normal, Student-\\(t\\)). Real-world wealth distributions and market participation, however, may be far from elliptical in their statistical properties—especially when only a small fraction of the population holds the majority of risky assets.\n\n\nImplications of Globally Concave CRRA Utility\nCRRA utility, with its global concavity, implies a declining marginal utility as consumption grows. If aggregate consumption (\\(C_t\\)) trends upward over time, the ratio of marginal utilities \\(\\bigl[u'(C_{t+1}) / u'(C_t)\\bigr]\\) naturally declines, ensuring an inverse relationship between the SDF (\\(m_{t+1}\\)) and any asset (or variable) with a long-term growth trend. In equity markets, returns \\(R_{t+1}\\) also tend to grow over time, so \\(m_t\\) and \\(R_{t+1}\\) end up negatively correlated by construction.\nIf one then chooses a suitably volatile variable (with sufficient high variance) to stand in for \\(m_{t+1}\\), one can reconcile observed excess returns with the theoretical predictions—effectively defusing the puzzle. In that sense, the puzzle may be an artifact of incomplete modeling of real-world heterogeneity."
  },
  {
    "objectID": "guides/finance_asset_puzzle.html#explaining-asset-premiums",
    "href": "guides/finance_asset_puzzle.html#explaining-asset-premiums",
    "title": "Asset Premium Puzzles",
    "section": "Explaining Asset Premiums",
    "text": "Explaining Asset Premiums\n\nThrough Market Heterogeneity\nRather than focusing exclusively on heterogeneous preferences (e.g., habit formation, behavioral biases), the perspective here is that heterogeneous economic environments—in particular, vast differences in wealth and market participation—play a decisive role:\n\nConcentrated Stock Market Participation\n\nA small fraction of wealthy households hold the majority of equity. Their risk attitudes and consumption patterns have disproportionate influence on asset prices.\n\nWealth and Consumption Inequality\n\nHigh-net-worth individuals exhibit markedly different consumption patterns from the average household, more closely aligning with equity market fluctuations.\n\n\nFrom this viewpoint, the high observed equity premium is not a puzzle at all once one acknowledges that a very small sub-population—namely, the extremely wealthy—holds large, volatile wealth positions that effectively determine marginal prices in the stock market. According to Federal Reserve data (Board 2025), 93% of households’ stock market wealth (though not 93% of total market capitalization) belongs to the wealthiest 10%. This implies that most aggregate equity risk and returns accrue to a relatively narrow stratum. For the “representative household,” which holds only about 7% of equity, stock price movements have minimal impact on its marginal utility of consumption. Hence, standard representative-agent formulations fail to capture the truly relevant marginal investor.\nEmpirical evidence from the following studies supports this line of reasoning:\n\nBasak and Cuoco (1998): Demonstrates that limited stock market participation elevates risk premia and depresses risk-free rates.\n\nYogo (2006): Shows that consumption by wealthy households closely tracks equity returns, reinforcing the link between high-end consumption dynamics and asset prices.\n\nGomes and Michaelides (2008): Connects growing income inequality with stock market participation patterns, resulting in rising equity premia.\n\nLettau, Ludvigson, and Ma (2019): Shows that a single macroeconomic factor tied to capital share (reflecting wealthy shareholders’ consumption) can explain a broad range of cross-sectional stock return premia.\n\n\n\nLimitations\nDespite offering a plausible explanation, this heterogeneity-based view faces practical hurdles:\n\nLow frequency data: Due to infrequent reporting on high-net-worth wealth, applying short-term no-arbitrage principles in cross-sectional asset pricing is problematic.\n\nDifficulty of reconciling EMH: Efficient Markets Hypothesis (EMH) posits that arbitrage opportunities vanish quickly, but wealth data often lack the granularity or frequency to confirm this.\n\nLong-run identification: Changes in upper-tier wealth or consumption may be valid for a long-run SDF (\\(m\\)), yet verifying this for short-run asset pricing remains challenging."
  },
  {
    "objectID": "guides/finance_asset_puzzle.html#conclusion",
    "href": "guides/finance_asset_puzzle.html#conclusion",
    "title": "Asset Premium Puzzles",
    "section": "Conclusion",
    "text": "Conclusion\nThe Equity Premium Puzzle and Risk-Free Rate Puzzle have dominated discussions in asset pricing for decades. However, labeling them as genuine “puzzles” may reflect an artifact of restrictive models that hinge on a single representative agent, uniform preferences, and high-level assumptions about consumption growth. By introducing heterogeneous market participation, particularly the reality that a small fraction of wealthy agents holds the lion’s share of risky assets, one finds that what appears to be a puzzle for the average consumer is, in fact, quite explicable among those who actually drive stock prices.\nIn short, when empirical ownership and wealth concentration data are properly accounted for, the puzzling gaps between theory and observation can diminish or disappear. The challenge remains to integrate heterogeneous agent frameworks with accurate micro-level data on wealth and consumption in order to provide a more comprehensive understanding of asset prices—an endeavor that holds promise for reconciling the so-called “puzzles” with empirical reality."
  },
  {
    "objectID": "guides/indicator_growth.html",
    "href": "guides/indicator_growth.html",
    "title": "경제 성장을 대표하는 지표?",
    "section": "",
    "text": "중앙은행이 대기업(‘고래’ 기업)들에게만 몰빵으로 현금 유동성을 공급하고, 이 기업들은 새로운 회사를 설립하고 그 자금을 투자에 사용한다. 투자된 돈이 중위 소득 상승과 소비로 연결되지 않고 다시 주식시장으로 유입되어 주가가 상승하면, 명목 GDP(Nominal GDP)는 증가할 수 있다. 하지만 이러한 명목 GDP 증가는 실제 경제 성장(real growth)을 반영하는 것일까?\n\n\n\n현금 유동성 공급으로 새롭게 만들어진 기업들이 소비자물가지수(CPI) 산정에 포함되지 않는 사업(B2B, 금융, AI, 데이터센터 등)을 한다면, CPI는 거의 변하지 않을 가능성이 매우 높다. 결과적으로, 명목 GDP 증가가 실질 GDP(Real GDP) 증가로 보이는 착시가 발생할 수 있다. 즉, 통계적으로 경제가 성장한 것처럼 보일 수 있지만, 실질적으로 대다수의 생활 수준은 나아지지 않아졌을 가능성이 매우 높다.\n\n\n\n공리주의 사회에서 경제 성장의 핵심 목표는 단순한 GDP 증가가 아니라 대다수 국민의 실질적 생활 수준 향상이다. 이를 더 잘 반영하는 지표는 Real GDP per capita 보다 Real Median Wage(실질 중위임금) 이 될 수 있다.\n\nReal GDP per capita는 평균적인 경제 생산량 증가를 보여주지만, 실제로 국민 개개인의 소득 분배를 고려하지 않는다.\n\nReal Median Wage는 국민 대다수가 체감하는 실질 소득의 변화를 나타내므로, 보다 정확한 경제 성장 지표가 될 수 있다.\n\n만약 명목 GDP가 증가하지만 중위임금이 정체되거나 하락한다면, 이는 대기업과 자산가 중심의 성장일 뿐, 일반 국민들에게 혜택이 돌아가지 않는 성장일 가능성이 크다.\n\n\n\n\n경제 성장의 지속 가능성을 예측하는 데 있어, 현재 사용되는 Natural GDP Growth Rate(자연 경제 성장률) 모델은 구조적인 문제를 가질 수 있다.\n- 이런 모델들은 과거 데이터를 기반으로 경제 성장률을 예측하지만, 금융시장 및 소비자 행동의 비선형적 변화를 제대로 반영하지 못한다.\n- 반면 Consumer Sentiment Index(소비자 심리 지수) 는 실시간으로 소비자들의 기대와 시장 심리를 측정한다.\n- 즉, 미래의 경제 성장 가능성을 예측하는 데 있어, 소비자들이 체감하는 경제 신뢰도와 구매 의향이 더 실질적인 지표가 될 수 있다.\n\n\n\n\n명목 GDP와 실질 GDP만으로 경제 성장을 평가하는 것은 한계가 있다.\n\nReal Median Wage가 상승하지 않는다면, GDP 성장은 일부 계층에만 혜택을 주는 비대칭적 성장일 가능성이 크다.\n경제 전망을 판단할 때, Consumer Sentiment Index와 같은 체감 지표가 전통적인 성장률 모델보다 더 현실적인 통찰을 제공할 수 있다.\n\n결국, 우리가 고민해야 할 것은 단순한 GDP 증가가 아니라 ’국민 개개인이 체감하는 실질적 경제 성장’이다."
  },
  {
    "objectID": "guides/indicator_growth.html#real-gdp-vs.-real-median-wage",
    "href": "guides/indicator_growth.html#real-gdp-vs.-real-median-wage",
    "title": "경제 성장을 대표하는 지표?",
    "section": "",
    "text": "중앙은행이 대기업(‘고래’ 기업)들에게만 몰빵으로 현금 유동성을 공급하고, 이 기업들은 새로운 회사를 설립하고 그 자금을 투자에 사용한다. 투자된 돈이 중위 소득 상승과 소비로 연결되지 않고 다시 주식시장으로 유입되어 주가가 상승하면, 명목 GDP(Nominal GDP)는 증가할 수 있다. 하지만 이러한 명목 GDP 증가는 실제 경제 성장(real growth)을 반영하는 것일까?\n\n\n\n현금 유동성 공급으로 새롭게 만들어진 기업들이 소비자물가지수(CPI) 산정에 포함되지 않는 사업(B2B, 금융, AI, 데이터센터 등)을 한다면, CPI는 거의 변하지 않을 가능성이 매우 높다. 결과적으로, 명목 GDP 증가가 실질 GDP(Real GDP) 증가로 보이는 착시가 발생할 수 있다. 즉, 통계적으로 경제가 성장한 것처럼 보일 수 있지만, 실질적으로 대다수의 생활 수준은 나아지지 않아졌을 가능성이 매우 높다.\n\n\n\n공리주의 사회에서 경제 성장의 핵심 목표는 단순한 GDP 증가가 아니라 대다수 국민의 실질적 생활 수준 향상이다. 이를 더 잘 반영하는 지표는 Real GDP per capita 보다 Real Median Wage(실질 중위임금) 이 될 수 있다.\n\nReal GDP per capita는 평균적인 경제 생산량 증가를 보여주지만, 실제로 국민 개개인의 소득 분배를 고려하지 않는다.\n\nReal Median Wage는 국민 대다수가 체감하는 실질 소득의 변화를 나타내므로, 보다 정확한 경제 성장 지표가 될 수 있다.\n\n만약 명목 GDP가 증가하지만 중위임금이 정체되거나 하락한다면, 이는 대기업과 자산가 중심의 성장일 뿐, 일반 국민들에게 혜택이 돌아가지 않는 성장일 가능성이 크다.\n\n\n\n\n경제 성장의 지속 가능성을 예측하는 데 있어, 현재 사용되는 Natural GDP Growth Rate(자연 경제 성장률) 모델은 구조적인 문제를 가질 수 있다.\n- 이런 모델들은 과거 데이터를 기반으로 경제 성장률을 예측하지만, 금융시장 및 소비자 행동의 비선형적 변화를 제대로 반영하지 못한다.\n- 반면 Consumer Sentiment Index(소비자 심리 지수) 는 실시간으로 소비자들의 기대와 시장 심리를 측정한다.\n- 즉, 미래의 경제 성장 가능성을 예측하는 데 있어, 소비자들이 체감하는 경제 신뢰도와 구매 의향이 더 실질적인 지표가 될 수 있다.\n\n\n\n\n명목 GDP와 실질 GDP만으로 경제 성장을 평가하는 것은 한계가 있다.\n\nReal Median Wage가 상승하지 않는다면, GDP 성장은 일부 계층에만 혜택을 주는 비대칭적 성장일 가능성이 크다.\n경제 전망을 판단할 때, Consumer Sentiment Index와 같은 체감 지표가 전통적인 성장률 모델보다 더 현실적인 통찰을 제공할 수 있다.\n\n결국, 우리가 고민해야 할 것은 단순한 GDP 증가가 아니라 ’국민 개개인이 체감하는 실질적 경제 성장’이다."
  },
  {
    "objectID": "guides/indicator_growth.html#부록-gdp-성장률의-한계와-대안적-경제지표",
    "href": "guides/indicator_growth.html#부록-gdp-성장률의-한계와-대안적-경제지표",
    "title": "경제 성장을 대표하는 지표?",
    "section": "부록: GDP 성장률의 한계와 대안적 경제지표",
    "text": "부록: GDP 성장률의 한계와 대안적 경제지표\n\nGDP 성장률의 측정과 한계\nGross Domestic Product(GDP)는 전통적으로 국가 경제 성장을 나타내는 핵심 지표로 활용되어 왔다. 그러나 최근 학계에서는 GDP 중심의 성장률 평가가 실제 국민의 삶의 질을 정확히 반영하지 못할 수 있다는 우려가 꾸준히 제기되고 있다(Kubiszewski et al., 2013; Stiglitz et al., 2009). 특히 금융 및 자산시장 중심의 성장 패턴이 강해질수록, GDP가 증가함에도 불구하고 소득 격차가 벌어지고 중산층의 생활 수준은 오히려 하락할 수 있다(Piketty, 2014).\n\nStiglitz, Sen, and Fitoussi (2009) 는 ’GDP가 단순한 생산물의 양적 증가만을 측정할 뿐, 불평등 심화와 실제 국민들이 체감하는 경제적 안녕(well-being)을 제대로 반영하지 못한다’는 문제를 공식적으로 제기한 바 있다. 이들은 GDP의 한계를 극복할 새로운 경제지표가 필요하다고 주장했다.\nCoyle (2014) 역시 GDP 수치 자체가 금융시장의 과열 현상으로 인해 실질적 경제 생산성 및 개개인의 복지 향상과 크게 괴리될 가능성을 지적했다. 그는 특히 금융시장의 투자 활동이 명목 GDP를 상승시키지만, 이러한 상승이 곧 일반 대중의 삶의 질 향상과 연결되지 않을 수 있음을 명시했다.\n\n\n\n대안적 경제지표로서의 실질 중위임금(Real Median Wage)\n경제학자들은 경제 성과 측정의 대안으로 소득분포의 중위값(median) 또는 중간층의 실질 소득 변화에 주목할 필요가 있다고 강조한다(Saez & Zucman, 2016; Chetty et al., 2017). 특히 중위임금(Median Wage)은 경제 성장이 국민 개개인의 생활 수준에 실제로 기여하는지를 명확히 나타내는 핵심적 지표로 주목받는다.\n\nChetty et al. (2017) 의 연구에 따르면, 미국 경제에서 GDP의 꾸준한 상승에도 불구하고 최근 30년간 실질 중위 소득이 정체된 사례가 발견되었으며, 이는 GDP 중심의 경제 성장이 반드시 대다수의 국민에게 이득을 가져다주지 않는다는 사실을 보여준다. 이 연구는 중위 소득을 핵심적 지표로 삼아 경제 정책의 우선순위를 재조정할 필요가 있다고 제안한다.\nSaez and Zucman (2016) 은 소득불평등의 증가가 GDP 성장률과 무관하게 진행되며, 중위 소득의 정체 또는 감소가 발생하면 경제 성장은 지속가능성을 잃게 된다고 경고한다. 그들은 중위 소득과 실질 생활 수준을 함께 고려하는 새로운 정책 프레임워크의 필요성을 강조한다.\n\n\n\n경제적 기대지표: Consumer Sentiment Index의 우월성\n경제 성장의 미래 전망을 평가할 때, 일반적으로 Central Bank 및 국제기구는 자연 경제 성장률(natural GDP rate)과 같은 구조적 모형 기반의 지표를 선호해왔다. 하지만 최근 연구들은 구조적 모형이 금융시장과 실물경제의 복잡한 상호작용을 잘 반영하지 못할 가능성이 있다고 지적한다(Sims, 2010; Coibion & Gorodnichenko, 2015). 이에 따라 소비자심리지수(Consumer Sentiment Index)가 경제 전망의 보다 정확한 선행지표로 주목받기 시작했다.\n\nCoibion and Gorodnichenko (2015) 는 소비자심리지수가 GDP 성장률, 특히 실질 소비지출의 미래 경로를 잘 예측하는 능력을 가졌음을 실증적으로 증명했다. 이 연구는 특히 불확실성이 높은 시기일수록 소비자심리지수가 공식적인 성장률 모형보다 경제 예측력에서 뛰어남을 보였다.\nSims (2010) 는 구조적 경제 예측 모형이 과거 데이터의 패턴에 과도하게 의존하여 금융 위기와 같은 비선형적 충격을 놓치는 경향이 있다고 지적했다. 반면 소비자 기대감은 그러한 경제적 충격을 보다 신속히 반영하며, 실물경제의 미래 변화에 대한 중요한 통찰력을 제공한다.\n\n\n\n시사점\n위 연구들의 공통된 결론은 명확하다. 전통적 GDP 성장률 측정 방식은 경제 성장과 국민 생활 수준 향상을 반드시 보장하지 않으며, 때로는 현실과 심각한 괴리를 일으킬 수 있다. 실질 중위임금(Real Median Wage)은 국민들의 실제 생활수준 개선 여부를 평가하는 데 있어 GDP보다 더욱 정확한 지표이며, 미래 경제에 대한 소비자의 심리를 측정하는 소비자심리지수는 구조적 예측모델보다 현실적 통찰력을 제공한다.\n경제 정책은 GDP라는 숫자 증가에 매몰되지 않고, 개개인의 실질 생활수준과 소비자의 체감적 경제 기대감을 더 정확히 반영하는 지표 중심으로 전환해야 한다."
  },
  {
    "objectID": "guides/indicator_growth.html#참고문헌",
    "href": "guides/indicator_growth.html#참고문헌",
    "title": "경제 성장을 대표하는 지표?",
    "section": "참고문헌",
    "text": "참고문헌\n\nChetty, R., Grusky, D., Hell, M., Hendren, N., Manduca, R., & Narang, J. (2017). The fading American dream: Trends in absolute income mobility since 1940. Science, 356(6336), 398-406.\nCoibion, O., & Gorodnichenko, Y. (2015). Information Rigidity and the Expectations Formation Process: A Simple Framework and New Facts. American Economic Review, 105(8), 2644-2678.\nCoyle, D. (2014). GDP: A brief but affectionate history. Princeton University Press.\nKubiszewski, I., Costanza, R., Franco, C., Lawn, P., Talberth, J., Jackson, T., & Aylmer, C. (2013). Beyond GDP: Measuring and achieving global genuine progress. Ecological Economics, 93, 57-68.\nPiketty, T. (2014). Capital in the Twenty-First Century. Harvard University Press.\nSaez, E., & Zucman, G. (2016). Wealth inequality in the United States since 1913: Evidence from capitalized income tax data. Quarterly Journal of Economics, 131(2), 519-578.\nSims, C. A. (2010). Rational Inattention and Monetary Economics. Handbook of Monetary Economics, 3, 155-181.\nStiglitz, J., Sen, A., & Fitoussi, J. P. (2009). Report by the commission on the measurement of economic performance and social progress. Paris: Commission on the Measurement of Economic Performance and Social Progress."
  },
  {
    "objectID": "guides/pareto_index.html",
    "href": "guides/pareto_index.html",
    "title": "Approximating Wealth Distribution",
    "section": "",
    "text": "The Pareto distribution has long been used to model wealth or income distributions, starting from the seminal work of Vilfredo Pareto in the late 19th century (Pareto 1964). One of the most intuitive ways to visualize the inequality within such a distribution is through the Lorenz curve, introduced by Lorenz in 1905 (Lorenz 1905). The Lorenz curve describes the cumulative proportion of total wealth (or resources) held by the bottom fraction of the population. Closely related to the Lorenz curve is the Gini coefficient, which provides a scalar measure of inequality (Gini 1912).\nThis document first derives the Lorenz curve for a Pareto distribution, then demonstrates how to estimate the Pareto index (shape parameter) from Lorenz curve observations. We additionally relate the Pareto distribution to Constant Relative Risk Aversion (CRRA) utility function, highlighting the same shared structure in a differential equation that governs both."
  },
  {
    "objectID": "guides/pareto_index.html#introduction",
    "href": "guides/pareto_index.html#introduction",
    "title": "Approximating Wealth Distribution",
    "section": "",
    "text": "The Pareto distribution has long been used to model wealth or income distributions, starting from the seminal work of Vilfredo Pareto in the late 19th century (Pareto 1964). One of the most intuitive ways to visualize the inequality within such a distribution is through the Lorenz curve, introduced by Lorenz in 1905 (Lorenz 1905). The Lorenz curve describes the cumulative proportion of total wealth (or resources) held by the bottom fraction of the population. Closely related to the Lorenz curve is the Gini coefficient, which provides a scalar measure of inequality (Gini 1912).\nThis document first derives the Lorenz curve for a Pareto distribution, then demonstrates how to estimate the Pareto index (shape parameter) from Lorenz curve observations. We additionally relate the Pareto distribution to Constant Relative Risk Aversion (CRRA) utility function, highlighting the same shared structure in a differential equation that governs both."
  },
  {
    "objectID": "guides/pareto_index.html#deriving-the-lorenz-curve-of-a-pareto-distribution",
    "href": "guides/pareto_index.html#deriving-the-lorenz-curve-of-a-pareto-distribution",
    "title": "Approximating Wealth Distribution",
    "section": "2. Deriving the Lorenz Curve of a Pareto Distribution",
    "text": "2. Deriving the Lorenz Curve of a Pareto Distribution\n\n2.1. Definition of the Lorenz Curve\nFor a continuous distribution of wealth (X), the Lorenz curve \\(L(p)\\) is defined as the fraction of total wealth owned by the bottom \\(p\\) fraction of the population:\n\\[\nL(p) \\;=\\; \\frac{\\int_{x_m}^{x(p)} x\\,f(x)\\,dx}{\\int_{x_m}^{\\infty} x\\,f(x)\\,dx},\n\\]\nwhere\n\n\\(p = F\\bigl(x(p)\\bigr)\\) is the cumulative proportion of individuals with wealth below \\(x(p)\\),\nThe numerator represents the cumulative wealth of the bottom \\(p\\) fraction,\nThe denominator represents the total wealth in the system, given by the expected value of \\(X\\) over its support.\n\n\n\n2.2. Pareto Distribution\nA Pareto distribution with shape parameter \\(\\alpha&gt;0\\) and scale parameter \\(x_m&gt;0\\) is defined by the PDF\n\\[\nf(x) \\;=\\; \\alpha\\,\\frac{x_m^\\alpha}{x^{\\alpha+1}},\n\\quad x \\ge x_m,\n\\]\nand the corresponding CDF\n\\[\nF(x) \\;=\\; 1 \\;-\\;\\Bigl(\\tfrac{x_m}{x}\\Bigr)^\\alpha,\n\\quad x \\ge x_m.\n\\]\nEquivalently, for \\(0 &lt; p &lt; 1\\), the quantile \\(x(p)\\) satisfying \\(F\\bigl(x(p)\\bigr)=p\\) is\n\\[\np \\;=\\; 1 \\;-\\;\\Bigl(\\tfrac{x_m}{x(p)}\\Bigr)^\\alpha\n\\;\\;\\Longleftrightarrow\\;\\;\nx(p) \\;=\\; \\frac{x_m}{\\bigl(1 - p\\bigr)^{1/\\alpha}}.\n\\]\n\n\n2.3. Total Wealth\nLet the total wealth be \\(W_{\\text{total}} = E[X]\\), the expected value of \\(X\\). Substituting the PDF of the Pareto distribution into the definition of expectation,\n\\[\nE[X]\n\\;=\\; \\int_{x_m}^{\\infty} x\\,f(x)\\,dx\n\\;=\\; \\int_{x_m}^{\\infty} x \\,\\alpha\\,\\frac{x_m^\\alpha}{x^{\\alpha+1}}\\,dx\n\\;=\\; \\alpha\\,x_m^\\alpha \\int_{x_m}^{\\infty} x^{-\\alpha}\\,dx.\n\\]\nFor \\(\\alpha&gt;1\\), the improper integral converges and we obtain\n\\[\nW_{\\text{total}}\n\\;=\\; E[X]\n\\;=\\; \\frac{\\alpha\\,x_m}{\\alpha - 1}.\n\\]\n\n\n2.4. Cumulative Wealth for the Bottom \\(p\\) Fraction\nThe cumulative wealth held by the bottom \\(p\\) fraction is\n\\[\nW(p)\n\\;=\\;\\int_{x_m}^{x(p)} x\\,f(x)\\,dx\n\\;=\\;\\alpha\\,x_m^\\alpha \\int_{x_m}^{x(p)} x^{-\\alpha}\\,dx\n\\;=\\;\\alpha\\,x_m^\\alpha\n\\Bigl[\\frac{x^{-\\alpha+1}}{-\\alpha+1}\\Bigr]_{x_m}^{x(p)}.\n\\]\nSimplifying,\n\\[\nW(p)\n\\;=\\; \\frac{\\alpha\\,x_m}{\\alpha - 1}\n\\;\\Bigl(1 - \\bigl(\\tfrac{x_m}{x(p)}\\bigr)^{\\alpha - 1}\\Bigr).\n\\]\n\n\n2.5. Lorenz Curve for the Pareto Distribution\nBy definition,\n\\[\nL(p)\n\\;=\\; \\frac{W(p)}{W_{\\text{total}}}\n\\;=\\; \\frac{\\frac{\\alpha\\,x_m}{\\alpha - 1}\\,\\Bigl(1 - \\bigl(\\tfrac{x_m}{x(p)}\\bigr)^{\\alpha - 1}\\Bigr)}\n            {\\frac{\\alpha\\,x_m}{\\alpha - 1}}\n\\;=\\; 1 - \\bigl(1 - p\\bigr)^{\\frac{\\alpha - 1}{\\alpha}}.\n\\]\nHence, for a Pareto distribution, the Lorenz curve is\n\\[\nL(p) \\;=\\; 1 \\;-\\; \\bigl(1 - p\\bigr)^{\\tfrac{\\alpha - 1}{\\alpha}}.\n\\]\n\nIf \\(\\alpha \\gg 1\\), the distribution is more equal, and \\(L(p)\\) is closer to the 45-degree line of perfect equality.\n\nIf \\(\\alpha\\) is only slightly larger than 1, the distribution is more unequal, with significant concentration of wealth in the upper tail."
  },
  {
    "objectID": "guides/pareto_index.html#estimating-the-pareto-index-from-the-lorenz-curve",
    "href": "guides/pareto_index.html#estimating-the-pareto-index-from-the-lorenz-curve",
    "title": "Approximating Wealth Distribution",
    "section": "3. Estimating the Pareto Index from the Lorenz Curve",
    "text": "3. Estimating the Pareto Index from the Lorenz Curve\nSuppose empirical data or external studies indicate specific points \\((p, L(p))\\) on the Lorenz curve. We can use\n\\[\nL(p) \\;=\\; 1 - (1 - p)^{\\frac{\\alpha - 1}{\\alpha}}\n\\]\nto solve numerically for \\(\\alpha\\). Commonly cited examples:\nSeveral Empirical Illustrations\n\nPareto 80:20 Rule: \\(p=0.80\\), \\(L(p)=0.20\\)\n\\(\\Rightarrow\\) \\(\\alpha \\approx \\frac{\\ln(4)}{\\ln(5)} \\approx 1.16\\).\nPareto 90:10 Rule: \\(p=0.90\\), \\(L(p)=0.10\\)\n\\(\\Rightarrow\\) \\(\\alpha \\approx 1.05\\).\nU.S. Stock Market: According to a report by Axios (2024), the top 10% own about 93% of total equity wealth, implying \\(p=0.90\\) and \\(L(p)=0.07\\). Solving yields \\(\\alpha \\approx 1.03\\).\n\nCredit Suisse Global Wealth Report: In 2013, it was reported that the top 1% control about 50% of global wealth, which implies \\(p=0.99\\) and \\(L(p)=0.50\\). Solving gives \\(\\alpha \\approx 1.18\\). Additionally, the top 10% were said to own about 85% of global wealth (\\(p=0.90\\), \\(L(p)=0.15\\)), giving \\(\\alpha \\approx 1.08\\). Comparing such estimates across years (e.g., 2013 vs. 2020) can reveal the time dynamics of the global wealth distribution (Credit Suisse 2013, 2020)."
  },
  {
    "objectID": "guides/pareto_index.html#the-gini-coefficient",
    "href": "guides/pareto_index.html#the-gini-coefficient",
    "title": "Approximating Wealth Distribution",
    "section": "4. The Gini Coefficient",
    "text": "4. The Gini Coefficient\nThe Gini coefficient is a measure of wealth or income inequality that is closely related to the Lorenz curve. The Gini coefficient is defined as the ratio of the area between the Lorenz curve and the 45-degree equality line to the total area under the 45-degree line. Mathematically, the Gini coefficient ( G ) is given by:\n\\[\nG = 1 - 2 \\int_0^1 L(p) \\, dp.\n\\]\nSubstituting the Lorenz curve for a Pareto distribution:\n\\[\nL(p) = 1 - (1 - p)^{\\frac{\\alpha - 1}{\\alpha}},\n\\]\nwe obtain:\n\\[\nG = 1 - 2 \\int_0^1 \\big[1 - (1 - p)^{\\frac{\\alpha - 1}{\\alpha}}\\big] \\, dp = \\frac{1}{2\\alpha - 1}.\n\\]\nThus, for a Pareto distribution, the Gini coefficient is:\n\\[\nG = \\frac{1}{2\\alpha - 1}, \\quad \\text{for } \\alpha &gt; \\frac{1}{2}.\n\\]\nSome features of the Gini coefficient:\n\nAs \\(\\alpha \\to 1^+\\), the Gini coefficient approaches 1, indicating extreme inequality (a few individuals hold nearly all the wealth).\nAs \\(\\alpha \\to \\infty\\), the Gini coefficient approaches 0, indicating perfect equality.\nFor typical empirical values of \\(\\alpha\\) in wealth distributions (e.g., 1.1 to 1.8), the Gini coefficient ranges from 0.83 to 0.38, reflecting significant inequality\nRelative measure: \\(G\\) compares the distribution to perfect equality, but does not capture absolute differences.\n\nNon‐additivity: One cannot simply average the Gini coefficients of subpopulations to obtain an overall Gini coefficient.\n\nSensitivity: The Gini coefficient is sensitive to changes in the middle of the distribution, but less so at the tails."
  },
  {
    "objectID": "guides/pareto_index.html#pareto-distribution-and-crra-utility",
    "href": "guides/pareto_index.html#pareto-distribution-and-crra-utility",
    "title": "Approximating Wealth Distribution",
    "section": "5. Pareto Distribution and CRRA Utility",
    "text": "5. Pareto Distribution and CRRA Utility\nIn microeconomic theory, a widely used utility specification is the Constant Relative Risk Aversion (CRRA) form (Arrow et al. 1974; Pratt 1978). The CRRA utility function \\(u(x)\\) satisfies\n\\[\n-\\frac{x\\,u''(x)}{u'(x)} \\;=\\; \\gamma,\n\\]\nwhere \\(\\gamma&gt;0\\) is the coefficient of relative risk aversion. Solving for \\(u(x)\\) under boundary conditions such as \\(u(1)=0\\) yields:\n\nIf \\(\\gamma=1\\), \\(u(x)=\\ln x\\). (using the L’hopital’s Rule)\nIf \\(\\gamma\\neq 1\\), \\(u(x)=\\frac{x^{\\,1-\\gamma}-1}{\\,1-\\gamma\\,}\\).\n\nLarger \\(\\gamma\\) indicates higher risk aversion, while \\(\\gamma=0\\) corresponds to risk neutrality (\\(u(x)=x\\)).\nA Pareto PDF can also be derived from a differential equation with similar form. If \\(f(x)\\) is the PDF of a Pareto random variable on \\(x\\ge x_m&gt;0\\), one can write:\n\\[\n-\\frac{x\\,f'(x)}{\\,f(x)\\!}\\;=\\;(1+\\alpha)\\,x_m^{\\alpha},\n\\]\nwhich likewise has a “power‐law” solution structure. Thus, Pareto distributions and CRRA utilities each emerge from a linear differential equation of analogous form, underscoring a conceptual parallel in how “power‐type” functional solutions can appear in both economic choice models (through marginal utility) and in heavy‐tailed probability distributions.\nFurthermore, in mainstream economic theory, marginal utility \\(u'(x)\\) is assumed to be strictly positive, and \\(u''(x)\\) typically negative (diminishing marginal utility). In probability theory, any valid PDF \\(f(x)\\) must be positive, and for heavy‐tailed distributions like Pareto, \\(f(x)\\) decreases for large \\(x\\). These parallels lead to a one‐to‐one analogy between certain types of declining utilities and distributions whose density functions also decline in \\(x\\).\n\nRemark: There is a well‐known relationship via logarithmic transforms: if \\(X\\) is Pareto(\\(x_m,\\alpha\\)), then \\(Y=\\ln(X/x_m)\\) is exponentially distributed with rate \\(\\alpha\\). This exponential distribution also arises from a first‐order linear differential equation, reinforcing these structural similarities."
  },
  {
    "objectID": "guides/pareto_index.html#conclusion",
    "href": "guides/pareto_index.html#conclusion",
    "title": "Approximating Wealth Distribution",
    "section": "6. Conclusion",
    "text": "6. Conclusion\nBecause the Pareto distribution has only two parameters (\\(x_m\\) and \\(\\alpha\\)), even minimal distributional data—such as “the bottom 80% own 20% of total wealth”—enables one to solve directly for the Pareto index \\(\\alpha\\). This simplicity makes the Pareto distribution a convenient model or approximation for global wealth distribution, although in practice the estimated \\(\\alpha\\) can vary greatly depending on the dataset, sampling, and specific segment of the population observed.\nIn wealth and income distribution analysis, pairing empirical Lorenz curves with Pareto modeling remains a powerful—if simplified—approach to gauging inequality. For both theoretical and practical reasons, it continues to be integral in economic research, policy discussions, and broader studies of social welfare. Meanwhile, connections to CRRA utility function illustrate that core economic principles and certain types of heavy‐tailed probabilistic behavior can share similar mathematical underpinnings."
  },
  {
    "objectID": "guides/yield_curve.html",
    "href": "guides/yield_curve.html",
    "title": "Yield curve",
    "section": "",
    "text": "채권 가격 (bond price)과 시장 금리(yield rate, 채권 수익률)는 역의 관계(inverse relationship)\n\n금리가 오르면 채권 가격은 하락하고,\n금리가 내리면 채권 가격은 상승한다.\n\n왜?\n\n시장 금리가 상승하면 기존의 낮은 금리를 가진 채권의 상대적 매력이 떨어지므로 가격이 하락하고,\n시장 금리가 하락하면 기존의 높은 금리를 가진 채권이 더 매력적으로 보이므로 가격이 상승한다.\n\n채권의 현재 가치(PV)를 수식으로 표현하면 다음과 같다.\n\\[\nP = \\sum_{t=1}^{T} \\frac{C}{(1 + r)^t} + \\frac{F}{(1 + r)^T}\n\\]\n여기서,\n- \\(P\\) : 채권 가격 (Present Value)\n- \\(C\\) : 매년 지급되는 쿠폰 이자 (Coupon Payment)\n- \\(F\\) : 만기 시 원금(Face Value)\n- \\(r\\) : 시장 금리 (Yield Rate)\n- \\(T\\) : 잔존 만기\n시장금리(\\(r\\))가 증가하면 할인율이 커져서 내가 가진 채권의 현재 가치(\\(P\\))가 낮아짐\n\n\n가정:\n\n채권의 액면가: 1,000달러\n쿠폰 이자율: 5% (연간)\n만기: 3년\n현재 시장 금리: 5% → 2% (3% 하락)\n\n인플레이션 때문에 기준금리가 내려가서 시장 금리가 하락하면,\n새로 발행되는 2% 금리의 채권보다 기존 5%의 쿠폰을 지급하는 채권이 더 매력적으로 보이므로,\n기존 채권의 가격이 상승한다.\n\n\n\n채권 금리는 단순히 하나의 고정된 값이 아니라, 만기에 따라 다르게 형성됨.\n\n단기 채권(Short-term bonds, 1년 이하): 단기 금리는 중앙은행의 정책금리(예: 연준의 FFR, 한국은행의 기준금리)와 직접적으로 연결됨.\n중기 채권(Mid-term bonds, 2~10년): 경제 성장률, 인플레이션 기대치, 신용 위험 등에 영향을 받음.\n장기 채권(Long-term bonds, 10년 이상): 장기 인플레이션 기대, 국가 신용, 경기 사이클 등에 따라 수익률이 결정됨.\n\n\n\n\n채권은 듀레이션(Duration)이 길수록 금리 변화에 대한 가격 민감도가 커진다.\n- 장기 채권: 금리 변화에 더 민감\n- 단기 채권: 금리 변화에 덜 민감\n\n\n\nTerm Structure는 채권 시장에서 만기(term)에 따라 형성되는 금리 구조\nTerm Structure는 주로 Yield Curve(수익률 곡선)의 형태로 시각화된다.\n\n\n\n\n금리 상승 (Yield 상승) 예상 → 채권 가격 하락 → 기존 장기 채권 보유자는 손해. 기존 장기 채권을 매도하고, 새로운 금리가 반영된 채권을 매수. 기준금리 상승때문에 단기 채권은 괜찮음.\n금리 하락 (Yield 하락) 에상 → 채권 가격 상승 → 기존 장기 채권 보유자는 이득. 기존 채권을 보유하여 가격 상승에 따른 자본 이득(capital gain) 확보. 장기 채권을 보유하는 것이 유리."
  },
  {
    "objectID": "guides/yield_curve.html#채권-가격과-시장-금리의-관계",
    "href": "guides/yield_curve.html#채권-가격과-시장-금리의-관계",
    "title": "Yield curve",
    "section": "",
    "text": "채권 가격 (bond price)과 시장 금리(yield rate, 채권 수익률)는 역의 관계(inverse relationship)\n\n금리가 오르면 채권 가격은 하락하고,\n금리가 내리면 채권 가격은 상승한다.\n\n왜?\n\n시장 금리가 상승하면 기존의 낮은 금리를 가진 채권의 상대적 매력이 떨어지므로 가격이 하락하고,\n시장 금리가 하락하면 기존의 높은 금리를 가진 채권이 더 매력적으로 보이므로 가격이 상승한다.\n\n채권의 현재 가치(PV)를 수식으로 표현하면 다음과 같다.\n\\[\nP = \\sum_{t=1}^{T} \\frac{C}{(1 + r)^t} + \\frac{F}{(1 + r)^T}\n\\]\n여기서,\n- \\(P\\) : 채권 가격 (Present Value)\n- \\(C\\) : 매년 지급되는 쿠폰 이자 (Coupon Payment)\n- \\(F\\) : 만기 시 원금(Face Value)\n- \\(r\\) : 시장 금리 (Yield Rate)\n- \\(T\\) : 잔존 만기\n시장금리(\\(r\\))가 증가하면 할인율이 커져서 내가 가진 채권의 현재 가치(\\(P\\))가 낮아짐\n\n\n가정:\n\n채권의 액면가: 1,000달러\n쿠폰 이자율: 5% (연간)\n만기: 3년\n현재 시장 금리: 5% → 2% (3% 하락)\n\n인플레이션 때문에 기준금리가 내려가서 시장 금리가 하락하면,\n새로 발행되는 2% 금리의 채권보다 기존 5%의 쿠폰을 지급하는 채권이 더 매력적으로 보이므로,\n기존 채권의 가격이 상승한다.\n\n\n\n채권 금리는 단순히 하나의 고정된 값이 아니라, 만기에 따라 다르게 형성됨.\n\n단기 채권(Short-term bonds, 1년 이하): 단기 금리는 중앙은행의 정책금리(예: 연준의 FFR, 한국은행의 기준금리)와 직접적으로 연결됨.\n중기 채권(Mid-term bonds, 2~10년): 경제 성장률, 인플레이션 기대치, 신용 위험 등에 영향을 받음.\n장기 채권(Long-term bonds, 10년 이상): 장기 인플레이션 기대, 국가 신용, 경기 사이클 등에 따라 수익률이 결정됨.\n\n\n\n\n채권은 듀레이션(Duration)이 길수록 금리 변화에 대한 가격 민감도가 커진다.\n- 장기 채권: 금리 변화에 더 민감\n- 단기 채권: 금리 변화에 덜 민감\n\n\n\nTerm Structure는 채권 시장에서 만기(term)에 따라 형성되는 금리 구조\nTerm Structure는 주로 Yield Curve(수익률 곡선)의 형태로 시각화된다.\n\n\n\n\n금리 상승 (Yield 상승) 예상 → 채권 가격 하락 → 기존 장기 채권 보유자는 손해. 기존 장기 채권을 매도하고, 새로운 금리가 반영된 채권을 매수. 기준금리 상승때문에 단기 채권은 괜찮음.\n금리 하락 (Yield 하락) 에상 → 채권 가격 상승 → 기존 장기 채권 보유자는 이득. 기존 채권을 보유하여 가격 상승에 따른 자본 이득(capital gain) 확보. 장기 채권을 보유하는 것이 유리."
  },
  {
    "objectID": "guides/yield_curve.html#yield-curve수익률-곡선",
    "href": "guides/yield_curve.html#yield-curve수익률-곡선",
    "title": "Yield curve",
    "section": "Yield Curve(수익률 곡선)",
    "text": "Yield Curve(수익률 곡선)\nYield Curve는 만기별 수익률을 연결한 곡선으로, 주요 형태는 3가지:\n\n정상적인 형태(Normal Yield Curve): 장기 금리가 단기 금리보다 높음 → 경제 성장이 기대되는 경우\n역전된 형태(Inverted Yield Curve): 장기 금리가 단기 금리보다 낮음 → 경기 침체 신호\n평평한 형태(Flat Yield Curve): 단기와 장기 금리 차이가 거의 없음 → 경기 전환기\n\n이렇게 다양한 형태가 나타나는 이유를 설명하는 이론들에는, 금리 기대이론(Expectations Hypothesis), 유동성 프리미엄 이론(Liquidity Premium Theory), 시장 분할 이론(Market Segmentation Theory) 등이 있음.\nYield Curve(수익률 곡선)는 본질적으로 \\((\\text{만기}, \\text{yield})\\) 쌍에 대한 Conditional Expectation Model을 적용한 결과라고 볼 수 있다.\n채권 시장에서 다양한 만기의 국채(예: 1년, 2년, 5년, 10년, 30년 등)의 수익률 데이터를 수집하면, 기본적으로 (만기, yield) 쌍의 산점도(scatter plot)를 얻을 수 있다.\n이후, Yield Curve는 이러한 데이터를 조건부 기대값 모형 (Conditional Expectation model)을 사용하여 스무딩(Smoothing)하거나 추정(Fitting)한 것이라고 볼 수 있다.\n수익률 곡선을 추정하는 대표적인 방법으로 다음과 같은 모델들이 있습니다.\n\nPolynomial Regression\n\n가장 기본적인 방법은 2차 또는 3차 다항식을 사용하여 스무딩한 곡선을 추정.\n\\[\n\\mathbb{E}[Y | T] = \\beta_0 + \\beta_1 T + \\beta_2 T^2 + \\beta_3 T^3 + \\epsilon\n\\] 여기서:\n\n\\(T\\)는 채권의 만기,\n\\(Y\\)는 수익률(Yield),\n\\(\\mathbb{E}[Y | T]\\)는 조건부 기대값.\n\n\n\n\nSpline Regression\n\nCubic Spline 또는 B-spline을 사용하여 여러 구간에서 스무딩된 Yield Curve를 만듬.\n\n\n\nNelson-Siegel & Svensson model\n\n실무에서 많이 사용하는 Nelson-Siegel Model의 기본 식 \\[\nY(T) = \\beta_0 + \\beta_1 \\frac{1 - e^{-T/\\tau}}{T/\\tau} + \\beta_2 \\left(\\frac{1 - e^{-T/\\tau}}{T/\\tau} - e^{-T/\\tau} \\right)\n\\]\n여기서:\n\n\\(\\beta_0, \\beta_1, \\beta_2\\)는 모델의 파라미터\n\\(\\tau\\)는 시간 척도 조정 파라미터\n\\(T\\)는 만기 (term)\n\n\n\n\nGaussian Process, Neural Networks\n\nGaussian Process Regression (GPR) 또는 딥러닝 모델(Neural Networks)을 활용하여 Yield Curve를 추정하는 방법"
  },
  {
    "objectID": "guides/yield_curve.html#code-scatter-plot-yield-curve",
    "href": "guides/yield_curve.html#code-scatter-plot-yield-curve",
    "title": "Yield curve",
    "section": "Code: Scatter Plot + Yield Curve",
    "text": "Code: Scatter Plot + Yield Curve\n(만기, 수익률) 데이터 산점도를 그린 후, 스무딩된 Yield Curve를 적용.\n\n\nCode\n# Yield Curve(수익률 곡선)**는 본질적으로 (term,yield) 쌍에 대한 Conditional Expectation Model을 적용한 결과\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\n\n# 예제 데이터 (만기, 수익률)\nmaturity = np.array([1, 2, 3, 5, 7, 10, 20, 30])  # 만기 (years)\nyield_rates = np.array([3.2, 3.4, 3.5, 3.7, 3.8, 4.0, 4.2, 4.3])  # 수익률 (%)\n\n# Nelson-Siegel 모델 함수 정의\ndef nelson_siegel(T, beta0, beta1, beta2, tau):\n    return beta0 + beta1 * (1 - np.exp(-T/tau)) / (T/tau) + beta2 * ((1 - np.exp(-T/tau)) / (T/tau) - np.exp(-T/tau))\n\n# 초기값 설정 및 최적화\npopt, _ = curve_fit(nelson_siegel, maturity, yield_rates, p0=[4, -1, 1, 2])\n\n# 스무딩된 곡선 생성\nT_fit = np.linspace(0.5, 30, 100)  # 연속적인 만기 값\nY_fit = nelson_siegel(T_fit, *popt)\n\n# 산점도 및 수익률 곡선 그래프\nplt.figure(figsize=(10, 6))\nplt.scatter(maturity, yield_rates, color='blue', label=\"Observed Data (Scatter)\")\nplt.plot(T_fit, Y_fit, color='red', linestyle='-', label=\"Fitted Yield Curve (Nelson-Siegel)\")\nplt.xlabel(\"Maturity (Years)\")\nplt.ylabel(\"Yield (%)\")\nplt.title(\"Yield Curve: Scatter Plot with Nelson-Siegel Fit\")\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "projects/corner_solution.html",
    "href": "projects/corner_solution.html",
    "title": "Corner Solutions in Optimization Model",
    "section": "",
    "text": "1. Introduction\nIn standard economic theory, both consumer preferences and production sets are generally assumed to exhibit convexity (Arrow and Debreu 1954; Debreu 1959). This assumption supports foundational results, including the existence and uniqueness of equilibrium and the efficiency of market allocations. In practice, however, features such as network externalities (Katz and Shapiro 1985; Rochet and Tirole 2003), rent-seeking (Shleifer and Vishny 1993), and multiple equilibria—often culminating in pronounced market dominance—can produce outcomes resembling non-convex preferences (Arthur 1994). In many cases, corner solutions and path-dependent equilibria emerge from winner-takes-all dynamics, concentrated economic power, and barriers to entry.\n\n\n2. Convexity in Economic Theory\n2.1 Convex Preferences and Production Sets\n\nConsumer preferences are typically modeled with quasi-concave utility functions, yielding convex (or “bowl-shaped”) indifference curves. This setup implies a preference for diversity in consumption, rather than extreme or corner solutions (Debreu 1959).\nProducers are often assumed to face diminishing marginal returns, reflected in a convex production possibility set. Under such conditions, output expansions follow a predictable pattern, and average costs rise eventually.\n\n2.2 Existence and Efficiency of Equilibrium\n\nWith convexity, free market entry, symmetric information, and price-taking behavior, perfectly competitive markets are shown to possess a stable equilibrium that is Pareto efficient (Arrow and Debreu 1954).\nThese results typically rely on fixed-point theorems and the properties of convex sets, ensuring both the existence of equilibrium prices and (in many cases) uniqueness or stability (Debreu 1959).\n\n2.3 Normative Implications\n\nConvexity underpins the normative stance that, absent significant market failures, competitive markets gravitate toward Pareto-efficient resource allocations.\nConsequently, government interventions usually aim to correct externalities, public goods issues, or information asymmetries within a broader context of largely convex preferences and production sets.\n\n\n\n3. Non-Convexities in Reality\n3.1 Network Externalities and Increasing Returns to Scale\n\nIn contrast to diminishing returns, many digital or platform-based markets exhibit network externalities, or increasing returns to scale (Katz and Shapiro 1985; Rochet and Tirole 2003). As additional users join a platform, its value to each user grows, often driving corner solutions in both production and consumption.\nInstead of smoothly concave utility or production functions, certain markets feature segments of increasing marginal returns, leading to “winner-takes-all” or “winner-take-most” dynamics.\n\n3.2 Coordination Games and Multiple Equilibria\n\nNetwork externalities commonly create coordination games, where each agent’s optimal choice depends on the choices of others. Small initial advantages or random shocks may tip the market toward a specific product or standard, resulting in lock-in (Arthur 1994).\nSuch scenarios can produce multiple Nash equilibria, for instance everyone choosing Product A or everyone choosing Product B, with potentially large welfare differences between them.\n\n3.3 Extreme or Corner Solutions in Consumption and Production\n\nWith robust network effects, consumers or producers may converge on a single brand, platform, or location, effectively marginalizing other options—even if those alternatives might have been preferred under purely convex preferences.\nThese corner solutions deviate from the classical idea that diversification in consumption and moderate scales in production yield optimal outcomes.\n\n3.4 Rent-Seeking and Incumbent Power\n\nDominant firms or groups can exploit political influence—through lobbying or regulatory capture—to fortify their positions, reinforcing non-convex outcomes by stifling competition (Tirole 1988; Shleifer and Vishny 1993).\nRent-seeking intensifies the misallocation of resources, as efforts are diverted to defending or reinforcing incumbents’ power, often via barriers to entry, reduced competition, and growing inequalities.\n\n\n\n4. Government Interventions\n4.1 Theoretical View: Correcting Market Imperfections\n\nTraditionally, policy interventions focus on addressing market failures, assuming that preferences and technologies remain fundamentally convex and that interventions are limited and transparent.\n\n4.2 Empirical Evidence: Policy Amplifies Non-Convexities\n\nIn reality, incumbents can wield outsized influence through lobbying and political capture, prompting policies that strengthen market concentration (Tirole 1988).\nInstead of fostering genuinely competitive markets, such policies may lock in non-convex outcomes, creating a vicious cycle of entrenched monopolistic power and limited competition.\n\n4.3 Lock-in and Path Dependence\n\nWhen policy-making aligns with incumbent interests, even minor advantages can become self-reinforcing (Arthur 1994).\nConsequently, once a market tips toward a specific firm, region, or product, effective competition may prove infeasible without sweeping policy reforms or disruptive innovation.\n\n\n\n5. Conclusion\nAlthough classical economic models lean on convex preferences and technologies to assert the existence of unique, efficient equilibria, real-world dynamics often revolve around non-convex phenomena. Network externalities, coordination failures, and rent-seeking can drive corner solutions, multiple equilibria, and lock-in that preserve incumbent advantages. Far from mitigating these issues, government policies sometimes exacerbate them through preferential treatment of dominant actors. Recognizing these non-convex realities is crucial for crafting policy frameworks that transcend purely theoretical assumptions of convexity and address the path-dependent complexity characterizing modern markets.\n\n\nAppendix: Utilitarian Objective function\n\n\nCode\n#@title Utilitarian objective function\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom scipy.optimize import minimize\n\n# 함수 정의\ndef z_function(x, y, a, b):\n  return y * (x**a) + (1 - y) * ((b - x)**a)\n\n# x, y 범위 및 매개변수 설정\na = 0.3  # 매개변수 a 값 (0과 1 사이)\nb = 20  # 매개변수 b 값\n\nx = np.linspace(0, b, 100)  # x 범위: 0부터 20까지 100개의 점\ny = np.linspace(0, 1, 100)  # y 범위: 0부터 1까지 100개의 점\nX, Y = np.meshgrid(x, y)  # x, y 좌표 격자 생성\n\n\n# Z 값 계산\nZ = z_function(X, Y, a, b)\n\n\ndef negative_z_function(params):\n    x, y = params\n    return -z_function(x, y, a, b)  # 최솟값을 찾기 위해 음수 값 반환\n\n# 초기값 설정 (interior 범위 내)\ninitial_guess = [b / 2, 0.5]\n\n# 경계 조건 설정\nbounds = [(0, b), (0, 1)]\n\n# 최적화 실행\nresult = minimize(negative_z_function, initial_guess, bounds=bounds)\n\n# 결과 추출\nextreme_point_x, extreme_point_y = result.x\nextreme_point_z = z_function(extreme_point_x, extreme_point_y, a, b)\n\nprint(\"Extreme Point (x, y, z):\", extreme_point_x, extreme_point_y, extreme_point_z)\n\n# Calculate Hessian matrix\ndef hessian_matrix(x, y, a, b):\n  \"\"\"Calculates the Hessian matrix of the z_function.\"\"\"\n  d2z_dx2 = a * (a - 1) * (y * (x**(a - 2)) + (1 - y) * ((b - x)**(a - 2)))\n  d2z_dy2 = 0  # Second derivative with respect to y is 0\n  d2z_dxdy = a * (x**(a - 1) - (b - x)**(a - 1))\n  d2z_dydx = d2z_dxdy  # Mixed partial derivatives are equal\n\n  return [[d2z_dx2, d2z_dxdy], [d2z_dydx, d2z_dy2]]\n\n# Determine the type of extreme point\nhessian = hessian_matrix(extreme_point_x, extreme_point_y, a, b)\ndeterminant = np.linalg.det(hessian)\n\nif determinant &gt; 0 and hessian[0][0] &gt; 0:\n  extreme_type = \"Minimum\"\nelif determinant &gt; 0 and hessian[0][0] &lt; 0:\n  extreme_type = \"Maximum\"\nelse:\n  extreme_type = \"Saddle\"\n\nprint(\"Extreme Point Type:\", extreme_type)\n\n\n# 3D 그래프 그리기\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(X, Y, Z)\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('z')\nplt.title('3D Graph of z = y*x^a + (1-y)(b-x)^a')\n\n\n# global interior extreme point 표시\nax.scatter(extreme_point_x, extreme_point_y, extreme_point_z, color='red', marker='o', s=100)\nax.text(extreme_point_x, extreme_point_y, extreme_point_z, f'Extreme Point ({extreme_type})', color='red')\n\nplt.show()\n\n# Contour Plot 그리기\nfig, ax = plt.subplots()\ncontour = ax.contour(X, Y, Z)\nax.set_xlabel('x')\nax.set_ylabel('y')\nplt.title('Contour Plot of z = y*x^a + (1-y)(b-x)^a')\nplt.clabel(contour, inline=1, fontsize=10)\n\n# global interior extreme point 표시\nax.scatter(extreme_point_x, extreme_point_y, color='red', marker='o', s=100)\nax.text(extreme_point_x, extreme_point_y, 'Extreme Point', color='red')\n\nplt.show()\n\n\nExtreme Point (x, y, z): 10.0 0.5 1.9952623149688795\nExtreme Point Type: Saddle\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAppendix: Homogeneous function of degree 1\n\n\nCode\n# Define a return to scale\nscale = 1 # Constant return to scale, i.e. Homogeneous function of degree 1\n\n# Define parameter a\na = 1/4\n\n# total wealth of x\nk_x = 2\n# total wealth of y\nk_y = 2\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 경고 메시지 숨기기\nnp.seterr(invalid='ignore')\n\ndef numerical_derivative(f, X, Y, h=1e-5):\n    \"\"\" Compute numerical partial derivatives using central difference method.\"\"\"\n    dfdx = (f(X + h, Y) - f(X - h, Y)) / (2 * h)  # ∂f/∂x\n    dfdy = (f(X, Y + h) - f(X, Y - h)) / (2 * h)  # ∂f/∂y\n    return dfdx, dfdy\n\n# Define functions u_1(x,y) = x^a * y^(1-a) and u_2(x,y) = (2-x)(2-y)\ndef u1(x, y):\n    return x**(scale*a) * y**(scale*(1-a))\n\ndef u2(x, y):\n    return (k_x - x)**(scale*a) * (k_y - y)**(scale*(1-a))\n\n# Define the grid\nx = np.linspace(0, k_x, 15)\ny = np.linspace(0, k_y, 15)\nX, Y = np.meshgrid(x, y)\n\n# Compute the numerical derivatives (vector field components)\nU1, V1 = numerical_derivative(u1, X, Y)\nU2, V2 = numerical_derivative(u2, X, Y)\n\n# Reduce the density of vectors for better visualization\nx_sparse = np.linspace(0, k_x, 8)\ny_sparse = np.linspace(0, k_y, 8)\nX_sparse, Y_sparse = np.meshgrid(x_sparse, y_sparse)\nU1_sparse, V1_sparse = numerical_derivative(u1, X_sparse, Y_sparse)\nU2_sparse, V2_sparse = numerical_derivative(u2, X_sparse, Y_sparse)\n\n# Plot the combined vector fields and contour plots\n#plt.figure(figsize=(8, 8))\n\n# Contour plots of u_1 and u_2 (level curves only)\ncontour1 = plt.contour(X, Y, u1(X, Y), colors='blue', linestyles='solid', linewidths=1)\ncontour2 = plt.contour(X, Y, u2(X, Y), colors='red', linestyles='dashed', linewidths=1)\n\n# Overlay vector fields\nplt.quiver(X_sparse, Y_sparse, U1_sparse, V1_sparse, color='b', angles='xy', label='∇$u_1$')\nplt.quiver(X_sparse, Y_sparse, U2_sparse, V2_sparse, color='r', angles='xy', label='∇$u_2$')\n\n# Labels and grid\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Gradient Vector Fields & Level Curves of $u_1$ and $u_2$')\nplt.legend()\nplt.grid('scaled')\nplt.axis('square')\n\nplt.tight_layout()\n# Show the plot\nplt.show()\n\n# Compute the sum of gradients\nU_sum = U1 + U2\nV_sum = V1 + V2\n\n# Reduce the density of vectors for better visualization\nU_sum_sparse, V_sum_sparse = numerical_derivative(lambda x, y: u1(x, y) + u2(x, y), X_sparse, Y_sparse)\n\n# Plot the combined vector fields and contour plots\n#plt.figure(figsize=(8, 8))\n\n# Contour plots of u_1 and u_2 (level curves only)\ncontour1 = plt.contour(X, Y, u1(X, Y), colors='blue', linestyles='solid', linewidths=1)\ncontour2 = plt.contour(X, Y, u2(X, Y), colors='red', linestyles='dashed', linewidths=1)\n\n# Overlay sum of gradient vector fields\nplt.quiver(X_sparse, Y_sparse, U_sum_sparse, V_sum_sparse, color='g', angles='xy', label='∇($u_1 + u_2$)')\n\n# Labels and grid\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Sum of Gradient Vector Fields & Level Curves of $u_1$ and $u_2$')\nplt.legend()\n\nplt.grid('scaled')\nplt.axis('square')\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAppendix: Sigmoid utility function\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Define constants\nkx = (np.pi**3 / 2) ** (1/3)\nky = (2**(1/2)) * ((np.pi**3 / 2) ** (1/3))\n\n# Define the grid\nx = np.linspace(0, kx, 1000)\ny = np.linspace(0, ky, 1000)\nX, Y = np.meshgrid(x, y)\n\n# Define the functions\nu1 = 1 - np.cos(X**(1/3) * Y**(2/3))\nu2 = 1 - np.cos((kx - X)**(1/3) * (ky - Y)**(2/3))\n\n# Find intersection points where u1 == u2\nthreshold = 1e-3  # Numerical tolerance for equality\nintersection_mask = np.abs(u1 - u2) &lt; threshold\nX_intersect = X[intersection_mask]\nY_intersect = Y[intersection_mask]\nZ_intersect = u1[intersection_mask]  # u1 and u2 are nearly equal\n\n# Create 3D plot\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection='3d')\n\n# Plot intersection line\nax.scatter(X_intersect, Y_intersect, Z_intersect, color='black', s=10, label='Intersection Line')\n\n# Surface plots for reference\nax.plot_surface(X, Y, u1, cmap='Blues', alpha=0.5)\nax.plot_surface(X, Y, u2, cmap='Reds', alpha=0.5)\n\n# Labels and title\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\nax.set_title('3D Intersection of $u_1$ and $u_2$')\nax.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nArrow, Kenneth J., and Gerard Debreu. 1954. “Existence of an Equilibrium for a Competitive Economy.” Econometrica 22 (3): 265–90.\n\n\nArthur, W. Brian. 1994. Increasing Returns and Path Dependence in the Economy. Ann Arbor, MI: University of Michigan Press.\n\n\nDebreu, Gerard. 1959. Theory of Value: An Axiomatic Analysis of Economic Equilibrium. New Haven, CT: Yale University Press.\n\n\nKatz, Michael L., and Carl Shapiro. 1985. “Network Externalities, Competition, and Compatibility.” The American Economic Review 75 (3): 424–40.\n\n\nRochet, Jean-Charles, and Jean Tirole. 2003. “Platform Competition in Two-Sided Markets.” Journal of the European Economic Association 1 (4): 990–1029.\n\n\nShleifer, Andrei, and Robert W. Vishny. 1993. “Corruption.” The Quarterly Journal of Economics 108 (3): 599–617.\n\n\nTirole, Jean. 1988. The Theory of Industrial Organization. Cambridge, MA: MIT Press."
  },
  {
    "objectID": "projects/gilded_age.html",
    "href": "projects/gilded_age.html",
    "title": "The New Gilded Age",
    "section": "",
    "text": "The term ‘Gilded Age’ was originally coined by Mark Twain in his novel The Gilded Age: A Tale of Today (Twain and Warner 1873), describing an era characterized by rapid economic expansion, extreme wealth concentration, and political corruption. A similar dynamic is emerging today, where financial and technological elites dominate economic output while wealth inequality reaches historic highs (Piketty 2014). The late 19th century saw industrial monopolies like Standard Oil and U.S. Steel controlling markets; today, tech giants such as Amazon, Apple, and Google exhibit similar dominance (Zucman 2019).\nSimultaneously, the Federal Reserve’s response to financial instability, particularly through excessive monetary expansion, contrasts with past policy mistakes that led to severe economic contractions due to monetary shrinkage (Bernanke 2000). If current economic trends persist—marked by the increasing concentration of wealth, hyperinflation risks, and geopolitical tensions—then the U.S. may be heading toward another crisis akin to the 1929 stock market collapse (Kindleberger 1978)."
  },
  {
    "objectID": "projects/gilded_age.html#extreme-wealth-concentration-and-economic-disparities",
    "href": "projects/gilded_age.html#extreme-wealth-concentration-and-economic-disparities",
    "title": "The New Gilded Age",
    "section": "Extreme Wealth Concentration and Economic Disparities",
    "text": "Extreme Wealth Concentration and Economic Disparities\nIn the late 19th century, “Robber Barons” controlled vast industrial empires while working-class Americans suffered under exploitative labor conditions (Irwin 2017). Today, the economic landscape reflects a similar dynamic: the top 1% of Americans hold over 30% of total U.S. wealth, and financial markets remain dominated by a handful of institutional investors and corporations (Saez and Zucman 2020). If historical trends hold, wealth concentration at this level often precedes financial and political crises."
  },
  {
    "objectID": "projects/gilded_age.html#financial-market-distortions-due-to-federal-reserve-policies",
    "href": "projects/gilded_age.html#financial-market-distortions-due-to-federal-reserve-policies",
    "title": "The New Gilded Age",
    "section": "Financial Market Distortions Due to Federal Reserve Policies",
    "text": "Financial Market Distortions Due to Federal Reserve Policies\nHistorically, the Federal Reserve’s failure to manage monetary policy effectively has exacerbated financial downturns. During the Great Depression, the Fed allowed the money supply to contract, worsening deflation (Friedman and Schwartz 1993). Conversely, in the 2008 financial crisis, the Fed implemented massive QE programs to avoid liquidity shortages (Gopinath and Gourinchas 2020). If the Fed continues expanding the money supply unchecked while maintaining low interest rates, it could trigger runaway inflation or asset bubbles (Reinhart and Rogoff 2010)."
  },
  {
    "objectID": "projects/gilded_age.html#protectionist-policies-and-global-trade-disruptions",
    "href": "projects/gilded_age.html#protectionist-policies-and-global-trade-disruptions",
    "title": "The New Gilded Age",
    "section": "Protectionist Policies and Global Trade Disruptions",
    "text": "Protectionist Policies and Global Trade Disruptions\nIn response to financial instability, the U.S. may turn to protectionist measures similar to those seen in the early 20th century, such as the Smoot-Hawley Tariff Act (Irwin 2017). If the U.S. imposes broad tariffs on allies like Canada, Mexico, and the EU (excluding the UK), retaliatory tariffs could significantly reduce global trade, accelerating economic fragmentation (Acker 2020)."
  },
  {
    "objectID": "projects/gilded_age.html#capital-controls",
    "href": "projects/gilded_age.html#capital-controls",
    "title": "The New Gilded Age",
    "section": "Capital Controls",
    "text": "Capital Controls\nTo prevent capital flight, the U.S. government might implement capital controls, restricting the movement of funds outside the country (Dornbusch 1996). Such policies could initially stabilize domestic financial markets by preventing liquidity outflows, but they would ultimately deter foreign investment and reduce the credibility of the U.S. dollar (Prasad 2021). If capital controls are implemented alongside protectionist trade policies, the global financial system could realign, reducing reliance on the dollar (Eichengreen 2019)."
  },
  {
    "objectID": "projects/gilded_age.html#internal-conflict",
    "href": "projects/gilded_age.html#internal-conflict",
    "title": "The New Gilded Age",
    "section": "Internal Conflict",
    "text": "Internal Conflict\nWith increasing partisan division, the U.S. could experience state-led resistance against federal economic policies. Democratic-led states might oppose Republican federal mandates, leading to legal disputes over taxation, social policies, and trade regulations (Levitsky and Ziblatt 2018). In extreme cases, states like California could advocate for economic or political autonomy, mirroring secessionist movements of the 19th century, while Texas, despite its strong Republican leanings, might push for greater state sovereignty in response to federal overreach or shifting national policies (Acker 2020)."
  },
  {
    "objectID": "projects/gilded_age.html#u.s.-dollar-as-the-global-reserve-currency",
    "href": "projects/gilded_age.html#u.s.-dollar-as-the-global-reserve-currency",
    "title": "The New Gilded Age",
    "section": "U.S. Dollar as the Global Reserve Currency",
    "text": "U.S. Dollar as the Global Reserve Currency\nThe decline of the British pound post-World War II illustrates how global reserve currencies can lose dominance due to internal and external economic shifts (Eichengreen 2019). If U.S. political instability continues, central banks worldwide may accelerate diversification away from dollar holdings, increasing reliance on alternative financial networks such as BRICS payment systems, Bitcoin, and other emerging digital currencies. (Prasad 2021)."
  },
  {
    "objectID": "projects/pricing_equal.html",
    "href": "projects/pricing_equal.html",
    "title": "Pricing the seemingly Equal Opportunities under Structural Inequality",
    "section": "",
    "text": "By applying the EMM-based asset pricing approach, this study quantifies the impact of structural inequalities on the valuation of ostensibly equal opportunities. The results emphasize the importance of considering underlying economic disparities when evaluating the fairness and effectiveness of qualification standards in society."
  },
  {
    "objectID": "projects/pricing_equal.html#conclusion",
    "href": "projects/pricing_equal.html#conclusion",
    "title": "Pricing the seemingly Equal Opportunities under Structural Inequality",
    "section": "",
    "text": "By applying the EMM-based asset pricing approach, this study quantifies the impact of structural inequalities on the valuation of ostensibly equal opportunities. The results emphasize the importance of considering underlying economic disparities when evaluating the fairness and effectiveness of qualification standards in society."
  },
  {
    "objectID": "projects/pricing_equal.html#main",
    "href": "projects/pricing_equal.html#main",
    "title": "Pricing the seemingly Equal Opportunities under Structural Inequality",
    "section": "Main",
    "text": "Main\n\nModel Scenario\nIn short, we consider a society divided into two distinct classes:\n\nProletariat (P): Individuals with access to general opportunities.\nCapitalist (C): Individuals with access to both general and exclusive special opportunities.\n\nInternally, both classes operate within perfectly competitive markets that adhere to the no-arbitrage principle. However, due to capital constraints, members of the P class cannot access the special opportunities available to the C class, rendering these opportunities unattainable. Consequently, while the absolute value of special opportunities is equal to or greater than that of general opportunities, the P class cannot exploit potential arbitrage opportunities due to these constraints. Both classes have offspring who inherit the economic outcomes of their parents’ investments. To claim these inherited assets, all offspring must meet a minimum qualification standard (e.g., educational credentials), analogous to the strike price (\\(K\\)) in option pricing. Until this qualification is met, the offspring hold a call option on their parents’ assets.\nIn detail, consider two distinct social classes: the proletariat (P), representing working-class individuals, and the capitalist class (C), representing wealthy individuals. These two classes face fundamentally different opportunity sets due to deep-seated structural inequalities. Such societal barriers, from both democratic and utilitarian viewpoints, constitute a significant social inefficiency.\nTo analyze the structural origins of this inefficiency, we apply the concept of Limits to Arbitrage Theory, which suggests markets are not always fully efficient. This inefficiency arises because of various constraints, such as behavioral biases, risk constraints, and notably, capital constraints. Capital constraints imply that significant capital is required to engage in arbitrage, effectively isolating classes economically. We assume, therefore, that the proletariat and capitalist classes are economically segregated due to these capital constraints.\nWithin each class, economic opportunities exist in a perfectly competitive market, satisfying the no-arbitrage condition internally. Members of the proletariat (P) freely select from a common set of general opportunities available to them. Conversely, members of the capitalist class (C) not only share these general opportunities but also exclusively access additional “special opportunities.” From the viewpoint of proletariat class members, these special opportunities represent unattainable benefits (“grapes beyond reach”), carrying higher or equal absolute value compared to general opportunities.\nTheoretically, if members of the capitalist class could short-sell general opportunities and simultaneously long-position special opportunities, they could realize arbitrage profits. However, due to aforementioned capital constraints (“limits to arbitrage”), such arbitrage trading is practically impossible within this model.\nBoth classes have offspring who inherit the economic results (absolute asset values) of their parents’ investment choices. To claim these inherited assets, children from both classes must meet identical minimum qualification standards (e.g., university diplomas, basic educational credentials). Although the qualification standard is identical for both, outcomes differ significantly due to inherited assets. For instance, a business administration graduate from the capitalist class inherits and manages substantial capital (businesses), while an identically qualified individual from the proletariat class works as an employee, earning wages in capitalist-owned enterprises.\nThe minimum qualification standard can be understood as a fixed barrier or strike price (K) of a call option. Until the qualification requirement is fulfilled, the children effectively hold call options on their parents’ assets. This model aims to quantify the absolute value of these call options for each class, reflecting the inherited economic outcomes accessible to the offspring.\n\n\nEMM-based Call Option Valuation Theory\nAssume a simple 1 period setting with only two possible states (e.g. good or bad). Under the Equivalent Martingale Measure (EMM), the fair market value of a call option at time 0, denoted as \\(\\hat{f}_0\\), must satisfy:\n\\[\\hat{f_0}\\cdot R = E^q[f_1]\\] where \\(f_1=max(S_0 \\cdot U-K,0)\\)\n\\[S_0\\cdot R=E^q[S_1]\\] where \\(S_0\\) = the current price of risky underlying asset\nFrom these two equations, we have the EMM or the state price density for good state \\(q\\) as: \\[\nq = \\frac{R - D}{U - D}\n\\]\nThus, the present fair value of the call option is given by:\n\\[\n\\hat{f}_0 = \\frac{1}{R}\\left[ \\frac{R - D}{U - D}(S_0 \\cdot U - K) + \\frac{U - R}{U - D} \\cdot 0 \\right] = \\frac{(R - D)(U - 1)}{R(U - D)}\n\\]\nassuming that\n\nInitial Asset Price (normalized): \\(S_0=1\\)\n\nStrike Price (qualification threshold): \\(K=1\\)\n\nMaturity: 18 years (quarterly steps = 72 periods)\n\nThis formula indicates a clear proportional relationship for estimating current fair values representing inherited qualification-based claims for each class over a 18-year maturity period. While this valuation could be approximated using continuous distributions (e.g., Black-Scholes under symmetric assumptions \\(U \\cdot D=1\\)), our discrete binomial model allows straightforward interpretation without loss of economic intuition.\n\n\nEmpirical Analysis\nWe employ distinct underlying assets for each class. Using the historical dataset (Q1 1982–Q4 2019, 152 quarterly observations), we estimated the parameters and their associated fair values of call options for each class.\nFor Proletariat (P) Class Children:\n\nRisky Asset: US Median usual weekly Real earnings (LES1252881600Q)\n\nRisk-Free Asset: US Real GDP per capita (A939RX0Q048SBEA)\n\nParameters:\n\n\\(U_p\\):= 75th percentile growth of wage (risky asset)\n\n\\(D_p\\):= 25th percentile growth of wage\n\n\\(R_p\\):= Median growth rate of Real GDP per capita\n\n\nFor Capitalist (C) Class Children:\n\nRisky Asset: S&P 500 equity (SPX)\n\nRisk-Free Asset: US 10-Year Treasury Bond\nParameters:\n\n\\(U_c\\):= 75th percentile quarterly growth of the equity index\n\n\\(D_c\\):= 25th percentile quarterly growth of the equity index\n\n\\(R_c\\):= Median of quarterly US 10-Year Treasury Bond Yield (DGS10)\n\n\nEmpirical Results:\n\nFor Proletariat (P) class children, \\(\\hat{P}_0=?\\)\n\nFor Capitalist (C) class children, \\(\\hat{C}_0=?\\)\n\n\n\nCode\nimport yfinance as yf\nimport pandas_datareader.data as web\nimport pandas as pd\nimport numpy as np\n\n# 데이터 기간 설정\nstart_date = '1982-01-01'\nend_date = '2019-12-31'\n\n# S&P 500 데이터 가져오기\nsp500 = yf.download(\"^GSPC\", start=start_date, end=end_date, interval=\"1d\")\nsp500_q = sp500['Close'].resample('QE').last()  # 분기별 종가 데이터\n\n# FRED 데이터 가져오기\nus_median_weekly_earnings = web.DataReader('LES1252881600Q', 'fred', start_date, end_date)\nus_real_gdp_per_capita = web.DataReader('A939RX0Q048SBEA', 'fred', start_date, end_date)\nus_10yr_treasury_yield = web.DataReader('DGS10', 'fred', start_date, end_date)\n\n# 인덱스를 맞추기 위해 분기별로 재샘플링\nus_median_weekly_earnings = us_median_weekly_earnings.resample('QE').last()\nus_real_gdp_per_capita = us_real_gdp_per_capita.resample('QE').last()\nus_10yr_treasury_yield = us_10yr_treasury_yield.resample('QE').last()\n\n# 데이터프레임으로 변환\ndata = pd.DataFrame({\n    'SP500': sp500_q.squeeze(),\n    'Median_Weekly_Earnings': us_median_weekly_earnings['LES1252881600Q'].squeeze(),\n    'Real_GDP_per_Capita': us_real_gdp_per_capita['A939RX0Q048SBEA'].squeeze(),\n    '10yr_Treasury_Yield': us_10yr_treasury_yield['DGS10'].squeeze()\n}, index=sp500_q.index)\n\n# 결측값 제거\ndata.dropna(inplace=True)\n\n# 수익률 계산\ndata['SP500_Return'] = data['SP500'].pct_change()\ndata['Earnings_Growth'] = data['Median_Weekly_Earnings'].pct_change()\ndata['GDP_Growth'] = data['Real_GDP_per_Capita'].pct_change()\n\n# 통계치 계산\nU_p = data['Earnings_Growth'].quantile(0.75)+1\nD_p = data['Earnings_Growth'].quantile(0.25)+1\nR_p = data['GDP_Growth'].median()+1\n\nU_c = data['SP500_Return'].quantile(0.75)+1\nD_c = data['SP500_Return'].quantile(0.25)+1\nR_c = data['10yr_Treasury_Yield'].median()\nR_c = R_c / 100 +1\n\nprint(f\"Proletariat Class Children Parameters:\")\nprint(f\"U_p: {U_p:.2f}\")\nprint(f\"D_p: {D_p:.2f}\")\nprint(f\"R_p: {R_p:.2f}\")\n\nprint(f\"\\nCapitalist Class Children Parameters:\")\nprint(f\"U_c: {U_c:.2f}\")\nprint(f\"D_c: {D_c:.2f}\")\nprint(f\"R_c: {R_c:.2f}\")\n\n\n# Binomial Option Pricing Model\ndef binomial_option_pricing(K, S_0, T, U, D, R, dt):\n    \"\"\"\n    K: Strike price\n    S_0: Initial stock price\n    T: Time to maturity (in years)\n    U: Up factor\n    D: Down factor\n    R: Risk-free rate\n    dt: Number of steps for each year\n    \"\"\"\n    n = T*dt # Number of steps in the binomial tree\n    q = (R - D) / (U - D)\n\n    # Initialize option values at maturity\n    option_values = np.zeros((n + 1, 1))\n    for i in range(n + 1):\n        ST = S_0 * (U ** i) * (D ** (n - i))\n        option_values[i] = max(0, ST - K)\n\n    # Backward recursion for option values\n    for j in range(n - 1, -1, -1):\n        for i in range(j + 1):\n            option_values[i] = (q * option_values[i + 1] + (1 - q) * option_values[i]) / R\n\n    return option_values[0, 0]\n\n\n# Parameters\nK = 1  # Strike price\nS_0 = 1  # Initial stock price\nT = 18  # Time to maturity (18 years)\ndt = 4 # Number of steps for each year\n\n# Calculate option prices\noption_price_proletariat = binomial_option_pricing(K, S_0, T, U_p, D_p, R_p, dt)\noption_price_capitalist = binomial_option_pricing(K, S_0, T, U_c, D_c, R_c, dt)\n\nprint(f\"\\nFair price of Call Option, held by Proletariat Class Children:\\n {option_price_proletariat:.2f}\")\nprint(f\"Fair price of Call Option, held by Capitalist Class Children:\\n {option_price_capitalist:.2f}\")\n\n\nYF.download() has changed argument auto_adjust default to True\n\n\n[*********************100%***********************]  1 of 1 completed\n\n\nProletariat Class Children Parameters:\nU_p: 1.01\nD_p: 1.00\nR_p: 1.01\n\nCapitalist Class Children Parameters:\nU_c: 1.07\nD_c: 0.99\nR_c: 1.05\n\nFair price of Call Option, held by Proletariat Class Children:\n 0.30\nFair price of Call Option, held by Capitalist Class Children:\n 0.97\n\n\n\n\nDiscussion\nThis model clarifies the stark inequality underlying “ostensibly equal” qualification standards. Although formally identical, the call options’ absolute valuations significantly diverge, reflecting distinct economic inheritances accessible to each class. This disparity highlights structural inefficiencies and deep-rooted inequalities, persisting despite nominally identical qualification standards.\nUltimately, this analysis underscores how asset-based class differentiation profoundly impacts the perceived and realized absolute value of educational and economic opportunities, illuminating critical implications for economic policy, educational equity, and social justice frameworks."
  },
  {
    "objectID": "projects/pricing_equal.html#introduction",
    "href": "projects/pricing_equal.html#introduction",
    "title": "Pricing the seemingly Equal Opportunities under Structural Inequality",
    "section": "Introduction",
    "text": "Introduction\nIn societies characterized by pronounced economic stratification, opportunities presented as equal often yield disparate outcomes across different social strata. This disparity arises from inherent structural inefficiencies that restrict access to certain opportunities based on class. The Limits to Arbitrage Theory posits that market inefficiencies can persist due to various constraints, including capital limitations, preventing rational traders from correcting mispricings (Shleifer and Vishny 1997). This paper explores how such constraints contribute to the unequal valuation of opportunities between the proletariat (P) and capitalist (C) classes."
  },
  {
    "objectID": "projects/pricing_equal.html#literature-review",
    "href": "projects/pricing_equal.html#literature-review",
    "title": "Pricing the seemingly Equal Opportunities under Structural Inequality",
    "section": "Literature Review",
    "text": "Literature Review\nThe theoretical foundation relies primarily on the limits to arbitrage theory, initially articulated by Shleifer and Vishny (Shleifer and Vishny 1997). Their seminal work shows how market inefficiencies persist due to practical constraints, particularly capital constraints, restricting the ability of arbitrageurs to exploit and correct mispricings. These constraints arise from significant capital requirements that effectively segregate participants into distinct economic spheres, as emphasized by subsequent studies on financially constrained arbitrageurs (Gromb and Vayanos 2002; Xiong 2001).\nGeanakoplos’ research introduces the leverage cycle, which explains how fluctuations in leverage and capital availability perpetuate systemic inequality and financial instability (Geanakoplos 2010). Complementary studies, such as that by Gromb and Vayanos, also demonstrate the welfare implications of constrained arbitrageurs operating under capital limitations, further exacerbating persistent inequality (Gromb and Vayanos 2002). Moreover, Barberis and Thaler’s comprehensive survey on behavioral finance indicates how cognitive and behavioral constraints exacerbate market inefficiencies, reinforcing the structural barriers that differentiate economic outcomes across social strata (Barberis and Thaler 2003).\nExtending beyond purely financial contexts, sociological and economic research provides additional perspectives on structural inequalities and opportunity valuation. Bourdieu’s concept of social reproduction underscores how cultural capital perpetuates socioeconomic inequalities across generations (Bourdieu 1973). Recent empirical evidence by Stansbury (Stansbury 2024) and findings by the Social Mobility Commission (Social Mobility Commission 2023) further demonstrate how economic capital inherited through generations shapes differential outcomes, even when individuals ostensibly possess identical qualifications.\nChetty et al. provide compelling evidence linking parental economic conditions to children’s educational outcomes and future earnings, strongly supporting the relevance of inherited economic positions in determining opportunity valuations (Chetty et al. 2014). Similarly, Piketty’s influential book highlights the crucial role inherited wealth plays in perpetuating structural economic disparities, emphasizing the critical nature of capital inheritance in shaping individuals’ economic trajectories and their access to opportunities (Piketty 2014).\nPolicy implications regarding these structural inequalities and efforts to enhance social mobility have been explored extensively by institutions such as the OECD. Their analyses suggest policy frameworks that might alleviate the persistent inequalities discussed herein (OECD 2018). Reeves’ concept of the “glass floor” further illustrates how affluent socioeconomic backgrounds systematically maintain class advantages despite equal or even lesser merit-based qualifications (Reeves 2017).\nCollectively, these studies underline a coherent narrative: ostensibly equal opportunities often conceal significant disparities rooted in inherited structural inequalities, persistent capital constraints, and behavioral limitations to arbitrage. Our model complements this literature by quantitatively evaluating how these structural factors systematically influence the absolute value of identical qualification standards across distinct socioeconomic groups."
  },
  {
    "objectID": "thesis/02_lit_review.html",
    "href": "thesis/02_lit_review.html",
    "title": "Too Big to Fail?",
    "section": "",
    "text": "Linear Models for Asset Management - Dual Approaches to Asset Management\n- Limitations of Linear asset pricing Models\n- Practical Asset Management\nTesting Linear Models - Data and Descriptive Statistics\n- Time-Series Split: 2010-01-01: Rationale for choosing this date as a structural break (QE onset, ETF growth).\n- Data Sources: CRSP (stocks), Fama-French (industry classifications), Yahoo Finance (indices/ETFs).\n- Appendix: Timeline of key events (e.g., QE phases, ETF milestones). - Industry Market-Cap Concentration Trends: Time-series analysis using SIC (10) and FF (10, 49) classifications.\n- Industry Beta Dynamics: Evolution of systematic risk by industry (CRSP data).\n- High-Tech Sector Dominance: Focus on chips and software industries (FF 10 classification).\n- Challenging Industry CAPM\n- Single-Factor Model: Fama-MacBeth results show significant unexplained alpha (CRSP, SIC 10).\n- Two-Factor Model: Market premium insignificant, reverse size effect significant (FF 49).\n- Challenging Linear Factor Models\n- Factor Selection: Highlight multicollinearity (mkt_excess, macro_bm) and limited explanatory power.\n- Lasso Regression: Apply regularization to select predictors for FF 10 industry portfolio excess returns.\n- Challenging Betting Against Beta\n- Out-of-Sample Performance: Optimal assets have betas near 1, not high or low (SIC 10 portfolios).\n- Challenging the traditional Mean-Variance weight Optimization\n- Extreme Weights: Unconstrained MV yields impractical results; constraints (e.g., no short sales) added.\n- Passive vs. Active: Passive value-weighted portfolios outperform MV with lower costs.\n- Challenging the Parametric Mean-Variance Weight Optimization\n- Bayesian update : Black and Litterman (1992) - Momentum and Size Tilting: Brandt et al. (2009) approach underperforms passive benchmarks."
  },
  {
    "objectID": "thesis/02_lit_review.html#dual-approaches-to-asset-management",
    "href": "thesis/02_lit_review.html#dual-approaches-to-asset-management",
    "title": "Too Big to Fail?",
    "section": "2.1 Dual Approaches to Asset Management",
    "text": "2.1 Dual Approaches to Asset Management\nIn economics, a limited resource allocation involves relative price vectors (\\(P\\)) and quantity vectors (\\(Q\\)), often constrained by a structural inner product:\n\\[P \\cdot Q = \\text{constant}\\]\nWith a convex or concave scalar function of \\(P\\) or \\(Q\\) which are complete and compact Euclidean spaces, this structure enables convex optimization tools. Duality in convex optimization posits that solving for \\(Q\\) given \\(P\\) (primal problem) or \\(P\\) given \\(Q\\) (dual problem) yields equivalent results under linear constraints, reflecting the Hahn-Banach theorem’s guarantee of consistent linear functionals.\n\nAsset Weight Optimization:\n\nGoal: Given \\(P\\) (joint distribution of returns), find \\(Q\\) (asset weights) to optimize an objective (e.g., maximize Sharpe ratio).\nCritique: Modern Portfolio Theory (MPT) assumes elliptical distributions and time-separable utility (e.g., CRRA), which are unrealistic. Non-stationary or fat-tailed distributions (e.g., Cauchy) render moment estimates unreliable, and utility may not reflect diminishing marginal returns for future wealth.\n\nAsset Pricing Optimization:\n\nGoal: Given \\(Q\\) (future cash flows), find \\(P\\) (state prices) to satisfy no-arbitrage conditions via the Euler equation.\nCritique: If risk-return tradeoffs break down (as with mega-caps), linear factor models derived from local approximations fail, undermining theoretical predictions."
  },
  {
    "objectID": "thesis/02_lit_review.html#limitations-of-linear-asset-pricing-models",
    "href": "thesis/02_lit_review.html#limitations-of-linear-asset-pricing-models",
    "title": "Too Big to Fail?",
    "section": "2.2 Limitations of Linear Asset Pricing Models",
    "text": "2.2 Limitations of Linear Asset Pricing Models\nTraditional asset pricing theories operate on the fundamental premise that expected returns directly compensate for systematic risk exposure. However, the empirical reality of top-performing mega-cap stocks presents a profound contradiction: these entities often display disproportionate returns relative to their risk profiles. Consider Apple, which despite having relatively high beta compared to other mega-caps, has generated returns that consistently exceed what would be predicted by its systematic risk exposure alone.\n\n2.2.1 Statistical Limitations\nThe conventional approach to addressing anomalies involves extending traditional asset pricing models by incorporating additional factors. These extensions represent a fundamentally inadequate methodology as they merely perpetuate the linear structure of existing frameworks while appending explanatory variables. The standard multi-factor model takes the form:\n\\[E(R_i) = R_f + \\beta_{i,1}F_1 + \\beta_{i,2}F_2 + ... + \\beta_{i,n}F_n + \\alpha_i\\]\nWhere \\(E(R_i)\\) represents expected returns, \\(R_f\\) is the risk-free rate, \\(\\beta_{i,j}\\) are factor loadings, and \\(F_j\\) are risk factors. The proposed extensions for capturing phenomena such as network effects, winner-take-all dynamics, and regulatory capture simply add terms to this equation.\nThis approach has several critical flaws related to both model specification and parameter estimation.\nLinear additive models inherently assume factor orthogonality, normal distribution of returns, and stable risk premiums—assumptions violated by the complex, non-linear relationships between institutional protection, market power, and returns. These models rely on the core statistical assumptions:\n\n\\(E(\\varepsilon_i) = 0\\) (zero-mean residuals)\n\\(Cov(F_j, \\varepsilon_i) = 0\\) (factors uncorrelated with residuals)\n\\(Cov(F_j, F_k) = 0\\) for \\(j \\neq k\\) (orthogonal factors)\n\\(\\varepsilon_i \\sim N(0, \\sigma^2)\\) (normally distributed residuals)\n\nYet empirical tests would consistently reject these assumptions for mega-cap returns. A particularly problematic issue is multicollinearity between factors. When adding variables such as institutional protection, regulatory capture, and network effects to explain mega-cap performance, these factors often exhibit high correlation amongst themselves. This multicollinearity leads to unstable coefficient estimates, inflated standard errors, and ultimately, unreliable inference. For instance, a firm’s market power is often highly correlated with its regulatory influence, making it difficult to disentangle their individual effects.\nMoreover, adding qualitative variables like “network effects” to statistical models is particularly vulnerable to p-hacking, as these constructs can be operationalized in numerous ways. The improvement in in-sample fitting (\\(R^2\\)) often comes at the expense of out-of-sample prediction accuracy—a fundamental manifestation of the bias-variance tradeoff in mean squared error:\n\\[MSE(prediction) = Bias^2 + Variance + Irreducible\\_Error\\]\nAdding complex factors to capture mega-cap outperformance typically increases model complexity, which may reduce in-sample bias but simultaneously increases estimation variance. This increased variance occurs because more complex models with additional parameters require more data for stable estimation and are more prone to overfitting noise rather than signal. This relationship is particularly problematic for stock returns, which violate ergodicity assumptions necessary for consistent parameter estimation.\nFurthermore, traditional frameworks remain epistemologically constrained by their reliance on backward-looking statistical relationships. Consider the standard time-series approach to estimating factor models:\n\\[R_{it} = \\alpha_i + \\beta_{i,MKT}R_{mt} + \\beta_{i,SMB}SMB_t + \\beta_{i,HML}HML_t + \\varepsilon_{it}\\]\nThis specification assumes parameter stability over time (\\(\\beta_{i,j,t} = \\beta_{i,j,t+1}\\)), an assumption violated when institutional dynamics create structural breaks. Microsoft’s evolving regulatory landscape demonstrates this challenge—its systematic risk profile has fundamentally changed as its market and political power have increased, rendering historical beta estimates increasingly irrelevant for forward-looking predictions.\n\n\n2.2.2 The Inverse Relationship Between Risk and Return for Dominant Firms\nThe conventional understanding of systematic risk requires fundamental reconceptualization. The Capital Asset Pricing Model posits:\n\\[E(R_i) = R_f + \\beta_i(E(R_m) - R_f)\\]\nWhere \\(\\beta_i = \\frac{Cov(R_i, R_m)}{Var(R_m)} = \\rho_{i,m}\\frac{\\sigma_i}{\\sigma_m}\\) measures systematic risk exposure. This can be decomposed into correlation (\\(\\rho_{i,m}\\)) and relative volatility components (\\(\\frac{\\sigma_i}{\\sigma_m}\\)).\nAn empirical analysis of market data reveals a critical paradox for mega-cap stocks versus non-mega-cap stocks. Both categories typically exhibit similar correlation coefficients with the market (\\(\\rho_{i,m} \\approx 1\\)), indicating strong co-movement with overall market trends. Similarly, their expected returns (\\(E(R_i)\\)) are often comparable or even favor mega-caps. However, the crucial distinction emerges in their volatility profiles: mega-cap stocks consistently demonstrate lower idiosyncratic volatility (\\(\\sigma_i^{mega} &lt; \\sigma_i^{non-mega}\\)) due to their institutional protection, diversified revenue streams, and market power.\nThis creates a fundamental contradiction in the risk-return paradigm. When calculating the Sharpe ratio or other risk-adjusted return metrics:\n\\[Sharpe_i = \\frac{E(R_i) - R_f}{\\sigma_i}\\]\nMega-cap stocks consistently outperform on a risk-adjusted basis. This superior risk-adjusted performance cannot be reconciled with traditional asset pricing theory, which predicts that lower risk should be associated with lower returns. The empirical reality suggests the opposite for market dominant stocks:\n\\[\\frac{E(R_i^{mega}) - R_f}{\\sigma_i^{mega}} &gt; \\frac{E(R_i^{non-mega}) - R_f}{\\sigma_i^{non-mega}}\\]\nDespite comparable raw returns and market correlations, the risk-adjusted outperformance of mega-cap stocks directly contradicts the foundational risk-return trade-off of asset pricing theory. Amazon exemplifies this phenomenon—its volatility has decreased relative to smaller competitors as its market dominance has increased, yet its returns have remained exceptional, generating superior risk-adjusted performance that cannot be explained by traditional models.\n\n\n2.2.3 Market Efficiency and Structural Advantages\nMarket efficiency assumptions underlying traditional models presuppose:\n\\[E[R_{i,t+1} | \\Omega_t] := E_t[R_{i,t+1}]\\]\nWhere \\(\\Omega_t\\) represents the information set available at time \\(t\\), and \\(E_t[\\cdot]\\) denotes expectation conditional on information at time \\(t\\). However, the persistence of abnormal returns for dominant firms suggests either:\n\nInformation about institutional advantages is not fully incorporated in prices\nThese advantages create structural market inefficiencies that cannot be arbitraged away\n\nGoogle’s digital advertising dominance illustrates these dynamics. Its network effects create increasing returns to scale that can be modeled as:\n\\[Profit_t = f(MarketShare_{t-1})^{\\gamma}\\]\nWhere \\(\\gamma &gt; 1\\) indicates increasing returns to scale. Traditional asset pricing models assume competitive markets where \\(\\gamma \\leq 1\\), making them structurally incapable of capturing the valuation implications of dominant market positions.\nThe fundamental inadequacy of traditional models becomes evident when examining their predictive accuracy for top market-cap stocks. A comparison of realized returns versus CAPM-predicted returns for top 10 stocks from 2010-2020 shows systematic underestimation:\n\\[\\alpha_i:= E(R_i) - [R_f + \\beta_i(R_m - R_f)] &gt; 0 \\text{ for top 10 stocks}\\]\nThis persistent alpha cannot be explained as compensation for omitted risk factors, as these firms typically enjoy reduced risk through institutional protection and market dominance.\nThis theoretical crisis demands not incremental additions to existing models but a comprehensive reconceptualization of capital market dynamics. The failure to accurately predict returns for dominant firms reflects not merely model misspecification but a systematic failure to capture the fundamental nature of contemporary capitalism, where returns increasingly flow not as compensation for risk-bearing but as rents extracted through structural market advantages.\nA more appropriate specification might acknowledge the non-linear relationship between institutional position and returns:\n\\[E(R_i) = R_f + \\beta_i(R_m - R_f) + g(MarketPower_i, InstitutionalProtection_i)\\]\nWhere \\(g(\\cdot)\\) is a non-linear function capturing the complex relationship between market power, institutional protection, and returns—a relationship that fundamentally contradicts the risk-return paradigm underlying traditional asset pricing theory."
  },
  {
    "objectID": "thesis/02_lit_review.html#practical-asset-management",
    "href": "thesis/02_lit_review.html#practical-asset-management",
    "title": "Too Big to Fail?",
    "section": "2.3 Practical Asset Management",
    "text": "2.3 Practical Asset Management\n\n2.3.1 Implications of Model Failure for Asset Management\nAsset management involves making investment decisions to achieve financial objectives, such as maximizing returns or minimizing risk, typically by constructing portfolios based on expected returns and risk estimates derived from asset pricing models. However, the inadequacy of traditional models when applied to dominant firms—those “superstar” companies that capture disproportionate market value—creates significant practical challenges:\n\nBreakdown of Market Efficiency Assumptions\nTraditional models assume markets are efficient, with asset prices reflecting all available information. Yet, dominant firms consistently generate abnormal returns that persist over time, suggesting inefficiencies tied to structural advantages—like brand loyalty or data monopolies—that markets fail to fully price in. For asset managers, this means traditional models underestimate the value of these firms, leading to underweighting in portfolios.\nInverse Risk-Return Dynamics\nModels like the CAPM posit a linear relationship where higher risk (measured as beta) yields higher expected returns. For dominant firms, however, the opposite often holds: their entrenched positions reduce idiosyncratic risk—through diversified revenue streams or regulatory buffers—while delivering superior returns. This inversion misleads asset managers into overestimating the risk of mega-cap stocks and underestimating their return potential, skewing portfolio optimization.\nFactor Instability and Omission\nMulti-factor models, such as those based on Fama-French factors, depend on stable relationships between risk factors (e.g., size, value) and returns. Yet, the market power of dominant firms evolves dynamically—through technological innovation or policy shifts—causing factor loadings to fluctuate. Moreover, these models lack factors that explicitly capture rents from structural advantages, leaving asset managers with incomplete tools to assess true risk-adjusted performance.\nInability to Model Non-Linear Effects\nThe linear structure of traditional models cannot accommodate the non-linear dynamics of contemporary capitalism, such as increasing returns to scale or tipping points in market dominance. For example, a firm like Amazon benefits from self-reinforcing network effects that amplify its returns beyond what risk-based models predict, rendering traditional allocations ineffective.\n\nThese shortcomings result in portfolios that miss out on the concentrated returns of dominant firms, which increasingly drive market performance. Asset managers relying on outdated frameworks risk underperformance in a landscape where structural advantages, not risk exposure, dictate success.\n\n\n2.3.2 A Data-Driven Alternative: Stochastic Dominance\nTo address these challenges, asset management must shift toward empirical, data-driven approaches that eschew the restrictive assumptions of traditional models. One such method is stochastic dominance, which evaluates assets by comparing their entire return distributions rather than relying on summary statistics like mean and variance. This approach offers a practical way to identify investments that align with the empirical realities of contemporary markets.\n\nHow It Works\n\nFirst-Order Stochastic Dominance (FSD): Asset A dominates Asset B if its cumulative distribution function (CDF) lies below B’s for all return levels, meaning A offers higher returns across all outcomes.\n\nSecond-Order Stochastic Dominance (SSD): For risk-averse investors, A dominates B if the area under A’s CDF (its integral) is less than B’s up to any point, indicating a better risk-return tradeoff.\n\nAdvantages\n\nRobustness: Unlike mean-variance optimization, stochastic dominance requires no assumptions about return distributions (e.g., normality), making it suitable for markets with fat tails or skewness—common in mega-cap stock performance.\n\nEmpirical Focus: By analyzing historical return distributions, it captures the real-world impact of structural advantages, such as the persistent outperformance of firms like Apple or Microsoft.\n\nFlexibility: It accommodates diverse investor risk preferences without specifying a utility function, broadening its applicability.\n\nPractical Implementation\nAsset managers can apply stochastic dominance by:\n\nHistorical Simulations: Using past return data to estimate CDFs and calculate the likelihood of one asset dominating another.\n\nResampling: Employing techniques like bootstrapping to test dominance under varying market conditions, enhancing robustness.\n\nChallenges\n\nData Requirements: Accurate CDF estimation demands extensive historical data, which may be scarce for newer firms or asset classes.\n\nNon-Stationarity: Return distributions shift over time—due to regulatory changes or competitive disruptions—limiting the reliability of historical dominance.\n\nPortfolio Complexity: Extending stochastic dominance to multi-asset portfolios is computationally intensive, often requiring simplifications or heuristic rules.\n\n\nDespite these hurdles, stochastic dominance provides a rigorous framework that sidesteps the theoretical pitfalls of traditional models. It allows asset managers to focus on observable performance, directly addressing the rent extraction that defines dominant firms.\n\n\n2.3.3 Complementary Tools\nWhile stochastic dominance serves as a strong foundation, asset managers can bolster their strategies with additional tools to capture the nuances of contemporary markets:\n\nFundamental Analysis\nDetailed assessments of a firm’s financials, competitive moat (e.g., patents, scale), and industry trends can reveal structural advantages missed by quantitative models. For instance, analyzing Tesla’s innovation pipeline or Alphabet’s data ecosystem offers insights into their sustained dominance.\nMachine Learning\nAdvanced algorithms can detect non-linear patterns in returns or predict shifts in market power, though they require careful tuning to avoid overfitting and ensure interpretability.\nLiquidity Considerations\nMega-cap stocks typically offer high liquidity, reducing transaction costs and making them practical choices for large portfolios—a factor stochastic dominance alone might not prioritize.\n\nMoreover, the concentration of returns in a few dominant firms suggests that a focused investment strategy—overweighting current mega-caps or scouting emerging leaders—may outperform broad diversification. However, identifying future dominants demands foresight into technological, regulatory, and economic trends, adding complexity to the process.\n\n\n2.3.4 Conclusion\nIn a world where returns stem from structural market advantages rather than risk-bearing, traditional asset pricing models fail to guide effective asset management. Their inability to predict the performance of dominant firms—rooted in flawed assumptions about efficiency, risk, and linearity—leads to misallocated portfolios that undervalue mega-cap opportunities. A data-driven approach using stochastic dominance offers a compelling alternative, leveraging empirical return distributions to identify superior investments without theoretical baggage. While challenges like data demands and non-stationarity persist, combining this method with fundamental analysis and modern tools equips asset managers to navigate contemporary capitalism. By embracing flexibility and empiricism, asset management can better capture the rents that define today’s markets, delivering superior outcomes in an era of concentrated dominance."
  },
  {
    "objectID": "thesis/04_model.html",
    "href": "thesis/04_model.html",
    "title": "Too Big to Fail?",
    "section": "",
    "text": "The TBTF Strategy\n\nLarge vs. Small Stocks: Analyze return distribution asymmetry and volatility differences over time.\n\nTop 10 Stocks: List membership (e.g., permno) and stability, dominated by high-tech (Nasdaq chips/software).\n\nTop 10 Transition Analysis: Transition matrix and stationary distribution (three states: top, mid, out).\n\nTBTF Performance: Compare risk-adjusted returns (Sharpe, Sortino, Omega) to market indices/ETFs.\n\n\n패널 데이터 상의 개인별 소득의 로그 (log-income) 가 AR(1) 과정을 따르며, 이 과정이 시간이 지남에 따라 두 개의 상태(regime) 간 전환되는 Markov-switching AR(1) 구조라고 가정하면, 상태 전이 행렬 (transition matrix) 를 포함한\n모형 파라미터를 추정 방법 - Expectation-Maximization (EM) 알고리즘 - Hamilton 필터와 같은 비선형 시계열 필터링 기법\n아래에서는 2-state Markov-switching AR(1) 모형을 panel data 상황에 맞춰 수식화하고, 상태 전이 행렬 추정 절차를 설명합니다.\n\n\n\n\n\n패널 데이터에서 개인 \\(i\\)의 시점 \\(t\\)의 로그 소득 \\(y_{it} = \\log(\\text{income}_{it})\\)가 다음과 같은 구조를 따른다고 가정합니다:\n\\[\ny_{it} = \\mu_{s_{it}} + \\phi_{s_{it}} y_{i,t-1} + \\epsilon_{it}, \\quad \\epsilon_{it} \\sim \\mathcal{N}(0, \\sigma_{s_{it}}^2)\n\\]\n여기서: - \\(s_{it} \\in \\{1, 2\\}\\): 시점 \\(t\\)에서 개인 \\(i\\)의 은닉 상태 (hidden state) (예: “저성장 상태”, “고성장 상태”) - 각 상태는 상태별 AR(1) 계수와 평균, 분산을 가짐\n\n\n\n\n개별 개인마다 상태가 전이되지만, 상태 전이 확률은 전체적으로 공통적이라고 가정:\n\\[\nP =\n\\begin{bmatrix}\np_{11} & p_{12} \\\\\np_{21} & p_{22}\n\\end{bmatrix}, \\quad \\text{where } p_{jk} = \\mathbb{P}(s_{it} = k \\mid s_{i,t-1} = j)\n\\]\n예:\n- \\(p_{11}\\) = 상태 1 (저성장)이 지속될 확률\n- \\(p_{12}\\) = 상태 1에서 2로 전이될 확률\n- …\n\n\n\n\nEM 알고리즘을 적용하여 다음을 추정: - 상태 전이 확률 \\(P\\) - 상태별 AR(1) 파라미터 \\(\\mu_1, \\mu_2, \\phi_1, \\phi_2, \\sigma_1^2, \\sigma_2^2\\) - 각 시점의 상태 posterior 확률 \\(\\mathbb{P}(s_{it} = k \\mid \\text{observed data})\\)\n\n\n\n초기 상태 전이행렬 \\(P^{(0)}\\)\n초기 파라미터 \\((\\mu_k^{(0)}, \\phi_k^{(0)}, \\sigma_k^{(0)})\\) for \\(k=1,2\\)\n\n\n\n\nHamilton filter 또는 Forward-Backward algorithm을 통해 다음을 계산:\n\n\\(\\gamma_{it}^{(k)} = \\mathbb{P}(s_{it} = k \\mid \\text{data})\\): 상태 k에 있을 posterior 확률\n\\(\\xi_{it}^{(j,k)} = \\mathbb{P}(s_{i,t-1} = j, s_{it} = k \\mid \\text{data})\\): 상태 전이 joint posterior\n\n\n\n\n\n\n\\[\np_{jk} = \\frac{\\sum_{i,t} \\xi_{it}^{(j,k)}}{\\sum_{i,t} \\sum_{k'} \\xi_{it}^{(j,k')}}\n\\]\n즉, 상태 \\(j \\to k\\)로의 전이가 얼마나 자주 발생했는지를 전체 joint posterior 기반으로 집계하여 전이 확률로 추정합니다.\n\n\n\n예를 들어 상태 \\(k\\)에 대해서:\n\n\\(\\phi_k, \\mu_k\\): \\(\\gamma_{it}^{(k)}\\)를 가중치로 사용한 weighted regression\n\\(\\sigma_k^2\\): weighted residual variance\n\n\n\n\n\n\n\n\n\n\n\n공통 전이행렬: 모든 개인에 대해 상태 전이 확률은 동일하다고 가정 (\\(P\\)는 공통)\n개인별 상이한 초기 분포: \\(\\mathbb{P}(s_{i1})\\)은 개인별로 추정하거나 공통으로 가정\n데이터가 충분히 길지 않은 경우, 시간 단위(\\(T\\))는 상태 추정을 어렵게 만들 수 있어 개인 간 정보 풀링이 중요\n\n\n\n\n\n예를 들어 다음과 같은 전이행렬이 있다고 가정:\n\\[\nP = \\begin{bmatrix}\n0.95 & 0.05 \\\\\n0.10 & 0.90\n\\end{bmatrix}\n\\]\n이 경우: - 상태 1 (저성장)은 매우 안정적으로 지속됨 (95%) - 상태 2 (고성장)은 상대적으로 덜 안정적이며, 10% 확률로 저성장으로 전이\n이 전이확률은 위 EM 알고리즘 M-step에서 \\(\\xi_{it}^{(j,k)}\\)를 평균하여 추정됩니다.\n\n\n\n\n패널 데이터를 대상으로 이 모형을 추정하려면 다음과 같은 라이브러리를 활용할 수 있습니다:\n\nstatsmodels.tsa.regime_switching.MarkovRegression (단일 시계열에 한정)\npyhsmm 또는 hmmlearn: HMM 기반 EM 추정\nlinearmodels.panel + custom EM 알고리즘: 패널 구조에 맞춘 추정"
  },
  {
    "objectID": "thesis/04_model.html#모형-구조-개인-i의-로그-소득",
    "href": "thesis/04_model.html#모형-구조-개인-i의-로그-소득",
    "title": "Too Big to Fail?",
    "section": "",
    "text": "패널 데이터에서 개인 \\(i\\)의 시점 \\(t\\)의 로그 소득 \\(y_{it} = \\log(\\text{income}_{it})\\)가 다음과 같은 구조를 따른다고 가정합니다:\n\\[\ny_{it} = \\mu_{s_{it}} + \\phi_{s_{it}} y_{i,t-1} + \\epsilon_{it}, \\quad \\epsilon_{it} \\sim \\mathcal{N}(0, \\sigma_{s_{it}}^2)\n\\]\n여기서: - \\(s_{it} \\in \\{1, 2\\}\\): 시점 \\(t\\)에서 개인 \\(i\\)의 은닉 상태 (hidden state) (예: “저성장 상태”, “고성장 상태”) - 각 상태는 상태별 AR(1) 계수와 평균, 분산을 가짐"
  },
  {
    "objectID": "thesis/04_model.html#상태-전이-행렬-공통-가정",
    "href": "thesis/04_model.html#상태-전이-행렬-공통-가정",
    "title": "Too Big to Fail?",
    "section": "",
    "text": "개별 개인마다 상태가 전이되지만, 상태 전이 확률은 전체적으로 공통적이라고 가정:\n\\[\nP =\n\\begin{bmatrix}\np_{11} & p_{12} \\\\\np_{21} & p_{22}\n\\end{bmatrix}, \\quad \\text{where } p_{jk} = \\mathbb{P}(s_{it} = k \\mid s_{i,t-1} = j)\n\\]\n예:\n- \\(p_{11}\\) = 상태 1 (저성장)이 지속될 확률\n- \\(p_{12}\\) = 상태 1에서 2로 전이될 확률\n- …"
  },
  {
    "objectID": "thesis/04_model.html#추정-방법-em-알고리즘-baumwelch-계열",
    "href": "thesis/04_model.html#추정-방법-em-알고리즘-baumwelch-계열",
    "title": "Too Big to Fail?",
    "section": "",
    "text": "EM 알고리즘을 적용하여 다음을 추정: - 상태 전이 확률 \\(P\\) - 상태별 AR(1) 파라미터 \\(\\mu_1, \\mu_2, \\phi_1, \\phi_2, \\sigma_1^2, \\sigma_2^2\\) - 각 시점의 상태 posterior 확률 \\(\\mathbb{P}(s_{it} = k \\mid \\text{observed data})\\)\n\n\n\n초기 상태 전이행렬 \\(P^{(0)}\\)\n초기 파라미터 \\((\\mu_k^{(0)}, \\phi_k^{(0)}, \\sigma_k^{(0)})\\) for \\(k=1,2\\)\n\n\n\n\nHamilton filter 또는 Forward-Backward algorithm을 통해 다음을 계산:\n\n\\(\\gamma_{it}^{(k)} = \\mathbb{P}(s_{it} = k \\mid \\text{data})\\): 상태 k에 있을 posterior 확률\n\\(\\xi_{it}^{(j,k)} = \\mathbb{P}(s_{i,t-1} = j, s_{it} = k \\mid \\text{data})\\): 상태 전이 joint posterior\n\n\n\n\n\n\n\\[\np_{jk} = \\frac{\\sum_{i,t} \\xi_{it}^{(j,k)}}{\\sum_{i,t} \\sum_{k'} \\xi_{it}^{(j,k')}}\n\\]\n즉, 상태 \\(j \\to k\\)로의 전이가 얼마나 자주 발생했는지를 전체 joint posterior 기반으로 집계하여 전이 확률로 추정합니다.\n\n\n\n예를 들어 상태 \\(k\\)에 대해서:\n\n\\(\\phi_k, \\mu_k\\): \\(\\gamma_{it}^{(k)}\\)를 가중치로 사용한 weighted regression\n\\(\\sigma_k^2\\): weighted residual variance"
  },
  {
    "objectID": "thesis/04_model.html#실용적인-가정-및-제약",
    "href": "thesis/04_model.html#실용적인-가정-및-제약",
    "title": "Too Big to Fail?",
    "section": "",
    "text": "공통 전이행렬: 모든 개인에 대해 상태 전이 확률은 동일하다고 가정 (\\(P\\)는 공통)\n개인별 상이한 초기 분포: \\(\\mathbb{P}(s_{i1})\\)은 개인별로 추정하거나 공통으로 가정\n데이터가 충분히 길지 않은 경우, 시간 단위(\\(T\\))는 상태 추정을 어렵게 만들 수 있어 개인 간 정보 풀링이 중요"
  },
  {
    "objectID": "thesis/04_model.html#간단한-예시",
    "href": "thesis/04_model.html#간단한-예시",
    "title": "Too Big to Fail?",
    "section": "",
    "text": "예를 들어 다음과 같은 전이행렬이 있다고 가정:\n\\[\nP = \\begin{bmatrix}\n0.95 & 0.05 \\\\\n0.10 & 0.90\n\\end{bmatrix}\n\\]\n이 경우: - 상태 1 (저성장)은 매우 안정적으로 지속됨 (95%) - 상태 2 (고성장)은 상대적으로 덜 안정적이며, 10% 확률로 저성장으로 전이\n이 전이확률은 위 EM 알고리즘 M-step에서 \\(\\xi_{it}^{(j,k)}\\)를 평균하여 추정됩니다."
  },
  {
    "objectID": "thesis/04_model.html#python-추정-예제-제공-가능",
    "href": "thesis/04_model.html#python-추정-예제-제공-가능",
    "title": "Too Big to Fail?",
    "section": "",
    "text": "패널 데이터를 대상으로 이 모형을 추정하려면 다음과 같은 라이브러리를 활용할 수 있습니다:\n\nstatsmodels.tsa.regime_switching.MarkovRegression (단일 시계열에 한정)\npyhsmm 또는 hmmlearn: HMM 기반 EM 추정\nlinearmodels.panel + custom EM 알고리즘: 패널 구조에 맞춘 추정"
  }
]