[
  {
    "objectID": "time-series/tfp.html",
    "href": "time-series/tfp.html",
    "title": "TFP Growth, Stationarity, and Structural Change",
    "section": "",
    "text": "The growth rate of Total Factor Productivity (TFP) is not a fixed constant; rather, it evolves over time and may be subject to long-term decline. While mainstream macroeconomic models frequently assume a constant TFP growth rate, this assumption is increasingly being questioned. Several structural factors—diminishing returns to R&D, demographic constraints, and sectoral shifts towards lower-productivity industries—suggest that aggregate TFP growth may continue to slow. However, TFP growth is not uniform across sectors. Certain industries, such as AI, biotechnology, and advanced energy technologies, exhibit significantly higher productivity gains, leading to growing sectoral disparities in innovation.\nFrom an econometric perspective, a decline in TFP growth could alter its stationarity properties, weakening its stochastic trend and making it more likely to transition from an \\(I(1)\\) (non-stationary) stochastic process to an \\(I(0)\\) (stationary) stochastic process. If structural breaks exist in TFP growth, traditional time-series models based on constant-trend cointegration assumptions may fail. Thus, accurately modeling TFP trends requires dynamic econometric techniques that account for time-varying structural changes."
  },
  {
    "objectID": "time-series/tfp.html#introduction",
    "href": "time-series/tfp.html#introduction",
    "title": "TFP Growth, Stationarity, and Structural Change",
    "section": "",
    "text": "The growth rate of Total Factor Productivity (TFP) is not a fixed constant; rather, it evolves over time and may be subject to long-term decline. While mainstream macroeconomic models frequently assume a constant TFP growth rate, this assumption is increasingly being questioned. Several structural factors—diminishing returns to R&D, demographic constraints, and sectoral shifts towards lower-productivity industries—suggest that aggregate TFP growth may continue to slow. However, TFP growth is not uniform across sectors. Certain industries, such as AI, biotechnology, and advanced energy technologies, exhibit significantly higher productivity gains, leading to growing sectoral disparities in innovation.\nFrom an econometric perspective, a decline in TFP growth could alter its stationarity properties, weakening its stochastic trend and making it more likely to transition from an \\(I(1)\\) (non-stationary) stochastic process to an \\(I(0)\\) (stationary) stochastic process. If structural breaks exist in TFP growth, traditional time-series models based on constant-trend cointegration assumptions may fail. Thus, accurately modeling TFP trends requires dynamic econometric techniques that account for time-varying structural changes."
  },
  {
    "objectID": "time-series/tfp.html#main",
    "href": "time-series/tfp.html#main",
    "title": "TFP Growth, Stationarity, and Structural Change",
    "section": "Main",
    "text": "Main\n\nUnderstanding TFP in Macroeconomic Models\nTFP represents technological progress that enhances economic output beyond capital and labor inputs. In the Solow-Swan growth model, TFP follows an exponential trend, expressed as:\n\\[\nA_t = A_0 e^{gt} \\epsilon_t\n\\]\nwhere \\(g\\) is the growth rate and \\(\\epsilon_t\\) captures short-term fluctuations. This implies that TFP is not strictly stationary; rather, its log form follows a unit root process with a deterministic trend:\n\\[\n\\log A_t = g t + u_t, \\quad u_t \\sim I(0)\n\\]\nThus, mainstream macroeconomic models do not assume TFP is stationary in levels, but often assume its deviation from trend is stationary.\n\n\nEmpirical Evidence on TFP Stationarity\nEmpirical studies show: 1. Long-Run TFP behaves as a non-stationary process (\\(I(1)\\)), meaning that it exhibits persistent growth and shocks that do not revert over time. 2. Short-Run TFP Shocks (deviations from trend) are often stationary (\\(I(0)\\)) process.\nIf GDP and capital stock are also \\(I(1)\\), Solow’s production function suggests they should be cointegrated:\n\\[\n\\log Y_t - \\alpha \\log K_t - (1-\\alpha) \\log L_t - g t = u_t, \\quad u_t \\sim I(0)\n\\]\nThus, GDP, capital, and labor may be cointegrated due to the common trend driven by TFP.\n\n\nIs TFP Growth Declining in the Long Run?\nMany studies suggest that TFP growth rates have declined since the mid-20th century.\nU.S. TFP Growth Trends (Annual Growth Rates): - 1950s–1970s: ~2.5% per year\n- 1980s–1990s: ~1.5% per year\n- 2000s–2020s: ~0.5%-1.0% per year\nThe productivity slowdown hypothesis suggests that long-term economic growth potential is becoming weaker than before.\n\n\nWhy Might TFP Growth Decline Over Time?\nSeveral key factors contribute to the observed decline in TFP growth:\n\nThe Low-Hanging Fruit Hypothesis (Gordon, 2016)\n\nPast technological revolutions (electricity, automobiles, antibiotics) were one-time events that provided massive boosts.\n\nRecent innovations (social media, fintech, AI) may not have the same economy-wide productivity effects.\n\nDeclining R&D Efficiency (Bloom et al., 2020)\n\n“Ideas are getting harder to find.”\n\nR&D investment has increased, but the number of researchers needed to sustain TFP growth has risen exponentially.\n\nDemographic Constraints and Human Capital Decay\n\nAging populations in advanced economies reduce labor force dynamism and innovation.\n\nEducation quality improvements may have plateaued, reducing skilled labor supply.\n\nSectoral Shift to Low-Productivity Industries\n\nAdvanced economies have shifted from manufacturing to services (e.g., healthcare, education, public sector).\n\nServices typically have lower TFP growth, as they rely heavily on human labor and are difficult to automate.\n\nIncreased Regulation and Market Distortions\n\nGrowing regulations, tax policies, and political uncertainty increase inefficiencies, reducing overall productivity.\n\n\n\n\nCould TFP Growth Accelerate Again?\nWhile aggregate TFP growth may appear to be declining, there are huge disparities across sectors. Just as wealth concentration is increasing, TFP growth could also be highly concentrated within specific industries:\n\nAI and Automation\n\nAI-driven automation could drastically improve productivity, particularly in certain white-collar jobs.\n\nHowever, productivity gains may be highly concentrated in tech-driven industries, rather than the economy as a whole.\n\nSectoral Disparities in TFP Growth\n\nWithin-market concentration: A small number of firms (e.g., winners in the winner-takes-all structure) are experiencing high TFP growth, while others stagnate.\n\nCross-market concentration: Some sectors (AI, biotech) may experience massive productivity boosts, while others (traditional services, public sector) may stagnate.\n\nEnergy Revolution and New Technological Frontiers\n\nAdvances in nuclear fusion, renewables, and space-based energy production could dramatically shift TFP growth.\n\nHowever, large-scale adoption is slow due to political, regulatory, and financial constraints."
  },
  {
    "objectID": "time-series/tfp.html#conclusion",
    "href": "time-series/tfp.html#conclusion",
    "title": "TFP Growth, Stationarity, and Structural Change",
    "section": "Conclusion",
    "text": "Conclusion\n\nImplications for Time Series Analysis\nIf the long-run TFP growth rate is declining, its stochastic trend may weaken, making it more likely to transition from a unit root (\\(I(1)\\)) process to a stationary (\\(I(0)\\)) process over time. This has important implications for empirical modeling and forecasting, particularly in cases where researchers assume a constant growth trend in TFP-driven models. To properly account for these shifts, researchers should:\n\nUse structural break tests (e.g., Bai-Perron) to detect shifts in TFP growth trends and assess whether different growth regimes exist.\n\nApply rolling unit root tests to examine whether TFP has moved from an \\(I(1)\\) process toward an \\(I(0)\\) process over time.\n\nIncorporate time-varying cointegration approaches to avoid biased estimations that assume stable relationships in economic growth models.\n\n\n\nRemarks\nTFP growth is not uniform across industries; while aggregate TFP growth may be declining, specific sectors are experiencing rapid technological advancements. Just as wealth and market concentration have increased, TFP gains are increasingly concentrated within high-tech and innovation-driven industries. The result is a growing disparity between leading-edge industries (AI, biotech, energy) and traditional sectors (public services, healthcare, education), which exhibit slower productivity improvements.\nAs sectoral divergence in productivity widens, traditional macroeconomic models that assume homogeneous TFP growth may require revision. Future research should focus on developing heterogeneous growth models that account for cross-sectoral differences, while also employing more dynamic econometric techniques to capture the evolving nature of TFP trends."
  },
  {
    "objectID": "time-series/joint_covariance_stationary.html",
    "href": "time-series/joint_covariance_stationary.html",
    "title": "Covariance Stationarity, Integration Order, and Cointegration",
    "section": "",
    "text": "Covariance stationarity and cointegration describe different statistical properties of time series data. Covariance stationarity applies to series where mean, variance, and autocovariance do not change over time, while cointegration describes cases where non-stationary (\\(I(1)\\)) time series maintain a stable long-run equilibrium. This study explores how these concepts relate and how cointegration allows for non-stationary series to form stationary linear combinations."
  },
  {
    "objectID": "time-series/joint_covariance_stationary.html#integration-order",
    "href": "time-series/joint_covariance_stationary.html#integration-order",
    "title": "Covariance Stationarity, Integration Order, and Cointegration",
    "section": "Integration Order",
    "text": "Integration Order\nThe integration order of a time series determines how many times it must be differenced to become stationary. A series is:\n\nI(0) (stationary) if it has a constant mean, variance, and autocovariance.\nI(1) (unit root process) if it is non-stationary but becomes stationary after first differencing.\nI(d) if it requires \\(d\\) differences to become stationary.\n\nEconomic time series such as GDP, money supply, and asset prices often exhibit I(1) behavior, meaning they contain stochastic trends and require differencing to achieve stationarity."
  },
  {
    "objectID": "time-series/joint_covariance_stationary.html#joint-covariance-stationarity-vs.-cointegration",
    "href": "time-series/joint_covariance_stationary.html#joint-covariance-stationarity-vs.-cointegration",
    "title": "Covariance Stationarity, Integration Order, and Cointegration",
    "section": "Joint Covariance Stationarity vs. Cointegration",
    "text": "Joint Covariance Stationarity vs. Cointegration\nJoint covariance stationarity applies when each time series maintains a constant mean, variance, and autocovariance over time. Cointegration, on the other hand, describes cases where two or more non-stationary (I(1)) series share a long-term equilibrium, forming a stationary linear combination.\n\nJoint Covariance Stationary Series (Weak-Sense Stationarity)\nA set of time series \\(X_t\\) and \\(Y_t\\) are jointly covariance stationary if they satisfy:\n\nConstant Mean: \\(E[X_t] = \\mu_X\\), \\(E[Y_t] = \\mu_Y\\) for all \\(t\\).\nConstant Variance: \\(Var(X_t)\\) and \\(Var(Y_t)\\) do not change over time.\nAutocovariance Depends Only on Lag: \\(Cov(X_t, X_{t-h})\\) and \\(Cov(Y_t, Y_{t-h})\\) depend only on the lag \\(h\\), not on \\(t\\).\n\nIf two time series are both weakly stationary, then any linear combination of them is also stationary.\n\n\nCointegrated Series\nA set of time series \\(X_t\\) and \\(Y_t\\) are cointegrated if:\n\nEach series is I(1) (non-stationary).\nA linear combination exists that is I(0) (stationary):\n\\[\n\\beta_1 X_t + \\beta_2 Y_t = u_t\n\\]\n\nwhere \\(u_t\\) is \\(I(0)\\) (stationary).\nThus, even though individual variables are non-stationary, their linear combination is stationary, implying a long-run equilibrium relationship.\n\n\nThe Relationship Between Covariance Stationarity and Cointegration\n\n\n\n\n\n\n\n\nProperty\nJoint Covariance Stationary Series\nCointegrated Series\n\n\n\n\nStationarity\nEach series is stationary (I(0))\nEach series is non-stationary (I(1)), but a linear combination is stationary\n\n\nUnit Root (I(d))\nI(0) for each series\nI(1) for each series, but a specific linear combination is I(0)\n\n\nMean & Variance Stability\nMean & variance are constant over time\nIndividual series do not have stable mean & variance, but the combination does\n\n\nLong-run Relationship\nNo long-term relationship constraint\nA long-run equilibrium relationship exists\n\n\n\n\nCointegrated Series Can Be Transformed into Covariance Stationary Series\nIf \\(X_t\\) and \\(Y_t\\) are cointegrated, their first differences \\(\\Delta X_t\\), \\(\\Delta Y_t\\) (or the residual \\(u_t\\)) are stationary.\n\nThe error correction term ( u_t ) is stationary ( I(0) ), meaning it satisfies the covariance stationarity conditions.\n\n\n\nJoint Covariance Stationary Series Are Not Cointegrated\nIf \\(X_t\\) and \\(Y_t\\) are both already \\(I(0)\\) (stationary), then any linear combination of them is also stationary.\n\nThey cannot be cointegrated because cointegration only applies to non-stationary (I(1)) series.\nIf all series are already covariance stationary, testing for cointegration is unnecessary."
  },
  {
    "objectID": "time-series/joint_covariance_stationary.html#implications-for-empirical-analysis",
    "href": "time-series/joint_covariance_stationary.html#implications-for-empirical-analysis",
    "title": "Covariance Stationarity, Integration Order, and Cointegration",
    "section": "Implications for Empirical Analysis",
    "text": "Implications for Empirical Analysis\n\nBefore testing for cointegration, check for stationarity. If all series are I(0), cointegration does not apply.\nIf series are cointegrated, their residuals (error correction term) should be covariance stationary.\nMany macroeconomic variables (e.g., GDP & consumption, money supply & inflation) are cointegrated rather than purely covariance stationary."
  },
  {
    "objectID": "projects/pricing_equal/pricing_equal.html",
    "href": "projects/pricing_equal/pricing_equal.html",
    "title": "Pricing the seemingly Equal Opportunities under Structural Inequality",
    "section": "",
    "text": "By applying the EMM-based asset pricing approach, this study quantifies the impact of structural inequalities on the valuation of ostensibly equal opportunities. The results emphasize the importance of considering underlying economic disparities when evaluating the fairness and effectiveness of qualification standards in society."
  },
  {
    "objectID": "projects/pricing_equal/pricing_equal.html#conclusion",
    "href": "projects/pricing_equal/pricing_equal.html#conclusion",
    "title": "Pricing the seemingly Equal Opportunities under Structural Inequality",
    "section": "",
    "text": "By applying the EMM-based asset pricing approach, this study quantifies the impact of structural inequalities on the valuation of ostensibly equal opportunities. The results emphasize the importance of considering underlying economic disparities when evaluating the fairness and effectiveness of qualification standards in society."
  },
  {
    "objectID": "projects/pricing_equal/pricing_equal.html#main",
    "href": "projects/pricing_equal/pricing_equal.html#main",
    "title": "Pricing the seemingly Equal Opportunities under Structural Inequality",
    "section": "Main",
    "text": "Main\n\nModel Scenario\nIn short, we consider a society divided into two distinct classes:\n\nProletariat (P): Individuals with access to general opportunities.\nCapitalist (C): Individuals with access to both general and exclusive special opportunities.\n\nInternally, both classes operate within perfectly competitive markets that adhere to the no-arbitrage principle. However, due to capital constraints, members of the P class cannot access the special opportunities available to the C class, rendering these opportunities unattainable. Consequently, while the absolute value of special opportunities is equal to or greater than that of general opportunities, the P class cannot exploit potential arbitrage opportunities due to these constraints. Both classes have offspring who inherit the economic outcomes of their parents’ investments. To claim these inherited assets, all offspring must meet a minimum qualification standard (e.g., educational credentials), analogous to the strike price (\\(K\\)) in option pricing. Until this qualification is met, the offspring hold a call option on their parents’ assets.\nIn detail, consider two distinct social classes: the proletariat (P), representing working-class individuals, and the capitalist class (C), representing wealthy individuals. These two classes face fundamentally different opportunity sets due to deep-seated structural inequalities. Such societal barriers, from both democratic and utilitarian viewpoints, constitute a significant social inefficiency.\nTo analyze the structural origins of this inefficiency, we apply the concept of Limits to Arbitrage Theory, which suggests markets are not always fully efficient. This inefficiency arises because of various constraints, such as behavioral biases, risk constraints, and notably, capital constraints. Capital constraints imply that significant capital is required to engage in arbitrage, effectively isolating classes economically. We assume, therefore, that the proletariat and capitalist classes are economically segregated due to these capital constraints.\nWithin each class, economic opportunities exist in a perfectly competitive market, satisfying the no-arbitrage condition internally. Members of the proletariat (P) freely select from a common set of general opportunities available to them. Conversely, members of the capitalist class (C) not only share these general opportunities but also exclusively access additional “special opportunities.” From the viewpoint of proletariat class members, these special opportunities represent unattainable benefits (“grapes beyond reach”), carrying higher or equal absolute value compared to general opportunities.\nTheoretically, if members of the capitalist class could short-sell general opportunities and simultaneously long-position special opportunities, they could realize arbitrage profits. However, due to aforementioned capital constraints (“limits to arbitrage”), such arbitrage trading is practically impossible within this model.\nBoth classes have offspring who inherit the economic results (absolute asset values) of their parents’ investment choices. To claim these inherited assets, children from both classes must meet identical minimum qualification standards (e.g., university diplomas, basic educational credentials). Although the qualification standard is identical for both, outcomes differ significantly due to inherited assets. For instance, a business administration graduate from the capitalist class inherits and manages substantial capital (businesses), while an identically qualified individual from the proletariat class works as an employee, earning wages in capitalist-owned enterprises.\nThe minimum qualification standard can be understood as a fixed barrier or strike price (K) of a call option. Until the qualification requirement is fulfilled, the children effectively hold call options on their parents’ assets. This model aims to quantify the absolute value of these call options for each class, reflecting the inherited economic outcomes accessible to the offspring.\n\n\nEMM-based Call Option Valuation Theory\nAssume a simple 1 period setting with only two possible states (e.g. good or bad). Under the Equivalent Martingale Measure (EMM), the fair market value of a call option at time 0, denoted as \\(\\hat{f}_0\\), must satisfy:\n\\[\\hat{f_0}\\cdot R = E^q[f_1]\\] where \\(f_1=max(S_0 \\cdot U-K,0)\\)\n\\[S_0\\cdot R=E^q[S_1]\\] where \\(S_0\\) = the current price of risky underlying asset\nFrom these two equations, we have the EMM or the state price density for good state \\(q\\) as: \\[\nq = \\frac{R - D}{U - D}\n\\]\nThus, the present fair value of the call option is given by:\n\\[\n\\hat{f}_0 = \\frac{1}{R}\\left[ \\frac{R - D}{U - D}(S_0 \\cdot U - K) + \\frac{U - R}{U - D} \\cdot 0 \\right] = \\frac{(R - D)(U - 1)}{R(U - D)}\n\\]\nassuming that\n\nInitial Asset Price (normalized): \\(S_0=1\\)\n\nStrike Price (qualification threshold): \\(K=1\\)\n\nMaturity: 18 years (quarterly steps = 72 periods)\n\nThis formula indicates a clear proportional relationship for estimating current fair values representing inherited qualification-based claims for each class over a 18-year maturity period. While this valuation could be approximated using continuous distributions (e.g., Black-Scholes under symmetric assumptions \\(U \\cdot D=1\\)), our discrete binomial model allows straightforward interpretation without loss of economic intuition.\n\n\nEmpirical Analysis\nWe employ distinct underlying assets for each class. Using the historical dataset (Q1 1982–Q4 2019, 152 quarterly observations), we estimated the parameters and their associated fair values of call options for each class.\nFor Proletariat (P) Class Children:\n\nRisky Asset: US Median usual weekly Real earnings (LES1252881600Q)\n\nRisk-Free Asset: US Real GDP per capita (A939RX0Q048SBEA)\n\nParameters:\n\n\\(U_p\\):= 75th percentile growth of wage (risky asset)\n\n\\(D_p\\):= 25th percentile growth of wage\n\n\\(R_p\\):= Median growth rate of Real GDP per capita\n\n\nFor Capitalist (C) Class Children:\n\nRisky Asset: S&P 500 equity (SPX)\n\nRisk-Free Asset: US 10-Year Treasury Bond\nParameters:\n\n\\(U_c\\):= 75th percentile quarterly growth of the equity index\n\n\\(D_c\\):= 25th percentile quarterly growth of the equity index\n\n\\(R_c\\):= Median of quarterly US 10-Year Treasury Bond Yield (DGS10)\n\n\nEmpirical Results:\n\nFor Proletariat (P) class children, \\(\\hat{P}_0=?\\)\n\nFor Capitalist (C) class children, \\(\\hat{C}_0=?\\)\n\n\n\nCode\nimport yfinance as yf\nimport pandas_datareader.data as web\nimport pandas as pd\nimport numpy as np\n\n# 데이터 기간 설정\nstart_date = '1982-01-01'\nend_date = '2019-12-31'\n\n# S&P 500 데이터 가져오기\nsp500 = yf.download(\"^GSPC\", start=start_date, end=end_date, interval=\"1d\")\nsp500_q = sp500['Close'].resample('QE').last()  # 분기별 종가 데이터\n\n# FRED 데이터 가져오기\nus_median_weekly_earnings = web.DataReader('LES1252881600Q', 'fred', start_date, end_date)\nus_real_gdp_per_capita = web.DataReader('A939RX0Q048SBEA', 'fred', start_date, end_date)\nus_10yr_treasury_yield = web.DataReader('DGS10', 'fred', start_date, end_date)\n\n# 인덱스를 맞추기 위해 분기별로 재샘플링\nus_median_weekly_earnings = us_median_weekly_earnings.resample('QE').last()\nus_real_gdp_per_capita = us_real_gdp_per_capita.resample('QE').last()\nus_10yr_treasury_yield = us_10yr_treasury_yield.resample('QE').last()\n\n# 데이터프레임으로 변환\ndata = pd.DataFrame({\n    'SP500': sp500_q.squeeze(),\n    'Median_Weekly_Earnings': us_median_weekly_earnings['LES1252881600Q'].squeeze(),\n    'Real_GDP_per_Capita': us_real_gdp_per_capita['A939RX0Q048SBEA'].squeeze(),\n    '10yr_Treasury_Yield': us_10yr_treasury_yield['DGS10'].squeeze()\n}, index=sp500_q.index)\n\n# 결측값 제거\ndata.dropna(inplace=True)\n\n# 수익률 계산\ndata['SP500_Return'] = data['SP500'].pct_change()\ndata['Earnings_Growth'] = data['Median_Weekly_Earnings'].pct_change()\ndata['GDP_Growth'] = data['Real_GDP_per_Capita'].pct_change()\n\n# 통계치 계산\nU_p = data['Earnings_Growth'].quantile(0.75)+1\nD_p = data['Earnings_Growth'].quantile(0.25)+1\nR_p = data['GDP_Growth'].median()+1\n\nU_c = data['SP500_Return'].quantile(0.75)+1\nD_c = data['SP500_Return'].quantile(0.25)+1\nR_c = data['10yr_Treasury_Yield'].median()\nR_c = R_c / 100 +1\n\nprint(f\"Proletariat Class Children Parameters:\")\nprint(f\"U_p: {U_p:.2f}\")\nprint(f\"D_p: {D_p:.2f}\")\nprint(f\"R_p: {R_p:.2f}\")\n\nprint(f\"\\nCapitalist Class Children Parameters:\")\nprint(f\"U_c: {U_c:.2f}\")\nprint(f\"D_c: {D_c:.2f}\")\nprint(f\"R_c: {R_c:.2f}\")\n\n\n# Binomial Option Pricing Model\ndef binomial_option_pricing(K, S_0, T, U, D, R, dt):\n    \"\"\"\n    K: Strike price\n    S_0: Initial stock price\n    T: Time to maturity (in years)\n    U: Up factor\n    D: Down factor\n    R: Risk-free rate\n    dt: Number of steps for each year\n    \"\"\"\n    n = T*dt # Number of steps in the binomial tree\n    q = (R - D) / (U - D)\n\n    # Initialize option values at maturity\n    option_values = np.zeros((n + 1, 1))\n    for i in range(n + 1):\n        ST = S_0 * (U ** i) * (D ** (n - i))\n        option_values[i] = max(0, ST - K)\n\n    # Backward recursion for option values\n    for j in range(n - 1, -1, -1):\n        for i in range(j + 1):\n            option_values[i] = (q * option_values[i + 1] + (1 - q) * option_values[i]) / R\n\n    return option_values[0, 0]\n\n\n# Parameters\nK = 1  # Strike price\nS_0 = 1  # Initial stock price\nT = 18  # Time to maturity (18 years)\ndt = 4 # Number of steps for each year\n\n# Calculate option prices\noption_price_proletariat = binomial_option_pricing(K, S_0, T, U_p, D_p, R_p, dt)\noption_price_capitalist = binomial_option_pricing(K, S_0, T, U_c, D_c, R_c, dt)\n\nprint(f\"\\nFair price of Call Option, held by Proletariat Class Children:\\n {option_price_proletariat:.2f}\")\nprint(f\"Fair price of Call Option, held by Capitalist Class Children:\\n {option_price_capitalist:.2f}\")\n\n\nYF.download() has changed argument auto_adjust default to True\n\n\n[*********************100%***********************]  1 of 1 completed\n\n\nProletariat Class Children Parameters:\nU_p: 1.01\nD_p: 1.00\nR_p: 1.01\n\nCapitalist Class Children Parameters:\nU_c: 1.07\nD_c: 0.99\nR_c: 1.05\n\nFair price of Call Option, held by Proletariat Class Children:\n 0.30\nFair price of Call Option, held by Capitalist Class Children:\n 0.97\n\n\n\n\nDiscussion\nThis model clarifies the stark inequality underlying “ostensibly equal” qualification standards. Although formally identical, the call options’ absolute valuations significantly diverge, reflecting distinct economic inheritances accessible to each class. This disparity highlights structural inefficiencies and deep-rooted inequalities, persisting despite nominally identical qualification standards.\nUltimately, this analysis underscores how asset-based class differentiation profoundly impacts the perceived and realized absolute value of educational and economic opportunities, illuminating critical implications for economic policy, educational equity, and social justice frameworks."
  },
  {
    "objectID": "projects/pricing_equal/pricing_equal.html#introduction",
    "href": "projects/pricing_equal/pricing_equal.html#introduction",
    "title": "Pricing the seemingly Equal Opportunities under Structural Inequality",
    "section": "Introduction",
    "text": "Introduction\nIn societies characterized by pronounced economic stratification, opportunities presented as equal often yield disparate outcomes across different social strata. This disparity arises from inherent structural inefficiencies that restrict access to certain opportunities based on class. The Limits to Arbitrage Theory posits that market inefficiencies can persist due to various constraints, including capital limitations, preventing rational traders from correcting mispricings (Shleifer and Vishny 1997). This paper explores how such constraints contribute to the unequal valuation of opportunities between the proletariat (P) and capitalist (C) classes."
  },
  {
    "objectID": "projects/pricing_equal/pricing_equal.html#literature-review",
    "href": "projects/pricing_equal/pricing_equal.html#literature-review",
    "title": "Pricing the seemingly Equal Opportunities under Structural Inequality",
    "section": "Literature Review",
    "text": "Literature Review\nThe theoretical foundation relies primarily on the limits to arbitrage theory, initially articulated by Shleifer and Vishny (Shleifer and Vishny 1997). Their seminal work shows how market inefficiencies persist due to practical constraints, particularly capital constraints, restricting the ability of arbitrageurs to exploit and correct mispricings. These constraints arise from significant capital requirements that effectively segregate participants into distinct economic spheres, as emphasized by subsequent studies on financially constrained arbitrageurs (Gromb and Vayanos 2002; Xiong 2001).\nGeanakoplos’ research introduces the leverage cycle, which explains how fluctuations in leverage and capital availability perpetuate systemic inequality and financial instability (Geanakoplos 2010). Complementary studies, such as that by Gromb and Vayanos, also demonstrate the welfare implications of constrained arbitrageurs operating under capital limitations, further exacerbating persistent inequality (Gromb and Vayanos 2002). Moreover, Barberis and Thaler’s comprehensive survey on behavioral finance indicates how cognitive and behavioral constraints exacerbate market inefficiencies, reinforcing the structural barriers that differentiate economic outcomes across social strata (Barberis and Thaler 2003).\nExtending beyond purely financial contexts, sociological and economic research provides additional perspectives on structural inequalities and opportunity valuation. Bourdieu’s concept of social reproduction underscores how cultural capital perpetuates socioeconomic inequalities across generations (Bourdieu 1973). Recent empirical evidence by Stansbury (Stansbury 2024) and findings by the Social Mobility Commission (Social Mobility Commission 2023) further demonstrate how economic capital inherited through generations shapes differential outcomes, even when individuals ostensibly possess identical qualifications.\nChetty et al. provide compelling evidence linking parental economic conditions to children’s educational outcomes and future earnings, strongly supporting the relevance of inherited economic positions in determining opportunity valuations (Chetty et al. 2014). Similarly, Piketty’s influential book highlights the crucial role inherited wealth plays in perpetuating structural economic disparities, emphasizing the critical nature of capital inheritance in shaping individuals’ economic trajectories and their access to opportunities (Piketty 2014).\nPolicy implications regarding these structural inequalities and efforts to enhance social mobility have been explored extensively by institutions such as the OECD. Their analyses suggest policy frameworks that might alleviate the persistent inequalities discussed herein (OECD 2018). Reeves’ concept of the “glass floor” further illustrates how affluent socioeconomic backgrounds systematically maintain class advantages despite equal or even lesser merit-based qualifications (Reeves 2017).\nCollectively, these studies underline a coherent narrative: ostensibly equal opportunities often conceal significant disparities rooted in inherited structural inequalities, persistent capital constraints, and behavioral limitations to arbitrage. Our model complements this literature by quantitatively evaluating how these structural factors systematically influence the absolute value of identical qualification standards across distinct socioeconomic groups."
  },
  {
    "objectID": "projects/gilded_age/gilded_age.html",
    "href": "projects/gilded_age/gilded_age.html",
    "title": "The New Gilded Age",
    "section": "",
    "text": "The term ‘Gilded Age’ was originally coined by Mark Twain in his novel The Gilded Age: A Tale of Today (Twain and Warner 1873), describing an era characterized by rapid economic expansion, extreme wealth concentration, and political corruption. A similar dynamic is emerging today, where financial and technological elites dominate economic output while wealth inequality reaches historic highs (Piketty 2014). The late 19th century saw industrial monopolies like Standard Oil and U.S. Steel controlling markets; today, tech giants such as Amazon, Apple, and Google exhibit similar dominance (Zucman 2019).\nSimultaneously, the Federal Reserve’s response to financial instability, particularly through excessive monetary expansion, contrasts with past policy mistakes that led to severe economic contractions due to monetary shrinkage (Bernanke 2000). If current economic trends persist—marked by the increasing concentration of wealth, hyperinflation risks, and geopolitical tensions—then the U.S. may be heading toward another crisis akin to the 1929 stock market collapse (Kindleberger 1978)."
  },
  {
    "objectID": "projects/gilded_age/gilded_age.html#extreme-wealth-concentration-and-economic-disparities",
    "href": "projects/gilded_age/gilded_age.html#extreme-wealth-concentration-and-economic-disparities",
    "title": "The New Gilded Age",
    "section": "Extreme Wealth Concentration and Economic Disparities",
    "text": "Extreme Wealth Concentration and Economic Disparities\nIn the late 19th century, “Robber Barons” controlled vast industrial empires while working-class Americans suffered under exploitative labor conditions (Irwin 2017). Today, the economic landscape reflects a similar dynamic: the top 1% of Americans hold over 30% of total U.S. wealth, and financial markets remain dominated by a handful of institutional investors and corporations (Saez and Zucman 2020). If historical trends hold, wealth concentration at this level often precedes financial and political crises."
  },
  {
    "objectID": "projects/gilded_age/gilded_age.html#financial-market-distortions-due-to-federal-reserve-policies",
    "href": "projects/gilded_age/gilded_age.html#financial-market-distortions-due-to-federal-reserve-policies",
    "title": "The New Gilded Age",
    "section": "Financial Market Distortions Due to Federal Reserve Policies",
    "text": "Financial Market Distortions Due to Federal Reserve Policies\nHistorically, the Federal Reserve’s failure to manage monetary policy effectively has exacerbated financial downturns. During the Great Depression, the Fed allowed the money supply to contract, worsening deflation (Friedman and Schwartz 1993). Conversely, in the 2008 financial crisis, the Fed implemented massive QE programs to avoid liquidity shortages (Gopinath and Gourinchas 2020). If the Fed continues expanding the money supply unchecked while maintaining low interest rates, it could trigger runaway inflation or asset bubbles (Reinhart and Rogoff 2010)."
  },
  {
    "objectID": "projects/gilded_age/gilded_age.html#protectionist-policies-and-global-trade-disruptions",
    "href": "projects/gilded_age/gilded_age.html#protectionist-policies-and-global-trade-disruptions",
    "title": "The New Gilded Age",
    "section": "Protectionist Policies and Global Trade Disruptions",
    "text": "Protectionist Policies and Global Trade Disruptions\nIn response to financial instability, the U.S. may turn to protectionist measures similar to those seen in the early 20th century, such as the Smoot-Hawley Tariff Act (Irwin 2017). If the U.S. imposes broad tariffs on allies like Canada, Mexico, and the EU (excluding the UK), retaliatory tariffs could significantly reduce global trade, accelerating economic fragmentation (Acker 2020)."
  },
  {
    "objectID": "projects/gilded_age/gilded_age.html#capital-controls",
    "href": "projects/gilded_age/gilded_age.html#capital-controls",
    "title": "The New Gilded Age",
    "section": "Capital Controls",
    "text": "Capital Controls\nTo prevent capital flight, the U.S. government might implement capital controls, restricting the movement of funds outside the country (Dornbusch 1996). Such policies could initially stabilize domestic financial markets by preventing liquidity outflows, but they would ultimately deter foreign investment and reduce the credibility of the U.S. dollar (Prasad 2021). If capital controls are implemented alongside protectionist trade policies, the global financial system could realign, reducing reliance on the dollar (Eichengreen 2019)."
  },
  {
    "objectID": "projects/gilded_age/gilded_age.html#internal-conflict",
    "href": "projects/gilded_age/gilded_age.html#internal-conflict",
    "title": "The New Gilded Age",
    "section": "Internal Conflict",
    "text": "Internal Conflict\nWith increasing partisan division, the U.S. could experience state-led resistance against federal economic policies. Democratic-led states might oppose Republican federal mandates, leading to legal disputes over taxation, social policies, and trade regulations (Levitsky and Ziblatt 2018). In extreme cases, states like California could advocate for economic or political autonomy, mirroring secessionist movements of the 19th century, while Texas, despite its strong Republican leanings, might push for greater state sovereignty in response to federal overreach or shifting national policies (Acker 2020)."
  },
  {
    "objectID": "projects/gilded_age/gilded_age.html#u.s.-dollar-as-the-global-reserve-currency",
    "href": "projects/gilded_age/gilded_age.html#u.s.-dollar-as-the-global-reserve-currency",
    "title": "The New Gilded Age",
    "section": "U.S. Dollar as the Global Reserve Currency",
    "text": "U.S. Dollar as the Global Reserve Currency\nThe decline of the British pound post-World War II illustrates how global reserve currencies can lose dominance due to internal and external economic shifts (Eichengreen 2019). If U.S. political instability continues, central banks worldwide may accelerate diversification away from dollar holdings, increasing reliance on alternative financial networks such as BRICS payment systems, Bitcoin, and other emerging digital currencies. (Prasad 2021)."
  },
  {
    "objectID": "projects/correlation_crypto/correlation_crypto.html",
    "href": "projects/correlation_crypto/correlation_crypto.html",
    "title": "Correlation within Crypto-currencies",
    "section": "",
    "text": "Abstract: 2025년 3월 현재, 시가총액이 크거나 투자자들에게 인기가 많은 주요 암호화폐(popular cryptocurrencies)를 선정하여 지난 1년간의 상관관계를 분석하였다. 대부분의 암호화폐 투자자들은 이러한 주요 암호화폐에 집중적으로 투자하는 경향이 있다. 한편, 암호화폐 자산에 대한 투자자의 평균 투자 기간은 단기(short-term)로, 일반적으로 1개월에서 3개월 사이에 해당한다. 이에 따라 본 연구에서는 데이터의 관측 빈도(observation frequency)를 일간(daily) 단위로 설정하고, 30일, 60일, 90일의 롤링 윈도우(rolling window)를 적용하여 주요 암호화폐 수익률의 선형 상관계수(Pearson’s coefficient)를 분석하였다. 이러한 분석은 변동성 헤징(volatility hedging)을 고려한 분산 투자(diversified investment) 전략 수립에 도움이 될 수 있다. 예를 들어, 일정한 투자 금액(예: 1억 원)을 주요 암호화폐 자산군 내에서 어떻게 배분할지 결정하는 데 있어, 상관계수 분석 결과가 투자 비중 조정에 유용한 정보를 제공할 것으로 기대된다."
  },
  {
    "objectID": "projects/correlation_crypto/correlation_crypto.html#서론",
    "href": "projects/correlation_crypto/correlation_crypto.html#서론",
    "title": "Correlation within Crypto-currencies",
    "section": "서론",
    "text": "서론\n비트코인(BTC)의 가격 및 수익률은, 단기적으로 다음과 같은 관계를 보여왔다.\n\nNDX (나스닥 100 지수)와 강한 양의 상관관계 (Nasdaq 2024, 2023),\nDXY (미국 달러 지수)와 강한 음의 상관관계 (Coindesk 2023; Coinglass 2023). 만약 비트코인 가격이 달러 가격과 장기적으로도 반대 방향으로 움직인다면, 이는 비트코인이 인플레이션 헤지 자산으로 간주될 수도 있을 가능성을 나타낸다 (Dyhrberg 2021).\n금 가격 (GOLD), 국내 실질 총생산량 (GDP) 과의 상관관계는 불명확하거나 간접적인 것으로 알려져 있음 (Cointelegraph 2023; Cryptoslate 2022).\n\n역사적 사례\n\n2020년 COVID-19 위기 이후 BTC와 NDX의 상관관계가 강화됨 (Nasdaq 2020),\n2022년 5월 연방준비제도(Fed)의 금리 인상 발표 당시, BTC와 NDX 모두 하락.\n2023년 비트코인 빙하기 기간 동안 BTC와 NDX의 상관관계 변화 분석 필요.\n2024년 3월 비트코인 ETF가 출시하여 기관 투자자 참여가 증가와 함께께 BTC과 NDX의 coupling이 심해짐.\n\n\n주요 암호화폐 목록 및 카테고리\n\n\n\n\n\n\n\n\n암호화폐 (Cryptocurrency)\n심볼 (Ticker)\n카테고리 (Category)\n\n\n\n\n비트코인 (Bitcoin)\nBTC/USD\nLayer 1\n\n\n이더리움 (Ethereum)\nETH/USD\nLayer 1, Smart Contract\n\n\n테더 (Tether)\nUSDT/USD\nStablecoin\n\n\n리플 (XRP)\nXRP/USD\nPayment Network\n\n\n솔라나 (Solana)\nSOL/USD\nLayer 1\n\n\n체인링크 (Chainlink)\nLINK/USD\nOracle\n\n\n온도 (Ondo)\nONDO/USD\nReal-World Asset (RWA)\n\n\n카르다노 (Cardano)\nADA/USD\nLayer 1\n\n\n트론 (Tron)\nTRX/USD\nLayer 1\n\n\n도지코인 (Dogecoin)\nDOGE/USD\nMeme Coin\n\n\n\n\n\n암호화폐 관련 정보 제공 매체 리뷰\n\n시세 데이터 (Price Data): 실시간 및 과거 가격 변동, 거래량(volume) 등\n\nCoinMarketCap\nCoinGecko\n\n온체인 데이터 (On-Chain Data): 거래량, 지갑 주소 변화, 네트워크 활성도 등\n\nGlassnode\nIntoTheBlock\n\n시장 분석 (Market Analysis): 전문가 및 AI 기반 분석 리포트\n\nMessari\nCryptoQuant\n\n뉴스 및 이벤트 (News & Events): 프로젝트 업데이트, 규제 변화 등\n\nCoinDesk\nThe Block\n\n소셜 미디어 분석 (Social Media Analysis): 트위터(X), 레딧(Reddit) 등에서의 커뮤니티 반응\n\nLunarCrush\nSantiment"
  },
  {
    "objectID": "projects/correlation_crypto/correlation_crypto.html#데이터-분석",
    "href": "projects/correlation_crypto/correlation_crypto.html#데이터-분석",
    "title": "Correlation within Crypto-currencies",
    "section": "데이터 분석",
    "text": "데이터 분석\n\n데이터\n\n데이터 소스: FinanceDataReader\n데이터 기간: 2024년 3월 1일 - 2025년 2월 28일\n데이터 빈도 (Data Frequency): 일간(Daily)\n분석 대상 암호화폐:\n\nBTC/USD, ETH/USD, USDT/USD, XRP/USD, SOL/USD, LINK/USD, ONDO/USD, ADA/USD, TRX/USD, DOGE/USD\n\n롤링 윈도우 크기 (Rolling Window Size): 30일, 60일, 90일\n\n\n\n분석 방법\n\n암호화폐의 일간 수익률(daily return)을 계산.\n각 롤링 윈도우 크기(30, 60, 90일)에 대해 롤링 상관 행렬(rolling correlation matrix)을 계산.\n평균 상관계수(mean of rolling correlation matrix)를 도출하여 암호화폐 간의 관계를 분석.\n\n\n\nCode\n# 분석 결과 (Results)\n\n# 여러 거래소에서 지원하는 거래쌍을 확인\n\nimport ccxt\nimport pandas as pd\n\n# 주요 암호화폐 목록\nTICKER_COIN = ['BTC/USDT', 'ETH/USDT', 'USDT/USD', 'XRP/USDT', 'SOL/USDT', 'LINK/USDT', 'ONDO/USDT', 'ADA/USDT', 'TRX/USDT', 'DOGE/USDT']\n\n# 지원하는 거래소 목록\nexchanges = ['binance', 'kraken', 'bitfinex', 'poloniex']\n\n# 각 거래소에서 지원하는 거래쌍 확인\nfor exchange_id in exchanges:\n    exchange = getattr(ccxt, exchange_id)()\n    markets = exchange.load_markets()\n    supported_pairs = [pair for pair in TICKER_COIN if pair in markets]\n    print(f\"{exchange_id} supports: {supported_pairs}\")\n\n# 주요 암호화폐 목록\nbinance_tickers = ['BTC/USDT', 'ETH/USDT', 'XRP/USDT', 'SOL/USDT', 'LINK/USDT', 'ADA/USDT', 'TRX/USDT', 'DOGE/USDT']\nkraken_tickers = ['USDT/USD']\npoloniex_tickers = ['ONDO/USDT']\n\n# 데이터 기간 설정\nSTART_DATE = '2024-03-01'\nEND_DATE = '2025-02-28'\n\n# 거래소 설정\nbinance = ccxt.binance()\nkraken = ccxt.kraken()\npoloniex = ccxt.poloniex()\n\n# 데이터 불러오기 함수\ndef fetch_crypto_data(exchange, tickers, start, end):\n    data = {}\n    start_timestamp = exchange.parse8601(f'{start}T00:00:00Z')\n    end_timestamp = exchange.parse8601(f'{end}T00:00:00Z')\n    for ticker in tickers:\n        try:\n            ohlcv = exchange.fetch_ohlcv(ticker, '1d', since=start_timestamp, limit=1000)\n            df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n            df.set_index('timestamp', inplace=True)\n            data[ticker] = df['close']\n        except Exception as e:\n            print(f\"Error fetching {ticker} from {exchange.id}: {e}\")\n    return pd.DataFrame(data)\n\n# 데이터 불러오기\nbinance_data = fetch_crypto_data(binance, binance_tickers, START_DATE, END_DATE)\nkraken_data = fetch_crypto_data(kraken, kraken_tickers, START_DATE, END_DATE)\npoloniex_data = fetch_crypto_data(poloniex, poloniex_tickers, START_DATE, END_DATE)\n\n# 모든 데이터를 하나의 DataFrame으로 병합\ncrypto_prices = pd.concat([binance_data, kraken_data, poloniex_data], axis=1)\n\n# 1) 일간 수익률 계산\ndef compute_returns(price_data: pd.DataFrame) -&gt; pd.DataFrame:\n    return price_data.pct_change().dropna(how='all')\n\ncrypto_returns = compute_returns(crypto_prices)\n\n# 2) 롤링 상관계수 계산\ndef rolling_correlation(returns: pd.DataFrame, window: int) -&gt; pd.DataFrame:\n    \"\"\"\n    returns: (date x tickers) DataFrame\n    window:  rolling window size (days)\n    \n    returns.rolling(window).corr() 결과는\n      - MultiIndex (date, ticker1)\n      - columns = ticker2\n    형태를 가집니다.\n    \"\"\"\n    corr_rolling = returns.rolling(window).corr()\n    return corr_rolling\n\n# 3) 날짜별 상관행렬을 모아서 평균 상관행렬을 산출\ndef average_correlation_matrix(returns: pd.DataFrame, window: int) -&gt; pd.DataFrame:\n    \"\"\"\n    - returns.rolling(window).corr() 결과를 사용\n    - 각 날짜별 (티커 x 티커) 상관행렬을 합산 후, 날짜 개수로 나누어 평균\n    \"\"\"\n    corr_rolling = rolling_correlation(returns, window)\n    \n    # MultiIndex에서 날짜(level=0) 목록을 추출\n    unique_dates = corr_rolling.index.get_level_values(0).unique()\n    tickers = returns.columns\n    \n    # 상관행렬 누적 합을 위한 (티커 x 티커) 형태의 빈 DataFrame\n    sum_matrix = pd.DataFrame(0.0, index=tickers, columns=tickers)\n    count = 0\n    \n    for date in unique_dates:\n        # (ticker1 x ticker2) 형태를 얻기 위해 xs(date, level=0)\n        date_corr = corr_rolling.xs(date, level=0)\n        # date_corr.index = ticker1, date_corr.columns = ticker2\n        \n        # 혹시 일부 티커에 대한 데이터가 누락되었을 경우를 대비하여 reindex\n        date_corr = date_corr.reindex(index=tickers, columns=tickers)\n        \n        # 날짜별 상관행렬(N x N)을 모두 누적\n        if date_corr.notna().all().all():\n            sum_matrix += date_corr.fillna(0.0)\n            count += 1\n    \n    # 평균 계산 (count가 0이 되지 않는다고 가정)\n    mean_matrix = sum_matrix / count\n    \n    return mean_matrix\n\n# 4) 롤링 상관계수 평균 계산\nrolling_corr_results = {}\nfor window in [30, 60, 90]:\n    mean_corr_matrix = average_correlation_matrix(crypto_returns, window)\n    rolling_corr_results[window] = mean_corr_matrix\n\n\nbinance supports: ['BTC/USDT', 'ETH/USDT', 'XRP/USDT', 'SOL/USDT', 'LINK/USDT', 'ADA/USDT', 'TRX/USDT', 'DOGE/USDT']\nkraken supports: ['BTC/USDT', 'ETH/USDT', 'USDT/USD', 'XRP/USDT', 'SOL/USDT', 'LINK/USDT', 'ADA/USDT', 'DOGE/USDT']\nbitfinex supports: ['BTC/USDT', 'ETH/USDT', 'USDT/USD', 'XRP/USDT', 'SOL/USDT', 'LINK/USDT', 'ADA/USDT', 'TRX/USDT', 'DOGE/USDT']\npoloniex supports: ['BTC/USDT', 'ETH/USDT', 'XRP/USDT', 'SOL/USDT', 'LINK/USDT', 'ONDO/USDT', 'ADA/USDT', 'TRX/USDT', 'DOGE/USDT']\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# 모든 행과 열이 출력되도록 설정\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.expand_frame_repr', False)\n\n# 결과 출력 및 시각화\nfor window, result in rolling_corr_results.items():\n    # 상관 행렬을 DataFrame으로 변환\n    result_df = result.dropna(how='all')\n    print(f\"\\n[Window = {window} days] Mean Correlation Matrix\\n\", result_df)\n    \n    # 히트맵 시각화\n    plt.figure(figsize=(10, 8))\n    \n    # 대각선 요소를 마스킹\n    mask = np.triu(np.ones(result_df.shape, dtype=bool))\n    \n    sns.heatmap(result_df, annot=True, cmap='coolwarm', center=0, mask=mask)\n    plt.title(f'Mean Rolling Correlation Matrix (Window Size: {window} days)')\n    plt.show()\n\n\n\n[Window = 30 days] Mean Correlation Matrix\n            BTC/USDT  ETH/USDT  XRP/USDT  SOL/USDT  LINK/USDT  ADA/USDT  TRX/USDT  DOGE/USDT  USDT/USD  ONDO/USDT\nBTC/USDT   1.000000  0.797105  0.565396  0.740181   0.681814  0.703330  0.381443   0.811459  0.448836   0.348155\nETH/USDT   0.797105  1.000000  0.554712  0.700525   0.734957  0.717653  0.377609   0.744033  0.281970   0.378043\nXRP/USDT   0.565396  0.554712  1.000000  0.548503   0.565133  0.648923  0.314937   0.587623  0.162215   0.226388\nSOL/USDT   0.740181  0.700525  0.548503  1.000000   0.670887  0.681713  0.316931   0.691400  0.249392   0.299814\nLINK/USDT  0.681814  0.734957  0.565133  0.670887   1.000000  0.757642  0.309791   0.665422  0.229266   0.413497\nADA/USDT   0.703330  0.717653  0.648923  0.681713   0.757642  1.000000  0.413841   0.723846  0.255016   0.327677\nTRX/USDT   0.381443  0.377609  0.314937  0.316931   0.309791  0.413841  1.000000   0.366327  0.180625   0.112381\nDOGE/USDT  0.811459  0.744033  0.587623  0.691400   0.665422  0.723846  0.366327   1.000000  0.311443   0.345945\nUSDT/USD   0.448836  0.281970  0.162215  0.249392   0.229266  0.255016  0.180625   0.311443  1.000000   0.239174\nONDO/USDT  0.348155  0.378043  0.226388  0.299814   0.413497  0.327677  0.112381   0.345945  0.239174   1.000000\n\n\n\n\n\n\n\n\n\n\n[Window = 60 days] Mean Correlation Matrix\n            BTC/USDT  ETH/USDT  XRP/USDT  SOL/USDT  LINK/USDT  ADA/USDT  TRX/USDT  DOGE/USDT  USDT/USD  ONDO/USDT\nBTC/USDT   1.000000  0.794220  0.512291  0.739069   0.661759  0.686840  0.315185   0.813363  0.439727   0.343463\nETH/USDT   0.794220  1.000000  0.493631  0.686624   0.707772  0.690285  0.302908   0.724075  0.288541   0.368641\nXRP/USDT   0.512291  0.493631  1.000000  0.498921   0.533043  0.627303  0.267398   0.535395  0.125374   0.201147\nSOL/USDT   0.739069  0.686624  0.498921  1.000000   0.649832  0.665617  0.269848   0.684101  0.234385   0.281718\nLINK/USDT  0.661759  0.707772  0.533043  0.649832   1.000000  0.742523  0.253491   0.636039  0.212387   0.396643\nADA/USDT   0.686840  0.690285  0.627303  0.665617   0.742523  1.000000  0.367449   0.707571  0.237445   0.310394\nTRX/USDT   0.315185  0.302908  0.267398  0.269848   0.253491  0.367449  1.000000   0.302247  0.151088   0.087378\nDOGE/USDT  0.813363  0.724075  0.535395  0.684101   0.636039  0.707571  0.302247   1.000000  0.308610   0.329430\nUSDT/USD   0.439727  0.288541  0.125374  0.234385   0.212387  0.237445  0.151088   0.308610  1.000000   0.239657\nONDO/USDT  0.343463  0.368641  0.201147  0.281718   0.396643  0.310394  0.087378   0.329430  0.239657   1.000000\n\n\n\n\n\n\n\n\n\n\n[Window = 90 days] Mean Correlation Matrix\n            BTC/USDT  ETH/USDT  XRP/USDT  SOL/USDT  LINK/USDT  ADA/USDT  TRX/USDT  DOGE/USDT  USDT/USD  ONDO/USDT\nBTC/USDT   1.000000  0.792785  0.469713  0.741462   0.643975  0.678632  0.265846   0.810099  0.425962   0.341469\nETH/USDT   0.792785  1.000000  0.446245  0.688565   0.694381  0.671355  0.237438   0.706158  0.281146   0.365844\nXRP/USDT   0.469713  0.446245  1.000000  0.452215   0.500434  0.606132  0.228845   0.490717  0.097375   0.189753\nSOL/USDT   0.741462  0.688565  0.452215  1.000000   0.633733  0.652648  0.246976   0.683198  0.235332   0.277572\nLINK/USDT  0.643975  0.694381  0.500434  0.633733   1.000000  0.727256  0.210145   0.608506  0.186046   0.384874\nADA/USDT   0.678632  0.671355  0.606132  0.652648   0.727256  1.000000  0.328958   0.693337  0.222492   0.300068\nTRX/USDT   0.265846  0.237438  0.228845  0.246976   0.210145  0.328958  1.000000   0.241865  0.123591   0.078513\nDOGE/USDT  0.810099  0.706158  0.490717  0.683198   0.608506  0.693337  0.241865   1.000000  0.296289   0.320652\nUSDT/USD   0.425962  0.281146  0.097375  0.235332   0.186046  0.222492  0.123591   0.296289  1.000000   0.242642\nONDO/USDT  0.341469  0.365844  0.189753  0.277572   0.384874  0.300068  0.078513   0.320652  0.242642   1.000000"
  },
  {
    "objectID": "projects/correlation_crypto/correlation_crypto.html#토론-discussions",
    "href": "projects/correlation_crypto/correlation_crypto.html#토론-discussions",
    "title": "Correlation within Crypto-currencies",
    "section": "토론 (Discussions)",
    "text": "토론 (Discussions)\n\n\n\n\n\n\nImportant\n\n\n\n변수들의 관찰 주기 (단기? 장기?)에 따라 또는 관찰 시기 (10년전? 지금?)에 따라 변수들 간의 선형관계는 유지되지 않을 수 있습니다. 2025년 현재 비트코인 (BTC) 가격은 시장 심리, 규제 변화, 기술적 요인 등에 크게 영향을 받고 있습니다.\n\n\n\n상관계수가 낮은 암호화폐 자산 조합을 식별하고, 헤지 투자 전략을 논의.\n특정 암호화폐 간의 높은 상관관계가 나타나는 이유 및 그에 따른 리스크 분석."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GitSAM",
    "section": "",
    "text": "내가 관심있는 것들…왜? 1."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "GitSAM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n그냥요.↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "깃샘학원\n“We Build Leaders, Not Just Winners.”\n깃샘학원의 목표는 성공한 엘리트 양성이 아니다: 단순한 시험 점수나 경제적 성공을 목표로 하지 않습니다. 깃샘학원의 목표는 훌륭한 리더 양성이다: 공감력, 윤리적 책임감, 논리적 사고력, 실질적 문제 해결 능력을 갖추어 사회적으로 선한 영향력을 발휘할 수 있는 진정한 리더 양성이 목표이다.\n\n\n문제: 고위직 엘리트들의 항진명제식 발언 분석\n\n1. 서론\n고위직 엘리트들은 정치, 법조, 경제, 경영 등 다양한 분야에서 영향력을 행사하며, 공적인 자리에서의 발언이 중요한 의미를 갖는다. 그러나 이들의 발언 중 상당수는 논리적으로 항진명제(tautology)에 가까우며, 이는 정보 비대칭을 활용한 책임 회피 전략으로 사용되는 경우가 많다. 여기 분석에서는 몇가지 대표적 사례들을 분석하고, 이러한 발언 방식을 만들어 내는 구조적 원인을 탐구하며, 개선 방안을 제안하고자 한다.\n\n\n2. 사례 분석\n\n법조 분야\n\n“최근 고위 공직자 비리 의혹 수사에 대한 검찰의 입장은?”\n\n검찰총장: “모든 수사는 법과 절차에 따라 공정하게 진행됩니다.”\n논리적 문제점: 법과 절차에 따른다는 것은 당연한 원칙이며, 구체적인 수사 진행 상황을 설명하지 않음.\n개선된 답변: “해당 의혹과 관련해 △△ 증거를 확보했으며, 3월 말까지 관계자 소환 조사를 진행할 예정입니다. 또한, 수사 결과는 국민에게 투명하게 공개할 것입니다.”\n\n\n\n\n경제 분야\n\n“금리 인상 가능성에 대해 어떻게 전망하나요?”\n\n한국은행 총재: “통화 정책은 물가 안정을 우선시할 것입니다.”\n논리적 문제점: 물가 안정을 목표로 한다는 것은 이미 알려진 사실로, 구체적 조건이 제시되지 않음.\n개선된 답변: “실업률은 5% 미만으로 유지되지만 연내 물가 상승률이 3%를 넘을 경우, 4분기 중 금리를 0.25%p 추가 인상할 계획입니다. 기준 금리 인상을 통해 물가의 안정성을 확보하고자 합니다.”\n\n\n\n\n정치 분야\n\n“차기 총선에서 여당의 승리 가능성은 어떻게 보는가?”\n\n여당 대변인: “선거 결과는 유권자의 선택에 달려 있습니다.”\n논리적 문제점: 선거 결과가 유권자의 선택에 따른다는 것은 자명한 사실이며, 선거 전략이나 예측을 제공하지 않음.\n개선된 답변: “현 지지율(45%)과 △△ 정책 공약을 바탕으로 20석 이상 확대를 목표로 합니다.”\n\n\n\n\n경영 분야\n\n“C사 제품의 안전성 논란에 대한 대응은?”\n\nC사 홍보팀장: “고객 안전을 최우선으로 삼고 있습니다.”\n논리적 문제점: 고객 안전을 최우선으로 한다는 것은 원칙적인 선언일 뿐, 실제 조치에 대한 설명이 없음.\n개선된 답변: “문제 제품 5만 개 전량 리콜하며, 피해 보상금을 제품 가격의 120%로 책정했습니다. 또한, 향후 안전성 강화를 위한 추가 조치를 마련할 것입니다.”\n\n\n\n\n\n3. 발언 방식 분석 및 원인\n\n(1) 정보 비대칭 활용 극대화\n고위직 인사들은 상대적으로 많은 정보를 보유하고 있으나, 이를 공개하지 않음으로써 해석의 여지를 남긴다. 이는 논란을 회피하고 의도적으로 불확실성을 유지하는 전략이다.\n\n\n(2) 항진명제 활용\n애매모호한 표현을 사용하여 논란을 피하는 동시에, 어느 쪽으로든 해석이 가능하도록 발언함으로써 책임을 최소화한다.\n\n\n(3) 자격 논란과 인신공격 전략\n논리적으로 반박하기 어려운 경우, 상대방의 자격을 문제 삼아 논의 자체를 흐리는 방식이 자주 활용된다.\n\n\n(4) 사회적·제도적 보상 구조\n책임을 명확히 지는 것보다 애매한 발언을 통해 책임을 회피하는 것이 생존 전략으로 작용하는 구조가 자리 잡고 있다. 이러한 환경에서는 모호한 표현이 오히려 유리하게 작용할 가능성이 크다.\n\n\n\n4. 해결 방안\n\n(1) 발언의 책임 명확화\n\n공적인 자리에서의 발언이 결과에 미치는 영향을 추적하는 피드백 시스템을 도입하여, 모호한 발언이 아니라 구체적인 답변을 유도해야 한다.\n“중대성이 낮다”고 발언한 경우, 이후 정책·법적 판단에서 이에 대한 책임을 질 수 있도록 제도적 장치를 마련해야 한다.\n\n\n\n(2) 논의 구조 개선\n\n논리적 토론을 강화하고, 인신공격을 차단하는 규칙을 도입해야 한다.\n예를 들어, 토론 중 상대의 과거나 자격을 논하는 발언이 나오면 자동으로 발언권을 제한하는 시스템을 고려할 수 있다.\n\n\n\n(3) 교육 및 사고방식 변화\n\n기계식 시험 중심이 아닌, 논리적 사고 및 창의적 문제 해결을 평가하는 교육 방식으로 변화해야 한다.\n특히, AI와 블록체인을 활용한 교육 평가 시스템을 통해 논리적 사고 과정을 명확하게 평가하는 방법이 필요하다.\n\n\n\n\n5. 결론\n고위직 엘리트들의 항진명제식 발언은 정보 비대칭을 활용한 책임 회피 전략의 일부로 작용하며, 이는 사회적·제도적 보상 구조와 결합하여 지속적으로 강화된다. 이러한 문제를 해결하기 위해서는 발언의 책임을 명확히 하고, 논의 구조를 개선하며, 교육 시스템의 변화를 통해 논리적 사고력을 강화하는 접근이 필요하다. 특히, AI와 블록체인을 활용한 평가 방식은 발언의 모호성을 줄이고 구체적인 논리적 사고를 촉진하는 데 기여할 수 있을 것이다.\n\n\n\n\n해결책: 사고력과 리더십을 키우는 혁신적 교육 프로그램\n\n혁신적 교육 및 평가 시스템 (AI & 블록체인 기반)\n\nPrompt-based Evaluation: AI를 활용하여 학생의 사고 과정 자체를 블록체인에 기록하고, 창의성·논리성·윤리적 책임감까지 정량적·객관적으로 평가\nProof of Thinking: 학습 과정을 블록체인에 저장하여 자기성찰 능력을 강화하고, 위변조 방지\n토론·윤리적 판단·전략적 사고 중심 교육: 동료와의 토론을 통해 사고력과 문제 해결력을 강화\n\n\n\n혁신적 강의 방식\n\n개념 시각화: python, desmos 등 기술을 활용하여 수학과 과학의 추상적 개념을 구체적으로 시각화\n실전 응용 모형 중심: 실전에 활용되고 있는 과학 모형들을 구체적으로 이해하고 적용하는 방식\n\n\n\n자연과학 모형 이해\n\n대수학, 기하학, 미적분학, 확률통계학\n고전역학, 통계물리학, 상대성이론, 양자역학, 유기화학, 분자생물학\n국제 경시대회(AMC 등) 및 입시 대비 문제풀이\nPython을 활용한 데이터 계산 및 시각화\n\n\n\nAI 및 기술 도구 이해와 활용법\n\nAI 활용 리서치 및 정보 검색 방법\n금융·투자 전략을 위한 선형대수학 (Google Colab 활용)\n창업·재테크를 위한 통계학 (Google Sheets 활용)\nLLM(대규모 언어 모델) 개발을 위한 확률 최적화 이론 및 미적분 해석학"
  },
  {
    "objectID": "projects/corner_solution/corner_solution.html",
    "href": "projects/corner_solution/corner_solution.html",
    "title": "Corner Solutions in Optimization Model",
    "section": "",
    "text": "1. Introduction\nIn standard economic theory, both consumer preferences and production sets are generally assumed to exhibit convexity (Arrow and Debreu 1954; Debreu 1959). This assumption supports foundational results, including the existence and uniqueness of equilibrium and the efficiency of market allocations. In practice, however, features such as network externalities (Katz and Shapiro 1985; Rochet and Tirole 2003), rent-seeking (Shleifer and Vishny 1993), and multiple equilibria—often culminating in pronounced market dominance—can produce outcomes resembling non-convex preferences (Arthur 1994). In many cases, corner solutions and path-dependent equilibria emerge from winner-takes-all dynamics, concentrated economic power, and barriers to entry.\n\n\n2. Convexity in Economic Theory\n2.1 Convex Preferences and Production Sets\n\nConsumer preferences are typically modeled with quasi-concave utility functions, yielding convex (or “bowl-shaped”) indifference curves. This setup implies a preference for diversity in consumption, rather than extreme or corner solutions (Debreu 1959).\nProducers are often assumed to face diminishing marginal returns, reflected in a convex production possibility set. Under such conditions, output expansions follow a predictable pattern, and average costs rise eventually.\n\n2.2 Existence and Efficiency of Equilibrium\n\nWith convexity, free market entry, symmetric information, and price-taking behavior, perfectly competitive markets are shown to possess a stable equilibrium that is Pareto efficient (Arrow and Debreu 1954).\nThese results typically rely on fixed-point theorems and the properties of convex sets, ensuring both the existence of equilibrium prices and (in many cases) uniqueness or stability (Debreu 1959).\n\n2.3 Normative Implications\n\nConvexity underpins the normative stance that, absent significant market failures, competitive markets gravitate toward Pareto-efficient resource allocations.\nConsequently, government interventions usually aim to correct externalities, public goods issues, or information asymmetries within a broader context of largely convex preferences and production sets.\n\n\n\n3. Non-Convexities in Reality\n3.1 Network Externalities and Increasing Returns to Scale\n\nIn contrast to diminishing returns, many digital or platform-based markets exhibit network externalities, or increasing returns to scale (Katz and Shapiro 1985; Rochet and Tirole 2003). As additional users join a platform, its value to each user grows, often driving corner solutions in both production and consumption.\nInstead of smoothly concave utility or production functions, certain markets feature segments of increasing marginal returns, leading to “winner-takes-all” or “winner-take-most” dynamics.\n\n3.2 Coordination Games and Multiple Equilibria\n\nNetwork externalities commonly create coordination games, where each agent’s optimal choice depends on the choices of others. Small initial advantages or random shocks may tip the market toward a specific product or standard, resulting in lock-in (Arthur 1994).\nSuch scenarios can produce multiple Nash equilibria, for instance everyone choosing Product A or everyone choosing Product B, with potentially large welfare differences between them.\n\n3.3 Extreme or Corner Solutions in Consumption and Production\n\nWith robust network effects, consumers or producers may converge on a single brand, platform, or location, effectively marginalizing other options—even if those alternatives might have been preferred under purely convex preferences.\nThese corner solutions deviate from the classical idea that diversification in consumption and moderate scales in production yield optimal outcomes.\n\n3.4 Rent-Seeking and Incumbent Power\n\nDominant firms or groups can exploit political influence—through lobbying or regulatory capture—to fortify their positions, reinforcing non-convex outcomes by stifling competition (Tirole 1988; Shleifer and Vishny 1993).\nRent-seeking intensifies the misallocation of resources, as efforts are diverted to defending or reinforcing incumbents’ power, often via barriers to entry, reduced competition, and growing inequalities.\n\n\n\n4. Government Interventions\n4.1 Theoretical View: Correcting Market Imperfections\n\nTraditionally, policy interventions focus on addressing market failures, assuming that preferences and technologies remain fundamentally convex and that interventions are limited and transparent.\n\n4.2 Empirical Evidence: Policy Amplifies Non-Convexities\n\nIn reality, incumbents can wield outsized influence through lobbying and political capture, prompting policies that strengthen market concentration (Tirole 1988).\nInstead of fostering genuinely competitive markets, such policies may lock in non-convex outcomes, creating a vicious cycle of entrenched monopolistic power and limited competition.\n\n4.3 Lock-in and Path Dependence\n\nWhen policy-making aligns with incumbent interests, even minor advantages can become self-reinforcing (Arthur 1994).\nConsequently, once a market tips toward a specific firm, region, or product, effective competition may prove infeasible without sweeping policy reforms or disruptive innovation.\n\n\n\n5. Conclusion\nAlthough classical economic models lean on convex preferences and technologies to assert the existence of unique, efficient equilibria, real-world dynamics often revolve around non-convex phenomena. Network externalities, coordination failures, and rent-seeking can drive corner solutions, multiple equilibria, and lock-in that preserve incumbent advantages. Far from mitigating these issues, government policies sometimes exacerbate them through preferential treatment of dominant actors. Recognizing these non-convex realities is crucial for crafting policy frameworks that transcend purely theoretical assumptions of convexity and address the path-dependent complexity characterizing modern markets.\n\n\nAppendix: Utilitarian Objective function\n\n\nCode\n#@title Utilitarian objective function\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom scipy.optimize import minimize\n\n# 함수 정의\ndef z_function(x, y, a, b):\n  return y * (x**a) + (1 - y) * ((b - x)**a)\n\n# x, y 범위 및 매개변수 설정\na = 0.3  # 매개변수 a 값 (0과 1 사이)\nb = 20  # 매개변수 b 값\n\nx = np.linspace(0, b, 100)  # x 범위: 0부터 20까지 100개의 점\ny = np.linspace(0, 1, 100)  # y 범위: 0부터 1까지 100개의 점\nX, Y = np.meshgrid(x, y)  # x, y 좌표 격자 생성\n\n\n# Z 값 계산\nZ = z_function(X, Y, a, b)\n\n\ndef negative_z_function(params):\n    x, y = params\n    return -z_function(x, y, a, b)  # 최솟값을 찾기 위해 음수 값 반환\n\n# 초기값 설정 (interior 범위 내)\ninitial_guess = [b / 2, 0.5]\n\n# 경계 조건 설정\nbounds = [(0, b), (0, 1)]\n\n# 최적화 실행\nresult = minimize(negative_z_function, initial_guess, bounds=bounds)\n\n# 결과 추출\nextreme_point_x, extreme_point_y = result.x\nextreme_point_z = z_function(extreme_point_x, extreme_point_y, a, b)\n\nprint(\"Extreme Point (x, y, z):\", extreme_point_x, extreme_point_y, extreme_point_z)\n\n# Calculate Hessian matrix\ndef hessian_matrix(x, y, a, b):\n  \"\"\"Calculates the Hessian matrix of the z_function.\"\"\"\n  d2z_dx2 = a * (a - 1) * (y * (x**(a - 2)) + (1 - y) * ((b - x)**(a - 2)))\n  d2z_dy2 = 0  # Second derivative with respect to y is 0\n  d2z_dxdy = a * (x**(a - 1) - (b - x)**(a - 1))\n  d2z_dydx = d2z_dxdy  # Mixed partial derivatives are equal\n\n  return [[d2z_dx2, d2z_dxdy], [d2z_dydx, d2z_dy2]]\n\n# Determine the type of extreme point\nhessian = hessian_matrix(extreme_point_x, extreme_point_y, a, b)\ndeterminant = np.linalg.det(hessian)\n\nif determinant &gt; 0 and hessian[0][0] &gt; 0:\n  extreme_type = \"Minimum\"\nelif determinant &gt; 0 and hessian[0][0] &lt; 0:\n  extreme_type = \"Maximum\"\nelse:\n  extreme_type = \"Saddle\"\n\nprint(\"Extreme Point Type:\", extreme_type)\n\n\n# 3D 그래프 그리기\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(X, Y, Z)\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('z')\nplt.title('3D Graph of z = y*x^a + (1-y)(b-x)^a')\n\n\n# global interior extreme point 표시\nax.scatter(extreme_point_x, extreme_point_y, extreme_point_z, color='red', marker='o', s=100)\nax.text(extreme_point_x, extreme_point_y, extreme_point_z, f'Extreme Point ({extreme_type})', color='red')\n\nplt.show()\n\n# Contour Plot 그리기\nfig, ax = plt.subplots()\ncontour = ax.contour(X, Y, Z)\nax.set_xlabel('x')\nax.set_ylabel('y')\nplt.title('Contour Plot of z = y*x^a + (1-y)(b-x)^a')\nplt.clabel(contour, inline=1, fontsize=10)\n\n# global interior extreme point 표시\nax.scatter(extreme_point_x, extreme_point_y, color='red', marker='o', s=100)\nax.text(extreme_point_x, extreme_point_y, 'Extreme Point', color='red')\n\nplt.show()\n\n\nExtreme Point (x, y, z): 10.0 0.5 1.9952623149688795\nExtreme Point Type: Saddle\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAppendix: Homogeneous function of degree 1\n\n\nCode\n# Define a return to scale\nscale = 1 # Constant return to scale, i.e. Homogeneous function of degree 1\n\n# Define parameter a\na = 1/4\n\n# total wealth of x\nk_x = 2\n# total wealth of y\nk_y = 2\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 경고 메시지 숨기기\nnp.seterr(invalid='ignore')\n\ndef numerical_derivative(f, X, Y, h=1e-5):\n    \"\"\" Compute numerical partial derivatives using central difference method.\"\"\"\n    dfdx = (f(X + h, Y) - f(X - h, Y)) / (2 * h)  # ∂f/∂x\n    dfdy = (f(X, Y + h) - f(X, Y - h)) / (2 * h)  # ∂f/∂y\n    return dfdx, dfdy\n\n# Define functions u_1(x,y) = x^a * y^(1-a) and u_2(x,y) = (2-x)(2-y)\ndef u1(x, y):\n    return x**(scale*a) * y**(scale*(1-a))\n\ndef u2(x, y):\n    return (k_x - x)**(scale*a) * (k_y - y)**(scale*(1-a))\n\n# Define the grid\nx = np.linspace(0, k_x, 15)\ny = np.linspace(0, k_y, 15)\nX, Y = np.meshgrid(x, y)\n\n# Compute the numerical derivatives (vector field components)\nU1, V1 = numerical_derivative(u1, X, Y)\nU2, V2 = numerical_derivative(u2, X, Y)\n\n# Reduce the density of vectors for better visualization\nx_sparse = np.linspace(0, k_x, 8)\ny_sparse = np.linspace(0, k_y, 8)\nX_sparse, Y_sparse = np.meshgrid(x_sparse, y_sparse)\nU1_sparse, V1_sparse = numerical_derivative(u1, X_sparse, Y_sparse)\nU2_sparse, V2_sparse = numerical_derivative(u2, X_sparse, Y_sparse)\n\n# Plot the combined vector fields and contour plots\n#plt.figure(figsize=(8, 8))\n\n# Contour plots of u_1 and u_2 (level curves only)\ncontour1 = plt.contour(X, Y, u1(X, Y), colors='blue', linestyles='solid', linewidths=1)\ncontour2 = plt.contour(X, Y, u2(X, Y), colors='red', linestyles='dashed', linewidths=1)\n\n# Overlay vector fields\nplt.quiver(X_sparse, Y_sparse, U1_sparse, V1_sparse, color='b', angles='xy', label='∇$u_1$')\nplt.quiver(X_sparse, Y_sparse, U2_sparse, V2_sparse, color='r', angles='xy', label='∇$u_2$')\n\n# Labels and grid\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Gradient Vector Fields & Level Curves of $u_1$ and $u_2$')\nplt.legend()\nplt.grid('scaled')\nplt.axis('square')\n\nplt.tight_layout()\n# Show the plot\nplt.show()\n\n# Compute the sum of gradients\nU_sum = U1 + U2\nV_sum = V1 + V2\n\n# Reduce the density of vectors for better visualization\nU_sum_sparse, V_sum_sparse = numerical_derivative(lambda x, y: u1(x, y) + u2(x, y), X_sparse, Y_sparse)\n\n# Plot the combined vector fields and contour plots\n#plt.figure(figsize=(8, 8))\n\n# Contour plots of u_1 and u_2 (level curves only)\ncontour1 = plt.contour(X, Y, u1(X, Y), colors='blue', linestyles='solid', linewidths=1)\ncontour2 = plt.contour(X, Y, u2(X, Y), colors='red', linestyles='dashed', linewidths=1)\n\n# Overlay sum of gradient vector fields\nplt.quiver(X_sparse, Y_sparse, U_sum_sparse, V_sum_sparse, color='g', angles='xy', label='∇($u_1 + u_2$)')\n\n# Labels and grid\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Sum of Gradient Vector Fields & Level Curves of $u_1$ and $u_2$')\nplt.legend()\n\nplt.grid('scaled')\nplt.axis('square')\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAppendix: Sigmoid utility function\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Define constants\nkx = (np.pi**3 / 2) ** (1/3)\nky = (2**(1/2)) * ((np.pi**3 / 2) ** (1/3))\n\n# Define the grid\nx = np.linspace(0, kx, 1000)\ny = np.linspace(0, ky, 1000)\nX, Y = np.meshgrid(x, y)\n\n# Define the functions\nu1 = 1 - np.cos(X**(1/3) * Y**(2/3))\nu2 = 1 - np.cos((kx - X)**(1/3) * (ky - Y)**(2/3))\n\n# Find intersection points where u1 == u2\nthreshold = 1e-3  # Numerical tolerance for equality\nintersection_mask = np.abs(u1 - u2) &lt; threshold\nX_intersect = X[intersection_mask]\nY_intersect = Y[intersection_mask]\nZ_intersect = u1[intersection_mask]  # u1 and u2 are nearly equal\n\n# Create 3D plot\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection='3d')\n\n# Plot intersection line\nax.scatter(X_intersect, Y_intersect, Z_intersect, color='black', s=10, label='Intersection Line')\n\n# Surface plots for reference\nax.plot_surface(X, Y, u1, cmap='Blues', alpha=0.5)\nax.plot_surface(X, Y, u2, cmap='Reds', alpha=0.5)\n\n# Labels and title\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\nax.set_title('3D Intersection of $u_1$ and $u_2$')\nax.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nArrow, Kenneth J., and Gerard Debreu. 1954. “Existence of an Equilibrium for a Competitive Economy.” Econometrica 22 (3): 265–90.\n\n\nArthur, W. Brian. 1994. Increasing Returns and Path Dependence in the Economy. Ann Arbor, MI: University of Michigan Press.\n\n\nDebreu, Gerard. 1959. Theory of Value: An Axiomatic Analysis of Economic Equilibrium. New Haven, CT: Yale University Press.\n\n\nKatz, Michael L., and Carl Shapiro. 1985. “Network Externalities, Competition, and Compatibility.” The American Economic Review 75 (3): 424–40.\n\n\nRochet, Jean-Charles, and Jean Tirole. 2003. “Platform Competition in Two-Sided Markets.” Journal of the European Economic Association 1 (4): 990–1029.\n\n\nShleifer, Andrei, and Robert W. Vishny. 1993. “Corruption.” The Quarterly Journal of Economics 108 (3): 599–617.\n\n\nTirole, Jean. 1988. The Theory of Industrial Organization. Cambridge, MA: MIT Press."
  },
  {
    "objectID": "projects/dichotomy/dichotomy.html",
    "href": "projects/dichotomy/dichotomy.html",
    "title": "Identifying the Most Significant Dichotomy",
    "section": "",
    "text": "Wealth concentration and inequality have long been central issues in economics, attracting renewed attention as wealth disparities continue to widen. The well-documented Pareto Principle suggests that the top 20% holds approximately 80% of total wealth, but empirical studies indicate that this ratio has become even more skewed over time (Piketty and Saez 2003; Alvaredo et al. 2013; Saez and Zucman 2016). Such trends raise fundamental questions about the underlying dynamics of wealth distribution: Is wealth accumulation a zero-sum game? If so, what is the most evident dichotomy that best exposes this phenomenon?\nAt every moment, newly created capital is introduced into the economic system through production, subsequently distributed among economic agents, and then either dissipated through consumption or retained through investment. Unlike a static framework where total wealth is fixed, real-world economic systems operate as dynamic entities where total wealth evolves over time. The interplay between production, distribution, consumption, and investment determines how wealth is allocated and reallocated across different socioeconomic groups. If the system maintains a stable and proportional allocation of new wealth over time, the relative shares of net worth for different groups remain unchanged. However, if allocation mechanisms fluctuate, then the wealth share of one group necessarily increases at the expense of another—reinforcing a zero-sum dynamic.\nThe key question, therefore, is: How should we partition the population to most clearly expose this zero-sum characteristic? In other words, at which wealth cutoff percentile do we observe the highest absolute correlation between two complementary wealth groups? Should we divide the population into the top 10% versus the bottom 90%? Or is a more extreme division, such as top 0.1% versus bottom 99.9%, more effective in revealing these dynamics?"
  },
  {
    "objectID": "projects/dichotomy/dichotomy.html#introduction",
    "href": "projects/dichotomy/dichotomy.html#introduction",
    "title": "Identifying the Most Significant Dichotomy",
    "section": "",
    "text": "Wealth concentration and inequality have long been central issues in economics, attracting renewed attention as wealth disparities continue to widen. The well-documented Pareto Principle suggests that the top 20% holds approximately 80% of total wealth, but empirical studies indicate that this ratio has become even more skewed over time (Piketty and Saez 2003; Alvaredo et al. 2013; Saez and Zucman 2016). Such trends raise fundamental questions about the underlying dynamics of wealth distribution: Is wealth accumulation a zero-sum game? If so, what is the most evident dichotomy that best exposes this phenomenon?\nAt every moment, newly created capital is introduced into the economic system through production, subsequently distributed among economic agents, and then either dissipated through consumption or retained through investment. Unlike a static framework where total wealth is fixed, real-world economic systems operate as dynamic entities where total wealth evolves over time. The interplay between production, distribution, consumption, and investment determines how wealth is allocated and reallocated across different socioeconomic groups. If the system maintains a stable and proportional allocation of new wealth over time, the relative shares of net worth for different groups remain unchanged. However, if allocation mechanisms fluctuate, then the wealth share of one group necessarily increases at the expense of another—reinforcing a zero-sum dynamic.\nThe key question, therefore, is: How should we partition the population to most clearly expose this zero-sum characteristic? In other words, at which wealth cutoff percentile do we observe the highest absolute correlation between two complementary wealth groups? Should we divide the population into the top 10% versus the bottom 90%? Or is a more extreme division, such as top 0.1% versus bottom 99.9%, more effective in revealing these dynamics?"
  },
  {
    "objectID": "projects/dichotomy/dichotomy.html#data",
    "href": "projects/dichotomy/dichotomy.html#data",
    "title": "Identifying the Most Significant Dichotomy",
    "section": "Data",
    "text": "Data\nOur empirical analysis is based on FRED (Federal Reserve Economic Data), covering quarterly observations from 1989 to 2024. The dataset is structured into wealth brackets representing net wealth shares at different percentile levels:\nGroups (stars)\n\n\\(X_4(t)\\): Share of Net Worth Held by the Top 0.1% (99.9th to 100th Wealth Percentiles) (WFRBSTP1300)\n\\(X_3(t)\\): Share of Net Worth Held by the 99th to 99.9th Wealth Percentiles (WFRBS99T999273)\n\nc.f. Share of Net Worth Held by the Top 1% (99th to 100th Wealth Percentiles) (WFRBST01134)\n\n\\(X_2(t)\\): Share of Net Worth Held by the 90th to 99th Wealth Percentiles (WFRBSN09161)\n\\(X_1(t)\\): Share of Net Worth Held by the 50th to 90th Wealth Percentiles (WFRBSN40188)\n\\(X_0(t)\\): Share of Net Worth Held by the Bottom 50% (1st to 50th Wealth Percentiles) (WFRBSB50215)\n\nAdditionally, we reference wealth cutoff amount to identify the minimum level of wealth required to belong to specific top percentile groups:\nCutoff Levels (bins)\n\n\\(p_4\\) or 99.9th: Minimum Wealth Cutoff for the Top 0.1% (99.9th to 100th Wealth Percentiles) (WFRBLTP1311)\n\\(p_3\\) or 99th: Minimum Wealth Cutoff for the 99th to 99.9th Wealth Percentiles (WFRBL99T999309)\n\\(p_2\\) or 90th: Minimum Wealth Cutoff for the 90th to 99th Wealth Percentiles (WFRBLN09304)\n\\(p_1\\) or 50th: Minimum Wealth Cutoff for the 50th to 90th Wealth Percentiles (WFRBLN40302)"
  },
  {
    "objectID": "projects/dichotomy/dichotomy.html#methodology",
    "href": "projects/dichotomy/dichotomy.html#methodology",
    "title": "Identifying the Most Significant Dichotomy",
    "section": "Methodology",
    "text": "Methodology\nGiven that total share of net wealth must always sum to one, any partition of the population into two groups remains complementary: \\[\nX_0 + X_1 + X_2 + X_3 + X_4 = 1.\n\\]\nTo quantify the most evident dichotomy, we define two complementary wealth groups for different percentile cutoffs:\n\nWhen \\(p = p_1\\):\n\\[\n  a(t) = X_4(t)+X_3(t)+X_2(t)+X_1(t), \\quad b(t) = X_0(t).\n\\]\nWhen \\(p = p_2\\):\n\\[\n  a(t) = X_4(t)+X_3(t)+X_2(t), \\quad b(t) = X_1(t)+X_0(t).\n\\]\nWhen \\(p = p_3\\):\n\\[\n  a(t) = X_4(t)+X_3(t), \\quad b(t) = X_2(t)+X_1(t)+X_0(t).\n\\]\nWhen \\(p = p_4\\):\n\\[\n  a(t) = X_4(t), \\quad b(t) = X_3(t)+X_2(t)+X_1(t)+X_0(t).\n\\]\n\nFor each cutoff \\(p\\), we compute the correlation:\n\\[\n   y(p) = \\mathrm{corr}\\bigl(a(t),\\,b(t)\\bigr).\n\\]\nWe seek the wealth cutoff \\(p\\) that maximizes the absolute correlation \\(|y(p)|\\), revealing the strongest inverse relationship between the two resulting wealth groups. A high absolute correlation suggests that fluctuations in one group’s net worth share are systematically mirrored by the other, reinforcing the zero-sum nature of wealth accumulation. This dichotomy provides insight into how different capital accumulation mechanisms—through labor or capital investment—shape long-term wealth distribution.\nStrong inverse correlations at certain percentiles may indicate critical thresholds where redistribution policies—such as capital taxation or inheritance taxation—could have the most pronounced effects (Piketty 2011; Piketty, Saez, and Stantcheva 2014; Saez and Zucman 2016)."
  },
  {
    "objectID": "projects/dichotomy/dichotomy.html#empirical-results",
    "href": "projects/dichotomy/dichotomy.html#empirical-results",
    "title": "Identifying the Most Significant Dichotomy",
    "section": "Empirical Results",
    "text": "Empirical Results\nAfter processing the quarterly dataset (excluding missing values), we find that the 90th percentile cutoff (\\(p_2\\)) exhibits the highest absolute correlation between the two complementary wealth groups. Specifically, as of 2022-07-01, the minimum wealth required to be in the top 10% was approximately $2,152,788.\nThis suggests that dividing the population into top 10% vs. bottom 90% most effectively reveals the zero-sum nature of wealth redistribution, compared to other partitions such as top 50% vs. bottom 50% or top 0.1% vs. bottom 99.9%.\nThese findings imply that the most structurally significant wealth division occurs between the top 10% and the rest, rather than between the ultra-rich and lower percentiles. This observation aligns with broader discussions on wealth polarization, where the top 10% increasingly dominates capital ownership while the bottom 90% exhibits a more recessive trajectory."
  },
  {
    "objectID": "projects/dichotomy/dichotomy.html#conclusions",
    "href": "projects/dichotomy/dichotomy.html#conclusions",
    "title": "Identifying the Most Significant Dichotomy",
    "section": "Conclusions",
    "text": "Conclusions\nThis study demonstrates that partitioning the population at the 90th wealth percentile provides the most evident dichotomy in revealing the zero-sum nature of capital accumulation. Over time, wealth redistribution mechanisms result in strong inverse correlations between net worth shares of different groups, underscoring the struggling aspects of capital accumulation. The analysis suggests that the structural division between the top 10% and the bottom 90% is more significant than commonly assumed top 1% vs. bottom 99% splits, reinforcing the notion that wealth concentration extends beyond the ultra-rich and affects broader socioeconomic strata.\nThese findings hold important implications for public policy, particularly in debates surrounding progressive taxation, capital gains policies, and inheritance tax structures. A strong inverse correlation at the 90th percentile threshold suggests that redistributive policies targeted at this level could have significant implications for long-term wealth dynamics. This aligns with prior research emphasizing the role of tax policy in shaping wealth accumulation patterns and mitigating excessive concentration of economic power (Piketty, Saez, and Stantcheva 2014; Saez and Zucman 2016).\nWhile this study primarily focuses on empirical correlation analysis, future research should explore additional macroeconomic variables to refine our understanding of wealth distribution dynamics. Incorporating GDP growth rates, investment patterns, labor market structures, and monetary policy changes may provide further insights into how systemic wealth flows evolve in response to economic shocks and policy interventions. Additionally, extending the dataset to include international comparisons could offer a broader perspective on whether the 90th percentile threshold serves as a critical inflection point for wealth inequality across different economies. Further research integrating both empirical and theoretical approaches will be essential in developing more effective strategies for addressing wealth concentration and economic mobility.\n\n\nCode\nimport pandas as pd\nimport pandas_datareader.data as web\nimport matplotlib.pyplot as plt\n\n# 데이터 기간 설정\nstart_date = '1989-07-01'\nend_date = '2024-07-01'\n\n# FRED 데이터 가져오기\nseries_ids = {\n    'X_4': 'WFRBSTP1300',\n    'X_3': 'WFRBS99T999273',\n    'X_2': 'WFRBSN09161',\n    'X_1': 'WFRBSN40188',\n    'X_0': 'WFRBSB50215'\n}\n\ndata = pd.DataFrame()\nfor name, series_id in series_ids.items():\n    data[name] = web.DataReader(series_id, 'fred', start_date, end_date)\n\n# 결측값 제거\ndata.dropna(inplace=True)\n\n\n# FRED에서 wealth level 데이터 가져오기\nwealth_level_ids = {\n    1: 'WFRBLN40302',\n    2: 'WFRBLN09304',\n    3: 'WFRBL99T999309',\n    4: 'WFRBLTP1311'\n}\n\nwealth_levels = {}\nfor p, series_id in wealth_level_ids.items():\n    latest_data = web.DataReader(series_id, 'fred', start_date, end_date).dropna().iloc[-1]\n    wealth_levels[p] = (latest_data.name, latest_data.iloc[0])  # 날짜와 값을 함께 저장\n    \n# 총 관측치 수 출력\nprint(f\"Total number of observations after removing NaN values: {len(data)}\")\n\n\nTotal number of observations after removing NaN values: 141\n\n\n\n\nCode\n# 상관관계 계산 함수\ndef calculate_correlation(a, b):\n    return a.corr(b)\n\n# 상관관계 계산\ncorrelations = {\n    1: calculate_correlation(data['X_4'] + data['X_3'] + data['X_2'] + data['X_1'], data['X_0']),\n    2: calculate_correlation(data['X_4'] + data['X_3'] + data['X_2'], data['X_1'] + data['X_0']),\n    3: calculate_correlation(data['X_4'] + data['X_3'], data['X_2'] + data['X_1'] + data['X_0']),\n    4: calculate_correlation(data['X_4'], data['X_3'] + data['X_2'] + data['X_1'] + data['X_0'])\n}\n\n# 결과 출력\nfor p, y in correlations.items():\n    print(f\"Correlation for p_{p}: {y:.4f}\")\n\n# 최소 상관관계를 가지는 wealth percentile 찾기\nmin_corr_p = min(correlations, key=correlations.get)\npercentile_map = {1: \"50th\", 2: \"90th\", 3: \"99th\", 4: \"99.9th\"}\nprint(f\"Minimum correlation is at p_{min_corr_p}: {percentile_map[min_corr_p]}\")\nwealth_date, wealth_level = wealth_levels[min_corr_p]\nprint(f\"Wealth level for {percentile_map[min_corr_p]} percentile on {wealth_date.date()}: ${wealth_level:,.0f}\")\n\n\n# 그래프 표현\np_values = list(correlations.keys())\ny_values = list(correlations.values())\n\nplt.plot(p_values, y_values, marker='o')\nplt.xlabel('Wealth Cutoff (p)')\nplt.ylabel('Correlation (y(p))')\nplt.title('Wealth Cutoff vs Correlation')\nplt.grid(True)\nplt.show()\n\n\nCorrelation for p_1: -0.9980\nCorrelation for p_2: -0.9998\nCorrelation for p_3: -0.9996\nCorrelation for p_4: -0.9987\nMinimum correlation is at p_2: 90th\nWealth level for 90th percentile on 2022-07-01: $2,152,788"
  },
  {
    "objectID": "projects/pareto_index/pareto_index.html",
    "href": "projects/pareto_index/pareto_index.html",
    "title": "Approximating Wealth Distribution",
    "section": "",
    "text": "The Pareto distribution has long been used to model wealth or income distributions, starting from the seminal work of Vilfredo Pareto in the late 19th century (Pareto 1964). One of the most intuitive ways to visualize the inequality within such a distribution is through the Lorenz curve, introduced by Lorenz in 1905 (Lorenz 1905). The Lorenz curve describes the cumulative proportion of total wealth (or resources) held by the bottom fraction of the population. Closely related to the Lorenz curve is the Gini coefficient, which provides a scalar measure of inequality (Gini 1912).\nThis document first derives the Lorenz curve for a Pareto distribution, then demonstrates how to estimate the Pareto index (shape parameter) from Lorenz curve observations. We additionally relate the Pareto distribution to Constant Relative Risk Aversion (CRRA) utility function, highlighting the same shared structure in a differential equation that governs both."
  },
  {
    "objectID": "projects/pareto_index/pareto_index.html#introduction",
    "href": "projects/pareto_index/pareto_index.html#introduction",
    "title": "Approximating Wealth Distribution",
    "section": "",
    "text": "The Pareto distribution has long been used to model wealth or income distributions, starting from the seminal work of Vilfredo Pareto in the late 19th century (Pareto 1964). One of the most intuitive ways to visualize the inequality within such a distribution is through the Lorenz curve, introduced by Lorenz in 1905 (Lorenz 1905). The Lorenz curve describes the cumulative proportion of total wealth (or resources) held by the bottom fraction of the population. Closely related to the Lorenz curve is the Gini coefficient, which provides a scalar measure of inequality (Gini 1912).\nThis document first derives the Lorenz curve for a Pareto distribution, then demonstrates how to estimate the Pareto index (shape parameter) from Lorenz curve observations. We additionally relate the Pareto distribution to Constant Relative Risk Aversion (CRRA) utility function, highlighting the same shared structure in a differential equation that governs both."
  },
  {
    "objectID": "projects/pareto_index/pareto_index.html#deriving-the-lorenz-curve-of-a-pareto-distribution",
    "href": "projects/pareto_index/pareto_index.html#deriving-the-lorenz-curve-of-a-pareto-distribution",
    "title": "Approximating Wealth Distribution",
    "section": "2. Deriving the Lorenz Curve of a Pareto Distribution",
    "text": "2. Deriving the Lorenz Curve of a Pareto Distribution\n\n2.1. Definition of the Lorenz Curve\nFor a continuous distribution of wealth (X), the Lorenz curve \\(L(p)\\) is defined as the fraction of total wealth owned by the bottom \\(p\\) fraction of the population:\n\\[\nL(p) \\;=\\; \\frac{\\int_{x_m}^{x(p)} x\\,f(x)\\,dx}{\\int_{x_m}^{\\infty} x\\,f(x)\\,dx},\n\\]\nwhere\n\n\\(p = F\\bigl(x(p)\\bigr)\\) is the cumulative proportion of individuals with wealth below \\(x(p)\\),\nThe numerator represents the cumulative wealth of the bottom \\(p\\) fraction,\nThe denominator represents the total wealth in the system, given by the expected value of \\(X\\) over its support.\n\n\n\n2.2. Pareto Distribution\nA Pareto distribution with shape parameter \\(\\alpha&gt;0\\) and scale parameter \\(x_m&gt;0\\) is defined by the PDF\n\\[\nf(x) \\;=\\; \\alpha\\,\\frac{x_m^\\alpha}{x^{\\alpha+1}},\n\\quad x \\ge x_m,\n\\]\nand the corresponding CDF\n\\[\nF(x) \\;=\\; 1 \\;-\\;\\Bigl(\\tfrac{x_m}{x}\\Bigr)^\\alpha,\n\\quad x \\ge x_m.\n\\]\nEquivalently, for \\(0 &lt; p &lt; 1\\), the quantile \\(x(p)\\) satisfying \\(F\\bigl(x(p)\\bigr)=p\\) is\n\\[\np \\;=\\; 1 \\;-\\;\\Bigl(\\tfrac{x_m}{x(p)}\\Bigr)^\\alpha\n\\;\\;\\Longleftrightarrow\\;\\;\nx(p) \\;=\\; \\frac{x_m}{\\bigl(1 - p\\bigr)^{1/\\alpha}}.\n\\]\n\n\n2.3. Total Wealth\nLet the total wealth be \\(W_{\\text{total}} = E[X]\\), the expected value of \\(X\\). Substituting the PDF of the Pareto distribution into the definition of expectation,\n\\[\nE[X]\n\\;=\\; \\int_{x_m}^{\\infty} x\\,f(x)\\,dx\n\\;=\\; \\int_{x_m}^{\\infty} x \\,\\alpha\\,\\frac{x_m^\\alpha}{x^{\\alpha+1}}\\,dx\n\\;=\\; \\alpha\\,x_m^\\alpha \\int_{x_m}^{\\infty} x^{-\\alpha}\\,dx.\n\\]\nFor \\(\\alpha&gt;1\\), the improper integral converges and we obtain\n\\[\nW_{\\text{total}}\n\\;=\\; E[X]\n\\;=\\; \\frac{\\alpha\\,x_m}{\\alpha - 1}.\n\\]\n\n\n2.4. Cumulative Wealth for the Bottom \\(p\\) Fraction\nThe cumulative wealth held by the bottom \\(p\\) fraction is\n\\[\nW(p)\n\\;=\\;\\int_{x_m}^{x(p)} x\\,f(x)\\,dx\n\\;=\\;\\alpha\\,x_m^\\alpha \\int_{x_m}^{x(p)} x^{-\\alpha}\\,dx\n\\;=\\;\\alpha\\,x_m^\\alpha\n\\Bigl[\\frac{x^{-\\alpha+1}}{-\\alpha+1}\\Bigr]_{x_m}^{x(p)}.\n\\]\nSimplifying,\n\\[\nW(p)\n\\;=\\; \\frac{\\alpha\\,x_m}{\\alpha - 1}\n\\;\\Bigl(1 - \\bigl(\\tfrac{x_m}{x(p)}\\bigr)^{\\alpha - 1}\\Bigr).\n\\]\n\n\n2.5. Lorenz Curve for the Pareto Distribution\nBy definition,\n\\[\nL(p)\n\\;=\\; \\frac{W(p)}{W_{\\text{total}}}\n\\;=\\; \\frac{\\frac{\\alpha\\,x_m}{\\alpha - 1}\\,\\Bigl(1 - \\bigl(\\tfrac{x_m}{x(p)}\\bigr)^{\\alpha - 1}\\Bigr)}\n            {\\frac{\\alpha\\,x_m}{\\alpha - 1}}\n\\;=\\; 1 - \\bigl(1 - p\\bigr)^{\\frac{\\alpha - 1}{\\alpha}}.\n\\]\nHence, for a Pareto distribution, the Lorenz curve is\n\\[\nL(p) \\;=\\; 1 \\;-\\; \\bigl(1 - p\\bigr)^{\\tfrac{\\alpha - 1}{\\alpha}}.\n\\]\n\nIf \\(\\alpha \\gg 1\\), the distribution is more equal, and \\(L(p)\\) is closer to the 45-degree line of perfect equality.\n\nIf \\(\\alpha\\) is only slightly larger than 1, the distribution is more unequal, with significant concentration of wealth in the upper tail."
  },
  {
    "objectID": "projects/pareto_index/pareto_index.html#estimating-the-pareto-index-from-the-lorenz-curve",
    "href": "projects/pareto_index/pareto_index.html#estimating-the-pareto-index-from-the-lorenz-curve",
    "title": "Approximating Wealth Distribution",
    "section": "3. Estimating the Pareto Index from the Lorenz Curve",
    "text": "3. Estimating the Pareto Index from the Lorenz Curve\nSuppose empirical data or external studies indicate specific points \\((p, L(p))\\) on the Lorenz curve. We can use\n\\[\nL(p) \\;=\\; 1 - (1 - p)^{\\frac{\\alpha - 1}{\\alpha}}\n\\]\nto solve numerically for \\(\\alpha\\). Commonly cited examples:\nSeveral Empirical Illustrations\n\nPareto 80:20 Rule: \\(p=0.80\\), \\(L(p)=0.20\\)\n\\(\\Rightarrow\\) \\(\\alpha \\approx \\frac{\\ln(4)}{\\ln(5)} \\approx 1.16\\).\nPareto 90:10 Rule: \\(p=0.90\\), \\(L(p)=0.10\\)\n\\(\\Rightarrow\\) \\(\\alpha \\approx 1.05\\).\nU.S. Stock Market: According to a report by Axios (2024), the top 10% own about 93% of total equity wealth, implying \\(p=0.90\\) and \\(L(p)=0.07\\). Solving yields \\(\\alpha \\approx 1.03\\).\n\nCredit Suisse Global Wealth Report: In 2013, it was reported that the top 1% control about 50% of global wealth, which implies \\(p=0.99\\) and \\(L(p)=0.50\\). Solving gives \\(\\alpha \\approx 1.18\\). Additionally, the top 10% were said to own about 85% of global wealth (\\(p=0.90\\), \\(L(p)=0.15\\)), giving \\(\\alpha \\approx 1.08\\). Comparing such estimates across years (e.g., 2013 vs. 2020) can reveal the time dynamics of the global wealth distribution (Credit Suisse 2013, 2020)."
  },
  {
    "objectID": "projects/pareto_index/pareto_index.html#the-gini-coefficient",
    "href": "projects/pareto_index/pareto_index.html#the-gini-coefficient",
    "title": "Approximating Wealth Distribution",
    "section": "4. The Gini Coefficient",
    "text": "4. The Gini Coefficient\nThe Gini coefficient is a measure of wealth or income inequality that is closely related to the Lorenz curve. The Gini coefficient is defined as the ratio of the area between the Lorenz curve and the 45-degree equality line to the total area under the 45-degree line. Mathematically, the Gini coefficient ( G ) is given by:\n\\[\nG = 1 - 2 \\int_0^1 L(p) \\, dp.\n\\]\nSubstituting the Lorenz curve for a Pareto distribution:\n\\[\nL(p) = 1 - (1 - p)^{\\frac{\\alpha - 1}{\\alpha}},\n\\]\nwe obtain:\n\\[\nG = 1 - 2 \\int_0^1 \\big[1 - (1 - p)^{\\frac{\\alpha - 1}{\\alpha}}\\big] \\, dp = \\frac{1}{2\\alpha - 1}.\n\\]\nThus, for a Pareto distribution, the Gini coefficient is:\n\\[\nG = \\frac{1}{2\\alpha - 1}, \\quad \\text{for } \\alpha &gt; \\frac{1}{2}.\n\\]\nSome features of the Gini coefficient:\n\nAs \\(\\alpha \\to 1^+\\), the Gini coefficient approaches 1, indicating extreme inequality (a few individuals hold nearly all the wealth).\nAs \\(\\alpha \\to \\infty\\), the Gini coefficient approaches 0, indicating perfect equality.\nFor typical empirical values of \\(\\alpha\\) in wealth distributions (e.g., 1.1 to 1.8), the Gini coefficient ranges from 0.83 to 0.38, reflecting significant inequality\nRelative measure: \\(G\\) compares the distribution to perfect equality, but does not capture absolute differences.\n\nNon‐additivity: One cannot simply average the Gini coefficients of subpopulations to obtain an overall Gini coefficient.\n\nSensitivity: The Gini coefficient is sensitive to changes in the middle of the distribution, but less so at the tails."
  },
  {
    "objectID": "projects/pareto_index/pareto_index.html#pareto-distribution-and-crra-utility",
    "href": "projects/pareto_index/pareto_index.html#pareto-distribution-and-crra-utility",
    "title": "Approximating Wealth Distribution",
    "section": "5. Pareto Distribution and CRRA Utility",
    "text": "5. Pareto Distribution and CRRA Utility\nIn microeconomic theory, a widely used utility specification is the Constant Relative Risk Aversion (CRRA) form (Arrow et al. 1974; Pratt 1978). The CRRA utility function \\(u(x)\\) satisfies\n\\[\n-\\frac{x\\,u''(x)}{u'(x)} \\;=\\; \\gamma,\n\\]\nwhere \\(\\gamma&gt;0\\) is the coefficient of relative risk aversion. Solving for \\(u(x)\\) under boundary conditions such as \\(u(1)=0\\) yields:\n\nIf \\(\\gamma=1\\), \\(u(x)=\\ln x\\). (using the L’hopital’s Rule)\nIf \\(\\gamma\\neq 1\\), \\(u(x)=\\frac{x^{\\,1-\\gamma}-1}{\\,1-\\gamma\\,}\\).\n\nLarger \\(\\gamma\\) indicates higher risk aversion, while \\(\\gamma=0\\) corresponds to risk neutrality (\\(u(x)=x\\)).\nA Pareto PDF can also be derived from a differential equation with similar form. If \\(f(x)\\) is the PDF of a Pareto random variable on \\(x\\ge x_m&gt;0\\), one can write:\n\\[\n-\\frac{x\\,f'(x)}{\\,f(x)\\!}\\;=\\;(1+\\alpha)\\,x_m^{\\alpha},\n\\]\nwhich likewise has a “power‐law” solution structure. Thus, Pareto distributions and CRRA utilities each emerge from a linear differential equation of analogous form, underscoring a conceptual parallel in how “power‐type” functional solutions can appear in both economic choice models (through marginal utility) and in heavy‐tailed probability distributions.\nFurthermore, in mainstream economic theory, marginal utility \\(u'(x)\\) is assumed to be strictly positive, and \\(u''(x)\\) typically negative (diminishing marginal utility). In probability theory, any valid PDF \\(f(x)\\) must be positive, and for heavy‐tailed distributions like Pareto, \\(f(x)\\) decreases for large \\(x\\). These parallels lead to a one‐to‐one analogy between certain types of declining utilities and distributions whose density functions also decline in \\(x\\).\n\nRemark: There is a well‐known relationship via logarithmic transforms: if \\(X\\) is Pareto(\\(x_m,\\alpha\\)), then \\(Y=\\ln(X/x_m)\\) is exponentially distributed with rate \\(\\alpha\\). This exponential distribution also arises from a first‐order linear differential equation, reinforcing these structural similarities."
  },
  {
    "objectID": "projects/pareto_index/pareto_index.html#conclusion",
    "href": "projects/pareto_index/pareto_index.html#conclusion",
    "title": "Approximating Wealth Distribution",
    "section": "6. Conclusion",
    "text": "6. Conclusion\nBecause the Pareto distribution has only two parameters (\\(x_m\\) and \\(\\alpha\\)), even minimal distributional data—such as “the bottom 80% own 20% of total wealth”—enables one to solve directly for the Pareto index \\(\\alpha\\). This simplicity makes the Pareto distribution a convenient model or approximation for global wealth distribution, although in practice the estimated \\(\\alpha\\) can vary greatly depending on the dataset, sampling, and specific segment of the population observed.\nIn wealth and income distribution analysis, pairing empirical Lorenz curves with Pareto modeling remains a powerful—if simplified—approach to gauging inequality. For both theoretical and practical reasons, it continues to be integral in economic research, policy discussions, and broader studies of social welfare. Meanwhile, connections to CRRA utility function illustrate that core economic principles and certain types of heavy‐tailed probabilistic behavior can share similar mathematical underpinnings."
  },
  {
    "objectID": "time-series/indicator_growth.html",
    "href": "time-series/indicator_growth.html",
    "title": "경제 성장을 대표하는 지표?",
    "section": "",
    "text": "중앙은행이 대기업(‘고래’ 기업)들에게만 몰빵으로 현금 유동성을 공급하고, 이 기업들은 새로운 회사를 설립하고 그 자금을 투자에 사용한다. 투자된 돈이 중위 소득 상승과 소비로 연결되지 않고 다시 주식시장으로 유입되어 주가가 상승하면, 명목 GDP(Nominal GDP)는 증가할 수 있다. 하지만 이러한 명목 GDP 증가는 실제 경제 성장(real growth)을 반영하는 것일까?\n\n\n\n현금 유동성 공급으로 새롭게 만들어진 기업들이 소비자물가지수(CPI) 산정에 포함되지 않는 사업(B2B, 금융, AI, 데이터센터 등)을 한다면, CPI는 거의 변하지 않을 가능성이 매우 높다. 결과적으로, 명목 GDP 증가가 실질 GDP(Real GDP) 증가로 보이는 착시가 발생할 수 있다. 즉, 통계적으로 경제가 성장한 것처럼 보일 수 있지만, 실질적으로 대다수의 생활 수준은 나아지지 않아졌을 가능성이 매우 높다.\n\n\n\n공리주의 사회에서 경제 성장의 핵심 목표는 단순한 GDP 증가가 아니라 대다수 국민의 실질적 생활 수준 향상이다. 이를 더 잘 반영하는 지표는 Real GDP per capita 보다 Real Median Wage(실질 중위임금) 이 될 수 있다.\n\nReal GDP per capita는 평균적인 경제 생산량 증가를 보여주지만, 실제로 국민 개개인의 소득 분배를 고려하지 않는다.\n\nReal Median Wage는 국민 대다수가 체감하는 실질 소득의 변화를 나타내므로, 보다 정확한 경제 성장 지표가 될 수 있다.\n\n만약 명목 GDP가 증가하지만 중위임금이 정체되거나 하락한다면, 이는 대기업과 자산가 중심의 성장일 뿐, 일반 국민들에게 혜택이 돌아가지 않는 성장일 가능성이 크다.\n\n\n\n\n경제 성장의 지속 가능성을 예측하는 데 있어, 현재 사용되는 Natural GDP Growth Rate(자연 경제 성장률) 모델은 구조적인 문제를 가질 수 있다.\n- 이런 모델들은 과거 데이터를 기반으로 경제 성장률을 예측하지만, 금융시장 및 소비자 행동의 비선형적 변화를 제대로 반영하지 못한다.\n- 반면 Consumer Sentiment Index(소비자 심리 지수) 는 실시간으로 소비자들의 기대와 시장 심리를 측정한다.\n- 즉, 미래의 경제 성장 가능성을 예측하는 데 있어, 소비자들이 체감하는 경제 신뢰도와 구매 의향이 더 실질적인 지표가 될 수 있다.\n\n\n\n\n명목 GDP와 실질 GDP만으로 경제 성장을 평가하는 것은 한계가 있다.\n\nReal Median Wage가 상승하지 않는다면, GDP 성장은 일부 계층에만 혜택을 주는 비대칭적 성장일 가능성이 크다.\n경제 전망을 판단할 때, Consumer Sentiment Index와 같은 체감 지표가 전통적인 성장률 모델보다 더 현실적인 통찰을 제공할 수 있다.\n\n결국, 우리가 고민해야 할 것은 단순한 GDP 증가가 아니라 ’국민 개개인이 체감하는 실질적 경제 성장’이다."
  },
  {
    "objectID": "time-series/indicator_growth.html#real-gdp-vs.-real-median-wage",
    "href": "time-series/indicator_growth.html#real-gdp-vs.-real-median-wage",
    "title": "경제 성장을 대표하는 지표?",
    "section": "",
    "text": "중앙은행이 대기업(‘고래’ 기업)들에게만 몰빵으로 현금 유동성을 공급하고, 이 기업들은 새로운 회사를 설립하고 그 자금을 투자에 사용한다. 투자된 돈이 중위 소득 상승과 소비로 연결되지 않고 다시 주식시장으로 유입되어 주가가 상승하면, 명목 GDP(Nominal GDP)는 증가할 수 있다. 하지만 이러한 명목 GDP 증가는 실제 경제 성장(real growth)을 반영하는 것일까?\n\n\n\n현금 유동성 공급으로 새롭게 만들어진 기업들이 소비자물가지수(CPI) 산정에 포함되지 않는 사업(B2B, 금융, AI, 데이터센터 등)을 한다면, CPI는 거의 변하지 않을 가능성이 매우 높다. 결과적으로, 명목 GDP 증가가 실질 GDP(Real GDP) 증가로 보이는 착시가 발생할 수 있다. 즉, 통계적으로 경제가 성장한 것처럼 보일 수 있지만, 실질적으로 대다수의 생활 수준은 나아지지 않아졌을 가능성이 매우 높다.\n\n\n\n공리주의 사회에서 경제 성장의 핵심 목표는 단순한 GDP 증가가 아니라 대다수 국민의 실질적 생활 수준 향상이다. 이를 더 잘 반영하는 지표는 Real GDP per capita 보다 Real Median Wage(실질 중위임금) 이 될 수 있다.\n\nReal GDP per capita는 평균적인 경제 생산량 증가를 보여주지만, 실제로 국민 개개인의 소득 분배를 고려하지 않는다.\n\nReal Median Wage는 국민 대다수가 체감하는 실질 소득의 변화를 나타내므로, 보다 정확한 경제 성장 지표가 될 수 있다.\n\n만약 명목 GDP가 증가하지만 중위임금이 정체되거나 하락한다면, 이는 대기업과 자산가 중심의 성장일 뿐, 일반 국민들에게 혜택이 돌아가지 않는 성장일 가능성이 크다.\n\n\n\n\n경제 성장의 지속 가능성을 예측하는 데 있어, 현재 사용되는 Natural GDP Growth Rate(자연 경제 성장률) 모델은 구조적인 문제를 가질 수 있다.\n- 이런 모델들은 과거 데이터를 기반으로 경제 성장률을 예측하지만, 금융시장 및 소비자 행동의 비선형적 변화를 제대로 반영하지 못한다.\n- 반면 Consumer Sentiment Index(소비자 심리 지수) 는 실시간으로 소비자들의 기대와 시장 심리를 측정한다.\n- 즉, 미래의 경제 성장 가능성을 예측하는 데 있어, 소비자들이 체감하는 경제 신뢰도와 구매 의향이 더 실질적인 지표가 될 수 있다.\n\n\n\n\n명목 GDP와 실질 GDP만으로 경제 성장을 평가하는 것은 한계가 있다.\n\nReal Median Wage가 상승하지 않는다면, GDP 성장은 일부 계층에만 혜택을 주는 비대칭적 성장일 가능성이 크다.\n경제 전망을 판단할 때, Consumer Sentiment Index와 같은 체감 지표가 전통적인 성장률 모델보다 더 현실적인 통찰을 제공할 수 있다.\n\n결국, 우리가 고민해야 할 것은 단순한 GDP 증가가 아니라 ’국민 개개인이 체감하는 실질적 경제 성장’이다."
  },
  {
    "objectID": "time-series/indicator_growth.html#부록-gdp-성장률의-한계와-대안적-경제지표-분석",
    "href": "time-series/indicator_growth.html#부록-gdp-성장률의-한계와-대안적-경제지표-분석",
    "title": "경제 성장을 대표하는 지표?",
    "section": "부록: GDP 성장률의 한계와 대안적 경제지표 분석",
    "text": "부록: GDP 성장률의 한계와 대안적 경제지표 분석\n\nGDP 성장률의 측정과 한계\nGross Domestic Product(GDP)는 전통적으로 국가 경제 성장을 나타내는 핵심 지표로 활용되어 왔다. 그러나 최근 학계에서는 GDP 중심의 성장률 평가가 실제 국민의 삶의 질을 정확히 반영하지 못할 수 있다는 우려가 꾸준히 제기되고 있다(Kubiszewski et al., 2013; Stiglitz et al., 2009). 특히 금융 및 자산시장 중심의 성장 패턴이 강해질수록, GDP가 증가함에도 불구하고 소득 격차가 벌어지고 중산층의 생활 수준은 오히려 하락할 수 있다(Piketty, 2014).\n\nStiglitz, Sen, and Fitoussi (2009) 는 ’GDP가 단순한 생산물의 양적 증가만을 측정할 뿐, 불평등 심화와 실제 국민들이 체감하는 경제적 안녕(well-being)을 제대로 반영하지 못한다’는 문제를 공식적으로 제기한 바 있다. 이들은 GDP의 한계를 극복할 새로운 경제지표가 필요하다고 주장했다.\nCoyle (2014) 역시 GDP 수치 자체가 금융시장의 과열 현상으로 인해 실질적 경제 생산성 및 개개인의 복지 향상과 크게 괴리될 가능성을 지적했다. 그는 특히 금융시장의 투자 활동이 명목 GDP를 상승시키지만, 이러한 상승이 곧 일반 대중의 삶의 질 향상과 연결되지 않을 수 있음을 명시했다.\n\n\n\n대안적 경제지표로서의 실질 중위임금(Real Median Wage)\n경제학자들은 경제 성과 측정의 대안으로 소득분포의 중위값(median) 또는 중간층의 실질 소득 변화에 주목할 필요가 있다고 강조한다(Saez & Zucman, 2016; Chetty et al., 2017). 특히 중위임금(Median Wage)은 경제 성장이 국민 개개인의 생활 수준에 실제로 기여하는지를 명확히 나타내는 핵심적 지표로 주목받는다.\n\nChetty et al. (2017) 의 연구에 따르면, 미국 경제에서 GDP의 꾸준한 상승에도 불구하고 최근 30년간 실질 중위 소득이 정체된 사례가 발견되었으며, 이는 GDP 중심의 경제 성장이 반드시 대다수의 국민에게 이득을 가져다주지 않는다는 사실을 보여준다. 이 연구는 중위 소득을 핵심적 지표로 삼아 경제 정책의 우선순위를 재조정할 필요가 있다고 제안한다.\nSaez and Zucman (2016) 은 소득불평등의 증가가 GDP 성장률과 무관하게 진행되며, 중위 소득의 정체 또는 감소가 발생하면 경제 성장은 지속가능성을 잃게 된다고 경고한다. 그들은 중위 소득과 실질 생활 수준을 함께 고려하는 새로운 정책 프레임워크의 필요성을 강조한다.\n\n\n\n경제적 기대지표: Consumer Sentiment Index의 우월성\n경제 성장의 미래 전망을 평가할 때, 일반적으로 Central Bank 및 국제기구는 자연 경제 성장률(natural GDP rate)과 같은 구조적 모형 기반의 지표를 선호해왔다. 하지만 최근 연구들은 구조적 모형이 금융시장과 실물경제의 복잡한 상호작용을 잘 반영하지 못할 가능성이 있다고 지적한다(Sims, 2010; Coibion & Gorodnichenko, 2015). 이에 따라 소비자심리지수(Consumer Sentiment Index)가 경제 전망의 보다 정확한 선행지표로 주목받기 시작했다.\n\nCoibion and Gorodnichenko (2015) 는 소비자심리지수가 GDP 성장률, 특히 실질 소비지출의 미래 경로를 잘 예측하는 능력을 가졌음을 실증적으로 증명했다. 이 연구는 특히 불확실성이 높은 시기일수록 소비자심리지수가 공식적인 성장률 모형보다 경제 예측력에서 뛰어남을 보였다.\nSims (2010) 는 구조적 경제 예측 모형이 과거 데이터의 패턴에 과도하게 의존하여 금융 위기와 같은 비선형적 충격을 놓치는 경향이 있다고 지적했다. 반면 소비자 기대감은 그러한 경제적 충격을 보다 신속히 반영하며, 실물경제의 미래 변화에 대한 중요한 통찰력을 제공한다.\n\n\n\n결론적 시사점\n위 연구들의 공통된 결론은 명확하다. 전통적 GDP 성장률 측정 방식은 경제 성장과 국민 생활 수준 향상을 반드시 보장하지 않으며, 때로는 현실과 심각한 괴리를 일으킬 수 있다. 실질 중위임금(Real Median Wage)은 국민들의 실제 생활수준 개선 여부를 평가하는 데 있어 GDP보다 더욱 정확한 지표이며, 미래 경제에 대한 소비자의 심리를 측정하는 소비자심리지수는 구조적 예측모델보다 현실적 통찰력을 제공한다.\n따라서, 경제 정책은 GDP라는 숫자 증가에 매몰되지 않고, 개개인의 실질 생활수준과 소비자의 체감적 경제 기대감을 더 정확히 반영하는 지표 중심으로 전환해야 한다.\n\n\n참고문헌\n\nChetty, R., Grusky, D., Hell, M., Hendren, N., Manduca, R., & Narang, J. (2017). The fading American dream: Trends in absolute income mobility since 1940. Science, 356(6336), 398-406.\nCoibion, O., & Gorodnichenko, Y. (2015). Information Rigidity and the Expectations Formation Process: A Simple Framework and New Facts. American Economic Review, 105(8), 2644-2678.\nCoyle, D. (2014). GDP: A brief but affectionate history. Princeton University Press.\nKubiszewski, I., Costanza, R., Franco, C., Lawn, P., Talberth, J., Jackson, T., & Aylmer, C. (2013). Beyond GDP: Measuring and achieving global genuine progress. Ecological Economics, 93, 57-68.\nPiketty, T. (2014). Capital in the Twenty-First Century. Harvard University Press.\nSaez, E., & Zucman, G. (2016). Wealth inequality in the United States since 1913: Evidence from capitalized income tax data. Quarterly Journal of Economics, 131(2), 519-578.\nSims, C. A. (2010). Rational Inattention and Monetary Economics. Handbook of Monetary Economics, 3, 155-181.\nStiglitz, J., Sen, A., & Fitoussi, J. P. (2009). Report by the commission on the measurement of economic performance and social progress. Paris: Commission on the Measurement of Economic Performance and Social Progress."
  },
  {
    "objectID": "time-series/labor_decoupling.html",
    "href": "time-series/labor_decoupling.html",
    "title": "노동: 기여한 만큼 분배가 되었나?",
    "section": "",
    "text": "Neoclassical Growth Model(신고전적 성장 모형)은 경제성장의 핵심 지표로 총생산량(Gross Domestic Product, GDP)을 중시한다. 이 모형에서는 생산의 핵심 요소를 다음과 같이 세 가지로 구분한다 (Solow 1956).\n\n총 노동량(Labor Input, \\(L\\))\n\n총 자본량(Capital Input, \\(K\\))\n\n노동증강기술(Labor-Augmenting Technology, \\(A\\))\n\n그러나 자본량(\\(K\\))은 직접적으로 측정하기 어려우며, 일반적으로 Perpetual Inventory Method(영속적 재고법)과 같은 방식을 활용하여 추정한다 (Hall and Jones 1999). 이에 더하여, 노동증강기술(\\(A\\))은 더욱 측정이 어려운 정성적 요소이기 때문에, 많은 연구에서는 Constant Returns to Scale(규모에 대한 수익 불변)을 가정한 Cobb-Douglas 생산함수를 이용하고, 추정 과정에서 발생하는 오차항(Error Term)을 기술진보(\\(A\\))로 대체하기도 한다 (Barro 1991). 이러한 방식이 현실을 얼마나 정확하게 반영하는지는 논란의 여지가 크다.\n모형(Model)은 본질적으로 이론적 가설이며, 경제성장에 대한 다양한 견해를 반영할 수 있는 도구이기 때문에, 각자의 가설과 주장을 펼치는 것은 학문적으로 허용될 수 있다. 그러나 문제의 본질은 해당 모형이 현실을 얼마나 정확하게 설명하고 예측할 수 있는지에 있다. 현실을 반영하지 못하고, 지속적으로 예측에 실패하는 경제성장 모형은 실질적으로 활용 가치가 떨어질 수밖에 없다 (Stiglitz 2017).\n경제성장과 관련하여 실용적인 측면에서 가장 중요한 변수는 총 노동량(\\(L\\))이라고 볼 수 있다. 이는 비교적 객관적인 측정이 가능하며, 경제의 실질적 변화를 분석하는 데 유용하다. 이러한 맥락에서 노동 생산성(Productivity)과 노동 분배(Labor Compensation)라는 두 가지 핵심 개념을 살펴볼 필요가 있다."
  },
  {
    "objectID": "time-series/labor_decoupling.html#서론",
    "href": "time-series/labor_decoupling.html#서론",
    "title": "노동: 기여한 만큼 분배가 되었나?",
    "section": "",
    "text": "Neoclassical Growth Model(신고전적 성장 모형)은 경제성장의 핵심 지표로 총생산량(Gross Domestic Product, GDP)을 중시한다. 이 모형에서는 생산의 핵심 요소를 다음과 같이 세 가지로 구분한다 (Solow 1956).\n\n총 노동량(Labor Input, \\(L\\))\n\n총 자본량(Capital Input, \\(K\\))\n\n노동증강기술(Labor-Augmenting Technology, \\(A\\))\n\n그러나 자본량(\\(K\\))은 직접적으로 측정하기 어려우며, 일반적으로 Perpetual Inventory Method(영속적 재고법)과 같은 방식을 활용하여 추정한다 (Hall and Jones 1999). 이에 더하여, 노동증강기술(\\(A\\))은 더욱 측정이 어려운 정성적 요소이기 때문에, 많은 연구에서는 Constant Returns to Scale(규모에 대한 수익 불변)을 가정한 Cobb-Douglas 생산함수를 이용하고, 추정 과정에서 발생하는 오차항(Error Term)을 기술진보(\\(A\\))로 대체하기도 한다 (Barro 1991). 이러한 방식이 현실을 얼마나 정확하게 반영하는지는 논란의 여지가 크다.\n모형(Model)은 본질적으로 이론적 가설이며, 경제성장에 대한 다양한 견해를 반영할 수 있는 도구이기 때문에, 각자의 가설과 주장을 펼치는 것은 학문적으로 허용될 수 있다. 그러나 문제의 본질은 해당 모형이 현실을 얼마나 정확하게 설명하고 예측할 수 있는지에 있다. 현실을 반영하지 못하고, 지속적으로 예측에 실패하는 경제성장 모형은 실질적으로 활용 가치가 떨어질 수밖에 없다 (Stiglitz 2017).\n경제성장과 관련하여 실용적인 측면에서 가장 중요한 변수는 총 노동량(\\(L\\))이라고 볼 수 있다. 이는 비교적 객관적인 측정이 가능하며, 경제의 실질적 변화를 분석하는 데 유용하다. 이러한 맥락에서 노동 생산성(Productivity)과 노동 분배(Labor Compensation)라는 두 가지 핵심 개념을 살펴볼 필요가 있다."
  },
  {
    "objectID": "time-series/labor_decoupling.html#본론",
    "href": "time-series/labor_decoupling.html#본론",
    "title": "노동: 기여한 만큼 분배가 되었나?",
    "section": "본론",
    "text": "본론\n\n1. 노동생산성(Labor Productivity)의 변화\n노동생산성은 단위 노동량당 산출량을 의미하며, 다음과 같이 정의할 수 있다 (Acemoglu et al. 2014).\n\\[\n\\text{노동생산성} = \\frac{Y_t}{L_t}\n\\]\n여기서 \\(Y_t\\)는 총생산량(GDP)의 대표적 대리 변수(proxy)이며, \\(L_t\\)는 총 노동량을 나타낸다. 각각의 변수는 다음과 같이 측정하였다.\n\n총생산량 Proxy (\\(Y_t\\)): Real Gross Domestic Product per Capita\n\n장기적으로 볼 때, 총생산량은 시간에 따라 증가하는 추세를 보였다.\n\n총노동량 Proxy (\\(L_t\\)): Hours Worked by Full-Time and Part-Time Employees\n\n이 역시 시간에 따라 증가하는 경향을 보였다.\n\n\n이로부터 계산한 노동생산성(\\(Y_t/L_t\\)) 역시 시간에 따라 증가하는 함수임을 확인할 수 있다. 즉, 경제가 성장함에 따라 노동자 1인당 산출하는 생산량은 꾸준히 증가해 왔다.\n\n\n2. 노동분배량(Labor Compensation)의 변화\n노동생산성이 증가하면, 일반적으로 노동자에게 돌아가는 보상 역시 증가해야 한다는 것이 경제적 정의(economic fairness)와 균형적 성장(balanced growth)의 핵심 원칙이다 (Piketty 2014). 그렇다면 노동 분배(Labor Compensation) 역시 시간에 따라 증가하였을까?\n이를 확인하기 위해, 노동자의 평균 실질 소득을 대리 변수로 활용하였다.\n\n노동에 분배된 총량 Proxy (\\(X_t\\)): Employed Full-Time: Median Usual Weekly Real Earnings\n\n장기적으로 증가하는 경향을 보였으나, 변동성이 존재하였다.\n\n노동분배량 (\\(\\frac{X_t}{L_t}\\))\n\n노동생산성이 증가하는 것과는 대조적으로, 노동자에게 분배된 소득의 비율(\\(X_t/L_t\\))은 오히려 시간에 따라 감소하는 경향을 보였다.\n\n\n이를 시각적으로 명확히 비교하기 위해, 노동분배량을 \\(\\frac{X_t}{L_t} \\times 100\\)으로 스케일링하여 그래프로 나타냈다.\nFRED Graph (1980년 이후 노동생산성과 노동분배량 비교)\n그래프를 살펴보면, 1980년대 이후 노동생산성과 노동분배량 사이의 격차가 점점 더 커지고 있음을 확인할 수 있다. 이는 무엇을 의미하는가? 노동생산성이 증가함에도 불구하고, 노동자에게 돌아가는 보상의 증가 속도가 이에 미치지 못하고 있다는 점을 시사한다. 다시 말해, “노동생산성과 노동자 보상의 분리 현상(decoupling)”이 지속적으로 심화되고 있음을 보여준다 (Stansbury and Summers 2020). 이러한 현상은 장기적으로 경제적 불평등(economic inequality)을 악화시키는 주요 원인 중 하나로 작용할 수 있다 (Stiglitz 2012).\n\n\n3. 노동 소득 분배율 감소의 원인\n노동소득 분배율(Labor Share)의 감소에 대한 경제학적 분석은 다양한 요인을 고려해야 하지만, 주요한 원인으로 다음 두 가지를 지적할 수 있다.\n\n기술 진보(Technological Progress)\n\nOECD의 분석에 따르면, 인공지능(AI)과 같은 첨단 기술의 발전은 특정 고숙련 노동자(high-skilled workers)에게는 유리하게 작용하지만, 그렇지 않은 노동자들에게는 불리한 영향을 미칠 수 있다 (OECD 2018). 이는 노동시장 내 임금 불평등(wage inequality)을 심화시키는 요인으로 작용한다.\n\n세계화(Globalization)\n\n생산 공정의 해외 이전(offshoring)과 국제 무역의 확대는 저임금 노동력을 활용한 생산 방식을 증가시켜, 선진국 내 노동자의 소득 증가율을 둔화시키는 결과를 초래할 수 있다 (Autor, Dorn, and Hanson 2013).\n\n\n\n\n4. 노동참여율(Labor Force Participation Rate)의 변화\n또한, 노동소득 분배율 감소와 노동시장 변화를 분석하기 위해, 노동참여율(Labor Force Participation Rate)을 함께 고려할 필요가 있다. 이를 위해 Current Population Survey (CPS)에서 조사한 Labor Force Participation Rate를 분석하였다.\n\nCOVID-19 팬데믹이 발생한 2020년에는 노동참여율이 일시적으로 급락했으나, 이후 빠르게 정상 수준으로 회복되었다 (Coibion, Gorodnichenko, and Weber 2020).\n\n장기적으로는 완만한 하락세를 보이고 있으며, 이는 인구 고령화(demographic aging) 등의 요인과도 관련이 있을 것으로 추정된다 (Krueger 2017)."
  },
  {
    "objectID": "time-series/labor_decoupling.html#결론",
    "href": "time-series/labor_decoupling.html#결론",
    "title": "노동: 기여한 만큼 분배가 되었나?",
    "section": "결론",
    "text": "결론\n노동생산성과 노동자 보상의 분리 현상(decoupling)은 실증적 데이터에 의해 명확히 확인된다. 노동생산성이 지속적으로 증가함에도 불구하고, 노동자에게 분배되는 소득의 비율은 점진적으로 감소해 왔다 (Karabarbounis and Neiman 2014). 이러한 현상의 주요 원인으로 기술 진보와 세계화가 지목되며, 장기적으로는 노동시장 내 불평등 심화 및 경제적 불안정성을 초래할 가능성이 크다 (Milanovic 2016). 향후 연구에서는 정책적 대응 방안과 노동소득 분배율 회복을 위한 제도적 개선책을 추가적으로 검토할 필요가 있다."
  }
]