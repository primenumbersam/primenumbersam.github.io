[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "gitsam 기초통계학",
    "section": "",
    "text": "cover\n\n\n우리는 “외국인 직접투자가 증가하면 경제성장률도 상승할까?” 혹은 “광고를 하면 실제로 매출이 늘어날까?” 같은 질문에 자주 직면합니다. 이처럼 명백한 답을 내리기 어려운 문제에 대해 보다 합리적으로 사고하고 판단하기 위한 도구가 바로 통계학입니다. 통계학은, 불완전하고 잡음 섞인 데이터로부터 일정한 결론을 도출할 수 있도록 도와주는 방법론(methodology) 입니다.\n그런데 여기서 말하는 결론이란 무엇일까요? 통계학의 궁극적 목적은 단순히 숫자를 계산하거나 도표를 만드는 것이 아닙니다. 우리가 관찰한 데이터가 어떤 가설을 지지하는지, 아니면 그것을 반박하는지를 정량적으로 판단할 수 있는 근거를 마련하는 것이 바로 통계학의 역할입니다.\n예를 들어, 광고 캠페인 전후의 매출 데이터를 비교하면서 “광고가 효과가 있었다”고 주장하고 싶다고 가정해 봅시다. 이 주장을 통계학적으로 입증하려면 먼저 몇 가지 질문을 해결해야 합니다. 광고의 영향을 측정하기 위해 어떤 기간의 매출을 사용할지, 동일한 시기 다른 마케팅 요인은 없었는지, 그리고 단순한 계절 요인에 의한 변화는 아닌지를 분명히 해야 합니다.\n또 다른 예로, “노동시장에서 여성에 대한 차별이 존재했는가?”라는 질문도 살펴볼 수 있습니다. 이 질문 역시 간단해 보이지만, 실질적으로 분석하기 위해서는 차별의 정의, 분석 대상이 되는 산업군, 분석 시기의 설정, 그리고 성별 이외에 소득에 영향을 줄 수 있는 다른 변수들(예: 경력, 학력 등)을 고려해야 합니다. 즉, 통계적 분석이 가능하려면 먼저 개념의 정제와 변수의 정의가 선행되어야 합니다.\n통계학은 이러한 문제들을 세 가지 층위에서 접근합니다. 첫째는 기술 통계학(descriptive statistics)으로, 주어진 데이터를 요약하고 구조를 파악하는 데 목적이 있습니다. 둘째는 추론 통계학(inferential statistics)으로, 전체 모집단에 대한 결론을 내리기 위해 표본을 분석합니다. 마지막으로 확률 모형(probabilistic model)은 이러한 표본과 모집단 사이의 관계를 수학적으로 설명하는 이론적 기반입니다. 예컨대, 우리가 모집단의 평균을 추정하려 한다면, 단순한 표본 평균이 그 역할을 대신할 수 있는 이유는 확률이론에서 나온 대수의 법칙이나 중심극한정리 같은 정리에 근거하기 때문입니다.\n더불어, 통계학은 단순히 정보를 요약하거나 모형을 세우는 데 그치지 않고, 데이터 분석 과정에서의 두 가지 상반된 태도를 구분하도록 요구합니다. 바로 탐색(exploration)과 활용(exploitation)입니다. 데이터를 탐색하는 단계에서는 그래프나 기초 통계량을 마음껏 살펴보면서 다양한 가설을 상상할 수 있습니다. 그러나 어떤 분석 결과를 바탕으로 실제 정책을 시행하거나 투자를 결정하는 것처럼 ’결정’을 내리는 순간에는, 분석의 전제와 그 한계에 대해 매우 엄격하게 검토해야 합니다. 탐색은 자유롭게 하되, 활용은 조심스럽게 해야 한다는 말입니다.\n그렇다면 통계학이 유용하다는 것은 무엇을 의미할까요? 그것은 곧 우리가 실제 세계의 모호함을 명확하게 측정할 수 있도록 도와주는 구조화된 사고 도구를 제공한다는 것입니다. 현실의 문제들은 대부분 모호합니다. 그러나 이 모호함을 아예 다루지 않거나 부정확하게 처리하느니, 오히려 명확한 가정 위에서 근사적으로 사고하는 것이 더 합리적일 수 있습니다.\n\n모든 모형은 틀렸다. 다만, 몇몇 모형은 유용하다.\n잘못된 모형을 정밀하게 푸는 것보다, 정확한 모형을 근사적으로 푸는 것이 낫다.\n\n이러한 인식이 통계학의 출발점이며, 합리적 사고를 위한 훈련의 첫 걸음입니다.",
    "crumbs": [
      "Apps",
      "gitsam 기초통계학"
    ]
  },
  {
    "objectID": "index.html#경제통계-분석-입문",
    "href": "index.html#경제통계-분석-입문",
    "title": "gitsam 기초통계학",
    "section": "",
    "text": "cover\n\n\n우리는 “외국인 직접투자가 증가하면 경제성장률도 상승할까?” 혹은 “광고를 하면 실제로 매출이 늘어날까?” 같은 질문에 자주 직면합니다. 이처럼 명백한 답을 내리기 어려운 문제에 대해 보다 합리적으로 사고하고 판단하기 위한 도구가 바로 통계학입니다. 통계학은, 불완전하고 잡음 섞인 데이터로부터 일정한 결론을 도출할 수 있도록 도와주는 방법론(methodology) 입니다.\n그런데 여기서 말하는 결론이란 무엇일까요? 통계학의 궁극적 목적은 단순히 숫자를 계산하거나 도표를 만드는 것이 아닙니다. 우리가 관찰한 데이터가 어떤 가설을 지지하는지, 아니면 그것을 반박하는지를 정량적으로 판단할 수 있는 근거를 마련하는 것이 바로 통계학의 역할입니다.\n예를 들어, 광고 캠페인 전후의 매출 데이터를 비교하면서 “광고가 효과가 있었다”고 주장하고 싶다고 가정해 봅시다. 이 주장을 통계학적으로 입증하려면 먼저 몇 가지 질문을 해결해야 합니다. 광고의 영향을 측정하기 위해 어떤 기간의 매출을 사용할지, 동일한 시기 다른 마케팅 요인은 없었는지, 그리고 단순한 계절 요인에 의한 변화는 아닌지를 분명히 해야 합니다.\n또 다른 예로, “노동시장에서 여성에 대한 차별이 존재했는가?”라는 질문도 살펴볼 수 있습니다. 이 질문 역시 간단해 보이지만, 실질적으로 분석하기 위해서는 차별의 정의, 분석 대상이 되는 산업군, 분석 시기의 설정, 그리고 성별 이외에 소득에 영향을 줄 수 있는 다른 변수들(예: 경력, 학력 등)을 고려해야 합니다. 즉, 통계적 분석이 가능하려면 먼저 개념의 정제와 변수의 정의가 선행되어야 합니다.\n통계학은 이러한 문제들을 세 가지 층위에서 접근합니다. 첫째는 기술 통계학(descriptive statistics)으로, 주어진 데이터를 요약하고 구조를 파악하는 데 목적이 있습니다. 둘째는 추론 통계학(inferential statistics)으로, 전체 모집단에 대한 결론을 내리기 위해 표본을 분석합니다. 마지막으로 확률 모형(probabilistic model)은 이러한 표본과 모집단 사이의 관계를 수학적으로 설명하는 이론적 기반입니다. 예컨대, 우리가 모집단의 평균을 추정하려 한다면, 단순한 표본 평균이 그 역할을 대신할 수 있는 이유는 확률이론에서 나온 대수의 법칙이나 중심극한정리 같은 정리에 근거하기 때문입니다.\n더불어, 통계학은 단순히 정보를 요약하거나 모형을 세우는 데 그치지 않고, 데이터 분석 과정에서의 두 가지 상반된 태도를 구분하도록 요구합니다. 바로 탐색(exploration)과 활용(exploitation)입니다. 데이터를 탐색하는 단계에서는 그래프나 기초 통계량을 마음껏 살펴보면서 다양한 가설을 상상할 수 있습니다. 그러나 어떤 분석 결과를 바탕으로 실제 정책을 시행하거나 투자를 결정하는 것처럼 ’결정’을 내리는 순간에는, 분석의 전제와 그 한계에 대해 매우 엄격하게 검토해야 합니다. 탐색은 자유롭게 하되, 활용은 조심스럽게 해야 한다는 말입니다.\n그렇다면 통계학이 유용하다는 것은 무엇을 의미할까요? 그것은 곧 우리가 실제 세계의 모호함을 명확하게 측정할 수 있도록 도와주는 구조화된 사고 도구를 제공한다는 것입니다. 현실의 문제들은 대부분 모호합니다. 그러나 이 모호함을 아예 다루지 않거나 부정확하게 처리하느니, 오히려 명확한 가정 위에서 근사적으로 사고하는 것이 더 합리적일 수 있습니다.\n\n모든 모형은 틀렸다. 다만, 몇몇 모형은 유용하다.\n잘못된 모형을 정밀하게 푸는 것보다, 정확한 모형을 근사적으로 푸는 것이 낫다.\n\n이러한 인식이 통계학의 출발점이며, 합리적 사고를 위한 훈련의 첫 걸음입니다.",
    "crumbs": [
      "Apps",
      "gitsam 기초통계학"
    ]
  },
  {
    "objectID": "index.html#목차-contents",
    "href": "index.html#목차-contents",
    "title": "gitsam 기초통계학",
    "section": "목차 (Contents)",
    "text": "목차 (Contents)\n\n통계 데이터 (Statistical Data)\n\n경제지표 (Economic indicators)\n경제데이터 분석준비\n활용목적에 따른 데이터의 질\n부록-무작위 표본추출법 (Random sampling)\n\n데이터 탐색 (Exploration): 데이터를 요약하고, 눈으로 보는 힘\n\n1D Statistics and Plots\n2D Relationship\n\n확률 - 데이터와 가설의 연결고리\n\n수학적 확률 vs. 통계적 확률\n함수로서의 확률변수 (random variable): 확률분포, 확률밀도함수.\n대수의 법칙 (Law of Large numbers)\n중심극한정리 (Central Limit Theorem): 모집단 vs 표본. 표본평균의 분포. 신뢰도.\n\n데이터 활용 (Exploitation)\n\n단기 - 외생성 (exogeneity) vs. 내생성 (endogeneity): 경제 데이터로 인과 추론이 어려운 이유\n중장기 - 인과관계 (Granger causality) vs. 공적분관계 (co-integration)\n통계적 추론 (Statistical Inference) - 빈도주의자 vs. 베이지언\n\n빈도주의자 (frequentist)들의 주요 가정 : 외생성 \\(E(\\epsilon | x)=0\\) 과 \\(\\epsilon \\sim N(0,\\sigma^2)\\)\n베이지언 (Bayesian)들의 주요 가정 : Joint distribution \\(f(x,y)\\)\n\n통계적 학습 (Statistical Learning) - Supervised vs. Unsupervised\n부록 - 베이지안 통계학과 응용\n\nModel Parameter Estimation and Statistical Hypothesis\n\nMultivariate Regression Model\nEstimation Method - from Frequentist perspective\nEstimation Method - from Bayesian perspective\n\n통계적 가설 검정 (Hypothesis test)\n\n통계적 가설 검정의 오류: Type I and II error\n유의수준, 통계량, p-value\n두 집단 평균 차이 검정\n여러 집단을 한꺼번에 비교",
    "crumbs": [
      "Apps",
      "gitsam 기초통계학"
    ]
  },
  {
    "objectID": "6-3.html",
    "href": "6-3.html",
    "title": "두 집단을 비교",
    "section": "",
    "text": "우리가 통계 분석을 수행하는 가장 실용적인 이유 중 하나는,\n서로 다른 두 집단 간에 어떤 특성이 차이를 보이는가를 검증하는 데 있습니다.\n예를 들어 다음과 같은 질문들을 떠올려 봅시다.\n이러한 질문들은 모두 두 집단 간의 특성 평균값이 유의미하게 다른가를 묻는 것이며, 이는 통계학의 가장 기본적인 가설 검정 문제 중 하나입니다.",
    "crumbs": [
      "Apps",
      "가설 점정 (Hypothesis Test)",
      "두 집단을 비교"
    ]
  },
  {
    "objectID": "6-1.html",
    "href": "6-1.html",
    "title": "통계적 가설 검정의 오류",
    "section": "",
    "text": "모든 통계학은 기본적으로 불확실성 하의 의사결정(decision under uncertainty)을 전제로 합니다.\n우리는 완전한 정보를 가지지 않은 상태에서, 제한된 표본과 관측값을 바탕으로 어떤 판단을 내려야 합니다. 그 판단이 정책일 수도 있고, 신약 개발일 수도 있으며, 단순한 마케팅 캠페인일 수도 있습니다.\n통계적 추론은 이렇게 말할 수 있습니다: “우리가 가진 표본에서 관측된 결과가 우연히 발생할 확률이 충분히 작다면, 이는 단순한 우연이라 보기 어렵고, 대안적 설명이 더 그럴듯하다.”\n이것이 바로 통계적 가설 검정(hypothesis testing)의 출발점입니다.\n하지만, 이런 추론 구조는 본질적으로 오류 가능성을 내포합니다.\n표본은 우연의 산물이기 때문에, 우연히 잘못된 결론에 도달할 가능성은 언제나 존재합니다.\n통계적 가설 검정을 통과했다고 해서 그 가설이 “진실”임을 의미하지는 않습니다.\n단지 주어진 데이터 내에서 우연히 발생하기 어려운 정도의 효과가 관측되었다는 뜻일 뿐입니다.\n그렇기에 통계적 판단은 항상 다음의 균형을 요구합니다:",
    "crumbs": [
      "Apps",
      "가설 점정 (Hypothesis Test)",
      "통계적 가설 검정의 오류"
    ]
  },
  {
    "objectID": "6-1.html#귀무가설과-대립가설-판단의-기준",
    "href": "6-1.html#귀무가설과-대립가설-판단의-기준",
    "title": "통계적 가설 검정의 오류",
    "section": "1 귀무가설과 대립가설: 판단의 기준",
    "text": "1 귀무가설과 대립가설: 판단의 기준\n모든 통계적 검정은 두 개의 상반된 가설을 세우는 것에서 시작됩니다:\n\n귀무가설(null hypothesis, \\(H_0\\)): 기존 구조, 효과 없음, 차이 없음\n예: “이 약은 효과가 없다”, “이 정책은 실업률을 낮추지 않는다”, “\\(\\beta = 0\\)”\n대립가설(alternative hypothesis, \\(H_1\\)): 변화 있음, 효과 존재\n예: “이 약은 효과가 있다”, “이 정책은 실업률을 낮춘다”, “\\(\\beta \\neq 0\\)”\n\n통계학자는 귀무가설을 기각할 충분한 증거가 있는지를 표본으로부터 판단합니다.\n즉, 기본값(\\(H_0\\))을 유지할지 아니면 바꿀지 결정하는 의사결정의 틀을 제공합니다.",
    "crumbs": [
      "Apps",
      "가설 점정 (Hypothesis Test)",
      "통계적 가설 검정의 오류"
    ]
  },
  {
    "objectID": "6-1.html#오류의-구조-type-i과-type-ii",
    "href": "6-1.html#오류의-구조-type-i과-type-ii",
    "title": "통계적 가설 검정의 오류",
    "section": "2 오류의 구조: Type I과 Type II",
    "text": "2 오류의 구조: Type I과 Type II\n통계적 가설 검정은 본질적으로 두 가지 판단 오류 가능성을 내포합니다. 이는 현실 세계에서의 법적 판단, 의료 진단, 정책 결정 등 다양한 영역에 깊이 관여합니다.\n\n2.1 Type I error (1종 오류, False Positive)\n실제로는 효과가 없지만, 있다고 잘못 판단하는 오류입니다.\n이는 귀무가설 \\(H_0\\)이 참인데도 이를 기각하는 상황입니다:\n\\[H_0 \\text{ is true, but we reject } H_0\\]\n이 오류는 과잉 반응(overreaction) 또는 불필요한 개입을 야기합니다.\n\n효과 없는 약이 효과 있다고 판단되어 시판됨\n정책이 실제로 효과가 없음에도 “유의미하다”고 주장하며 지속됨\n무고한 시민이 죄를 저질렀다고 오판되어 처벌당함\n“빈대 잡으려다 초가삼간 태운다”는 속담\n\n\n\n2.2 Type II error (2종 오류, False Negative)\n실제로는 효과가 있음에도 없다고 판단하는 오류입니다.\n즉, 대립가설 \\(H_1\\)이 참인데도 이를 채택하지 못하는 상황입니다:\n\\[H_1 \\text{ is true, but we fail to reject } H_0\\]\n이 오류는 무기력하거나 소극적인 대응을 초래합니다.\n\n실제로 효과 있는 신약이 통계적으로 유의하지 않다고 판단되어 폐기됨\n의미 있는 사회현상이 “유의하지 않다”며 간과됨\n권력자의 범죄가 증거 부족으로 무죄 처분됨\n“정의의 실현보다 오히려 불공정의 방조”라는 비판이 따라붙습니다.\n\n\n\n2.3 사례 비교: 현실 속 Type I vs. Type II\n\n\n\n분야\nType I Error\nType II Error\n\n\n\n\n코로나 검사\n음성인 사람을 확진자로 판단\n감염자를 놓침\n\n\n신약 승인\n효과 없는 약을 승인함\n효과 있는 약을 거부함\n\n\n범죄 수사\n무고한 시민을 기소함\n진범을 방면함\n\n\n정책 평가\n효과 없는 정책을 계속 유지\n효과 있는 정책을 폐기\n\n\n학교 시험\n공부 안 한 학생이 고득점\n열심히 한 학생이 탈락\n\n\n\n\n\n2.4 법치주의적 해석: 정의와 자유의 균형\n이 두 오류는 법적 판단과 민주주의 시스템에서도 그대로 적용됩니다.\n\nType I error는 무고한 시민을 벌주는 것으로, 자유의 침해를 의미합니다.\nType II error는 권력층의 불법을 놓치는 것으로, 정의의 훼손을 의미합니다.\n\n어떤 오류를 더 심각하게 보는가는 사회의 철학적 전제에 따라 달라집니다.\n\n자유주의적 민주주의는 Type I error를 줄이는 것을 우선시합니다. (예: 무죄 추정 원칙)\n공리주의적 효율성을 중시하는 체제는 Type II error의 위험에 더 민감하게 반응합니다. (예: 시스템의 무력화 방지)",
    "crumbs": [
      "Apps",
      "가설 점정 (Hypothesis Test)",
      "통계적 가설 검정의 오류"
    ]
  },
  {
    "objectID": "5-2.html",
    "href": "5-2.html",
    "title": "중회귀모형 - 빈도주의자 접근",
    "section": "",
    "text": "직교 투영과 점추정의 논리\n\n선형 회귀모형은 수학적으로 단순하지만, 그 이면에는 명확한 철학적 전제가 존재합니다.\n빈도주의자는 ’한 번 주어진 모집단’과 ’반복 가능한 표본추출’이라는 두 가지 전제를 바탕으로,\n관측된 데이터로부터 고정된 모수(parameter)를 추정하는 접근을 취합니다.\n회귀모형에서는 이 고정된 모수가 바로 회귀계수 \\(\\beta\\)입니다.\n회귀모형에서 가장 단순한 형태는 다음과 같습니다:\n\\[Y = X\\beta + \\varepsilon, \\quad \\varepsilon \\sim \\text{mean-zero}\\]\n여기서 \\(Y\\)는 \\(n \\times 1\\) 벡터, \\(X\\)는 \\(n \\times p\\) 행렬이며, 각 행은 한 관측치를 나타냅니다.\n오차항 \\(\\varepsilon\\)는 \\(Y\\)에 포함된 설명되지 않는 구성요소이며, \\(X\\)와는 상관되지 않는다는 전제를 둡니다.\n빈도주의자의 추론 방식은 매우 명확합니다.\n데이터는 우연히 관측된 결과이지만, 그 뒤에 있는 \\(\\beta\\)는 고정된 값이라고 전제하며,\n이 고정된 \\(\\beta\\)를 가장 합리적인 방식으로 추정하려고 합니다.\n그렇다면, 어떤 방식이 ’합리적인 방식’일까요?\n빈도주의자는 일반적으로 오차항의 제곱합이 최소가 되는 cost function을 가정하고 \\(\\beta\\)를 추정합니다. 즉, 다음 문제를 푸는 것이 목적입니다:\n\\[\\hat{\\beta} = \\arg\\min_{\\beta} \\|Y - X\\beta\\|^2\\]\n이 문제는 기하학적으로 다음과 같은 의미를 갖습니다.\n\\(Y\\)는 \\(\\mathbb{R}^n\\)의 벡터이며, \\(X\\)의 열공간(column space of \\(X\\))에 직교(projection)되는 점이 바로 \\(\\hat{Y} = X\\hat{\\beta}\\)입니다. 이때 오차 벡터 \\(e = Y - X\\hat{\\beta}\\)는 \\(X\\)의 열공간에 수직입니다. 즉, \\[X^\\top (Y - X\\hat{\\beta}) = 0\\]\n이라는 직교 조건이 바로 OLS 추정량의 정의 조건입니다.\n이 조건을 정리하면 바로 우리가 익숙하게 알고 있는 정규방정식(normal equation)이 됩니다: \\[\\hat{\\beta} = (X^\\top X)^{-1} X^\\top Y\\]\n이 계산 방식은 순전히 기하학적 정리에서 출발합니다. 모수의 분포에 대한 어떤 가정도 없이, 오로지 관측값 \\(Y\\)가 \\(X\\)의 선형 조합으로 잘 설명될 수 있도록 만든 것입니다. 이러한 점에서 빈도주의적 회귀는 회귀계수 \\(\\beta\\)의 설명력을 최대화하는 방향으로 잔차를 최소화하는 구조’를 우선시하는 접근이라고 할 수 있습니다.\nOLS 추정량의 평균과 분산을 계산해 보면, \\(\\varepsilon\\)가 평균 0이고 분산 \\(\\sigma^2 I\\)를 갖는다는 조건 하에 \\(\\hat{\\beta}\\)는 다음과 같은 성질을 가집니다:\n\n\\(\\mathbb{E}[\\hat{\\beta}] = \\beta\\): 불편 추정량(unbiased estimator)\n\\(\\mathrm{Var}(\\hat{\\beta}) = \\sigma^2 (X^\\top X)^{-1}\\): 선형 불편 추정량 중 최솟값 (BLUE)\n\n이러한 결과는 Gauss-Markov 정리에 의해 보장되며, 빈도주의자는 이 구조를 전제로\n\\(\\hat{\\beta}\\)라는 단일 값, 즉 점추정치(point estimate)에 주된 관심을 둡니다.\n하지만 이 추정값의 ’정확성’은 여전히 표본의 크기와 구조에 의존합니다. 표본이 클수록 \\(\\hat{\\beta}\\)는 실제 \\(\\beta\\)에 가까워지며, 작은 표본일수록 불확실성이 커지게 됩니다. 이 불확실성은 이후 신뢰구간(confidence interval)이나 가설 검정(hypothesis testing)으로 표현됩니다.\n이러한 구조는 빈도주의자에게 있어 모형이란 ’현실의 불확실한 일부를 가장 잘 설명해 줄 수 있는 단일한 요약값을 구하는 절차’이며, 그 절차는 오로지 반복가능한 데이터 구조와 수학적 정당성 위에서만 정당화될 수 있다는 철학을 따릅니다.\n\n\n\nReuseCC BY-NC-SA 4.0",
    "crumbs": [
      "Apps",
      "모형계수 추정법 (Estimation)",
      "중회귀모형 - 빈도주의자 접근"
    ]
  },
  {
    "objectID": "4-5.html",
    "href": "4-5.html",
    "title": "부록-베이지안 통계학과 응용",
    "section": "",
    "text": "베이지안 통계학(Bayesian Statistics)은 확률을 믿음의 정도(belief)로 해석합니다.\n관측 이전의 불확실성은 사전 확률(Prior)로,\n관측 이후의 지식은 사후 확률(Posterior)로 표현되며,\n이 둘은 베이즈 정리(Bayes’ Theorem)에 의해 연결됩니다:\n\\[P(\\theta \\mid y) = \\frac{P(y \\mid \\theta) \\cdot P(\\theta)}{P(y)} \\propto P(y \\mid \\theta) \\cdot P(\\theta)\\]\n여기서 \\(\\theta\\)는 모수, \\(y\\)는 데이터, \\(P(y \\mid \\theta)\\)는 가능도(likelihood)입니다.\n전체 확률 \\(P(y)\\)는 정규화 상수이며, 사후 분포를 확률분포로 만들기 위해 필요합니다.\n베이지안 추론은 다음 5단계로 요약할 수 있습니다:\n\n모형 설정: 데이터 \\(y\\)와 모수 \\(\\theta\\) 사이의 확률 구조를 정의\n사전 분포 설정: \\(\\theta\\)에 대한 사전 확률 \\(P(\\theta)\\)를 지정 (정보적 또는 무정보적)\n데이터 관측: 새로운 데이터를 수집\n사후 분포 계산:\n\n베이즈 정리를 이용해 사후 확률 \\(P(\\theta \\mid y)\\) 계산\n복잡한 경우에는 MCMC(Markov Chain Monte Carlo) 같은 수치적 근사 기법 필요\n\n사후 기반 추론: 점 추정, 구간 추정, 가설 검정, 예측 등 수행\n\n\n\n\n점 추정(point estimation)은 손실 함수 \\(L(\\theta, \\hat{\\theta})\\)에 따라 달라집니다.\n\n제곱 오차 손실(MSE, Mean Squared Error): 사후 평균 \\(\\mathbb{E}[\\theta \\mid y]\\) 사용\n절대 오차 손실(MAE, Mean Absolute Error): 사후 중앙값 사용\n\n\n\n\n\n베이지안 계산을 단순화하는 개념입니다.\n예: 베타-이항 모형 (Beta-Binomial Model)\n\n관측값: \\(k\\) successes in \\(n\\) trials\n가능도: \\(P(k \\mid \\theta, n) \\propto \\theta^k (1 - \\theta)^{n - k}\\)\n사전: \\(\\theta \\sim \\text{Beta}(\\alpha, \\beta)\\)\n사후: \\(\\theta \\mid k \\sim \\text{Beta}(\\alpha + k, \\beta + n - k)\\)\n\n사전과 사후가 같은 분포족을 유지하므로 계산이 용이합니다.\n\n\n\n베이지안 모델 비교에서는 다음과 같은 기준을 사용합니다:\n\nBayes Factor (BF):\n\\[BF_{12} = \\frac{P(y \\mid M_1)}{P(y \\mid M_2)}\\]\n→ 데이터가 주어졌을 때 모델 \\(M_1\\)이 \\(M_2\\)보다 얼마나 더 지지받는지를 정량화\n사후 모델 확률:\n\\[P(M_i \\mid y) \\propto P(y \\mid M_i) \\cdot P(M_i)\\]\nModel Averaging (모델 평균화):\n\\[P(\\theta \\mid y) = \\sum_i P(\\theta \\mid y, M_i) \\cdot P(M_i \\mid y)\\]\n\n\n\n\n어떤 질병의 유병률이 매우 낮은 상황(예: \\(P(\\text{Disease}) = 0.001\\))을 생각해 봅시다.\n\n민감도(Sensitivity): \\(P(\\text{Positive} \\mid \\text{Disease}) = 0.99\\)\n특이도(Specificity): \\(P(\\text{Negative} \\mid \\text{No Disease}) = 0.99\\)\n\n한 사람이 양성(Positive) 판정을 받았을 때, 실제 병에 걸렸을 확률(PPV: Positive Predictive Value)은?\n\\[P(\\text{Disease} \\mid \\text{Positive}) = \\frac{0.99 \\cdot 0.001}{0.99 \\cdot 0.001 + 0.01 \\cdot 0.999} \\approx 0.09\\]\n→ 양성 반응자의 9%만 실제 환자.\n검사의 정확도는 사전 확률에 강하게 의존함을 보여주는 고전적 예입니다.",
    "crumbs": [
      "Apps",
      "데이터 활용 (Exploitation)",
      "부록-베이지안 통계학과 응용"
    ]
  },
  {
    "objectID": "4-5.html#희귀병-검사-예시-사전-확률의-중요성",
    "href": "4-5.html#희귀병-검사-예시-사전-확률의-중요성",
    "title": "부록-베이지안 통계학과 응용",
    "section": "",
    "text": "어떤 질병의 유병률이 매우 낮은 상황(예: \\(P(\\text{Disease}) = 0.001\\))을 생각해 봅시다.\n\n민감도(Sensitivity): \\(P(\\text{Positive} \\mid \\text{Disease}) = 0.99\\)\n특이도(Specificity): \\(P(\\text{Negative} \\mid \\text{No Disease}) = 0.99\\)\n\n한 사람이 양성(Positive) 판정을 받았을 때, 실제 병에 걸렸을 확률(PPV: Positive Predictive Value)은?\n\\[P(\\text{Disease} \\mid \\text{Positive}) = \\frac{0.99 \\cdot 0.001}{0.99 \\cdot 0.001 + 0.01 \\cdot 0.999} \\approx 0.09\\]\n→ 양성 반응자의 9%만 실제 환자.\n검사의 정확도는 사전 확률에 강하게 의존함을 보여주는 고전적 예입니다.",
    "crumbs": [
      "Apps",
      "데이터 활용 (Exploitation)",
      "부록-베이지안 통계학과 응용"
    ]
  },
  {
    "objectID": "4-5.html#multi-armed-bandit-mab-다중-슬롯머신-문제",
    "href": "4-5.html#multi-armed-bandit-mab-다중-슬롯머신-문제",
    "title": "부록-베이지안 통계학과 응용",
    "section": "2.1 Multi-Armed Bandit (MAB, 다중 슬롯머신 문제)",
    "text": "2.1 Multi-Armed Bandit (MAB, 다중 슬롯머신 문제)\n\n문제: 여러 arm(옵션) 중 보상 확률이 가장 높은 arm을 찾아 탐험(Exploration)과 활용(Exploitation)을 균형 있게 수행\nThompson Sampling (톰슨 샘플링):\n\n각 arm \\(i\\)의 성공 확률 \\(\\theta_i\\)에 대해 Beta 사전분포 설정\n각 사후 분포 \\(P(\\theta_i \\mid \\text{data})\\)에서 \\(\\theta_i^*\\) 샘플링\n\\(\\theta_i^*\\)가 가장 큰 arm 선택\n선택 결과 관측, 사후 분포 업데이트\n반복 수행 → 탐험과 활용이 자연스럽게 균형됨\n\nContextual Bandit (문맥 기반 밴딧):\n\n사용자 상태나 환경 \\(x\\)에 따라 보상 확률 \\(\\theta(x, a)\\)를 모델링\n베이지안 회귀, 베이지안 신경망 등으로 확장 가능",
    "crumbs": [
      "Apps",
      "데이터 활용 (Exploitation)",
      "부록-베이지안 통계학과 응용"
    ]
  },
  {
    "objectID": "4-5.html#naive-bayes-classifier-나이브-베이즈-분류기",
    "href": "4-5.html#naive-bayes-classifier-나이브-베이즈-분류기",
    "title": "부록-베이지안 통계학과 응용",
    "section": "2.2 Naive Bayes Classifier (나이브 베이즈 분류기)",
    "text": "2.2 Naive Bayes Classifier (나이브 베이즈 분류기)\n\n문제: 텍스트 분류, 감성 분석, 스팸 필터링\n가정: 단어들은 조건부 독립\n공식: \\(P(c \\mid d) \\propto P(c) \\prod_i P(w_i \\mid c)\\)",
    "crumbs": [
      "Apps",
      "데이터 활용 (Exploitation)",
      "부록-베이지안 통계학과 응용"
    ]
  },
  {
    "objectID": "4-5.html#hedonic-pricing-model-헤도닉-가격-모형",
    "href": "4-5.html#hedonic-pricing-model-헤도닉-가격-모형",
    "title": "부록-베이지안 통계학과 응용",
    "section": "2.3 Hedonic Pricing Model (헤도닉 가격 모형)",
    "text": "2.3 Hedonic Pricing Model (헤도닉 가격 모형)\n\n문제: 재화의 가격을 품질 특성들의 함수로 설명\n베이지안 회귀(Bayesian Regression) 적용 시 장점:\n\n각 특성 기여도의 사후 분포 추정 가능\n사전 지식 반영\n예측 구간 등 불확실성 표현 가능",
    "crumbs": [
      "Apps",
      "데이터 활용 (Exploitation)",
      "부록-베이지안 통계학과 응용"
    ]
  },
  {
    "objectID": "4-3.html",
    "href": "4-3.html",
    "title": "통계적 추론 (Statistical Inference)",
    "section": "",
    "text": "빈도주의자 vs. 베이지언: 서로 다른 통계철학",
    "crumbs": [
      "Apps",
      "데이터 활용 (Exploitation)",
      "통계적 추론 (Statistical Inference)"
    ]
  },
  {
    "objectID": "4-3.html#세대-간-소득-이동성-측정-문제",
    "href": "4-3.html#세대-간-소득-이동성-측정-문제",
    "title": "통계적 추론 (Statistical Inference)",
    "section": "1 세대 간 소득 이동성 측정 문제",
    "text": "1 세대 간 소득 이동성 측정 문제\n우리는 종종 “부모 세대의 소득 수준이 자녀 세대의 소득 수준에 얼마나 영향을 미칠까?” 와 같은 질문에 답하고자 합니다. 이는 사회의 기회균등 수준을 보여주는 중요한 지표인 ‘세대 간 소득 이동성(intergenerational income mobility)’ 측정 문제입니다. 통계학은 이러한 질문에 답하기 위해 데이터를 분석하는 방법론을 제공합니다.\n예를 들어, 많은 가구의 부모 소득(\\(X\\))과 자녀 소득(\\(Y\\)) 데이터를 수집했다고 가정해 봅시다. 우리의 주된 관심은 \\(X\\)와 \\(Y\\) 사이의 관계, 특히 \\(X\\)가 1단위 변할 때 \\(Y\\)가 평균적으로 얼마나 변하는지 (\\(β_1\\)​)를 측정하는 것일 수 있습니다. \\(β_1​= 0\\) 이라면 완전한 이동성(부모 소득과 자녀 소득 무관), \\(β_1​=1\\) 이라면 완전한 고착(부모 소득이 그대로 자녀 소득 결정)을 시사할 수 있습니다. 이 \\(β_1\\)​ 값을 데이터로부터 어떻게 추정하고 그 추정치의 불확실성은 어떻게 평가할까요? 여기에 대해 통계학은 크게 두 가지 다른 접근 방식을 제시합니다. 바로 빈도주의(Frequentist)와 베이지안(Bayesian) 통계 철학입니다. 두 철학은 데이터로부터 결론을 도출하는 기본적인 방법론과 그 결과의 해석 방식에서 차이를 보입니다.",
    "crumbs": [
      "Apps",
      "데이터 활용 (Exploitation)",
      "통계적 추론 (Statistical Inference)"
    ]
  },
  {
    "objectID": "4-3.html#데이터의-질과-통계적-추론에-대한-철학",
    "href": "4-3.html#데이터의-질과-통계적-추론에-대한-철학",
    "title": "통계적 추론 (Statistical Inference)",
    "section": "2 데이터의 질과 통계적 추론에 대한 철학",
    "text": "2 데이터의 질과 통계적 추론에 대한 철학\n어떤 통계 철학을 따르든, 데이터로부터 의미 있는 결론을 얻기 위해서는 데이터 자체의 질이 중요합니다. 특히 다음 두 가지 개념은 대부분의 통계적 추론에서 필수적인 기반이 됩니다.\n\n실현된 결과의 편향되지 않은 표본 (Unbiased Sample of Realized Outcomes):\n\n우리가 분석하는 데이터(표본)는 관심을 갖는 더 큰 집단(모집단)이나 데이터 생성 과정을 편향 없이 잘 대표해야 합니다. 즉, 표본 추출 과정에서 특정 특성을 가진 개체들이 과도하게 혹은 과소하게 포함되지 않아야 합니다. 예를 들어, 소득 이동성 연구에서 고소득층 부모 가구만 표본에 포함된다면, 전체 사회의 이동성을 제대로 반영하지 못할 것입니다.\n표본이 편향되지 않아야 그 분석 결과를 모집단 전체나 일반적인 현상으로 일반화할 수 있습니다. 이는 빈도주의든 베이지안이든 추론의 타당성을 위한 기본 전제입니다.\n\n사건/관측치의 독립성 (Independence of Events/Observations):\n\n한 관측치(예: 한 가구의 소득 정보)의 결과가 다른 관측치의 결과에 영향을 주지 않는다는 가정입니다. 수학적으로는 \\(P(A∩B)=P(A)P(B)\\) 또는 \\(P(A∣B)=P(A)\\)로 표현됩니다.\n무작위 표본 추출(Random Sampling)은 종종 관측치 간 독립성을 확보하기 위한 방법입니다. 많은 통계 모델(예: 단순 회귀분석)은 계산과 해석의 편의를 위해 관측치들이 서로 독립적이라고 가정합니다(i.i.d. 가정의 ‘i’). 만약 특정 지역 가구들이 서로 영향을 주고받는다면 (예: 지역 경제 효과), 독립성 가정이 깨질 수 있으며 분석 시 이를 고려해야 합니다. 독립성 가정의 타당성은 두 관점 모두에서 중요하게 검토되어야 합니다.\n\n\n데이터의 질이 확보되었다는 전제 하에,\n데이터를 해석하고 결론을 도출하는 방식에 대한 체계적인 접근법을 통계 철학(Statistical Philosophy) 이라고 할 수 있습니다. 이는 다음을 포함하는 포괄적인 시스템입니다.\n\n확률의 정의와 해석 방식\n모수(Parameter)의 본질에 대한 관점\n데이터와 사전 지식의 역할\n통계적 추론(추정, 가설 검정 등)의 목표와 방법론\n결과(예: 신뢰구간, p-값 등)의 의미 해석\n\n현대 통계학에는 크게 두 가지 지배적인 통계 철학이 있습니다: 빈도주의와 베이지안.",
    "crumbs": [
      "Apps",
      "데이터 활용 (Exploitation)",
      "통계적 추론 (Statistical Inference)"
    ]
  },
  {
    "objectID": "4-3.html#빈도주의-철학-frequentist-philosophy",
    "href": "4-3.html#빈도주의-철학-frequentist-philosophy",
    "title": "통계적 추론 (Statistical Inference)",
    "section": "3 빈도주의 철학 (Frequentist Philosophy)",
    "text": "3 빈도주의 철학 (Frequentist Philosophy)\n\n핵심 아이디어: 통계적 절차(procedure)의 장기적 성과(long-run performance) 에 초점을 맞춥니다. 즉, 사용하는 추정 방법이나 검정 방법이 동일한 실험을 무한히 반복했을 때 평균적으로 얼마나 잘 작동하는지에 관심을 둡니다.\n확률 해석: 확률을 장기적 빈도(long-run frequency) 로 해석합니다.\n모수(Parameter): 알려지지 않은 고정된 상수(fixed unknown constant) 로 간주합니다. 예를 들어, 소득 이동성 예시에서 전체 모집단의 실제 이동성 계수 \\(β_1\\)​은 우리가 정확히 알지 못하지만, 어떤 고정된 참값을 가진다고 가정합니다.\n모델링 접근 및 가정:\n\n주로 데이터의 조건부 분포 \\(P(Y∣X)\\) 또는 조건부 기댓값 \\(E(Y∣X)\\)를 모델링하는데 집중합니다. 소득 이동성 예시에서는 자녀 소득 \\(Y\\)를 부모 소득 \\(X\\)로 설명하는 모델, 예를 들어 선형 회귀 모델 \\(Y=β_0​+β_1​X+ϵ\\) 을 가정할 수 있습니다.\n주요 목표: 표본 데이터 \\((x_i​,y_i​)\\)를 이용하여 고정된 참값 \\(β_1\\)​을 점 추정(point estimate) 하고, 그 추정치의 정확성(precision) 을 평가하며(예: 표준오차 계산), 모수에 대한 가설을 검정하고 (예: \\(H_0​:β_1​=0\\) 검정), 모수가 포함될 가능성이 높은 구간을 추정(신뢰구간 계산)하는 것입니다.\n통계적 방법: 최소제곱법(Ordinary Least Squares, OLS)을 사용하여 \\(β_1\\) ​을 추정 (estimate)하고, 추정된 계수 \\(\\hat{\\beta_1}\\)의 표준오차(Standard Error)를 계산할 수 있습니다. 이를 바탕으로 t-검정(t-test)을 수행하여 \\(\\hat{\\beta_1}\\)이 통계적으로 유의미한지(0과 다른지) 판단하고, \\(\\hat{\\beta_1}\\)에 대한 신뢰구간을 계산합니다.\n주요 가정 (예: 선형 회귀 모델): 모델 추론의 타당성을 위해 다음과 같은 가정들이 필요할 수 있습니다.\n\n선형성: E(Y∣X)가 X에 대해 선형 관계, i.e. \\(E(Y∣X)=β_0​+β_1​X\\).\n외생성 (Exogeneity): \\(E(ϵ∣X)=0\\). 즉, 설명 변수 X(부모 소득)가 오차항 ϵ(자녀 소득에 영향을 미치는 X 외의 모든 요인)과 상관관계가 없어야 합니다. 이 가정이 성립해야 \\(\\hat{\\beta_1}\\)이 \\(β_1\\)​의 불편향 추정량(unbiased estimator)이 됩니다.\n추가적 가정이지만 완화될 수 있는 등분산성 (Homoscedasticity): \\(Var(ϵ∣X)=σ^2\\). 오차항의 분산이 X 값에 관계없이 일정해야 합니다. 이 가정이 깨지면 OLS 추정량은 여전히 불편향이지만 효율적이지 않으며, 표준오차 계산이 잘못될 수 있습니다. c.f. Heteroscedasticity? GLS\n추가적 가정이지만 완화될 수 있는 정규성 (Normality): \\(ϵ∼N(0,σ^2)\\). 오차항이 정규분포를 따른다는 가정은 주로 작은 표본에서의 가설 검정이나 신뢰구간의 정확한 분포 계산을 위해 필요합니다. (큰 표본에서는 중심극한정리에 의해 자동적으로 완화될 수 있습니다)\n\n\n결과 해석:\n\n점 추정량 \\(\\hat{\\beta}_{1}\\)​: 표본으로부터 계산된 \\(β_1\\)​의 가장 가능성 높은 값.\np-값 (p-value): 귀무가설 H0​ (예: \\(β_1​=0\\))이 사실이라고 가정했을 때, 현재 표본에서 얻은 결과(예: \\(\\hat{\\beta_1}\\)) 또는 그보다 더 극단적인 결과를 얻을 확률입니다. p-값이 유의수준(예: 0.05)보다 작으면 귀무가설을 기각합니다. 이는 귀무가설이 틀렸다는 직접적인 확률이 아닙니다.\n신뢰구간 (Confidence Interval): 예를 들어 95% 신뢰구간은 “만약 우리가 동일한 방식으로 표본을 무수히 많이 뽑아 각각 신뢰구간을 계산한다면, 그 구간들 중 약 95%가 실제 참값 \\(β_1​\\)을 포함할 것”이라는 절차의 신뢰도를 의미합니다. 우리가 계산한 특정한 구간 (예: [0.3, 0.5])이 참값 \\(β_1\\)​을 포함할 확률이 95%라고 말할 수는 없습니다.",
    "crumbs": [
      "Apps",
      "데이터 활용 (Exploitation)",
      "통계적 추론 (Statistical Inference)"
    ]
  },
  {
    "objectID": "4-3.html#베이지안-철학-bayesian-philosophy",
    "href": "4-3.html#베이지안-철학-bayesian-philosophy",
    "title": "통계적 추론 (Statistical Inference)",
    "section": "4 베이지안 철학 (Bayesian Philosophy)",
    "text": "4 베이지안 철학 (Bayesian Philosophy)\n\n핵심 아이디어: 확률을 사용하여 모수에 대한 불확실성을 측정하고 업데이트합니다. 사전 지식(prior belief)을 명시적으로 통합하고, 데이터를 통해 이를 갱신하여 사후 지식(posterior belief)을 얻는 과정을 중시합니다.\n확률 해석: 확률을 믿음의 정도(degree of belief) 로 해석합니다.\n모수(Parameter): 고정된 상수가 아니라 확률 변수(random variable) 로 간주합니다. 즉, 모수 \\(β_1\\)​ 자체가 확률 분포를 가진다고 생각합니다. 우리는 데이터 분석을 통해 이 분포를 학습하고 업데이트합니다.\n모델링 접근 및 믿음 업데이트:\n\n관심 있는 모든 것(데이터와 모수)의 결합 확률 분포(joint probability distribution) 를 명시적으로 설정하는 것에서 시작합니다. 이는 보통 모수 \\(θ=(β_0​,β_1​,σ^2)\\)에 대한 사전 분포 \\(P(θ)\\) 와, 주어진 모수 값에서 데이터 \\((X,Y)\\)가 관측될 확률인 가능도 함수 \\(P(Y∣X,θ)\\) 를 곱하여 정의됩니다 \\(P(Y,X,θ)=P(Y∣X,θ)P(θ)\\).\n가능도 설정: \\(P(Y∣X,θ)\\)는 데이터 생성 과정을 기술합니다. 예를 들어, \\(\\bar{Y}\\)​가 \\(N(β_0​+β_1​ \\bar{X}​,σ^2)\\) 분포를 따른다고 가정할 수 있습니다. 이는 빈도주의의 오차항 가정과 유사하지만, 베이지안에서는 이를 데이터가 모수에 대해 제공하는 정보의 원천으로 해석합니다.\n사전 분포 설정: \\(P(θ)\\)는 데이터를 보기 전에 모수 \\(θ=(β_0​,β_1​,σ^2)\\)에 대해 가지고 있는 사전 지식이나 믿음을 확률 분포로 표현한 것입니다. 예를 들어, 소득 이동성 계수 \\(β_1\\)​은 0과 1 사이의 값을 가질 가능성이 높고, 음수는 아닐 것이라는 믿음을 사전 분포에 반영할 수 있습니다 (예: Beta 분포나 절단된 정규분포 사용). 사전 지식이 없다면, 데이터의 영향을 최대화하기 위해 무정보적(non-informative) 또는 약정보적(weakly informative) 사전 분포를 사용하는 것이 일반적입니다.\n베이즈 정리를 통한 업데이트: 사전 분포와 가능도 함수를 베이즈 정리를 이용해 결합하여 사후 분포(Posterior Distribution) \\(P(θ∣Y,X)\\) 를 계산합니다. \\(P(θ∣Y,X)=P(Y∣X)P(Y∣X,θ)P(θ)​∝P(Y∣X,θ)P(θ)\\)\n여기서 \\(P(Y∣X)=∫P(Y∣X,θ)P(θ)dθ\\)는 정규화 상수입니다. 이 사후 분포는 데이터를 관찰한 후 모수 θ에 대한 업데이트된 믿음을 나타냅니다. 이것이 베이지안 믿음 업데이트 과정의 핵심입니다.\n통계적 방법: 사후 분포가 계산되면(종종 MCMC와 같은 수치적 방법 사용), 이 분포로부터 필요한 모든 정보를 추출합니다. 예를 들어, \\(β_1\\)​의 사후 평균(posterior mean)이나 중앙값(median)을 점 추정치로 사용할 수 있고, 사후 분포의 특정 구간(예: 2.5% 분위수 ~ 97.5% 분위수)을 계산하여 \\(β_1\\)​에 대한 95% 신뢰구간(Credible Interval)을 얻습니다. 또한 사후 분포를 이용하여 \\(P(β_1​&gt;0∣Y,X)\\) 와 같이 특정 가설에 대한 확률을 직접 계산할 수 있습니다.\n\n결과 해석:\n\n사후 분포 P(θ∣Y,X): 데이터와 사전 정보를 고려했을 때, 모수 θ의 가능한 값들에 대한 우리의 업데이트된 믿음의 분포입니다.\n점 추정치 (예: 사후 평균): 사후 분포의 중심 경향을 나타내는 값으로, 데이터를 고려한 후 모수의 가장 그럴듯한 값으로 해석될 수 있습니다.\n신뢰구간 (Credible Interval): 예를 들어 95% 신뢰구간 [0.3, 0.5]는 “우리가 가진 데이터와 모델을 바탕으로 했을 때, 실제 \\(β_1​\\) 값이 0.3과 0.5 사이에 있을 확률(믿음의 정도)이 95%” 라는 직접적인 확률적 해석을 제공합니다.\n가설에 대한 직접 확률: \\(P(β_1​&gt;0∣Y,X)\\) 와 같이 관심 있는 가설이 사실일 확률을 사후 분포로부터 직접 계산하여 제시할 수 있습니다.",
    "crumbs": [
      "Apps",
      "데이터 활용 (Exploitation)",
      "통계적 추론 (Statistical Inference)"
    ]
  },
  {
    "objectID": "4-3.html#비교-요약",
    "href": "4-3.html#비교-요약",
    "title": "통계적 추론 (Statistical Inference)",
    "section": "5 비교 요약",
    "text": "5 비교 요약\n\n\n\n\n\n\n\n\n특징\n빈도주의 (Frequentist)\n베이지안 (Bayesian)\n\n\n\n\n철학 핵심\n절차의 장기적 성과 보장\n확률을 이용한 믿음의 측정 및 업데이트\n\n\n확률 의미\n장기적 빈도 (객관적)\n믿음의 정도 (주관적/논리적)\n\n\n모수(β1​)\n고정된 미지의 상수\n확률 분포를 갖는 확률 변수\n\n\n주요 입력\n데이터 (가능도)\n데이터 (가능도) + 사전 분포\n\n\n주요 출력\n점 추정치, 표준오차, p-값, 신뢰구간\n사후 분포, 점 추정치, 신뢰구간, 가설 확률\n\n\n결과 해석 (구간)\n특정 절차로 계산 시, 95% 확률로 참값을 포함하는 구간 생성\n계산된 특정 구간이 95% 확률로 참값을 포함한다는 믿음\n\n\n요구 사항\n모델 가정 (외생성, 동분산성 등) 충족 중요\n사전 분포 설정 필요, 모델 가정 및 가능도 함수 명시\n\n\n\n세대 간 소득 이동성(\\(β_1\\)​)을 추정할 때, 빈도주의는 \\(β_1\\)​의 ‘가장 좋은’ 추정치를 찾고 그 추정치가 얼마나 정밀한지, 그리고 \\(β_1​=0\\) 가설을 기각할 수 있는지에 집중합니다. 반면 베이지안은 \\(β_1\\)​의 가능한 값들에 대한 우리의 믿음을 사후 분포로 나타내고, 이 분포를 바탕으로 \\(β_1\\)​이 특정 범위(신뢰구간)에 있을 확률이나 특정 가설(예: \\(β_1​&gt;0\\))이 맞을 확률을 직접적으로 제시합니다. 두 접근법 모두 장단점이 있으며, 문제의 성격과 분석가의 목적에 따라 적합한 방법을 선택하거나 상호 보완적으로 활용할 수 있습니다.\n\n5.1 모델의 한계와 결과 해석의 주의점\n지금까지 빈도주의와 베이지안이라는 두 가지 통계 철학을 통해 세대 간 소득 이동성(β1​)을 추정하고 해석하는 방법을 살펴보았습니다. 그러나 어떤 철학을 따르든, 우리가 얻은 통계적 추정치(예: 빈도주의의 \\(\\hat{\\beta}_{1}\\)​ 또는 베이지안의 \\(β_1\\)​에 대한 사후 분포)가 현실 세계의 ‘진짜’ 세대 간 소득 이동성 메커니즘을 완벽하게 반영한다고 생각해서는 안 됩니다.\n이는 특히 사회과학적 현상을 다룰 때 더욱 중요합니다. 세대 간 소득 이동성 예시를 다시 생각해 봅시다. 우리가 고려한 단순 선형 모델 \\(Y=β_0​+β_1​X+ϵ\\) 은 부모 소득(\\(X\\))이 자녀 소득(\\(Y\\))에 미치는 평균적인 연관성을 포착하려 합니다. 하지만 현실은 훨씬 복잡합니다. 자녀의 소득에는 부모 소득 외에도 수많은 요인이 영향을 미칩니다. 예를 들면 다음과 같습니다.\n\n교육 수준: 부모 및 자녀의 교육 수준과 질\n가정 환경: 부모의 양육 방식, 가풍, 문화 자본\n사회적 자본: 부모의 인적 네트워크, 사회적 지위\n지역 환경: 거주 지역의 학교, 커뮤니티, 노동 시장 조건\n유전적 요인: 지능, 재능 등 부모로부터 유전되는 특성\n건강, 운, 차별, 경제 상황 변동, 측정 오차 등\n\n이러한 요인들 중 상당수는 관찰되지 않거나 모델에 포함되지 않았을 가능성이 높습니다. 더 심각한 문제는, 이 요인들 중 일부가 부모 소득(X)과 자녀 소득(Y) 모두에 영향을 미치는 교란 요인(confounding factor) 으로 작용할 수 있다는 점입니다. 예를 들어, 부모의 높은 교육 수준은 부모 소득(\\(X\\))을 높이는 동시에, 자녀에게 더 나은 교육 환경을 제공하여 자녀 소득(\\(Y\\))에도 긍정적인 영향을 줄 수 있습니다. 만약 모델에서 부모의 교육 수준을 통제하지 않는다면, 우리가 추정한 \\(\\hat{\\beta}_1\\)​은 순수한 부모 소득의 영향뿐 아니라 부모 교육 수준의 효과까지 일부 포함하게 되어, \\(β_1\\)​의 크기가 과대 또는 과소 추정될 수 있습니다. 이는 빈도주의 모델의 외생성 가정(\\(E(ϵ∣X)=0\\))이 깨지는 주요 원인이 됩니다.\n이러한 현실을 염두에 둘 때, 통계학자 조지 박스(George E. P. Box)의 유명한 말을 떠올리는 것이 도움이 됩니다:\n\n“All models are wrong, but some are useful.”\n(모든 모델은 틀렸다, 하지만 어떤 것들은 유용하다.)\n\n이 말의 의미는 다음과 같습니다.\n\n모든 모델은 틀렸다: 통계 모델은 복잡한 현실 세계를 단순화(simplification) 한 것입니다. 현실의 모든 복잡성을 완벽하게 포착하는 모델은 존재하지 않습니다. 따라서 우리가 사용하는 어떤 모델이든 현실을 정확히 반영하지 못한다는 점에서 ’틀렸다’고 할 수 있습니다. 소득 이동성 단순 모델도 마찬가지입니다.\n어떤 것들은 유용하다: 모델이 현실과 다를 수 있다고 해서 모델이 아예 가치가 없는 것은 아닙니다. 모델이 유용한(useful) 경우는 다음과 같을 수 있습니다.\n\n현상 이해의 출발점: 단순한 모델이라도 복잡한 현상을 이해하기 위한 좋은 시작점을 제공할 수 있습니다.\n데이터 요약 및 설명: 특정 데이터셋 내에서 변수들 간의 관계를 간결하게 요약하고 설명하는 데 유용할 수 있습니다 (​\\(\\hat{\\beta}_1\\)은 \\(X\\)와 \\(Y\\)의 관찰된 연관성을 나타냅니다).\n예측: 모델의 한계를 인지한다면, 특정 조건 하에서 결과를 예측하는 데 사용될 수 있습니다.\n점진적 개선: 단순 모델의 한계를 인식하고, 교란 요인을 통제하는 더 정교한 모델(예: 다중 회귀분석, 도구 변수법, 구조 방정식 모형 등)로 발전시켜 나갈 수 있습니다.\n\n\n결론적으로, 빈도주의든 베이지안이든 통계적 추론 결과(p-값, 신뢰구간, 사후 확률, 신뢰구간 등)를 해석할 때는 항상 다음을 명심해야 합니다.\n\n결과는 모델에 의존적이다: 모든 통계적 결론은 분석에 사용된 특정 모델과 그 가정 하에서 도출된 것입니다.\n모델은 현실의 근사치이다: 사용된 모델이 현실을 얼마나 잘 근사하는지에 따라 결과의 현실 설명력이 달라집니다. 교란 요인 통제가 미흡하거나 모델 설정이 잘못되었다면, 통계적으로 유의미한 결과(예: 작은 p-값, 0을 포함하지 않는 신뢰구간)를 얻었더라도 그것이 실제 인과관계나 현상의 본질을 반영한다고 단정하기 어렵습니다.\n비판적 사고가 필수적이다: 통계 분석가는 자신이 사용하는 모델의 가정을 명확히 이해하고, 그 가정이 현실적으로 타당한지, 통제되지 못한 교란 요인은 없는지 끊임없이 비판적으로 검토해야 합니다.\n\n따라서 세대 간 소득 이동성 \\(β_1\\)​에 대한 빈도주의적 또는 베이지안적 추정 결과를 제시할 때는, 이것이 특정 모델과 데이터에 기반한 근사치이며, 수많은 잠재적 교란 요인의 영향을 받을 수 있음을 명확히 밝히고 해석에 주의를 기울여야 합니다. 이것이 통계 모델을 책임감 있게 사용하는 자세일 것입니다.",
    "crumbs": [
      "Apps",
      "데이터 활용 (Exploitation)",
      "통계적 추론 (Statistical Inference)"
    ]
  },
  {
    "objectID": "4-1.html",
    "href": "4-1.html",
    "title": "외생성 (exogeneity) vs. 내생성 (endogeneity)",
    "section": "",
    "text": "Exogeneity vs. Endogeneity in Short-Run Models 왜 단기적 분석에서는 인과 추론이 어려운지 그리고 그 어려움을 해결하기 위한 통계적 접근들",
    "crumbs": [
      "Apps",
      "데이터 활용 (Exploitation)",
      "외생성 (exogeneity) vs. 내생성 (endogeneity)"
    ]
  },
  {
    "objectID": "3-3.html",
    "href": "3-3.html",
    "title": "대수의 법칙 (Law of Large numbers)",
    "section": "",
    "text": "한 투자자가 특정 주식(예: 삼성전자)에 매달 1회씩 투자한 후, 5년간의 월간 수익률을 기록했다고 가정해 봅시다. 매월의 수익률은 예측 불가능하게 등락을 반복하지만, 투자자는 “길게 보면 평균적으로 연 8%의 수익률을 기대할 수 있다”는 말을 듣고 투자를 유지하고 있습니다. 이런 질문을 해볼 수 있습니다:\n여기에서 등장하는 것이 바로 Law of Large Numbers (LLN)입니다. LLN은 경험적 평균(empirical average)이 수학적 기댓값(expected value)에 수렴하는 조건을 보여주는 확률론의 가장 핵심적인 정리 중 하나입니다.",
    "crumbs": [
      "Apps",
      "데이터와 가설의 연결고리 (Probability)",
      "대수의 법칙 (Law of Large numbers)"
    ]
  },
  {
    "objectID": "3-3.html#확률변수의-반복과-평균",
    "href": "3-3.html#확률변수의-반복과-평균",
    "title": "대수의 법칙 (Law of Large numbers)",
    "section": "1 확률변수의 반복과 평균",
    "text": "1 확률변수의 반복과 평균\n다음과 같은 확률 모델을 상정합니다.\n\n\\({X_1, X_2, \\dots, X_n}\\)는 i.i.d. random variables (독립 동일 분포 확률변수)입니다.\n각 \\(X_i\\)는 finite expected value를 가지며, \\(\\mathbb{E}[X_i] = \\mu &lt; \\infty\\)\n\n이때, 경험적 평균을 다음과 같이 정의합니다:\n\\[\\bar{X_n} = \\frac{1}{n} \\sum_{i=1}^n X_i\\]\nLaw of Large Numbers는 다음을 말합니다:\n\\(n \\to \\infty\\)일 때, \\(\\bar{X}_n \\to \\mu\\) with high probability (또는 almost surely)\n\n\n\n\n\n\n\n\n정리\n수렴 방식\n의미\n\n\n\n\nWLLN\nin probability\n확률적으로 거의 대부분은 \\(\\mu\\) 근처에 모인다\n\n\nSLLN\nalmost surely\n거의 모든 개별적 sample path에서 평균이 \\(\\mu\\)로 수렴",
    "crumbs": [
      "Apps",
      "데이터와 가설의 연결고리 (Probability)",
      "대수의 법칙 (Law of Large numbers)"
    ]
  },
  {
    "objectID": "3-3.html#weak-law-of-large-numbers-wlln",
    "href": "3-3.html#weak-law-of-large-numbers-wlln",
    "title": "대수의 법칙 (Law of Large numbers)",
    "section": "2 Weak Law of Large Numbers (WLLN)",
    "text": "2 Weak Law of Large Numbers (WLLN)\n확률변수 \\(X_1, X_2, \\dots, X_n\\)가 서로 독립이고 동일한 분포를 따르며, \\(\\mathbb{E}[X_i] = \\mu\\), \\(\\operatorname{Var}(X_i) = \\sigma^2 &lt; \\infty\\)일 때, 다음이 성립합니다:\n\\[\\forall \\varepsilon &gt; 0, \\quad \\lim_{n \\to \\infty} \\mathbb{P}(|\\bar{X}_n - \\mu| \\geq \\varepsilon) = 0\\]\n이는 확률의 관점에서 \\(\\bar{X_n}\\)이 \\(\\mu\\)에 수렴한다는 약한 의미의 수렴입니다. 수렴의 형태는 in probability입니다.",
    "crumbs": [
      "Apps",
      "데이터와 가설의 연결고리 (Probability)",
      "대수의 법칙 (Law of Large numbers)"
    ]
  },
  {
    "objectID": "3-3.html#strong-law-of-large-numbers-slln",
    "href": "3-3.html#strong-law-of-large-numbers-slln",
    "title": "대수의 법칙 (Law of Large numbers)",
    "section": "3 Strong Law of Large Numbers (SLLN)",
    "text": "3 Strong Law of Large Numbers (SLLN)\n같은 조건 하에서 다음이 성립하면 이를 강한 대수의 법칙이라고 부릅니다:\n\\[\\mathbb{P}\\left( \\lim_{n \\to \\infty} \\bar{X}_n = \\mu \\right) = 1\\]\n이는 거의 모든 표본 경로(sample path)에서 평균이 기대값으로 수렴한다는 강한 의미의 수렴입니다. 확률론적으로 훨씬 강한 조건입니다. 수렴의 형태는 almost sure convergence입니다.\n월간 수익률을 확률변수 \\(X_t\\)로 보고, 장기 투자자의 총 누적 수익률은 다음과 같이 평균화된 형태로 정리됩니다:\n\\[\\bar{X_n} = \\frac{1}{n} \\sum_{t=1}^n X_t \\to \\mu \\quad (\\text{as } n \\to \\infty)\\]\n이 수렴을 보장하는 것이 바로 LLN이며, 투자자에게 다음과 같은 의미를 제공합니다:\n\n“매월 수익률이 변동성 있게 움직이더라도, 충분히 긴 시간 동안 투자를 지속하면, 기대 수익률에 수렴하게 된다.”\n\n물론, 이 같은 법칙은 시장 구조가 stationarity를 갖고 있고, i.i.d. assumption이 현실적으로 적절한 경우에만 한정됩니다. 금융자산들의 수익률 분포는 흔히 time-varying하고 heavy-tailed하기 때문에, LLN이 적용되지 않을 수 있습니다.\n\nLaw of Large Numbers는 확률적 반복 실험에서의 평균이 수렴하는 경향을 수학적으로 증명합니다.\nWLLN은 확률적 수렴을, SLLN은 거의 확실한 수렴을 보장합니다.\n수학적으로는 \\(\\bar{X}_n \\to \\mu\\), 금융적으로는 “평균 수익률은 언제 신뢰 가능한가”라는 질문에 대한 답을 제공합니다.\n다음 절에서는, 평균값 뿐 아니라 평균값 주변에 어떤 분포가 형성되는지, 즉 표본 평균의 분포가 어떻게 정규분포로 근사되는지를 설명하는 중심극한정리(Central Limit Theorem)로 넘어가겠습니다.",
    "crumbs": [
      "Apps",
      "데이터와 가설의 연결고리 (Probability)",
      "대수의 법칙 (Law of Large numbers)"
    ]
  },
  {
    "objectID": "3-1.html",
    "href": "3-1.html",
    "title": "수학적 확률 vs. 통계적 확률",
    "section": "",
    "text": "우리는 일상에서 “확률”이라는 말을 자주 사용합니다. 비가 올 확률, 시험에 붙을 확률, 주식이 오를 확률. 그러나 이 확률이 어디서 오는 것인지를 물으면 쉽게 대답하기 어렵습니다. 어떤 확률은 수학적으로 미리 계산 가능한 반면, 어떤 확률은 데이터를 통해 경험적으로 추정해야 합니다. 이 둘은 통계학에서 각각 수학적 확률(mathematical probability)과 통계적 확률(empirical probability)로 구분됩니다.",
    "crumbs": [
      "Apps",
      "데이터와 가설의 연결고리 (Probability)",
      "수학적 확률 vs. 통계적 확률"
    ]
  },
  {
    "objectID": "3-1.html#수학적-확률-이론적-정의와-계산",
    "href": "3-1.html#수학적-확률-이론적-정의와-계산",
    "title": "수학적 확률 vs. 통계적 확률",
    "section": "1 수학적 확률: 이론적 정의와 계산",
    "text": "1 수학적 확률: 이론적 정의와 계산\n수학적 확률은 사건의 결과가 모두 균등하고, 확정된 규칙에 따라 결정되는 경우에 적용됩니다. 동전 던지기, 주사위 굴리기, 카드 섞기 등이 그 예입니다. 이 경우 각 사건의 확률은 전체 가능한 경우의 수에 대한 특정 사건의 경우의 수의 비율로 정의됩니다. 대표적인 예로 다음 두 가지 직관과 어긋나는 확률 문제는 수학적 확률의 개념을 정확히 이해하는 데 도움이 됩니다.\n\n1.1 생일 역설 (Birthday Paradox)\n문제: 30명이 있는 방에서 서로 생일이 겹치는 두 사람이 존재할 확률은 얼마일까요?\n많은 사람들이 50%보다 낮을 것이라 생각하지만, 실제 계산에 따르면 이 확률은 약 70.6%입니다. 놀랍게도 단지 23명만 있어도 겹칠 확률이 50%를 넘습니다.\n왜 그런가요? 핵심은 비교 쌍의 수가 빠르게 증가한다는 점입니다. \\(n\\)명이 있다면 가능한 쌍의 수는 \\(\\binom{n}{2} = \\frac{n(n-1)}{2}\\)입니다. 각 쌍이 서로 다른 생일을 가질 확률을 곱해 나가면, 전체 확률은 빠르게 감소합니다. 결국 문제는 “누군가와 겹칠 확률”이 아니라 “모든 쌍이 다 다를 확률”로 접근해야 합니다. 이 문제는 직관이 통하지 않는 상황에서 수학적 확률이 어떻게 명확한 판단을 제공하는지를 잘 보여줍니다.\n\n\n1.2 비서 문제 (Secretary Problem)\n문제: 한 명의 비서를 채용하려고 \\(n\\)명의 후보를 순서대로 인터뷰합니다. 한 번 지나친 후보는 다시 선택할 수 없습니다. 이때 최적의 전략과 그 전략을 통해 최고의 후보를 선택할 확률은?\n정답은 약 37%, 정확히는 \\(\\frac{1}{e} \\approx 0.368\\)입니다.\n최적 전략은 다음과 같습니다: 1. 먼저 전체 중 약 \\(n/e\\)명 (\\(\\approx 37\\%\\))을 관찰만 하면서 기록합니다. 2. 그 이후 등장하는 사람 중 관찰한 후보들 중 가장 뛰어났던 사람보다 뛰어난 사람이 나오면 즉시 채용합니다.\n이 문제는 사건의 확률을 수학적으로 정의하고 최적 전략을 설계하는 사고의 전형을 보여줍니다. 특히 한 번의 선택이 불확실성을 줄일 수 있다는 점에서 확률적 의사결정의 구조를 잘 설명합니다.",
    "crumbs": [
      "Apps",
      "데이터와 가설의 연결고리 (Probability)",
      "수학적 확률 vs. 통계적 확률"
    ]
  },
  {
    "objectID": "3-1.html#통계적-확률-경험에-기반한-추정",
    "href": "3-1.html#통계적-확률-경험에-기반한-추정",
    "title": "수학적 확률 vs. 통계적 확률",
    "section": "2 통계적 확률: 경험에 기반한 추정",
    "text": "2 통계적 확률: 경험에 기반한 추정\n수학적 확률은 사전적으로 계산 가능한 경우에만 적용됩니다. 하지만 현실의 대부분 문제는 규칙이 불분명하거나 경우의 수를 모두 나열할 수 없습니다.\n예를 들어:\n\n내일 주식시장이 오를 확률\n특정 지역에서 지진이 발생할 확률\n백신 접종 후 부작용이 생길 확률\n\n이러한 경우에는 데이터를 관측하고 그 빈도를 바탕으로 확률을 추정해야 합니다. 이것이 바로 통계적 확률(empirical probability)입니다.\n통계적 확률은 사건이 여러 번 반복되었을 때 관측되는 빈도로 정의되며, 대수의 법칙(Law of Large Numbers)에 따라 표본의 수가 증가할수록 모집단의 진짜 확률에 가까워집니다.",
    "crumbs": [
      "Apps",
      "데이터와 가설의 연결고리 (Probability)",
      "수학적 확률 vs. 통계적 확률"
    ]
  },
  {
    "objectID": "3-1.html#확률과-유용한-정보-엔트로피의-개념",
    "href": "3-1.html#확률과-유용한-정보-엔트로피의-개념",
    "title": "수학적 확률 vs. 통계적 확률",
    "section": "3 확률과 유용한 정보: 엔트로피의 개념",
    "text": "3 확률과 유용한 정보: 엔트로피의 개념\n확률은 불확실성과 정보량이라는 개념과도 밀접하게 연결되어 있습니다. 이때 핵심적인 개념이 바로 정보이론에서의 엔트로피(entropy)입니다.\n\n“내일 해가 뜰 것이다”라는 정보는 예측 가능성이 높기 때문에 불확실성이 0에 가까우며, 정보량도 거의 없습니다.\n반면 “내일 서울에서 눈이 올 것이다”라는 정보는 발생 확률이 낮아 불확실성이 크고, 따라서 정보량도 높습니다.\n\n하나의 사건 \\(x\\)가 발생할 확률이 \\(p(x)\\)일 때, 정보 이론에서는 해당 사건의 정보량을 다음과 같이 정의됩니다:\n\\[I(x) = -\\log p(x)\\]\n즉, 확률이 낮을수록 정보량이 크다는 뜻입니다.\n그리고 전체 엔트로피는 다음과 같이 정의됩니다:\n\\[H(X) = -\\sum_{x} p(x) \\log p(x)\\]\n이 값은 확률분포가 얼마나 퍼져 있는가, 즉 불확실성이 얼마나 큰가를 수치적으로 나타냅니다.\n\n모두가 똑같은 값을 가질 때: 엔트로피는 0 (정보 없음)\n결과가 고르게 분포할 때: 엔트로피는 최대 (정보 풍부)\n\n엔트로피는 데이터의 구조를 요약할 때도 유용합니다. 예를 들어 뉴스 추천 시스템에서는 사용자의 클릭 확률이 불확실할수록 추천 시스템의 정보이득(information gain)이 높아지며, 이는 더 효과적인 학습 기회를 제공하게 됩니다.",
    "crumbs": [
      "Apps",
      "데이터와 가설의 연결고리 (Probability)",
      "수학적 확률 vs. 통계적 확률"
    ]
  },
  {
    "objectID": "2-1.html",
    "href": "2-1.html",
    "title": "1D Statistics and Plots",
    "section": "",
    "text": "“우리는 숫자보다 숫자의 패턴을 읽어야 합니다”\n수치는 정밀함을 제공하고, 시각화는 구조적 직관을 제공합니다. 수익률 데이터처럼 크기와 분산, 방향성과 꼬리 두께까지 고려해야 하는 경우, 두 도구는 함께 사용하는 것이 유용합니다. 예컨대, 평균 수익률이 같다고 해도 표준편차, 왜도, 첨도, 이상값의 유무에 따라 투자자의 판단은 전혀 달라질 수 있습니다.\n두 자산, 삼성전자와 SK하이닉스 주식의 최근 24개월간 월간 수익률 데이터를 비교해 봅니다. 같은 시장에서 거래되는 두 종목이라도 수익률의 크기, 변동성, 비대칭성은 매우 다를 수 있습니다. 이 비교를 통해 우리가 알고 싶은 것은 단순히 “어느 쪽 수익률이 더 높은가?”가 아닙니다. 중요한 질문은 다음과 같습니다:\n이러한 질문에 합리적으로 답하기 위해 필요한 도구가 바로 수치 요약(Statistics)과 시각화(Plots)입니다.",
    "crumbs": [
      "Apps",
      "데이터 탐색 (Exploration)",
      "1D Statistics and Plots"
    ]
  },
  {
    "objectID": "2-1.html#중심-경향의-측정",
    "href": "2-1.html#중심-경향의-측정",
    "title": "1D Statistics and Plots",
    "section": "1.1 중심 경향의 측정",
    "text": "1.1 중심 경향의 측정\n\n평균 (mean): 수익률의 총합을 관측치 수로 나눈 값. 그러나 극단값(outlier)에 민감합니다.\n중앙값 (median): 데이터를 정렬했을 때 중간에 위치한 값. 비대칭 분포에서 더 안정적인 대표값입니다.\n최빈값 (mode): 가장 자주 등장한 값. 분포의 패턴이 뚜렷한 경우 유용합니다.\n\n예: 고소득자의 존재로 인해 평균소득이 높아진 분포에서는 중앙값이 오히려 실제 중간소득층을 더 잘 대표합니다. 이처럼 소득분포는 대개 오른쪽으로 긴 꼬리를 가진 비대칭 분포를 보입니다.",
    "crumbs": [
      "Apps",
      "데이터 탐색 (Exploration)",
      "1D Statistics and Plots"
    ]
  },
  {
    "objectID": "2-1.html#산포의-측정과-변동성의-해석",
    "href": "2-1.html#산포의-측정과-변동성의-해석",
    "title": "1D Statistics and Plots",
    "section": "1.2 산포의 측정과 변동성의 해석",
    "text": "1.2 산포의 측정과 변동성의 해석\n\n표준편차 (standard deviation)는 평균으로부터 값들이 얼마나 퍼져 있는지를 나타냅니다. 분산의 제곱근입니다.\nRMS (Root Mean Square)는 값들의 크기를 평균적으로 평가하며, 평균이 0일 때에도 유용합니다.\n\n\\[\\text{RMS} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n x_i^2}, \\quad s = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2}\\]\n수익률이 평균 0에 가까워도 변동성이 크면 RMS는 여전히 크며, 이는 평균 수익률 (mean return)이 같아도 변동성(volatility)이 다르다는 점을 반영합니다.",
    "crumbs": [
      "Apps",
      "데이터 탐색 (Exploration)",
      "1D Statistics and Plots"
    ]
  },
  {
    "objectID": "2-1.html#분포의-모양-왜도와-첨도",
    "href": "2-1.html#분포의-모양-왜도와-첨도",
    "title": "1D Statistics and Plots",
    "section": "1.3 분포의 모양: 왜도와 첨도",
    "text": "1.3 분포의 모양: 왜도와 첨도\n\n왜도 (skewness)는 분포의 비대칭성을 나타냅니다. 오른쪽 꼬리가 길면 양의 왜도, 왼쪽이 길면 음의 왜도입니다.\n첨도 (kurtosis)는 분포의 뾰족함과 꼬리의 두께를 설명합니다. 정규분포보다 꼬리가 두꺼우면 극단값의 발생 가능성이 높습니다.\n\n예를 들어, 시가총액이 작은 주식들(small size)의 수익률이 시가총액이 큰 주식들(big size)보다 평균적으로 더 높을 수 있습니다. 그러나 small size 주식의 수익률 분포가 더 큰 왜도(skewness)와 더 높은 첨도(kurtosis)를 가진다면, 해당 주식에 투자한 투자자는 높은 기대수익률과 함께 더 큰 변동성 및 극단적 손익 가능성을 동시에 감수해야 합니다.\n이러한 현상은 “more risk, more return”이라는 주류 금융경제학 이론의 핵심 철학과도 일치합니다. 특히 자산 가격 결정 이론에서는, 평균 수익률이 높은 자산은 일반적으로 더 큰 분산(variance), 더 긴 꼬리(tail), 더 뾰족한 분포(peakedness)를 동반한다고 가정합니다. 이는 왜도와 첨도가 단순한 통계량이 아니라, 자본시장에서 보상(risk premium)이 형성되는 구조를 이해하는 데도 중요한 역할을 함을 의미합니다.",
    "crumbs": [
      "Apps",
      "데이터 탐색 (Exploration)",
      "1D Statistics and Plots"
    ]
  },
  {
    "objectID": "2-1.html#자유도와-통계적-추정의-정확성",
    "href": "2-1.html#자유도와-통계적-추정의-정확성",
    "title": "1D Statistics and Plots",
    "section": "1.4 자유도와 통계적 추정의 정확성",
    "text": "1.4 자유도와 통계적 추정의 정확성\n자유도(degree of freedom)는 통계적 추정에서 독립적인 정보가 얼마나 있는가를 수량화한 개념입니다.\n표준편차를 계산할 때, 전체 편차 합이 0이 되기 때문에 마지막 하나는 나머지로부터 결정됩니다. 따라서 \\(n\\)개의 편차 중 자유로운 값은 \\(n-1\\)개입니다. 이 때문에 분산 계산에서 분모가 \\(n\\)이 아닌 \\((n-1)\\)이 되는 것입니다.\n선형회귀에서는 자유도가 더욱 중요한 의미를 가집니다.\n\nT-통계량(T-statistic)은 회귀계수가 0이라는 귀무가설을 검정할 때 사용됩니다. 이때의 자유도는 \\(n - k - 1\\)입니다. 여기서 \\(k\\)는 독립변수의 수이며, 1은 상수항(intercept) 때문입니다.\n\\[t = \\frac{\\hat{\\beta}_j}{\\text{SE}(\\hat{\\beta}_j)}, \\quad \\text{with df} = n - k - 1\\]\nAdjusted R-squared (조정된 결정계수)도 모델의 설명력을 평가할 때 자유도를 고려하여 계산됩니다. 단순한 R-squared는 변수의 수를 늘릴수록 무조건 커지기 때문에, 다음과 같이 자유도로 보정합니다: \\[ \\bar{R}^2 = 1 - \\left(1 - R^2\\right) \\cdot \\frac{n - 1}{n - k - 1} \\]\n\n이는 변수의 수가 증가할수록 모델이 더 복잡해지지만, 자유도가 줄어드는 만큼 설명력에 대한 과도한 낙관을 억제하려는 구조입니다. 이처럼 자유도는 통계적 추정의 신뢰성, 검정의 엄밀함, 모델 선택의 합리성에 직결되는 핵심 개념입니다.",
    "crumbs": [
      "Apps",
      "데이터 탐색 (Exploration)",
      "1D Statistics and Plots"
    ]
  },
  {
    "objectID": "2-1.html#히스토그램과-밀도-해석",
    "href": "2-1.html#히스토그램과-밀도-해석",
    "title": "1D Statistics and Plots",
    "section": "2.1 히스토그램과 밀도 해석",
    "text": "2.1 히스토그램과 밀도 해석\n히스토그램(histogram)은 데이터를 구간별로 나누고 각 구간의 빈도수를 막대그래프로 표현한 것입니다. 단순한 수치보다 분포의 모양을 직관적으로 파악할 수 있습니다.\n\n히스토그램은 확률 밀도(probability density)의 개념으로도 해석할 수 있습니다. 관측치가 많아질수록 히스토그램은 연속적인 확률 밀도 함수에 수렴하며, 특히 로그변환이나 비율 변화율(퍼센트 변화)을 적용하면 비대칭 구조를 완화할 수 있습니다.",
    "crumbs": [
      "Apps",
      "데이터 탐색 (Exploration)",
      "1D Statistics and Plots"
    ]
  },
  {
    "objectID": "2-1.html#boxplot-중앙값-사분위수-이상값",
    "href": "2-1.html#boxplot-중앙값-사분위수-이상값",
    "title": "1D Statistics and Plots",
    "section": "2.2 Boxplot: 중앙값, 사분위수, 이상값",
    "text": "2.2 Boxplot: 중앙값, 사분위수, 이상값\n상자그림(Boxplot)은 데이터를 5개의 지점(최소, Q1, 중앙값, Q3, 최대)으로 요약합니다. 이상값은 박스 밖의 점으로 나타납니다.\n\nIQR(Interquartile Range)이 크면 변동성이 크다는 뜻입니다.\n박스가 한쪽으로 치우쳐 있으면 분포가 비대칭임을 나타냅니다.\n\n\nBoxplot은 특히 변수 간 비교에서 수익률의 위치와 분산을 동시에 시각화할 수 있는 장점이 있습니다.",
    "crumbs": [
      "Apps",
      "데이터 탐색 (Exploration)",
      "1D Statistics and Plots"
    ]
  },
  {
    "objectID": "1-3.html",
    "href": "1-3.html",
    "title": "활용목적에 따른 데이터의 질",
    "section": "",
    "text": "데이터는 본래 침묵합니다. 데이터를 말하게 하는 것은, 우리가 던지는 질문과 그것을 해석하려는 태도입니다. 그러나 질문이 아무리 정교하더라도, 데이터 자체가 왜곡되어 있다면 그 결론 역시 신뢰하기 어렵습니다. 따라서 데이터 분석에서 가장 먼저 점검해야 할 것은, 우리가 다루는 데이터가 “무엇을 반영하고 있고, 무엇을 누락하고 있는가”입니다.",
    "crumbs": [
      "Apps",
      "통계 데이터 (Statistical Data)",
      "활용목적에 따른 데이터의 질"
    ]
  },
  {
    "objectID": "1-3.html#표본은-모집단을-대표하는가",
    "href": "1-3.html#표본은-모집단을-대표하는가",
    "title": "활용목적에 따른 데이터의 질",
    "section": "1 표본은 모집단을 대표하는가?",
    "text": "1 표본은 모집단을 대표하는가?\n통계학은 관찰할 수 있는 일부(표본)를 바탕으로 전체(모집단)에 대해 추론하는 학문입니다. 이때 표본은 반드시 모집단의 속성을 잘 반영해야 합니다. 이것이 대표성(representativeness)입니다.\n예를 들어, 특정 시간대의 대도시 중심부에서 시민들의 정치 성향을 조사한 결과를 전체 유권자의 의견이라 주장하는 것은 무리일 수 있습니다. 이처럼 대표성 오류가 생기면, 아무리 정밀한 분석이라 해도 결론은 잘못된 방향으로 흐를 수 있습니다.\n더불어 선택 편향(selection bias)도 자주 발생하는 오류입니다. “지방도로에서 고속도로보다 보행자 사고가 많다”는 통계가 그 예입니다. 겉으로는 지방도로가 더 위험한 듯 보이지만, 고속도로에는 보행자 자체가 거의 존재하지 않는다는 사실을 간과하면 판단을 그르칠 수 있습니다.\n이러한 오류를 줄이기 위해 통계학은 무작위 표본추출(random sampling)을 기본 전제로 삼습니다. 모든 대상이 동등한 확률로 선택될 수 있어야 하며, 경우에 따라서는 층화 표본추출(stratified sampling) 같은 보정 기법도 함께 사용합니다. 단지 많은 양의 데이터를 확보하는 것보다, 제대로 뽑은 적절한 표본을 확보하는 일이 더 중요합니다.\nc.f. Median Voter Theorem: 유권자의 선호가 1차원 선형 공간에 배열될 수 있는 특수한 경우, 다수결 제도 하에서는 중간 위치에 있는 유권자(중위 유권자, median voter)가 선호하는 정책이 선택됩니다. 이는 분포의 중앙값이 LAD (Least Absolute Deviation) 기준의 최적해이기 때문입니다. 두 개 정당이 경쟁하는 양당제(bipartisanship)에서는 양당 모두 중도 유권자층의 선호를 중심으로 정책을 조정하게 됩니다. 대표성 개념이 수학적으로 정당화되는 한 예입니다.",
    "crumbs": [
      "Apps",
      "통계 데이터 (Statistical Data)",
      "활용목적에 따른 데이터의 질"
    ]
  },
  {
    "objectID": "1-3.html#생존한-것만을-보면-진실은-보이지-않는다",
    "href": "1-3.html#생존한-것만을-보면-진실은-보이지-않는다",
    "title": "활용목적에 따른 데이터의 질",
    "section": "2 생존한 것만을 보면, 진실은 보이지 않는다",
    "text": "2 생존한 것만을 보면, 진실은 보이지 않는다\n통계적 분석에서 자주 간과되는 또 하나의 오류는 생존 편의(survivorship bias)입니다. 이는 성공하거나 계속 존재하는 사례만을 대상으로 분석하는 경우 발생합니다.\n예컨대 30년간 주식시장에 존재한 기업들의 수익률을 분석할 때, 현재까지 상장된 기업만을 기준으로 삼는다면 실제보다 훨씬 우수한 성과가 관측될 수 있습니다. 그러나 이미 도태된 수많은 기업들은 결과에서 빠져 있고, 이는 전체 시장의 평균 성과를 왜곡시킬 수 있습니다.\n 1980년대에는 비재벌 기업이 상대적으로 높은 수익률을 보였지만, 2000년대 이후로는 재벌 계열사가 시장을 지배하며 성과 우위를 고착화하는 흐름을 보입니다.",
    "crumbs": [
      "Apps",
      "통계 데이터 (Statistical Data)",
      "활용목적에 따른 데이터의 질"
    ]
  },
  {
    "objectID": "1-3.html#데이터는-수치일-뿐-해석은-맥락이다",
    "href": "1-3.html#데이터는-수치일-뿐-해석은-맥락이다",
    "title": "활용목적에 따른 데이터의 질",
    "section": "3 데이터는 수치일 뿐, 해석은 맥락이다",
    "text": "3 데이터는 수치일 뿐, 해석은 맥락이다\n수치는 객관적으로 보이지만, 해석은 언제나 주관적 판단과 연결되어 있습니다. 현직 프리미엄 현상처럼, 통계적으로 반복되는 경향이 있다고 해서 그것이 ’좋은 현상’이라는 결론으로 바로 이어지는 것은 아닙니다. 유권자의 만족, 단순한 인지도 효과, 지역별 정치 성향, 시대적 분위기 등의 요인은 통계값 속에 명시되어 있지 않기 때문입니다.\n마찬가지로, “소득이 높을수록 행복도 높다”는 명제 역시 절대적으로 성립하지 않습니다. 일정 수준 이상의 소득을 넘어서면, 행복은 더 이상 단순한 함수처럼 증가하지 않습니다. 이런 경향은 문화권에 따라 달라지기도 하며, 시대적 맥락에 따라 다르게 해석됩니다.\n요컨대, 통계적 수치를 해석할 때에는 언제나 수치 바깥에 존재하는 구조적 요인들을 함께 고려해야 합니다. 통계학은 수치 그 자체보다는, 그 수치가 어디서 왔고, 어디까지 말할 수 있는지를 따져보는 태도를 함께 요구합니다.",
    "crumbs": [
      "Apps",
      "통계 데이터 (Statistical Data)",
      "활용목적에 따른 데이터의 질"
    ]
  },
  {
    "objectID": "1-3.html#시간의-흐름은-데이터를-바꾼다",
    "href": "1-3.html#시간의-흐름은-데이터를-바꾼다",
    "title": "활용목적에 따른 데이터의 질",
    "section": "4 시간의 흐름은 데이터를 바꾼다",
    "text": "4 시간의 흐름은 데이터를 바꾼다\n횡단면 자료가 ‘여럿을 동시에 보는’ 것이라면, 시계열 자료는 ‘하나를 계속해서 지켜보는’ 것입니다. 경제지표, 주가, 물가지수처럼 시간에 따라 변화하는 데이터를 분석할 때 우리는 시계열적 사고를 필요로 합니다.\n시계열 분석에서는 다음과 같은 개념들이 기본이 됩니다:\n\n시차(lag)1: 과거의 값이 현재에 영향을 미치는 시간적 거리입니다. 예를 들어, \\(Y_{t-1}\\)은 이전 시점의 \\(Y\\) 값을 의미합니다.\n자기회귀모형(AR model)2: 현재의 값을 과거의 값들로 설명하는 모델로, \\(Y_t = \\phi_1 Y_{t-1} + \\phi_2 Y_{t-2} + \\cdots + \\varepsilon_t\\) 같은 형태를 가집니다.\n정상성(stationarity)3: 시계열이 시간에 따라 평균, 분산, 공분산 구조가 일정하게 유지되는 성질입니다. 정상성이 확보되어야 예측의 안정성과 신뢰성이 유지됩니다.\n\n예를 들어, 한 국가의 GDP가 꾸준히 상승한다면, 그 흐름은 장기 추세(long-term trend)4로 이해할 수 있습니다. 반면 경기순환이나 고용률의 등락처럼 주기적 구조(cyclic pattern)5를 보이는 변수는 일정한 파동을 따를 수 있습니다.\n시계열 자료의 분석에서는 또 하나 중요한 개념이 바로 변동성(volatility)6입니다. 이는 주로 금융 시장에서 사용되는 개념으로, 수익률이 시간에 따라 얼마나 흔들리는지를 나타냅니다. 변동성이 클수록 불확실성은 커지지만, 역설적으로 옵션(option) 상품의 가치는 이러한 불확실성 덕분에 커지기도 합니다. 콜옵션의 가격은 자산가격이 급등할 때 크게 상승하며, 이처럼 비대칭적 payoff 구조를 가지는 파생상품에서는 변동성 자체가 중요한 분석 변수로 작용합니다.\n마지막으로, 시계열 자료는 여러 가지 구조적 컴포넌트로 구성됩니다. 이에는 추세(trend), 주기(cycle), 계절성(seasonality)이 있으며, 각각은 경제 현상에서 고유한 리듬을 형성합니다. 이러한 구조를 부드럽게 추정하기 위해 통계학에서는 헨더슨 가중치 스무딩(Henderson weight smoothing)7 같은 경험적 기법을 사용하기도 합니다. 이 방법은 과거와 현재의 관측값을 가중 평균하여, 급격한 단기 변동성을 제거하고 장기적인 흐름을 보다 선명하게 파악하는 데 도움을 줍니다.",
    "crumbs": [
      "Apps",
      "통계 데이터 (Statistical Data)",
      "활용목적에 따른 데이터의 질"
    ]
  },
  {
    "objectID": "1-3.html#표본은-어디서-끊겼는가",
    "href": "1-3.html#표본은-어디서-끊겼는가",
    "title": "활용목적에 따른 데이터의 질",
    "section": "5 표본은 어디서 끊겼는가?",
    "text": "5 표본은 어디서 끊겼는가?\n마지막으로 주의해야 할 것은 결측 표본(truncated sample)의 문제입니다. 이는 표본 자체가 관측 대상의 일정 조건을 만족하는 일부만으로 구성되어 있을 때 발생합니다. 예를 들어, 범죄 사건 분석에서 특정 전과 기록이 있는 사람들만을 분석 대상으로 삼는다면, 우리는 ’범죄자’가 아닌 ’적발된 범죄자’만을 분석하게 됩니다. 마치 도박에서 딴 기억만 기억하는 도박꾼처럼, 통계 분석도 무엇이 관측되었고 무엇이 누락되었는가를 명확히 해야만 합리적 해석이 가능할 것입니다.",
    "crumbs": [
      "Apps",
      "통계 데이터 (Statistical Data)",
      "활용목적에 따른 데이터의 질"
    ]
  },
  {
    "objectID": "1-3.html#footnotes",
    "href": "1-3.html#footnotes",
    "title": "활용목적에 따른 데이터의 질",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLag는 시계열 데이터에서 변수의 과거 값을 기준 시점에 연결해 주는 시간 차이를 의미합니다. 예: \\(Y_{t-1}, Y_{t-2}\\) 등.↩︎\nAutoregressive (AR) model은 시계열 변수의 현재 값이 자신의 과거 값들의 선형 결합으로 표현되는 모형입니다.↩︎\nStationarity는 시계열의 평균, 분산, 자기상관 등이 시간에 따라 변하지 않는 성질로, 예측 가능성을 높이는 전제입니다. \\(\\mathbb{E}[Y_t] = \\mu\\) (불변인 평균), \\(\\mathrm{Var}(Y_t) = \\sigma^2\\) (불변인 분산), \\(\\mathrm{Cov}(Y_t, Y_{t+h}) = \\gamma(h)\\) (시차 \\(h\\)에만 의존하는 공분산)을 만족하는 경우를 말합니다. 이를 weak stationarity 또는 covariance stationarity라고 부릅니다.↩︎\nLong-term trend는 데이터가 시간에 따라 보여주는 지속적인 증가 또는 감소의 방향성을 의미합니다. \\(Y_t = \\tau_t + \\varepsilon_t\\) 와 같이 추세(trend) 성분 \\(\\tau_t\\)와 오차 \\(\\varepsilon_t\\)로 분해했을 때, \\(\\tau_t\\)의 증가 혹은 감소 방향성을 의미합니다. 보통 선형 함수 \\(\\tau_t = \\alpha + \\beta t\\)나 비선형 함수(예: 로그, 지수)를 사용해 모델링됩니다.↩︎\nCyclic function은 경기에 따라 반복적으로 나타나는 파동, 즉 비정기적이나 반복되는 상승과 하강의 패턴을 설명합니다. \\(f(t + T) = f(t) \\quad \\text{for all } t\\) 를 만족하는 함수를 의미합니다. 이는 정상적인 주기적 반복 구조를 가지는 이상적인 형태이며, 실제 시계열에서는 완전한 주기성은 드물고, 비정기적이나 유사한 파동이 반복되는 순환(cycle) 형태로 관측됩니다. 경기순환, 생산지수, 실업률 등의 경제지표가 이에 해당합니다.↩︎\nVolatility는 시간에 따른 수익률의 변화폭을 나타내며, 리스크의 정량적 지표로 사용됩니다. \\(\\sigma_t = \\sqrt{\\mathbb{E}[(r_t - \\mu)^2]}\\) 또는 시계열 모델에서는 \\(\\mathrm{Var}(r_t | \\mathcal{F}_{t-1})\\)처럼 조건부 분산 개념으로 확장됩니다. GARCH 등 변동성 모형에서는 시간이 지남에 따라 \\(\\sigma_t^2\\)가 동적으로 변화합니다.↩︎\nHenderson weight smoothing은 특정 시점 주변의 값을 가중 평균하여 노이즈를 제거하고, 데이터의 장기 추세를 부드럽게 표현하는 기법입니다. \\(\\hat{Y}_t = \\sum_{k=-h}^{h} w_k Y_{t+k}\\) 의 형태를 취합니다. Henderson은 13항 또는 23항 등으로 구성된 경험적 필터를 제안하였으며, 추세 성분의 부드러운 변화를 포착하는 데 유용합니다.↩︎",
    "crumbs": [
      "Apps",
      "통계 데이터 (Statistical Data)",
      "활용목적에 따른 데이터의 질"
    ]
  },
  {
    "objectID": "1-1.html",
    "href": "1-1.html",
    "title": "경제지표 (Economic Indicators)",
    "section": "",
    "text": "경제통계학에서 가장 먼저 마주하게 되는 대상은 ’지표(indicator)’입니다. 실업률, 경제성장률, 물가상승률과 같은 지표들은 우리가 경제 현상을 수량화하여 파악하는 데 도움을 줍니다. 그러나 이 지표들은 단순한 숫자가 아니라, 측정의 정의와 방법론에 따라 달라질 수 있는 추상화된 개념입니다. 따라서 통계적 지표를 올바르게 이해하기 위해서는 그 수치가 어떻게 만들어졌는지, 그 정의와 구성방식을 먼저 들여다보는 일이 필요합니다.\n지표란 현실의 복잡한 현상을 요약적으로 나타내기 위해 만든 숫자입니다. 이는 일종의 함수처럼 작동합니다. 예를 들어, ’실업률’이라는 지표는 특정 시점에서의 실업자의 수와 경제활동 인구를 입력값으로 받아 하나의 수치를 출력하는 함수라 할 수 있습니다. 중요한 것은 이 함수가 정확히 어떤 입력을 받아 어떤 규칙으로 출력을 산출하는지 이해하는 것입니다.\n모든 지표는 정의, 측정 단위, 표본 추출 방식, 그리고 가중치 보정이라는 네 가지 주요 요소에 의해 구성됩니다. 이 구성요소가 어떻게 설정되느냐에 따라 지표의 수치는 크게 달라질 수 있으며, 따라서 해석의 방식도 달라져야 합니다.",
    "crumbs": [
      "Apps",
      "통계 데이터 (Statistical Data)",
      "경제지표 (Economic Indicators)"
    ]
  },
  {
    "objectID": "1-1.html#경제지표의-정의와-구성",
    "href": "1-1.html#경제지표의-정의와-구성",
    "title": "경제지표 (Economic Indicators)",
    "section": "",
    "text": "경제통계학에서 가장 먼저 마주하게 되는 대상은 ’지표(indicator)’입니다. 실업률, 경제성장률, 물가상승률과 같은 지표들은 우리가 경제 현상을 수량화하여 파악하는 데 도움을 줍니다. 그러나 이 지표들은 단순한 숫자가 아니라, 측정의 정의와 방법론에 따라 달라질 수 있는 추상화된 개념입니다. 따라서 통계적 지표를 올바르게 이해하기 위해서는 그 수치가 어떻게 만들어졌는지, 그 정의와 구성방식을 먼저 들여다보는 일이 필요합니다.\n지표란 현실의 복잡한 현상을 요약적으로 나타내기 위해 만든 숫자입니다. 이는 일종의 함수처럼 작동합니다. 예를 들어, ’실업률’이라는 지표는 특정 시점에서의 실업자의 수와 경제활동 인구를 입력값으로 받아 하나의 수치를 출력하는 함수라 할 수 있습니다. 중요한 것은 이 함수가 정확히 어떤 입력을 받아 어떤 규칙으로 출력을 산출하는지 이해하는 것입니다.\n모든 지표는 정의, 측정 단위, 표본 추출 방식, 그리고 가중치 보정이라는 네 가지 주요 요소에 의해 구성됩니다. 이 구성요소가 어떻게 설정되느냐에 따라 지표의 수치는 크게 달라질 수 있으며, 따라서 해석의 방식도 달라져야 합니다.",
    "crumbs": [
      "Apps",
      "통계 데이터 (Statistical Data)",
      "경제지표 (Economic Indicators)"
    ]
  },
  {
    "objectID": "1-1.html#실업률",
    "href": "1-1.html#실업률",
    "title": "경제지표 (Economic Indicators)",
    "section": "2 실업률",
    "text": "2 실업률\n실업률은 가장 널리 알려진 경제지표 중 하나입니다. 흔히 “일할 의사가 있는 사람들 중 일하지 못하는 사람의 비율”로 이해됩니다. 공식적으로는 다음과 같이 정의됩니다:\n\\[\\text{실업률 (Unemployment Rate)} = \\frac{\\text{실업자 수}}{\\text{경제활동인구}} \\times 100\\]\n여기서 경제활동인구는 일정 기간 동안 취업했거나 구직활동을 한 만 15세 이상의 인구를 의미합니다. 실업자란 현재 일하지 않지만 일정 기간 내에 적극적으로 구직활동을 한 사람을 뜻합니다.\n한국의 통계청은 매달 약 3만여 가구를 대상으로 실업률을 포함한 경제활동지표를 조사합니다. 이는 미국의 CPS(Current Population Survey)와 유사한 방식으로, 조사 구역을 대표성 있게 설정하고, 적절한 표본 크기와 설계 가중치(weight)를 적용하여 모집단 전체를 추정할 수 있도록 합니다.\n그러나 여기에는 여러 전제가 포함됩니다. 예를 들어, ’실업자’로 분류되기 위해서는 적극적인 구직활동이 필요합니다. 따라서 구직을 포기한 사람은 실업자로 간주되지 않으며, 실업률 수치에서 제외됩니다. 이러한 분류 기준은 실업률의 해석에 중요한 영향을 미칩니다.\n또한, 특정 지역이나 연령대의 비중이 전체 모집단과 다를 경우, 가중치를 조정하여 이를 보완합니다. 이는 비확률적 표본 편향(non-response bias)을 줄이고 지표의 대표성을 확보하기 위한 통계적 보정 과정입니다.",
    "crumbs": [
      "Apps",
      "통계 데이터 (Statistical Data)",
      "경제지표 (Economic Indicators)"
    ]
  },
  {
    "objectID": "1-1.html#경제활동참가율",
    "href": "1-1.html#경제활동참가율",
    "title": "경제지표 (Economic Indicators)",
    "section": "3 경제활동참가율",
    "text": "3 경제활동참가율\n실업률과 함께 자주 활용되는 지표는 경제활동참가율입니다. 이는 다음과 같이 정의됩니다:\n\\[\\text{경제활동참가율} = \\frac{\\text{경제활동인구}}{\\text{만 15세 이상 인구}} \\times 100\\]\n이 지표는 노동시장에 참여하고 있는 인구가 전체 성인 인구 중에서 어느 정도인지를 보여줍니다. 단순히 실업률만으로는 노동시장의 전체 상황을 파악하기 어렵기 때문에, 실업률과 경제활동참가율을 함께 해석해야 보다 정확한 분석이 가능합니다.\n예컨대, 실업률이 낮다고 해서 노동시장이 반드시 건강하다고 단정할 수 없습니다. 경제활동참가율이 매우 낮아 많은 사람들이 노동시장에서 이탈해 있다면, 실업률은 낮더라도 고용의 질적 수준은 매우 낮을 수 있기 때문입니다.",
    "crumbs": [
      "Apps",
      "통계 데이터 (Statistical Data)",
      "경제지표 (Economic Indicators)"
    ]
  },
  {
    "objectID": "1-1.html#물가와-인플레이션",
    "href": "1-1.html#물가와-인플레이션",
    "title": "경제지표 (Economic Indicators)",
    "section": "4 물가와 인플레이션",
    "text": "4 물가와 인플레이션\n물가 수준과 인플레이션률은 국민의 실질 구매력을 판단하는 데 핵심적인 지표입니다. 물가는 소비자물가지수(CPI)를 통해 측정되며, CPI는 대표적인 소비재 묶음의 평균 가격을 바탕으로 산출됩니다.\n\\[\\text{인플레이션률} = \\frac{CPI_t - CPI_{t-1}}{CPI_{t-1}} \\times 100\\]\n하지만 어떤 품목을 소비재 묶음에 포함시킬 것인지, 가중치는 어떻게 줄 것인지에 따라 결과는 크게 달라질 수 있습니다. 이는 인플레이션의 측정이 단순한 계산 문제가 아니라, 측정의 구성 방식이 경제적 해석에 깊이 영향을 준다는 점을 잘 보여줍니다.\n이러한 맥락에서 필립스 커브(Phillips Curve)는 단기적으로 실업률과 인플레이션 간에 역의 관계가 존재한다는 경험적 관찰을 제시합니다. 하지만 이 관계는 시대나 국가에 따라 약화되거나 반전되기도 하며, 통계적 추정의 불확실성을 보여주는 대표적인 사례이기도 합니다.",
    "crumbs": [
      "Apps",
      "통계 데이터 (Statistical Data)",
      "경제지표 (Economic Indicators)"
    ]
  },
  {
    "objectID": "1-1.html#소득-분포와-불평등",
    "href": "1-1.html#소득-분포와-불평등",
    "title": "경제지표 (Economic Indicators)",
    "section": "5 소득 분포와 불평등",
    "text": "5 소득 분포와 불평등\n소득과 재산 분포는 통계적 측면에서 다루기 어려운 지표입니다. 일반적으로 정규분포와 달리 비대칭적이고 heavy-tailed한 형태를 보이기 때문입니다. 이 때문에 평균보다는 분위수(percentile)나 상대적 위치가 분석에 더 적합할 수 있습니다.\n예를 들어, 상위 10%와 하위 10%의 소득을 비교하거나, 상위 1%의 소득 점유율을 계산하는 방식이 대표적입니다. 이러한 접근은 단순한 중앙값이나 평균만으로는 드러나지 않는 분포의 비대칭성과 계층 간 격차를 드러내는 데 유용합니다.\n이러한 분석을 바탕으로 만들어진 지표 중 가장 널리 쓰이는 것이 지니 계수(Gini index)입니다. 지니 계수는 0에 가까울수록 평등한 분포, 1에 가까울수록 불평등한 분포를 의미합니다. 하지만 지니 계수 역시 분포의 ’모양’에 민감하기 때문에, 하나의 수치만으로 불평등을 진단하는 데는 주의가 필요합니다.",
    "crumbs": [
      "Apps",
      "통계 데이터 (Statistical Data)",
      "경제지표 (Economic Indicators)"
    ]
  },
  {
    "objectID": "1-1.html#행복과-통계-그리고-통계적-착시",
    "href": "1-1.html#행복과-통계-그리고-통계적-착시",
    "title": "경제지표 (Economic Indicators)",
    "section": "6 행복과 통계, 그리고 통계적 착시",
    "text": "6 행복과 통계, 그리고 통계적 착시\n현대 경제학에서는 행복과 소득 간의 관계에 주목하는 연구들도 많습니다. 특히 ‘행복 경제학(Happiness Economics)’ 분야에서는 일정 수준 이상의 소득이 삶의 만족도를 결정짓는 주요 요인이 아닐 수 있다는 연구들이 보고되고 있습니다.\n또한 통계적 분석에서는 간단한 평균 비교나 회귀분석에서 발생할 수 있는 오류, 예컨대 심슨의 역설(Simpson’s paradox)에 대한 주의가 필요합니다. 이는 하위 집단에선 명확한 경향이 나타나는데, 전체 집단에선 그 경향이 반대로 나타나는 현상으로, 분석 단위의 설정이 결과 해석에 결정적인 영향을 줄 수 있음을 보여주는 대표적 사례입니다.",
    "crumbs": [
      "Apps",
      "통계 데이터 (Statistical Data)",
      "경제지표 (Economic Indicators)"
    ]
  },
  {
    "objectID": "1-2.html",
    "href": "1-2.html",
    "title": "경제데이터 분석준비",
    "section": "",
    "text": "Preparing for Economic Data Analysis\n통계학에서 분석은 곧 계산이 아닙니다. 계산은 언제나 준비된 구조를 바탕으로 이뤄지며, 그 구조의 이해가 없다면 아무리 정확한 계산이라도 의미 있는 결론에 도달할 수 없습니다. 따라서 본격적인 분석에 앞서, 우리가 다룰 데이터의 구조와 유형, 그리고 그 데이터를 어떻게 다룰 것인지에 대한 이해가 선행되어야 합니다.",
    "crumbs": [
      "Apps",
      "통계 데이터 (Statistical Data)",
      "경제데이터 분석준비"
    ]
  },
  {
    "objectID": "1-2.html#데이터는-행row과-열column로-구성된-표",
    "href": "1-2.html#데이터는-행row과-열column로-구성된-표",
    "title": "경제데이터 분석준비",
    "section": "1 데이터는 행(row)과 열(column)로 구성된 표",
    "text": "1 데이터는 행(row)과 열(column)로 구성된 표\n경제 데이터를 다룰 때 가장 기본이 되는 형식은 데이터 테이블 (data table, data matrix, dataframe)입니다. 여기서 행은 하나의 관측값(개별 사례, 시점, 개체)을, 열은 변수(variable)를 의미합니다. 예컨대 어떤 나라의 매월 GDP, 실업률, 물가 등을 모은 데이터라면, 각 행은 ‘월별 관측치’, 열은 ’경제 변수’들이 됩니다.\n중요한 것은 이 행과 열의 수에 관한 구조적 조건입니다. 통계적으로 유의미한 추론이 가능하려면, 행의 수(n)가 열의 수(p)보다 충분히 많아야 합니다. 이는 단순한 숫자의 문제가 아니라, 식별성(identification)과 관련된 수학적 조건이기 때문입니다. 이를 수학적으로 생각해보면, 열은 변수의 개수이고, 우리가 구하고자 하는 미지수의 수입니다.\n반면 행은 우리가 가진 독립적인 정보, 즉 방정식의 수에 해당합니다. 연립 1차방정식을 풀 때 방정식의 수가 미지수보다 적다면 해를 유일하게 결정할 수 없듯이, 통계에서도 데이터가 충분하지 않으면 변수 간 관계를 식별해낼 수 없습니다. 따라서 “데이터가 많다”는 것은 단지 행이 많다는 뜻이 아니라, 변수에 비해 정보가 충분히 많고, 독립적이며, 잡음보다 신호가 강하다는 전제를 담고 있는 것입니다.",
    "crumbs": [
      "Apps",
      "통계 데이터 (Statistical Data)",
      "경제데이터 분석준비"
    ]
  },
  {
    "objectID": "1-2.html#자료의-구조-횡단면-시계열-그리고-패널",
    "href": "1-2.html#자료의-구조-횡단면-시계열-그리고-패널",
    "title": "경제데이터 분석준비",
    "section": "2 자료의 구조: 횡단면, 시계열, 그리고 패널",
    "text": "2 자료의 구조: 횡단면, 시계열, 그리고 패널\n통계 데이터는 수집 방식과 시간 개념의 유무에 따라 자료 구조(data structure)가 다릅니다. 이 구조를 이해하는 일은 분석 방법을 선택하는 데 결정적인 영향을 줍니다.\n첫째, 횡단면 자료(cross-sectional data)는 같은 시점에서 여러 개체(사람, 국가, 기업 등)에 대해 수집한 자료입니다. 예를 들어, 2023년 현재 서울시 1,000가구의 소득을 조사한 자료는 전형적인 횡단면 자료입니다.\n둘째, 시계열 자료(time-series data)는 하나의 개체에 대해 시간의 흐름에 따라 관측한 자료입니다. 예컨대 한국은행의 월별 소비자물가지수나 분기별 GDP 통계는 시계열 자료에 해당합니다.\n셋째, 패널 자료(panel data)는 여러 개체를 일정한 시간 동안 반복적으로 추적하여 관측한 자료입니다. 패널 자료는 횡단면 자료와 시계열 자료의 장점을 결합한 형태로, 개체 간 이질성(heterogeneity)과 시간에 따른 동태성(dynamic change) 모두를 반영할 수 있어 구조적 분석에 매우 유용합니다.\n대표적인 패널 데이터셋으로는 다음과 같은 것들이 있습니다:\n\nCRSP: 미국 주식시장 상장기업의 월별·일별 재무 및 수익률 자료를 포함하는 금융 시계열 중심의 패널 데이터\nPSID (Panel Study of Income Dynamics): 미국 가계의 소득, 고용, 건강, 교육 등 다양한 정보를 장기적으로 추적한 사회경제 패널\n한국노동패널조사 (KLIPS): 한국노동연구원이 수행하는 조사로, 개인과 가구 단위의 노동시장, 소득, 교육, 건강 정보 등을 지속적으로 기록한 대표적 국내 패널 데이터\n\n이러한 패널 자료는 단면에서는 보이지 않던 개체별 이질성과 시간적 구조를 식별하는 데 중요한 통찰을 제공하며, 고정효과(fixed effect), 동태 패널 모델(dynamic panel model) 등 다양한 통계 기법의 적용이 가능합니다.\n한편, 단순한 형태의 pooled panel data는, 시간 구조를 고려하지 않고 서로 다른 시점의 데이터를 하나로 묶어 횡단면처럼 다루는 방식입니다. 예컨대 2020년과 2021년의 가구소득 데이터를 단순히 하나의 테이블로 병합하여 분석하는 것이 이에 해당합니다. 이는 시계열적 상관 구조나 개체 고정 효과를 무시하므로 분석의 전제와 해석에 유의해야 합니다.",
    "crumbs": [
      "Apps",
      "통계 데이터 (Statistical Data)",
      "경제데이터 분석준비"
    ]
  },
  {
    "objectID": "1-2.html#자료의-측정척도-명목-순서-비율",
    "href": "1-2.html#자료의-측정척도-명목-순서-비율",
    "title": "경제데이터 분석준비",
    "section": "3 자료의 측정척도: 명목, 순서, 비율",
    "text": "3 자료의 측정척도: 명목, 순서, 비율\n데이터는 수치로 보이지만, 그 수치가 실제로 어떤 의미를 담고 있는지에 따라 해석 방식이 달라집니다. 이것이 바로 측정 척도(scale of measurement)의 문제입니다.\n먼저, 명목척도(nominal scale)는 숫자가 단지 ‘이름표’ 역할만 하는 경우입니다. 예를 들어, 성별을 1(남성), 2(여성)으로 표시해도, 이 숫자에 수학적 연산 의미는 없습니다. 단지 범주를 구분하는 데 쓰일 뿐입니다.\n다음으로, 순서척도(ordinal scale)는 숫자에 순서가 부여된 경우입니다. 예를 들어 ’성적 등급’을 1등급, 2등급, 3등급으로 나눈다면, 1이 2보다 우수함을 의미하긴 하지만, 1등급과 2등급 사이의 차이가 2등급과 3등급 사이의 차이와 같다고 보장할 수는 없습니다.\n마지막으로, 비율척도(ratio scale)는 숫자가 ‘간격’뿐 아니라 ‘배율’의 의미도 가지며, 절대적 0점(zero point)이 정의되어 있습니다. 키, 몸무게, 재산, 점수 등은 모두 비율척도로 측정되며, 0은 ’없음’을 의미합니다. 예를 들어 180cm는 90cm의 정확히 두 배이고, 재산이 0이면 정말로 ‘없는’ 상태입니다.\n이처럼 척도의 종류에 따라 우리가 적용할 수 있는 수학적 연산과 통계 기법이 달라지기 때문에, 데이터의 숫자가 무엇을 뜻하는지부터 묻는 일이 통계학의 출발점이 됩니다.",
    "crumbs": [
      "Apps",
      "통계 데이터 (Statistical Data)",
      "경제데이터 분석준비"
    ]
  },
  {
    "objectID": "1-2.html#경제-데이터를-구할-수-있는-곳",
    "href": "1-2.html#경제-데이터를-구할-수-있는-곳",
    "title": "경제데이터 분석준비",
    "section": "4 경제 데이터를 구할 수 있는 곳",
    "text": "4 경제 데이터를 구할 수 있는 곳\n경제통계 분석을 위해 사용할 수 있는 신뢰할 만한 데이터는 여러 곳에서 제공됩니다.\n\n통계청(KOSIS)은 인구, 가구, 고용, 물가, 소득, 산업 등 다양한 범주의 국가통계를 제공합니다.\n한국은행(ECOS)은 금융·경제 지표, 국민계정, 국제수지, 통화량 등 거시지표를 중심으로 데이터를 제공합니다.\n\n이 외에도 세계은행(World Bank), IMF, OECD 등 국제기구에서도 양질의 시계열 및 단면 데이터를 구할 수 있습니다. 데이터의 접근성과 투명성은 경제학 연구뿐 아니라 공공 정책의 설계에도 큰 영향을 미칩니다.",
    "crumbs": [
      "Apps",
      "통계 데이터 (Statistical Data)",
      "경제데이터 분석준비"
    ]
  },
  {
    "objectID": "1-2.html#데이터를-다루는-도구",
    "href": "1-2.html#데이터를-다루는-도구",
    "title": "경제데이터 분석준비",
    "section": "5 데이터를 다루는 도구",
    "text": "5 데이터를 다루는 도구\n데이터 분석에는 여러 도구가 쓰이지만, 사용 목적과 데이터 양에 따라 적합한 도구는 달라집니다.\n가장 보편적으로 쓰이는 도구는 여전히 엑셀(Excel)입니다. 직관적이고 간편하지만, 수식 추적이 어렵고 대용량 분석에는 부적합하다는 점에서 한계도 분명합니다. 이에 비해 구글 시트(Google Sheets)는 무료이며, 간단한 시각화와 공유가 용이하다는 점에서 장점이 있습니다.\n보다 본격적인 분석을 원하는 경우에는 파이썬(Python)의 판다스(Pandas) 라이브러리를 추천할 수 있습니다. Pandas는 대규모 자료의 정제, 그룹 분석, 결측치 처리 등 다양한 기능을 제공하며, 재현성과 자동화 측면에서도 장점이 큽니다.\n통계학을 배우는 이유는 ’엑셀을 능숙하게 다루기 위해서’가 아니라, 어떤 도구를 쓰든 올바른 분석 구조를 설계하고 해석할 수 있도록 훈련되기 위함입니다.",
    "crumbs": [
      "Apps",
      "통계 데이터 (Statistical Data)",
      "경제데이터 분석준비"
    ]
  },
  {
    "objectID": "1-4.html",
    "href": "1-4.html",
    "title": "부록-무작위 표본추출법 (Random Sampling)",
    "section": "",
    "text": "통계학은 불확실성을 다루는 학문입니다. 그러나 이 불확실성이 무질서함을 의미하지는 않습니다. 통계적 추론은 매우 엄격한 수학적 조건과 철학적 전제를 필요로 하며, 그 출발점은 표본을 어떻게 추출하느냐에 있습니다. 무작위 표본추출(random sampling)은 표본이 모집단의 특성을 왜곡 없이 반영하도록 보장하는 가장 기본적인 원칙입니다.",
    "crumbs": [
      "Apps",
      "통계 데이터 (Statistical Data)",
      "부록-무작위 표본추출법 (Random Sampling)"
    ]
  },
  {
    "objectID": "1-4.html#무작위성과-독립성의-개념",
    "href": "1-4.html#무작위성과-독립성의-개념",
    "title": "부록-무작위 표본추출법 (Random Sampling)",
    "section": "1 무작위성과 독립성의 개념",
    "text": "1 무작위성과 독립성의 개념\n무작위 표본추출이란, 모집단의 각 요소가 동일한 확률로 선택될 기회를 가지며, 그 선택이 서로 독립적으로 이루어지는 과정을 말합니다. 여기서 ’동일한 확률’만큼이나 중요한 것이 ’독립성(independence)’입니다. 표본들 간의 추출 결과가 서로 영향을 미치지 않아야 모집단의 구조를 정확히 반영할 수 있습니다.\n예를 들어, 어떤 설문조사에서 첫 번째로 추출된 응답자의 특성이 두 번째 응답자 선정에 영향을 준다면, 이 표본은 더 이상 독립적으로 추출되었다고 볼 수 없습니다. 이러한 경우, 표본 사이의 상관관계로 인해 정보가 중복되거나 편향이 발생하게 됩니다. 이는 추정치의 분산을 과소평가하게 만들며, 결과적으로 신뢰구간이나 통계적 검정의 타당성이 크게 훼손됩니다.\n통계학은 이러한 독립성을 다음의 수학적 관계로 표현합니다: \\(P(A \\cap B) = P(A) \\cdot P(B)\\) 이 조건은 사건 \\(A\\)와 \\(B\\)가 서로 독립일 때의 곱셈 법칙이며, 무작위 추출된 표본들 간에도 동일하게 적용됩니다.",
    "crumbs": [
      "Apps",
      "통계 데이터 (Statistical Data)",
      "부록-무작위 표본추출법 (Random Sampling)"
    ]
  },
  {
    "objectID": "1-4.html#무작위-수를-이용한-파이-값-추정",
    "href": "1-4.html#무작위-수를-이용한-파이-값-추정",
    "title": "부록-무작위 표본추출법 (Random Sampling)",
    "section": "2 무작위 수를 이용한 파이 값 추정",
    "text": "2 무작위 수를 이용한 파이 값 추정\n무작위성과 확률 개념은 단지 철학적 원리에 그치지 않고, 수치 계산에도 직접 응용됩니다. 대표적인 예로 몬테카를로(Monte Carlo) 방법을 이용한 원주율 \\(\\pi\\) 근사 계산이 있습니다.\n단위 정사각형 안에 반지름 1인 사분원을 그려 놓고, 그 안에 무작위로 점을 뿌려서 이 점이 원 안에 들어갈 확률을 계산하면, 전체 면적의 비율을 통해 \\(\\pi\\)를 근사할 수 있습니다. 이때 계산 공식은 다음과 같습니다: \\(\\pi \\approx 4 \\times \\frac{\\text{원이 포함된 점의 수}}{\\text{총 점의 수}}\\)\n이 방법은 무작위로 생성된 점들이 독립적으로 분포되어 있다는 전제 하에서만 유효합니다. 무작위성과 독립성은 계산의 정확성을 보장하는 핵심 전제입니다.",
    "crumbs": [
      "Apps",
      "통계 데이터 (Statistical Data)",
      "부록-무작위 표본추출법 (Random Sampling)"
    ]
  },
  {
    "objectID": "1-4.html#이항분포와-부트스트랩의-구조",
    "href": "1-4.html#이항분포와-부트스트랩의-구조",
    "title": "부록-무작위 표본추출법 (Random Sampling)",
    "section": "3 이항분포와 부트스트랩의 구조",
    "text": "3 이항분포와 부트스트랩의 구조\n통계학에서 자주 등장하는 이항분포도 무작위성과 독립성을 전제로 구성됩니다. 성공 확률이 \\(p\\)인 시행을 \\(n\\)번 반복할 때, 성공 횟수는 이항분포를 따르게 됩니다. 각 시행이 독립적으로 이루어졌다는 전제는 이 분포의 핵심 구성 요소입니다.\n이와 유사한 개념은 부트스트랩(bootstrap) 방법에서도 확인할 수 있습니다. 부트스트랩은 하나의 표본에서 복원추출을 반복하여 여러 개의 재표본을 구성하고, 이를 통해 통계량의 분포를 추정하는 방법입니다. 이때 각 추출이 독립적으로 이루어지는 것이 전제되어야 하며, 이러한 구조 아래에서 중심극한정리가 적용되고 신뢰구간을 설정할 수 있습니다1",
    "crumbs": [
      "Apps",
      "통계 데이터 (Statistical Data)",
      "부록-무작위 표본추출법 (Random Sampling)"
    ]
  },
  {
    "objectID": "1-4.html#신뢰구간과-대표성-문제",
    "href": "1-4.html#신뢰구간과-대표성-문제",
    "title": "부록-무작위 표본추출법 (Random Sampling)",
    "section": "4 신뢰구간과 대표성 문제",
    "text": "4 신뢰구간과 대표성 문제\n통계학에서 자주 사용되는 신뢰구간(confidence interval) 개념 역시 무작위성과 독립성을 전제로 합니다. 동일한 방식으로 95% 신뢰구간을 구성하고 이를 여러 번 반복하면, 약 95%의 경우에 해당 구간이 실제 모수를 포함하게 됩니다. 그러나 이 해석은 표본이 올바르게 무작위로, 그리고 독립적으로 추출되었을 때만 성립합니다.\n예를 들어 특정 후보의 지지율을 조사하면서 그 후보의 지지층이 많은 지역만을 표본으로 삼는다면, 이는 오버샘플링(over-sampling) 문제를 유발하며 통계적 추론은 왜곡될 수밖에 없습니다. 표본의 대표성은 단순히 표본의 수가 많은 것으로 보장되지 않으며, 무작위성과 독립성이 확보되었는지 여부에 따라 결정됩니다.",
    "crumbs": [
      "Apps",
      "통계 데이터 (Statistical Data)",
      "부록-무작위 표본추출법 (Random Sampling)"
    ]
  },
  {
    "objectID": "1-4.html#난수-발생과-역변환법",
    "href": "1-4.html#난수-발생과-역변환법",
    "title": "부록-무작위 표본추출법 (Random Sampling)",
    "section": "5 난수 발생과 역변환법",
    "text": "5 난수 발생과 역변환법\n컴퓨터 시뮬레이션에서는 무작위 수, 즉 난수를 생성하는 방법이 필수적입니다. 일반적으로는 \\([0,1]\\) 구간에서 균등한 확률로 생성된 난수 \\(U\\)를 사용하며, 이를 원하는 분포의 확률변수로 변환하기 위해 역변환법(inverse transform method)이 사용됩니다.\n확률분포 \\(F(x)\\)에 대해 누적분포함수 \\(F\\)의 역함수를 취하면, 다음과 같은 방식으로 새로운 확률변수를 정의할 수 있습니다: \\(X = F^{-1}(U)\\)\n이 방식은 간단하면서도 다양한 분포에 적용이 가능하며, 난수의 구조가 이론적으로 잘 정의되어 있을 때 특히 효과적입니다2.",
    "crumbs": [
      "Apps",
      "통계 데이터 (Statistical Data)",
      "부록-무작위 표본추출법 (Random Sampling)"
    ]
  },
  {
    "objectID": "1-4.html#마르코프-체인과-mcmc-방식",
    "href": "1-4.html#마르코프-체인과-mcmc-방식",
    "title": "부록-무작위 표본추출법 (Random Sampling)",
    "section": "6 마르코프 체인과 MCMC 방식",
    "text": "6 마르코프 체인과 MCMC 방식\n더 복잡한 확률분포에서 표본을 추출하려면 마르코프 체인 몬테카를로(MCMC) 방식이 사용됩니다. 이 방식은 표본 간의 독립성을 전제로 하지 않지만, 장기적으로 특정 분포에 수렴하도록 설계된 확률적 과정입니다.\nMCMC는 마르코프 체인(Markov Chain)을 기반으로 하며, 현재 상태만이 다음 상태를 결정하는 확률적 규칙에 따라 새로운 표본을 생성합니다. 이러한 상태 전이가 충분히 반복되면, 체인은 하나의 스테이셔너리 분포(stationary distribution)에 수렴하게 됩니다. 이 상태에서는 더 이상 분포가 변화하지 않으며, 이 분포는 타겟 분포로 간주할 수 있습니다3.",
    "crumbs": [
      "Apps",
      "통계 데이터 (Statistical Data)",
      "부록-무작위 표본추출법 (Random Sampling)"
    ]
  },
  {
    "objectID": "1-4.html#메트로폴리스헤이스팅스-알고리즘",
    "href": "1-4.html#메트로폴리스헤이스팅스-알고리즘",
    "title": "부록-무작위 표본추출법 (Random Sampling)",
    "section": "7 메트로폴리스–헤이스팅스 알고리즘",
    "text": "7 메트로폴리스–헤이스팅스 알고리즘\nMCMC 기법 중 대표적인 알고리즘은 메트로폴리스–헤이스팅스(Metropolis–Hastings) 방법입니다. 이 방법은 우리가 직접 샘플링할 수 없는 복잡한 분포에서 샘플을 생성하기 위해, 비교적 단순한 제안 분포(proposal distribution)를 사용합니다.\n새로운 상태 \\(x'\\)이 제안되면, 기존 상태 \\(x\\)와의 상대적인 가능성을 평가하는 수용 확률(acceptance probability)을 통해 그 상태를 채택하거나 유지합니다. 수용 확률은 다음과 같이 계산됩니다: \\[\\alpha = \\min\\left(1, \\frac{p(x') q(x|x')}{p(x) q(x'|x)}\\right)\\]\n이 방식을 사용하면, 타겟 분포 \\(p(x)\\)가 정규화 상수를 알 수 없는 경우에도 효과적으로 샘플링할 수 있습니다4.",
    "crumbs": [
      "Apps",
      "통계 데이터 (Statistical Data)",
      "부록-무작위 표본추출법 (Random Sampling)"
    ]
  },
  {
    "objectID": "1-4.html#footnotes",
    "href": "1-4.html#footnotes",
    "title": "부록-무작위 표본추출법 (Random Sampling)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n중심극한정리(Central Limit Theorem)는 독립이고 동일분포를 따르는 확률변수들의 합이 정규분포로 수렴한다는 정리입니다.↩︎\n역변환법은 \\(X \\sim F\\)를 얻기 위해 \\(X = F^{-1}(U)\\)를 계산하는 방식입니다. \\(U\\)는 \\([0,1]\\)에서 균등하게 추출된 난수입니다.↩︎\n스테이셔너리 분포란 확률분포 \\(\\pi\\)가 상태 전이행렬 \\(P\\)에 대해 \\(\\pi P = \\pi\\)를 만족하는 경우를 의미합니다.↩︎\n메트로폴리스–헤이스팅스 알고리즘은 복잡한 분포에서 직접 샘플링이 어려울 때, 수용확률을 계산하여 효율적으로 근사 샘플을 생성하는 MCMC 기법입니다.↩︎",
    "crumbs": [
      "Apps",
      "통계 데이터 (Statistical Data)",
      "부록-무작위 표본추출법 (Random Sampling)"
    ]
  },
  {
    "objectID": "2-2.html",
    "href": "2-2.html",
    "title": "2D Relationship",
    "section": "",
    "text": "합리적 분석 흐름: 수치 요약 → 시각적 탐색 → 구조 해석 → 단위 해석 (탄력성)\n현실 세계의 대부분의 분석 대상은 하나의 변수로 설명되지 않습니다. 중요한 구조는 두 변수 이상 간의 관계에서 나타나며, 이러한 관계의 형태와 강도를 파악하는 것이 데이터 분석의 핵심입니다.",
    "crumbs": [
      "Apps",
      "데이터 탐색 (Exploration)",
      "2D Relationship"
    ]
  },
  {
    "objectID": "2-2.html#d-시각화의-중요성",
    "href": "2-2.html#d-시각화의-중요성",
    "title": "2D Relationship",
    "section": "1 2D 시각화의 중요성",
    "text": "1 2D 시각화의 중요성\n숫자만으로는 파악할 수 없는 구조를 시각적으로 드러내는 것이 산점도(scatterplot)입니다. 두 변수의 관계를 점으로 표시함으로써, 관계의 방향성, 선형성, 변동성, 이상값(outlier) 등을 직관적으로 파악할 수 있습니다. 이를 보다 정교하게 확장한 것이 Joint Plot으로, 각 변수의 단변량 분포와 이변량 산점도를 동시에 보여줍니다. 분포의 왜도(skewness), 집중도, 변동성을 함께 파악할 수 있어 강력한 시각화 도구입니다. 특히 Anscombe’s quartet은 시각화의 필요성을 강조하는 대표적 사례입니다. 네 개의 데이터셋이 동일한 평균, 분산, 상관계수, 회귀선을 가지지만, 시각화해 보면 전혀 다른 구조임을 확인할 수 있습니다. \nAnscombe’s quartet. 이처럼 수치는 관계의 정량적 요약을 가능하게 하지만, 그 구조를 정확히 이해하기 위해서는 시각화가 필수입니다.",
    "crumbs": [
      "Apps",
      "데이터 탐색 (Exploration)",
      "2D Relationship"
    ]
  },
  {
    "objectID": "2-2.html#상관관계와-인과관계는-다릅니다",
    "href": "2-2.html#상관관계와-인과관계는-다릅니다",
    "title": "2D Relationship",
    "section": "2 상관관계와 인과관계는 다릅니다",
    "text": "2 상관관계와 인과관계는 다릅니다\n상관관계(correlation)는 단지 함께 움직이는 경향(co-movement)을 수치적으로 표현한 것이며, 인과관계(causation)와는 전혀 다른 개념입니다. 상관계수는 다음과 같이 계산됩니다:\n\\[\\rho_{X,Y} = \\frac{\\mathrm{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\\]\n이는 두 변수 간의 선형 관계의 강도와 방향을 측정할 뿐이며, \\(|\\rho| = 1\\)이면 완벽한 선형 관계, \\(\\rho = 0\\)이면 선형성이 없음을 의미합니다. 하지만 상관관계는 결코 인과관계(causality)를 보장하지 않습니다.\n예를 들어 운동화 가격과 교수의 월급이 모두 시간이 흐름에 따라 증가하고 있다면, 상관계수는 높게 나올 수 있지만 이 둘 사이에는 직접적 인과관계가 없으며, 이는 단지 공통된 시간 추세(time trend)의 영향일 수 있습니다. 이러한 경우는 허위 상관관계(spurious correlation)로 분류됩니다.\n또한, 두 변수 간에 세 번째 요인(third variable)이 존재할 경우, 상관계수는 실제 인과 구조를 왜곡할 수 있습니다. 예를 들어, 교육 수준과 소득 간에는 상관관계가 관찰되지만, 이 관계는 가정환경, 건강, 지역, 사회 자본 등 다양한 변수의 영향을 받을 수 있습니다.\n이 때문에 인과관계를 분석하려면 단순한 상관계수를 넘어 모형 기반 추정(structural modeling), 혼란변수 통제(confounder control), 또는 실험적 설계(experimental design)가 필요합니다.",
    "crumbs": [
      "Apps",
      "데이터 탐색 (Exploration)",
      "2D Relationship"
    ]
  },
  {
    "objectID": "2-2.html#해석이-쉬운-관계와-어려운-관계",
    "href": "2-2.html#해석이-쉬운-관계와-어려운-관계",
    "title": "2D Relationship",
    "section": "3 해석이 쉬운 관계와 어려운 관계",
    "text": "3 해석이 쉬운 관계와 어려운 관계\n\n3.1 단기적·선형적 관계\n시간의 개입이 거의 없거나, 실험적 통제가 가능한 경우, 또는 물리적으로 정의된 상황에서 특정 변수 간의 관계는 선형 관계로 관측될 수 있습니다. 이러한 관계는 보통 산점도에서도 직선에 가깝게 나타나며, 회귀분석 결과도 안정적입니다. - 키와 몸무게 - 용수철의 늘어난 길이와 매달린 추의 무게 (후크의 법칙) - 중간고사 점수와 기말고사 점수 - 주식 시장의 하루 전 거래량과 당일 거래량\n이러한 관계는 cross-sectional 데이터 또는 단기적 시간관측자료에서 자주 등장하며, 분석과 해석이 비교적 쉽습니다. 그러나 대부분 단순한 기술적 이해에 그치며, 사회과학적 의사결정이나 정책 수립에는 한계가 있습니다.\n\n\n3.2 장기적·비선형적 관계\n반면 사회적, 경제적, 제도적 요소들이 복잡하게 얽혀 있는 장기 관계는 해석이 어렵습니다. 이 경우는 수많은 혼란 요인(confounder), 피드백 효과, 동태적 변화 등이 작용하여 단순한 회귀선이나 상관계수로는 충분히 설명되지 않습니다:\n\n교육 수준과 장기적 연평균 소득\n국가별 자유무역 정책과 경제 성장률\n흡연과 건강 지표\n기후정책과 장기 에너지 소비량\n\n이런 관계들은 정책적 판단이나 경제적 해석에서 매우 중요하지만, 대부분 비선형적 구조를 가지며, 설명변수와 종속변수 간에 시간 지연(lag), 누적 효과, 상호작용 효과가 존재합니다. 또한 국가 간, 세대 간, 제도 간의 이질성까지 고려해야 하므로, 단순 상관계수 해석은 거의 무의미하다고 볼 수 있습니다.",
    "crumbs": [
      "Apps",
      "데이터 탐색 (Exploration)",
      "2D Relationship"
    ]
  },
  {
    "objectID": "2-2.html#로그-변환을-통한-비선형-관계의-선형화",
    "href": "2-2.html#로그-변환을-통한-비선형-관계의-선형화",
    "title": "2D Relationship",
    "section": "4 로그 변환을 통한 비선형 관계의 선형화",
    "text": "4 로그 변환을 통한 비선형 관계의 선형화\n경제 변수들은 곱셈적(multiplicative) 관계를 갖는 경우가 많으며, 이러한 구조는 선형 회귀모형에 부적합합니다. 이때 사용하는 대표적 방법이 로그 변환(log transformation)입니다.\n예를 들어 다음과 같은 관계를 생각해 봅니다:\n\n임금이 5%씩 증가할수록, 소비량도 비례적으로 증가\n물가가 2배로 오르면, 실질 수요량이 20% 감소\n\n이런 곱셈 구조는 로그를 취하면 덧셈 구조로 바뀌며, 회귀모형 내에서 선형화됩니다:\n\\[\\log Y = \\alpha + \\beta \\log X + \\varepsilon\\]\n이 형태에서 \\(\\beta\\)는 탄력성(elasticity)으로 해석됩니다.\n탄력성(elasticity)은 \\(X\\)가 1% 변화할 때 \\(Y\\)가 몇 퍼센트 변하는지를 나타내는 경제학적 해석 지표입니다. 로그-로그 회귀모형에서 탄력성은 다음과 같이 정의됩니다:\n\\[\\text{Elasticity} := \\frac{\\partial \\log Y}{\\partial \\log X} = \\beta\\]\n즉, \\(\\beta\\)는 \\(X\\)가 1% 변화할 때 \\(Y\\)가 \\(\\beta\\)%만큼 변화함을 의미합니다.\n\n소득 탄력성: 소득이 1% 증가할 때 소비가 몇 % 증가하는가?\n재산 탄력성: 총자산이 증가할 때 비소비적 지출(예: 투자, 저축)이 얼마나 증가하는가?\n수요의 가격탄력성: 가격이 1% 증가할 때 수요는 몇 % 감소하는가?\n\n이 개념은 단기적 관계뿐 아니라 장기적 구조 추정(long-run structural estimation)에서 매우 중요한 해석을 제공합니다. 단순한 절대 변화가 아닌 상대적 비율의 변화를 분석한다는 점에서 경제지표에 특화된 해석도구입니다.",
    "crumbs": [
      "Apps",
      "데이터 탐색 (Exploration)",
      "2D Relationship"
    ]
  },
  {
    "objectID": "3-2.html",
    "href": "3-2.html",
    "title": "확률변수 (random variable)",
    "section": "",
    "text": "확률변수는 숫자가 아니라 확률공간 위에 정의된 함수입니다.",
    "crumbs": [
      "Apps",
      "데이터와 가설의 연결고리 (Probability)",
      "확률변수 (random variable)"
    ]
  },
  {
    "objectID": "3-2.html#probability-space의-구조",
    "href": "3-2.html#probability-space의-구조",
    "title": "확률변수 (random variable)",
    "section": "1 Probability space의 구조",
    "text": "1 Probability space의 구조\n확률이 정의되기 위한 수학적 공간은 Probability space로 구성되며, 이는 다음과 같은 세 요소의 집합입니다: \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\)\n\n\\(\\Omega\\)는 sample space: 가능한 모든 결과들의 집합입니다. 예: bullish, bearish, stagnant market의 세 가지 시장 상태.\n\\(\\mathcal{F}\\)는 sigma-algebra (event space): 부분집합들로 구성된 \\(\\Omega\\)의 집합이며, 우리가 “사건(event)”이라 부를 수 있는 것들의 체계입니다.\n\\(\\mathbb{P}\\)는 probability measure: \\(\\mathcal{F}\\)에 정의된 함수로, 각 event에 확률을 할당합니다.\n\n이 구조에서 사건 \\(A \\in \\mathcal{F}\\)는 “시장이 bullish”과 같이 참인지 거짓인지 판단 가능한 조건에 해당합니다.",
    "crumbs": [
      "Apps",
      "데이터와 가설의 연결고리 (Probability)",
      "확률변수 (random variable)"
    ]
  },
  {
    "objectID": "3-2.html#event와-filtration",
    "href": "3-2.html#event와-filtration",
    "title": "확률변수 (random variable)",
    "section": "2 Event와 Filtration",
    "text": "2 Event와 Filtration\nEvent는 sample space의 부분집합입니다. 예를 들어, \\[A = \\{\\omega \\in \\Omega: \\text{market is bullish or stagnant} \\}\\] 와 같이 정의할 수 있습니다.\n시장 정보가 시간에 따라 누적된다고 가정하면, 시간 \\(t\\)까지의 모든 관측 가능한 정보의 집합을 \\(\\mathcal{F}_t\\)라고 표기합니다. 이러한 정보의 축적 구조 \\({\\mathcal{F}_t}\\)는 filtration이라 불리며, 확률적 정보가 시간의 흐름에 따라 점진적으로 드러나는 구조를 반영합니다.\n\n2.1 Random variable는 함수\nRandom variable \\(X\\)는 다음과 같은 함수입니다: \\[X: \\Omega \\rightarrow \\mathbb{R}\\]\n즉, 각 가능한 결과 \\(\\omega \\in \\Omega\\)에 대해, 그에 상응하는 수치 \\(X(\\omega)\\)를 반환하는 함수입니다. 예를 들어,\n\n\\(\\Omega = {\\text{bullish}, \\text{bearish}, \\text{stagnant}}\\)\n\\(X(\\omega) = \\text{monthly return under market state } \\omega\\) 라면, 각 시장 상태에 따라 수익률이 결정됩니다.\n\n이때 중요한 점은 확률변수는 확률을 직접적으로 가지는 대상이 아니라, 확률이 정의된 공간 위에 정의된 함수라는 점입니다. 확률은 여전히 event에 대해 정의되며, \\(X\\)는 그 event를 수치로 변환하는 역할을 합니다.",
    "crumbs": [
      "Apps",
      "데이터와 가설의 연결고리 (Probability)",
      "확률변수 (random variable)"
    ]
  },
  {
    "objectID": "3-2.html#discrete-vs.-continuous-확률변수의-정의역",
    "href": "3-2.html#discrete-vs.-continuous-확률변수의-정의역",
    "title": "확률변수 (random variable)",
    "section": "3 Discrete vs. Continuous: 확률변수의 정의역",
    "text": "3 Discrete vs. Continuous: 확률변수의 정의역\n\n3.1 Discrete sample space\n\\(\\Omega\\)가 유한하거나 countable한 경우, 즉 시장이 bullish 또는 bearish와 같이 명확히 구분될 수 있는 경우, 확률변수 \\(X\\)는 다음과 같은 구조로 해석됩니다:\n\nProbability mass function (PMF): \\(p(x) = \\mathbb{P}(X = x)\\) 확률변수가 특정 값 \\(x\\)를 가질 확률입니다. 예를 들어,\n\n\\(X(\\text{bullish}) = 0.06\\)\n\\(X(\\text{bearish}) = -0.04\\)\n\nCumulative distribution function (CDF): \\(F(x) = \\mathbb{P}(X \\leq x)\\) 확률변수가 주어진 수 이하일 누적 확률입니다. 이는 항상 non-decreasing이며, \\(\\lim_{x \\to -\\infty} F(x) = 0\\), \\(\\lim_{x \\to +\\infty} F(x) = 1\\)을 만족합니다.\n\n\n\n3.2 Continuous sample space\n\\(\\Omega\\)가 실수 전체 등 연속적인 결과 공간일 경우, PMF는 의미가 없으며 대신 확률밀도함수(probability density function)를 사용합니다.\n\nProbability density function (PDF): \\(f(x): \\text{a function such that } \\mathbb{P}(a \\leq X \\leq b) = \\int_a^b f(x)\\, dx\\) 개별 값에 대한 확률은 0이며, 구간의 확률만이 의미를 가집니다.\nCDF는 여전히 다음과 같이 정의됩니다: \\(F(x) = \\int_{-\\infty}^x f(t)\\, dt\\)\n\nPDF \\(f(x)\\)는 확률 그 자체가 아니라 밀도(density)이며, \\(\\int_{-\\infty}^{\\infty} f(x)\\, dx = 1\\) 을 만족해야 합니다.\n다음 절에서는 이러한 확률변수가 대규모 표본 속에서 어떻게 수렴하고, 정규성으로 전개되는지를 탐색합니다.",
    "crumbs": [
      "Apps",
      "데이터와 가설의 연결고리 (Probability)",
      "확률변수 (random variable)"
    ]
  },
  {
    "objectID": "3-4.html",
    "href": "3-4.html",
    "title": "중심극한정리 (Central Limit Theorem)",
    "section": "",
    "text": "투자자 A는 60일간의 일간 수익률을 평균해 보고, 투자자 B는 120일간의 일간 수익률을 평균합니다.\n수익률은 매일 불규칙하게 오르내리지만, 그 이동평균은 일정한 경향을 보일 수 있습니다.\n여기서 중요한 질문이 제기됩니다:\n이 질문에 대한 답을 제공할 수 있는 조건이 중심극한정리(Central Limit Theorem, CLT)입니다.",
    "crumbs": [
      "Apps",
      "데이터와 가설의 연결고리 (Probability)",
      "중심극한정리 (Central Limit Theorem)"
    ]
  },
  {
    "objectID": "3-4.html#classical-clt",
    "href": "3-4.html#classical-clt",
    "title": "중심극한정리 (Central Limit Theorem)",
    "section": "1 Classical CLT",
    "text": "1 Classical CLT\n확률변수열 \\({X_i}\\)가 서로 독립이며 동일한 분포(i.i.d.)를 따르고, 각 \\(X_i\\)에 대해 \\(\\mathbb{E}[X_i] = \\mu,\\quad \\mathrm{Var}(X_i) = \\sigma^2 &lt; \\infty\\) 일 때, \\[\\frac{1}{\\sqrt{n}} \\sum_{i=1}^n \\left( \\frac{X_i - \\mu}{\\sigma} \\right) \\xrightarrow{d} \\mathcal{N}(0,1)\\]\n즉, \\(Z_n = \\frac{\\bar{X}_n - \\mu}{\\sigma / \\sqrt{n}}\\) 처럼 정의된 표준화된 표본평균은\n\\(n \\to \\infty\\)일 때\n표준정규분포 \\(\\mathcal{N}(0,1)\\)로 분포수렴(in distribution)합니다.\n\n1.1 Classical CLT의 작동 조건\n중심극한정리(CLT)가 작동하는 수학적 구조의 핵심에는 두 가지 조건이 있습니다:\n(1) 확률변수 간 독립성(independence)\n(2) 덧셈 연산(additivity)의 대칭성(symmetry)\n이 두 조건은 각각 정보의 비중복성과 대칭성의 수렴 구조를 형성합니다.\n\n1.1.1 (1) Independence: 모평균을 추정하기 위한 정보의 비중복성\n확률변수열 \\({X_i}_{i=1}^n\\)이 서로 독립적이다라는 것은\n\\[\\mathbb{P}(X_i \\in A \\mid X_1, \\dots, X_{i-1}) = \\mathbb{P}(X_i \\in A)\\]\n임을 의미하며, 이는 각 \\(X_i\\)가 새로운 정보를 제공함을 뜻합니다.\n여기서 ’정보’는 모집단의 평균 \\(\\mu\\)에 대한 추론에 기여하는 요소로 해석됩니다.\n만약 \\(X_i\\)와 \\(X_{i-1}\\)이 상관되어 있다면, \\(X_i\\)의 실현은 이미 \\(X_{i-1}\\)을 통해 부분적으로 예측 가능하며, 이는 정보의 중복입니다. 이처럼 독립성은 각 샘플이 동일한 기여를 하고 있으며, 관측이 서로 겹치지 않는 구조를 보장합니다.\n그러나 현실의 시계열 자료(time-series data)에서는 이 독립 조건이 쉽게 깨집니다.\n예를 들어 일간 수익률 시계열 \\({r_t}\\)에서\n\\[\\mathrm{Cov}(r_t, r_{t-1}) \\neq 0\\]\n이면, CLT의 전통적 형태는 적용될 수 없습니다. 대신 다음과 같은 조건 하에서 일반화된 CLT가 성립합니다:\n\nStrong mixing condition: 시간 간격이 커질수록 상관이 사라지는 약한 의존 구조\nMartingale difference sequence: \\(\\mathbb{E}[X_t \\mid \\mathcal{F}_{t-1}] = 0\\) 인 조건부 불편성\nWeak dependence: 공분산이 시차에 따라 급속히 감소하는 구조\n\n이러한 조건들은 독립성과 비슷한 추론 성질을 유지하면서도 현실적 의존성을 허용합니다.\n\n\n1.1.2 (2) 덧셈 연산과 대칭성: 왜 정규분포인가?\n중심극한정리는 확률변수들의 합(sum) 또는 평균(mean)에 대해 작동합니다.\n여기서 ’합’이라는 연산이 단순한 연산이 아니라, 특정 대수 구조의 성질을 가짐을 주목해야 합니다. 수학적으로 \\((\\mathbb{R}, +)\\)는 Abelian group입니다:\n\nClosure: \\(a, b \\in \\mathbb{R} \\Rightarrow a + b \\in \\mathbb{R}\\)\nAssociativity: \\((a + b) + c = a + (b + c)\\)\nIdentity: \\(0\\)이 항등원 (additive identity)\nInverse: 각 \\(a\\)에 대해 \\(-a\\) 존재\nCommutativity: \\(a + b = b + a\\)\n\n이 구조에서 중요한 성질은 commutativity입니다. 즉, 덧셈의 순서에 관계없이 결과가 같다는 것인데, 이것이 바로 합의 대칭적 성질을 수학적으로 보장하는 핵심입니다.\n이 연산을 반복하면 다음과 같은 일이 발생합니다:\n\n각 확률변수 \\(X_i\\)의 대칭적 noise가 축적되며,\n평균 중심으로 대칭성이 강화되고,\n꼬리(tail)의 비대칭성은 약화되며,\n결국 대칭적 형태의 종곡선 (bell-shaped curve)으로 수렴합니다.\n\n이로부터 정규분포(normal distribution)가 등장합니다.\n정규분포는 다음과 같은 성질을 갖습니다:\n\\[f(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left( -\\frac{(x - \\mu)^2}{2\\sigma^2} \\right)\\]\n\n이 함수는 평균 \\(\\mu\\)를 중심으로 좌우 대칭입니다.\n분산 \\(\\sigma^2\\)에 의해 퍼짐 정도가 결정됩니다.\n\\(\\mu\\)와 \\(\\sigma^2\\)라는 두 개의 모수(parameter)만으로 전체 분포가 완전히 규정됩니다.\n\n즉, CLT는 독립된 대칭적 noise들의 평균을 반복할수록, Abelian group의 commutativity와 additive symmetry에 의해 정규분포로 수렴한다는 수학적 귀결입니다.\n\n\n\n1.2 Standard deviation vs. Standard error\n많은 학습자들이 혼동하는 개념이 standard deviation (SD)와 standard error (SE)입니다.\n두 개념은 다음과 같은 차이를 가집니다:\n\n\n\n\n\n\n\n\n\n개념\n정의\n의미\n수식\n\n\n\n\nSD\n\\(\\sigma\\)\n개별 값이 평균에서 얼마나 흩어져 있는가\n\\(\\sqrt{\\mathbb{E}[(X - \\mu)^2]}\\)\n\n\nSE\n\\(\\sigma / \\sqrt{n}\\)\n표본평균 자체가 얼마나 변동하는가\n\\(\\mathrm{SD}(\\bar{X}_n)\\)\n\n\n\n즉, SD는 단일 관측값의 변동성을 측정하고, SE는 표본평균이라는 통계량의 불확실성을 측정합니다.\n\n“표본이 많아질수록 평균은 더욱 정확해진다.”\n이것이 바로 CLT의 실질적 의미이며, 그 공식적 표현이 standard error의 감소입니다.\n\n\n\n1.3 Classical CLT 활용\nCLT가 만들어내는 종착지는 바로 정규분포 \\(\\mathcal{N}(\\mu, \\sigma^2/n)\\)입니다.\n이 분포는 단지 “예쁜 곡선”이 아니라, 다음과 같은 강력한 특징을 가집니다:\n\n평균 \\(\\mu\\)를 중심으로 완벽한 대칭성\n분포가 두 개의 모수 \\((\\mu, \\sigma^2)\\)로만 완전히 정의됨\n68–95–99.7 Rule: 정규분포에서 전체 데이터의 약 68%는 평균으로부터 ±1표준편차, 95%는 ±2표준편차, 99.7%는 ±3표준편차 범위 안에 위치한다는 경험적 법칙입니다.\n모든 선형 통계량(linear statistic)이 정규분포로 귀속됨\n\nCLT에 의해 복잡한 분포라도 평균을 기준으로 하면 정규 근사 가능하기 때문에,\n다양한 추정, 검정, 신뢰구간 설정이 정규분포를 전제로 성립합니다.\n정규분포의 중심성을 실용적으로 활용하기 위해서는 표준화(standardization)가 필요합니다: \\[Z = \\frac{X - \\mu}{\\sigma}\\]\n이 과정을 통해 다음이 가능합니다:\n\n서로 다른 단위의 변수들 간 비교\n표준점수(Z-score) 기반의 해석 및 가설 검정\n예: 대학수학능력시험의 표준점수, IQ, 금융 stress test 등\n\n표준화된 분포는 \\(\\mathcal{N}(0, 1)\\)이 되며, 모든 확률 해석이 상대 위치로 환원됩니다.\n\n모집단의 평균 \\(\\mu\\)를 표본평균 \\(\\bar{X_n}\\)을 이용해 추정할 때,\n\n\\(\\bar{X_n} \\pm z_{\\alpha/2} \\cdot \\mathrm{SE}\\) 는 \\(1 - \\alpha\\)의 신뢰수준을 가지는 신뢰구간이 됩니다. 이때 \\(z_{\\alpha/2}\\)는 표준정규분포의 분위수(quantile)이며, CLT가 이 분포를 보장해 주기 때문에 합리적 추론이 가능해집니다.",
    "crumbs": [
      "Apps",
      "데이터와 가설의 연결고리 (Probability)",
      "중심극한정리 (Central Limit Theorem)"
    ]
  },
  {
    "objectID": "4-2.html",
    "href": "4-2.html",
    "title": "인과관계 (Granger causality) vs. 공적분관계 (co-integration)",
    "section": "",
    "text": "Causality and Long-Term Economic Relationships\n통계적으로 유의미한 상관관계가 발견되었다고 해서, 그 관계가 인과적(causal)이라고 말할 수는 없습니다. 특히 경제·사회 데이터에서는 변수 간 상호작용이 복잡하며, 외부 요인(confounders), 시간 지연, 피드백 루프 등으로 인해 단순한 인과 추론은 오히려 왜곡을 초래할 수 있습니다.\n이 절에서는 시계열 분석에서 자주 사용되는 Granger causality를 출발점으로 삼고, 이를 넘어서는 구조적 접근(예: Pearl의 구조 모형, Rubin의 잠재적 결과 프레임워크), 그리고 장기적 상호작용을 설명하는 공적분관계(cointegration)로까지 확장합니다.",
    "crumbs": [
      "Apps",
      "데이터 활용 (Exploitation)",
      "인과관계 (Granger causality) vs. 공적분관계 (co-integration)"
    ]
  },
  {
    "objectID": "4-2.html#granger-causality-예측-가능성이-인과인가",
    "href": "4-2.html#granger-causality-예측-가능성이-인과인가",
    "title": "인과관계 (Granger causality) vs. 공적분관계 (co-integration)",
    "section": "1 Granger Causality: 예측 가능성이 인과인가?",
    "text": "1 Granger Causality: 예측 가능성이 인과인가?\nGranger causality는 시계열 변수 \\(X_t\\)가 다른 변수 \\(Y_t\\)의 미래 값을 예측하는 데 유의미한 기여를 하는지를 검정합니다.\n\n정의: 과거의 \\(X\\) 값이 \\(Y\\)의 미래값에 통계적으로 유의미한 설명력을 가진다면, \\(X\\)는 \\(Y\\)에 대해 Granger 원인(granger cause)이라고 합니다.\n형식적 표현: \\[Y_t = \\sum_{i=1}^{p} \\alpha_i Y_{t-i} + \\sum_{j=1}^{q} \\beta_j X_{t-j} + \\varepsilon_t\\]\n여기서 \\(\\beta_j \\neq 0\\)이면, \\(X\\)가 \\(Y\\)를 Granger-cause 한다고 판단합니다.\n해석 주의점: Granger causality는 예측 가능성(prediction)에 기반할 뿐, 진정한 인과 구조(mechanistic causality)를 보장하지 않습니다.",
    "crumbs": [
      "Apps",
      "데이터 활용 (Exploitation)",
      "인과관계 (Granger causality) vs. 공적분관계 (co-integration)"
    ]
  },
  {
    "objectID": "4-2.html#구조적-인과-모형-pearl과-rubin",
    "href": "4-2.html#구조적-인과-모형-pearl과-rubin",
    "title": "인과관계 (Granger causality) vs. 공적분관계 (co-integration)",
    "section": "2 구조적 인과 모형: Pearl과 Rubin",
    "text": "2 구조적 인과 모형: Pearl과 Rubin\nGranger 접근은 “과거 정보 → 미래 예측”이라는 시계열적 인과성을 평가하지만, 더 근본적인 인과 해석을 위해서는 구조적 모형이 필요합니다.\n\n2.1 (1) Pearl의 SCM (Structural Causal Model)\n\n변수 간 관계를 Directed Acyclic Graph (DAG)로 표현하고,\n개입(do-calculus)이라는 연산을 통해 인과 효과를 정량화합니다.\n\n예: 흡연 → 폐암 관계에서, 관찰된 상관관계뿐 아니라 “흡연을 강제로 제거했을 때 폐암 발생률이 어떻게 바뀌는가?”라는 질문을 모형화합니다.\n\n\n2.2 (2) Rubin의 Potential Outcomes Framework\n\n개별 단위(i)의 잠재적 결과를 두 개로 상정:\n\\(Y_i(1)\\) = 처치받았을 때 결과, \\(Y_i(0)\\) = 처치받지 않았을 때 결과\n실제로는 하나의 결과만 관측 가능 → counterfactual inference가 핵심\n무작위 대조 실험(RCT)이 가장 신뢰 가능한 방법",
    "crumbs": [
      "Apps",
      "데이터 활용 (Exploitation)",
      "인과관계 (Granger causality) vs. 공적분관계 (co-integration)"
    ]
  },
  {
    "objectID": "4-2.html#혼란-변수와-결과론적-해석의-오류",
    "href": "4-2.html#혼란-변수와-결과론적-해석의-오류",
    "title": "인과관계 (Granger causality) vs. 공적분관계 (co-integration)",
    "section": "3 혼란 변수와 결과론적 해석의 오류",
    "text": "3 혼란 변수와 결과론적 해석의 오류\n모든 인과 추론은 통계적으로 유의미한 설명력을 단일 원인으로 환원하는 오류를 경계해야 합니다.\n특히 결과 중심적 해석(post hoc reasoning)은 다음의 오류를 범하기 쉽습니다:\n\nConfounders: \\(Z\\)가 \\(X\\)와 \\(Y\\) 모두에 영향을 미친다면, \\(X \\to Y\\)의 인과 관계는 왜곡됩니다.\n생존자 편향: 성공한 사례만 분석하면, 실패한 경우의 데이터는 누락됩니다.\n과도한 단순화: 복잡한 사회·경제적 조건을 단일 변수로 설명하려 함\n\n\n예: 부자가 된 사람이 과거 높은 투자 수익률을 보였다고 해서, 그 수익률이 부의 “원인”이었다고 단정짓는 것은 위험합니다. 초기 자본, 시장 상승장, 정보 접근성 등 다양한 구조적 요인이 작용했을 수 있습니다.",
    "crumbs": [
      "Apps",
      "데이터 활용 (Exploitation)",
      "인과관계 (Granger causality) vs. 공적분관계 (co-integration)"
    ]
  },
  {
    "objectID": "4-2.html#비선형성-문턱-효과-조건부-인과",
    "href": "4-2.html#비선형성-문턱-효과-조건부-인과",
    "title": "인과관계 (Granger causality) vs. 공적분관계 (co-integration)",
    "section": "4 비선형성, 문턱 효과, 조건부 인과",
    "text": "4 비선형성, 문턱 효과, 조건부 인과\n현실 세계의 인과 관계는 종종 다음과 같은 비선형적 또는 조건적 특성을 가집니다:\n\nNonlinearity: \\(X\\)가 \\(Y\\)에 미치는 효과가 수준에 따라 달라짐 (e.g., 금리가 5%에서 4%로 내려갈 때는 효과가 크지만, 1%에서 0%로는 작음)\nInteraction: 변수 \\(X\\)의 효과가 다른 변수 \\(Z\\)의 수준에 따라 달라짐 (e.g., 교육이 소득에 미치는 효과가 지역 또는 성별에 따라 다름)\nThreshold effect: 특정 구간에서는 영향력이 없다가, 일정 수준을 넘으면 강한 효과",
    "crumbs": [
      "Apps",
      "데이터 활용 (Exploitation)",
      "인과관계 (Granger causality) vs. 공적분관계 (co-integration)"
    ]
  },
  {
    "objectID": "4-2.html#cointegration-단기적-분산-장기적-균형",
    "href": "4-2.html#cointegration-단기적-분산-장기적-균형",
    "title": "인과관계 (Granger causality) vs. 공적분관계 (co-integration)",
    "section": "5 Cointegration: 단기적 분산, 장기적 균형",
    "text": "5 Cointegration: 단기적 분산, 장기적 균형\n서로 비정상(non-stationary)인 시계열 변수 \\(X_t, Y_t\\)가 각각 따로 보면 평균이 존재하지 않고, 분산이 무한대로 커지지만,\n어떤 선형 조합 \\(\\beta_1 X_t + \\beta_2 Y_t\\)는 stationary하다면, 이 두 변수는 공적분(cointegrated) 관계에 있다고 말합니다.\n\n경제적 의미: \\(X\\)와 \\(Y\\)는 단기적으로는 따로 움직이지만, 장기적으로는 균형관계를 유지합니다.\n예시: 소비와 소득, 환율과 물가, 부동산 가격과 임대료 등은 공적분 관계를 가질 수 있습니다.\n\nCointegration은 인과관계와는 구분되지만, 장기적 관계라는 점에서 실질적인 경제 메커니즘의 존재를 시사합니다.\n\n\n\n\n\n\n\n\n\n분석 방식\n초점\n수단\n한계\n\n\n\n\nGranger Causality\n예측 가능성\n과거 정보의 통계적 설명력\n인과성 아님, 제3요인 영향 가능\n\n\nPearl SCM\n구조적 인과\nDAG, 개입(do-operator)\n모델 구조의 정확성 필요\n\n\nRubin Framework\n처치 효과\n반사실적 비교\n관찰되지 않는 결과 추정 필요\n\n\nCointegration\n장기 균형 관계\n비정상 시계열의 선형 조합\n인과 아님, 단기 예측력 없음",
    "crumbs": [
      "Apps",
      "데이터 활용 (Exploitation)",
      "인과관계 (Granger causality) vs. 공적분관계 (co-integration)"
    ]
  },
  {
    "objectID": "4-4.html",
    "href": "4-4.html",
    "title": "통계적 학습 (Statistical Learning)",
    "section": "",
    "text": "통계적 학습(Statistical Learning)은 데이터로부터 패턴을 학습하여 예측이나 설명을 수행하는 알고리즘적 절차를 의미합니다.\n\n\n\n학습 방식\n입력\n출력\n목적\n예시\n\n\n\n\n지도 학습\n\\(X\\)\n\\(Y\\)\n예측\n가격 예측, 분류\n\n\n비지도 학습\n\\(X\\)\n없음\n구조 파악\n클러스터링, PCA\n\n\n강화 학습\n상태, 행동\n보상\n정책 최적화\n게임 AI, 로봇",
    "crumbs": [
      "Apps",
      "데이터 활용 (Exploitation)",
      "통계적 학습 (Statistical Learning)"
    ]
  },
  {
    "objectID": "4-4.html#통계적-학습-개요",
    "href": "4-4.html#통계적-학습-개요",
    "title": "통계적 학습 (Statistical Learning)",
    "section": "",
    "text": "통계적 학습(Statistical Learning)은 데이터로부터 패턴을 학습하여 예측이나 설명을 수행하는 알고리즘적 절차를 의미합니다.\n\n\n\n학습 방식\n입력\n출력\n목적\n예시\n\n\n\n\n지도 학습\n\\(X\\)\n\\(Y\\)\n예측\n가격 예측, 분류\n\n\n비지도 학습\n\\(X\\)\n없음\n구조 파악\n클러스터링, PCA\n\n\n강화 학습\n상태, 행동\n보상\n정책 최적화\n게임 AI, 로봇",
    "crumbs": [
      "Apps",
      "데이터 활용 (Exploitation)",
      "통계적 학습 (Statistical Learning)"
    ]
  },
  {
    "objectID": "4-4.html#지도-학습-supervised-learning",
    "href": "4-4.html#지도-학습-supervised-learning",
    "title": "통계적 학습 (Statistical Learning)",
    "section": "2 지도 학습 (Supervised Learning)",
    "text": "2 지도 학습 (Supervised Learning)\n\n지도 학습은 입력 벡터 \\(X\\)와 대응하는 출력 \\(Y\\) 간의 함수적 관계를 추정하는 문제입니다.\n예: 특정 특성을 가진 개인이 상품을 구매했는지 (\\(Y = 1\\)), 구매하지 않았는지 (\\(Y = 0\\)) 예측\n입력 특성 (\\(X\\)): 나이, 지역, 교육 수준, 성별 등\n\n\n2.1 회귀분석 (선형/로지스틱 회귀)\n\n\n2.2 결정트리 (Decision Tree)\n\n데이터의 특성을 기준으로 이진 분할하여 예측을 수행하는 구조화된 모델\n해석 가능성이 높으나 과적합 위험 존재\n랜덤 포레스트 (Random Forest):\n\n여러 개의 결정트리를 앙상블하여 예측 안정성을 확보.\n무작위 변수 선택 + 배깅(Bagging)을 통해 과적합 완화.\n비선형적인 데이터 분류 및 회귀에 강건\n\n\n\n\n2.3 SVM (Support Vector Machine)\n\n\n2.4 신경망 모형(Neural Network Model)",
    "crumbs": [
      "Apps",
      "데이터 활용 (Exploitation)",
      "통계적 학습 (Statistical Learning)"
    ]
  },
  {
    "objectID": "4-4.html#비지도-학습-unsupervised-learning",
    "href": "4-4.html#비지도-학습-unsupervised-learning",
    "title": "통계적 학습 (Statistical Learning)",
    "section": "3 비지도 학습 (Unsupervised Learning)",
    "text": "3 비지도 학습 (Unsupervised Learning)\n\n정답 없이 주어진 데이터 \\(X\\)로부터 군집 구조, 차원 축소, 밀도 추정 등을 수행합니다.\n목표는 데이터의 패턴이나 구조적 특징을 파악하는 것입니다.\n\n\n3.1 주성분 분석 (Principal Component Analysis, PCA)\n\n목적: 고차원의 데이터를 정보 손실을 최소화하면서 저차원으로 축소\n방법:\n\n\\(p\\)개의 특성을 선형 결합하여 새로운 축(주성분)을 생성\n분산이 가장 큰 방향을 기준으로 차원 선택\n\n수식적 정의: 주성분 \\(Z_1 = a_1^\\top X\\)는 \\(\\text{Var}(Z_1)\\)을 최대화하는 \\(a_1\\)을 찾는 문제\n\n\n\n3.2 군집 분석 (Clustering)\n\n정의: 유사한 관측치를 하나의 군집으로 묶는 방법\n기본 원칙:\n\n군집 간 이질성: 서로 다른 군집은 명확히 구분되어야 함\n군집 내 동질성: 같은 군집 내 관측치는 유사해야 함\n\n\nK-평균 알고리즘\n\n\n초기 중심 \\(k\\)개 설정 → 각 데이터에 가장 가까운 중심 할당 → 중심 재계산 → 반복\n초기값 의존성 있음 → 여러 초기값으로 반복 시도\n\n\n계층적 클러스터링\n\n\n각 데이터를 클러스터로 시작 → 가장 가까운 쌍을 반복적으로 병합\ndendrogram으로 시각화 가능\n\n\n거리 측정 방법\n\n\n최소 거리 (single linkage)\n최대 거리 (complete linkage)\n평균 거리 (average linkage)",
    "crumbs": [
      "Apps",
      "데이터 활용 (Exploitation)",
      "통계적 학습 (Statistical Learning)"
    ]
  },
  {
    "objectID": "4-4.html#강화-학습-reinforcement-learning",
    "href": "4-4.html#강화-학습-reinforcement-learning",
    "title": "통계적 학습 (Statistical Learning)",
    "section": "4 강화 학습 (Reinforcement Learning)",
    "text": "4 강화 학습 (Reinforcement Learning)\n\n학습자는 환경과 상호작용하며 행동(action)에 대한 보상(reward)을 통해 최적 정책을 학습합니다.\n보상은 지연되어 주어질 수 있으며, 장기적 기대보상을 극대화하는 방향으로 학습이 진행됩니다.\nCarrot & Stick: 올바른 행동에 보상, 잘못된 행동에 벌칙\n알파고: 수천 번의 게임을 반복하며 승리 확률을 극대화하는 전략을 학습",
    "crumbs": [
      "Apps",
      "데이터 활용 (Exploitation)",
      "통계적 학습 (Statistical Learning)"
    ]
  },
  {
    "objectID": "5-1.html",
    "href": "5-1.html",
    "title": "중회귀모형 도입",
    "section": "",
    "text": "회귀모형은 단지 회귀계수를 계산하는 수단이 아닙니다. Predictive or Explanatory",
    "crumbs": [
      "Apps",
      "모형계수 추정법 (Estimation)",
      "중회귀모형 도입"
    ]
  },
  {
    "objectID": "5-1.html#예측prediction-vs.-설명attribution",
    "href": "5-1.html#예측prediction-vs.-설명attribution",
    "title": "중회귀모형 도입",
    "section": "1 예측(Prediction) vs. 설명(Attribution)",
    "text": "1 예측(Prediction) vs. 설명(Attribution)\n통계적 회귀모형을 설계할 때, 입력변수 \\(X\\)와 결과변수 \\(Y\\)의 시간적 관계는\n모형이 “미래 예측을 위한 것인지”, 아니면 “현재 설명을 위한 것인지”를 결정하는 핵심입니다.\n\n\\(X_{t-1} \\to Y_t\\): 시간적으로 선행하는 변수를 사용하면, 이는 예측 모형입니다.\n\\(X_t \\to Y_t\\): 동시적 변수를 사용하면, 이는 해석(attribution) 또는 설명적 구조로 간주됩니다.\n\n예를 들어, Risk-based return prediction 모형에서는 lagged variable을 사용해 미래 수익률을 예측합니다. (예: linear factor pricing model) 반면, risk attribution 모형에서는 현재 수익률의 분산을 요인별로 분해합니다.",
    "crumbs": [
      "Apps",
      "모형계수 추정법 (Estimation)",
      "중회귀모형 도입"
    ]
  },
  {
    "objectID": "5-1.html#아파트-가격을-예측할-수-있는가",
    "href": "5-1.html#아파트-가격을-예측할-수-있는가",
    "title": "중회귀모형 도입",
    "section": "2 아파트 가격을 예측할 수 있는가?",
    "text": "2 아파트 가격을 예측할 수 있는가?\n부동산 시장에서 “아파트 가격은 어떻게 결정되는가?”라는 질문은 누구나 던져볼 수 있지만,\n이를 정량적으로 예측하거나 설명하려면 먼저 분석 가능한 모형이 필요합니다.\n예를 들어, 다음과 같은 feature (설명변수)들을 고려해 봅시다:\n\n\\(X_1\\): 전용면적 (m²)\n\\(X_2\\): 건축연도 (혹은 노후도)\n\\(X_3\\): 최근 거래 횟수\n\\(X_4\\): 역세권 여부 (dummy)\n\\(X_5\\): 해당 학군 내 평균 수능 점수\n\n이제 이 변수들과 실제 아파트 실거래 가격 \\(Y\\) 간의 관계를 수식으로 표현하면,\n다변량 회귀모형(multiple linear regression)의 형태가 됩니다:\n\\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p + \\varepsilon\\]\n여기서 \\(\\varepsilon\\)은 관측되지 않은 충격(예: 가족 사정, 급매 여부 등)을 나타내는 오차항입니다.\n이러한 선형 회귀모형은 계산하기 쉬운 수학적 구조로 시작하지만, \\(X\\)에 들어가는 변수들은 다음과 같이 변형될 수 있습니다:\n\n\\(X_1^2\\), \\(X_1^3\\)처럼 다항식(polyomial features)\n\\(\\log X_1\\), \\(\\sqrt{X_1}\\) 같은 함수 변환(transformations)\n두 변수의 곱 \\(X_1 \\cdot X_2\\)와 같은 상호작용 항(interaction terms)\n\n따라서 선형 회귀모형은 형식적으로 선형(linear in parameter)이지만, 입력변수는 비선형적 형태로 변환되어 모형에 들어갈 수 있습니다. 이 경우에도 회귀계수는 직교 투영 조건(orthogonal projection condition) 하에서 추정은 가능합니다. 하지만, 추정이 가능하다고 해서 해석도 가능한 것은 아닙니다.\n빈도주의적 회귀에서는, 주어진 설계행렬 \\(X\\)가 주어졌을 때,\n\\(Y\\)를 \\(X\\)의 열공간(column space)에 직교 투영(projection)하여 계수를 추정합니다.\n이는 기하학적으로 잘 정의된 연산이며, 수학적으로는 일관되게 계산됩니다:\n\\[\\hat{\\beta} = (X^\\top X)^{-1} X^\\top Y\\]\n그러나 \\(X\\)의 열들이 변형된 다항식, 로그, 조건부 상호작용항 등으로 구성되어 있다면,\n각 \\(\\hat{\\beta}_j\\)가 의미하는 바는 매우 불분명해집니다.\n예를 들어, \\(X = {1, X_1, X_1^2, \\log X_2, X_1 \\cdot X_2}\\)와 같이 구성된 경우,\n\\(\\beta_3\\)의 변화가 \\(Y\\)에 미치는 영향은 \\(X_1\\)과 \\(X_2\\)의 수준에 따라 비선형적으로 달라지므로,\n그 값 자체가 어떤 해석 가능한 의미를 갖는다고 말하기 어렵습니다.",
    "crumbs": [
      "Apps",
      "모형계수 추정법 (Estimation)",
      "중회귀모형 도입"
    ]
  },
  {
    "objectID": "5-3.html",
    "href": "5-3.html",
    "title": "중회귀모형 - 베이지언 접근",
    "section": "",
    "text": "사전정보와 분포 추정\n빈도주의자는 모수를 고정된 상수로 간주하고, 반복적 표본 추출을 통해 그 값에 접근하려 합니다. 반면, 베이지언 접근은 처음부터 모수(parameter)를 확률변수로 간주합니다. 즉, 모수에 대해 사전 분포(prior distribution)를 설정하고, 실제 데이터를 관측한 후 사후 분포(posterior distribution)를 통해 모수의 불확실성을 정량적으로 표현합니다.\n이러한 철학적 전환은 “불확실성”을 다루는 방식 자체를 바꾸어 놓습니다.\n빈도주의자에게 불확실성은 표본추출의 산물이라면, 베이지언에게는 모형화의 본질적인 출발점입니다.",
    "crumbs": [
      "Apps",
      "모형계수 추정법 (Estimation)",
      "중회귀모형 - 베이지언 접근"
    ]
  },
  {
    "objectID": "5-3.html#베이지언-회귀의-기본-구조",
    "href": "5-3.html#베이지언-회귀의-기본-구조",
    "title": "중회귀모형 - 베이지언 접근",
    "section": "1 베이지언 회귀의 기본 구조",
    "text": "1 베이지언 회귀의 기본 구조\n다변량 회귀모형은 다음과 같이 표현됩니다:\n\\[Y = X\\beta + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)\\]\n이때 베이지언은 \\(\\beta\\) 자체를 확률변수로 취급하여, 사전 분포를 다음과 같이 설정합니다:\n\\[\\beta \\sim \\mathcal{N}(\\mu_0, \\Sigma_0)\\]\n이처럼 정규분포를 사전 분포로 선택하는 이유는, 정규 오차 구조와 conjugate pair(켤레쌍)을 이루기 때문입니다. 즉, 정규 likelihood와 정규 prior를 곱한 결과는 다시 정규 분포가 되어 계산이 닫히는(closed-form) 구조를 유지합니다.\n이를 기반으로, 사후 분포는 다음과 같은 정규분포가 됩니다:\n\\[\\beta \\mid Y, X \\sim \\mathcal{N}(\\mu_n, \\Sigma_n)\\] where \\[\\Sigma_n = \\left( \\Sigma_0^{-1} + \\frac{1}{\\sigma^2} X^\\top X \\right)^{-1}, \\quad \\mu_n = \\Sigma_n \\left( \\Sigma_0^{-1} \\mu_0 + \\frac{1}{\\sigma^2} X^\\top Y \\right)\\]\n이러한 표현은 모수 \\(\\beta\\)에 대한 완전한 분포적 추정을 제공하며, 추후 예측(prediction)이나 불확실성 평가에서 강력한 도구가 됩니다.",
    "crumbs": [
      "Apps",
      "모형계수 추정법 (Estimation)",
      "중회귀모형 - 베이지언 접근"
    ]
  },
  {
    "objectID": "5-3.html#joint-distribution의-관점에서-보는-회귀모형",
    "href": "5-3.html#joint-distribution의-관점에서-보는-회귀모형",
    "title": "중회귀모형 - 베이지언 접근",
    "section": "2 joint distribution의 관점에서 보는 회귀모형",
    "text": "2 joint distribution의 관점에서 보는 회귀모형\n베이지언 회귀는 단지 \\(\\beta\\)의 분포를 추정하는 데 그치지 않고,\n모수 \\(\\beta\\)와 데이터 \\(Y\\) 사이의 결합 확률분포(joint distribution) 전체를 모형화합니다:\n\\[p(Y, \\beta) = p(Y \\mid \\beta) \\cdot p(\\beta)\\] 여기서 \\(p(Y \\mid \\beta)\\)는 likelihood이고, \\(p(\\beta)\\)는 prior입니다.\n베이즈 정리는 이 구조를 사후분포로 업데이트하는 절차로 정리합니다: \\[p(\\beta \\mid Y) = \\frac{p(Y \\mid \\beta) \\cdot p(\\beta)}{p(Y)}\\]\n이 전체 joint distribution 구조를 이해하면, 나중에 예측분포(prediction for new \\(Y^\\ast\\))나 불확실성 전파(uncertainty propagation)까지 자연스럽게 확장할 수 있습니다.",
    "crumbs": [
      "Apps",
      "모형계수 추정법 (Estimation)",
      "중회귀모형 - 베이지언 접근"
    ]
  },
  {
    "objectID": "5-3.html#예측-분포와-분포-추정",
    "href": "5-3.html#예측-분포와-분포-추정",
    "title": "중회귀모형 - 베이지언 접근",
    "section": "3 예측 분포와 분포 추정",
    "text": "3 예측 분포와 분포 추정\n베이지언 회귀의 또 다른 강점은 미래 관측값에 대한 분포적 예측입니다.\n예를 들어, 새로운 입력값 \\(x^\\ast\\)에 대한 \\(Y^\\ast\\)의 예측 분포는 다음과 같습니다:\n\\[Y^\\ast \\mid x^\\ast, Y, X \\sim \\mathcal{N}\\left( x^{\\ast \\top} \\mu_n, x^{\\ast \\top} \\Sigma_n x^\\ast + \\sigma^2 \\right)\\]\n여기서 \\(x^{\\ast \\top} \\mu_n\\)는 예측 평균이고, 뒤의 항은 예측 오차의 분산입니다. 이처럼 베이지언은 추정치를 하나의 숫자(point)로 요약하지 않고, 전체 분포(distribution)를 통해 불확실성을 기술합니다.\n\n3.1 상관관계 구조와 Copula\njoint distribution 구조를 이해할 때, 각 변수 간의 결합 구조가 얼마나 유연한지가 핵심이 됩니다. 베이지언 회귀에서는 \\(\\beta\\)의 분포 구조가 사전 분포와 데이터 구조에 의해 결정되며, 공분산 행렬 \\(\\Sigma_n\\)이 변수 간의 의존 관계를 암묵적으로 표현합니다.\n하지만 이러한 구조가 항상 선형일 필요는 없습니다. 특히 다변수 분포에서 주변분포(marginal distribution)는 각각 다른 분포를 따르더라도, 이들을 결합하는 구조를 일반화하는 방법이 바로 Copula 함수입니다.\nCopula는 \\(F_{X,Y}(x,y) = C(F_X(x), F_Y(y))\\) 와 같이 결합함수를 분리하는 방식으로,\n비정규적이고 비선형적인 상관관계 구조까지도 유연하게 모델링할 수 있는 확장 도구로 사용됩니다.\n베이지언 회귀 자체는 정규 구조를 가정하지만, copula 기반 베이지언 모형은 비정규성(non-Gaussianity)까지 포함하는 확장적 구조를 가능하게 합니다.",
    "crumbs": [
      "Apps",
      "모형계수 추정법 (Estimation)",
      "중회귀모형 - 베이지언 접근"
    ]
  },
  {
    "objectID": "6-2.html",
    "href": "6-2.html",
    "title": "유의수준, 통계량, p-value",
    "section": "",
    "text": "우리는 불확실한 현실 속에서, 어떤 주장이 “신뢰할 만한 것인지”를 판단해야 할 때가 많습니다.\n통계학은 이런 판단을 내릴 수 있도록 정량적 기준을 제시합니다.\n그 대표적인 틀이 바로 유의성 검정(significance testing)이며,\n그 중심에는 검정통계량, 유의수준(α), 그리고 p-value라는 세 가지 도구가 놓여 있습니다.\n가설 검정의 기본 원리는 단순합니다.\n“귀무가설(null hypothesis)이 참이라고 가정했을 때, 실제 관측된 결과가 얼마나 드문 일인가?”를 따지는 것입니다. 관측된 값이 너무 극단적이라면, 그 가설을 그대로 믿는 것은 불합리하다는 결론에 도달할 수 있습니다.\n이때 검정통계량(test statistic)은 관측값을 수치로 요약한 것이며, 그 수치가 귀무가설이 옳다고 가정했을 때의 기준 분포에서 얼마나 멀리 떨어져 있는지를 측정합니다.",
    "crumbs": [
      "Apps",
      "가설 점정 (Hypothesis Test)",
      "유의수준, 통계량, p-value"
    ]
  },
  {
    "objectID": "6-4.html",
    "href": "6-4.html",
    "title": "여러 집단을 한꺼번에 비교",
    "section": "",
    "text": "ANOVA와 카이제곱 검정은 서로 다른 데이터 유형에 대해\n“여러 집단 간의 차이가 우연을 넘어서 존재하는가?”라는 동일한 질문에 답하려는 도구입니다.\n이 두 검정법은 사회과학, 의학, 정책분석 등 다양한 분야에서 그룹 간의 차이와 독립성을 검정하는 가장 표준적인 방법입니다. 그러나 유의성 결과를 해석할 때에는 항상 실질적 중요성, 표본의 특성, 그리고 자료 수집 과정의 전제 조건을 함께 고려해야 합니다.",
    "crumbs": [
      "Apps",
      "가설 점정 (Hypothesis Test)",
      "여러 집단을 한꺼번에 비교"
    ]
  },
  {
    "objectID": "4-5.html#베이지안-통계학-개요",
    "href": "4-5.html#베이지안-통계학-개요",
    "title": "부록-베이지안 통계학과 응용",
    "section": "",
    "text": "베이지안 통계학(Bayesian Statistics)은 확률을 믿음의 정도(belief)로 해석합니다.\n관측 이전의 불확실성은 사전 확률(Prior)로,\n관측 이후의 지식은 사후 확률(Posterior)로 표현되며,\n이 둘은 베이즈 정리(Bayes’ Theorem)에 의해 연결됩니다:\n\\[P(\\theta \\mid y) = \\frac{P(y \\mid \\theta) \\cdot P(\\theta)}{P(y)} \\propto P(y \\mid \\theta) \\cdot P(\\theta)\\]\n여기서 \\(\\theta\\)는 모수, \\(y\\)는 데이터, \\(P(y \\mid \\theta)\\)는 가능도(likelihood)입니다.\n전체 확률 \\(P(y)\\)는 정규화 상수이며, 사후 분포를 확률분포로 만들기 위해 필요합니다.\n베이지안 추론은 다음 5단계로 요약할 수 있습니다:\n\n모형 설정: 데이터 \\(y\\)와 모수 \\(\\theta\\) 사이의 확률 구조를 정의\n사전 분포 설정: \\(\\theta\\)에 대한 사전 확률 \\(P(\\theta)\\)를 지정 (정보적 또는 무정보적)\n데이터 관측: 새로운 데이터를 수집\n사후 분포 계산:\n\n베이즈 정리를 이용해 사후 확률 \\(P(\\theta \\mid y)\\) 계산\n복잡한 경우에는 MCMC(Markov Chain Monte Carlo) 같은 수치적 근사 기법 필요\n\n사후 기반 추론: 점 추정, 구간 추정, 가설 검정, 예측 등 수행\n\n\n\n\n점 추정(point estimation)은 손실 함수 \\(L(\\theta, \\hat{\\theta})\\)에 따라 달라집니다.\n\n제곱 오차 손실(MSE, Mean Squared Error): 사후 평균 \\(\\mathbb{E}[\\theta \\mid y]\\) 사용\n절대 오차 손실(MAE, Mean Absolute Error): 사후 중앙값 사용\n\n\n\n\n\n베이지안 계산을 단순화하는 개념입니다.\n예: 베타-이항 모형 (Beta-Binomial Model)\n\n관측값: \\(k\\) successes in \\(n\\) trials\n가능도: \\(P(k \\mid \\theta, n) \\propto \\theta^k (1 - \\theta)^{n - k}\\)\n사전: \\(\\theta \\sim \\text{Beta}(\\alpha, \\beta)\\)\n사후: \\(\\theta \\mid k \\sim \\text{Beta}(\\alpha + k, \\beta + n - k)\\)\n\n사전과 사후가 같은 분포족을 유지하므로 계산이 용이합니다.\n\n\n\n베이지안 모델 비교에서는 다음과 같은 기준을 사용합니다:\n\nBayes Factor (BF):\n\\[BF_{12} = \\frac{P(y \\mid M_1)}{P(y \\mid M_2)}\\]\n→ 데이터가 주어졌을 때 모델 \\(M_1\\)이 \\(M_2\\)보다 얼마나 더 지지받는지를 정량화\n사후 모델 확률:\n\\[P(M_i \\mid y) \\propto P(y \\mid M_i) \\cdot P(M_i)\\]\nModel Averaging (모델 평균화):\n\\[P(\\theta \\mid y) = \\sum_i P(\\theta \\mid y, M_i) \\cdot P(M_i \\mid y)\\]\n\n\n\n\n어떤 질병의 유병률이 매우 낮은 상황(예: \\(P(\\text{Disease}) = 0.001\\))을 생각해 봅시다.\n\n민감도(Sensitivity): \\(P(\\text{Positive} \\mid \\text{Disease}) = 0.99\\)\n특이도(Specificity): \\(P(\\text{Negative} \\mid \\text{No Disease}) = 0.99\\)\n\n한 사람이 양성(Positive) 판정을 받았을 때, 실제 병에 걸렸을 확률(PPV: Positive Predictive Value)은?\n\\[P(\\text{Disease} \\mid \\text{Positive}) = \\frac{0.99 \\cdot 0.001}{0.99 \\cdot 0.001 + 0.01 \\cdot 0.999} \\approx 0.09\\]\n→ 양성 반응자의 9%만 실제 환자.\n검사의 정확도는 사전 확률에 강하게 의존함을 보여주는 고전적 예입니다.",
    "crumbs": [
      "Apps",
      "데이터 활용 (Exploitation)",
      "부록-베이지안 통계학과 응용"
    ]
  },
  {
    "objectID": "4-5.html#베이지안-통계학-응용",
    "href": "4-5.html#베이지안-통계학-응용",
    "title": "부록-베이지안 통계학과 응용",
    "section": "2 베이지안 통계학 응용",
    "text": "2 베이지안 통계학 응용\n\n2.1 Multi-Armed Bandit (MAB, 다중 슬롯머신 문제)\n\n문제: 여러 arm(옵션) 중 보상 확률이 가장 높은 arm을 찾아 탐험(Exploration)과 활용(Exploitation)을 균형 있게 수행\nThompson Sampling (톰슨 샘플링):\n\n각 arm \\(i\\)의 성공 확률 \\(\\theta_i\\)에 대해 Beta 사전분포 설정\n각 사후 분포 \\(P(\\theta_i \\mid \\text{data})\\)에서 \\(\\theta_i^*\\) 샘플링\n\\(\\theta_i^*\\)가 가장 큰 arm 선택\n선택 결과 관측, 사후 분포 업데이트\n반복 수행 → 탐험과 활용이 자연스럽게 균형됨\n\nContextual Bandit (문맥 기반 밴딧):\n\n사용자 상태나 환경 \\(x\\)에 따라 보상 확률 \\(\\theta(x, a)\\)를 모델링\n베이지안 회귀, 베이지안 신경망 등으로 확장 가능\n\n\n\n\n2.2 Naive Bayes Classifier (나이브 베이즈 분류기)\n\n문제: 텍스트 분류, 감성 분석, 스팸 필터링\n가정: 단어들은 조건부 독립\n공식: \\(P(c \\mid d) \\propto P(c) \\prod_i P(w_i \\mid c)\\)\n\n\n\n2.3 Hedonic Pricing Model (헤도닉 가격 모형)\n\n문제: 재화의 가격을 품질 특성들의 함수로 설명\n베이지안 회귀(Bayesian Regression) 적용 시 장점:\n\n각 특성 기여도의 사후 분포 추정 가능\n사전 지식 반영\n예측 구간 등 불확실성 표현 가능",
    "crumbs": [
      "Apps",
      "데이터 활용 (Exploitation)",
      "부록-베이지안 통계학과 응용"
    ]
  },
  {
    "objectID": "6-2.html#유의수준과-판단-기준",
    "href": "6-2.html#유의수준과-판단-기준",
    "title": "유의수준, 통계량, p-value",
    "section": "1 유의수준과 판단 기준",
    "text": "1 유의수준과 판단 기준\n유의수준(significance level, α)은 우리가 얼마나 작은 확률까지를 우연으로 받아들일 준비가 되어 있는지를 나타냅니다.\n즉, 귀무가설이 옳을 때에도 우리가 그 가설을 기각할 수 있는 “허용된 오판의 확률”입니다.\n가장 일반적으로 쓰이는 유의수준은 5%입니다. 이는 다음과 같이 해석할 수 있습니다:\n\n“귀무가설이 참인 상황에서, 100번 중 5번 이하로 관측될 정도로 드문 결과가 나왔다면, 우리는 그 가설을 기각하겠다.”\n\n이는 Type I error의 허용 가능성을 명시적으로 설정하는 것이기도 합니다.",
    "crumbs": [
      "Apps",
      "가설 점정 (Hypothesis Test)",
      "유의수준, 통계량, p-value"
    ]
  },
  {
    "objectID": "6-2.html#p-value란-무엇인가",
    "href": "6-2.html#p-value란-무엇인가",
    "title": "유의수준, 통계량, p-value",
    "section": "2 p-value란 무엇인가?",
    "text": "2 p-value란 무엇인가?\np-value는 관측된 통계량보다 극단적인 결과가\n귀무가설 하에서 실제로 관측될 확률(probability)을 의미합니다.\np-value는 본질적으로 “이런 결과가 정말 우연히 나올 수 있었을까?”라는 질문에 대한 답입니다.\np-value가 작을수록, “이건 우연으로 보기엔 너무 극단적인 결과야”라는 판단에 가까워지며,\n그 값이 우리가 설정한 유의수준 \\(\\alpha\\)보다 작다면 귀무가설을 기각할 수 있습니다.\n예를 들어, 어떤 정책 변화 이후 실업률이 크게 줄어들었고, 이에 대한 분석 결과 p-value가 0.003이라면,\n이는 “그 정도의 변화가 단순한 우연으로 발생할 확률은 0.3%에 불과하다”는 의미입니다.\n만약 유의수준을 5%(0.05)로 설정했다면, 우리는 “정책의 효과가 통계적으로 유의하다”고 판단할 수 있습니다.",
    "crumbs": [
      "Apps",
      "가설 점정 (Hypothesis Test)",
      "유의수준, 통계량, p-value"
    ]
  },
  {
    "objectID": "6-2.html#주의-사항",
    "href": "6-2.html#주의-사항",
    "title": "유의수준, 통계량, p-value",
    "section": "3 주의 사항",
    "text": "3 주의 사항\n“95% 신뢰수준의 신뢰구간(confidence interval)”이 의미하는 것은\n“무수히 많은 표본을 반복해서 추출했을 때, 그로부터 만들어지는 구간 중 약 95%는 진짜 모수를 포함할 것이다”라는 것이지, 지금 눈앞에 있는 이 구간이 참값을 포함할 확률이 95%라는 뜻은 아닙니다.\n확률은 미래의 사건에 대한 가능성이고,\n신뢰도는 통계적 절차의 일관성과 반복 안정성에 관한 수치입니다.\n한편, p-value가 0.0001이라고 해서, 그것이 반드시 “실질적으로 중요한 결과”를 의미하는 것은 아닙니다.\n아주 큰 표본에서는 미세한 차이도 쉽게 통계적으로 유의해질 수 있기 때문입니다.\n반대로, 표본이 작다면 실제로 중요한 차이도 통계적으로 유의하지 않을 수 있습니다.\n그러므로, 통계적 유의성(statistical significance)과 실질적 중요성(practical significance)은 항상 구분되어야 하며, 해석은 문제의 맥락과 의사결정의 목적에 따라 달라져야 합니다.",
    "crumbs": [
      "Apps",
      "가설 점정 (Hypothesis Test)",
      "유의수준, 통계량, p-value"
    ]
  },
  {
    "objectID": "6-3.html#가설-설정-차이가-없다는-가정을-먼저-둡니다",
    "href": "6-3.html#가설-설정-차이가-없다는-가정을-먼저-둡니다",
    "title": "두 집단을 비교",
    "section": "1 가설 설정: ’차이가 없다’는 가정을 먼저 둡니다",
    "text": "1 가설 설정: ’차이가 없다’는 가정을 먼저 둡니다\n통계적 가설 검정은 늘 귀무가설(null hypothesis)에서 시작합니다.\n귀무가설은 기본적으로 “차이가 없다”는 전제를 의미하며, 이 가설이 참이라는 가정 하에서\n관측된 데이터가 얼마나 우연히 나올 수 있었는지를 살펴보는 것이 검정의 본질입니다.\n두 집단 A와 B에 대해 비교하고자 할 때, 보통 다음과 같은 가설이 설정됩니다:\n\n귀무가설 \\(H_0\\): \\(\\mu_A = \\mu_B\\) (두 집단의 평균 차이는 없다)\n대립가설 \\(H_1\\): \\(\\mu_A \\neq \\mu_B\\) (두 집단의 평균 차이가 있다)\n\n귀무가설이 참이라면, 두 집단 간의 관측된 평균 차이는 단순한 표본 오차일 수 있습니다. 그러나 그 차이가 표준오차(standard error)에 비해 충분히 클 경우, 우연으로 보기 어려운 판단이 가능해집니다.",
    "crumbs": [
      "Apps",
      "가설 점정 (Hypothesis Test)",
      "두 집단을 비교"
    ]
  },
  {
    "objectID": "6-3.html#표준편차와-표준오차의-구분",
    "href": "6-3.html#표준편차와-표준오차의-구분",
    "title": "두 집단을 비교",
    "section": "2 표준편차와 표준오차의 구분",
    "text": "2 표준편차와 표준오차의 구분\n여기서 중요한 개념이 바로 표준편차(standard deviation)와 표준오차(standard error)의 차이입니다.\n\n표준편차는 한 집단의 데이터가 평균으로부터 얼마나 퍼져 있는지를 나타내는 지표입니다.\n표준오차는 여러 번의 표본추출을 했을 때, 표본평균의 변동성을 측정하는 값입니다.\n즉, 집단의 평균을 추정할 때 발생하는 불확실성입니다.\n\n표준오차는 다음과 같은 공식으로 계산됩니다:\n\\[\\text{Standard Error} = \\frac{\\text{Standard Deviation}}{\\sqrt{n}}\\]\n따라서 표본의 크기 \\(n\\)이 커질수록 표준오차는 작아지고, 평균 추정은 더 정밀해집니다.",
    "crumbs": [
      "Apps",
      "가설 점정 (Hypothesis Test)",
      "두 집단을 비교"
    ]
  },
  {
    "objectID": "6-3.html#검정통계량과-신뢰-구간",
    "href": "6-3.html#검정통계량과-신뢰-구간",
    "title": "두 집단을 비교",
    "section": "3 검정통계량과 신뢰 구간",
    "text": "3 검정통계량과 신뢰 구간\n두 집단 간 평균 차이를 검정할 때는 다음과 같은 검정통계량(test statistic)을 사용합니다:\n\\[t = \\frac{\\bar{X}_A - \\bar{X}_B}{\\text{SE}(\\bar{X}_A - \\bar{X}_B)}\\]\n여기서 \\(\\text{SE}(\\bar{X}_A - \\bar{X}_B)\\)는 두 표본 평균 차이의 표준오차이며, 두 집단의 분산과 표본크기를 함께 고려하여 계산됩니다.\n검정통계량 \\(t\\)가 클수록, 두 평균 차이가 우연히 발생했을 가능성은 작아집니다. 이때 계산된 p-value가 유의수준 \\(\\alpha\\) (예: 0.05)보다 작으면, 우리는 귀무가설을 기각할 수 있습니다.\n또한 신뢰구간(confidence interval)을 이용하면, 두 집단 평균 차이에 대한 추정값이 실제로 유의미한지를 직관적으로 확인할 수도 있습니다. 만약 95% 신뢰구간이 0을 포함하지 않는다면, 이는 통계적으로 유의한 차이로 해석됩니다.",
    "crumbs": [
      "Apps",
      "가설 점정 (Hypothesis Test)",
      "두 집단을 비교"
    ]
  },
  {
    "objectID": "6-3.html#분산의-비교-f-test",
    "href": "6-3.html#분산의-비교-f-test",
    "title": "두 집단을 비교",
    "section": "4 분산의 비교: F-test",
    "text": "4 분산의 비교: F-test\n평균뿐 아니라 두 집단 간 분산의 차이를 비교하고 싶을 때는 F-test를 사용합니다.\n예를 들어, 두 집단의 수입 변동성이 얼마나 다른지를 평가할 수 있으며, 이는 다음과 같은 형태를 가집니다:\n\\[F = \\frac{s_A^2}{s_B^2}\\]\n\\(F\\)-분포는 비대칭이며, 정규성을 가정하는 한편 두 표본이 독립적이어야 한다는 전제가 필요합니다.\n분산의 비교는 가끔 사소해 보이지만, 실제론 중요한 함의를 갖습니다. 예를 들어, 두 집단이 평균적으로는 비슷하더라도, 불평등이나 불안정성이라는 관점에서는 분산이 더 중요한 분석 대상일 수 있습니다.",
    "crumbs": [
      "Apps",
      "가설 점정 (Hypothesis Test)",
      "두 집단을 비교"
    ]
  },
  {
    "objectID": "6-4.html#anova",
    "href": "6-4.html#anova",
    "title": "여러 집단을 한꺼번에 비교",
    "section": "1 ANOVA",
    "text": "1 ANOVA\nANOVA의 핵심 질문은 다음과 같습니다:\n\n“세 개 이상의 집단이 동일한 평균을 공유하고 있다고 볼 수 있는가?”\n\n예를 들어, 세 개의 교육 정책에 따라 학생의 수학 성적 평균이 다른지를 비교하고자 한다면, 우리는 다음과 같은 가설을 설정하게 됩니다.\n\n귀무가설 \\(H_0\\): 모든 집단의 평균이 같다 (\\(\\mu_1 = \\mu_2 = \\mu_3\\))\n대립가설 \\(H_1\\): 적어도 하나의 집단 평균이 다르다\n\nANOVA는 각 집단의 집단 내 변동(within-group variance)과 집단 간 변동(between-group variance)을 비교합니다. 이 비율이 충분히 크면, “집단 간 평균 차이가 단순한 우연 이상의 것일 수 있다”고 판단합니다.\n검정통계량은 다음과 같은 F-통계량으로 계산됩니다:\n\\[F = \\frac{\\text{Between-group Mean Square}}{\\text{Within-group Mean Square}}\n\\]\n이 \\(F\\) 값이 크면, 집단 간 평균 차이가 무시할 수 없는 수준이라는 뜻이며,\n유의수준 \\(\\alpha\\)와 비교하여 귀무가설의 기각 여부를 결정합니다.",
    "crumbs": [
      "Apps",
      "가설 점정 (Hypothesis Test)",
      "여러 집단을 한꺼번에 비교"
    ]
  },
  {
    "objectID": "6-4.html#카이제곱-검정",
    "href": "6-4.html#카이제곱-검정",
    "title": "여러 집단을 한꺼번에 비교",
    "section": "2 카이제곱 검정",
    "text": "2 카이제곱 검정\nANOVA가 연속형 변수들 간의 평균을 다루는 반면,\n카이제곱 검정(Chi-square test)은 범주형 변수들 간의 독립성을 분석하는 데 사용됩니다.\n예를 들어, 다음과 같은 질문을 생각해볼 수 있습니다:\n\n성별에 따라 투표 참여율이 차이가 있을까?\n고용 형태(정규직/비정규직)에 따라 노조 가입률이 다를까?\n지역별로 정당 지지도 분포는 독립적일까?\n\n이러한 질문에 대해 우리는 두 범주형 변수 간의 독립성(independence)을 검정하고자 합니다.\n가장 기본적인 형태는 교차표(contingency table)입니다. 예를 들어 성별(Gender)과 투표참여 여부(Participation)를 2×2로 나눈 표를 생각해봅시다. 각 셀에는 관측된 빈도(observed count)가 들어가고, 카이제곱 검정은 각 셀에 대해 다음과 같은 값을 계산합니다:\n\\[\\chi^2 = \\sum_{i=1}^{r} \\sum_{j=1}^{c} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\]\n여기서 - \\(O_{ij}\\)는 관측값(observed count), - \\(E_{ij}\\)는 기대값(expected count), - \\(r, c\\)는 행과 열의 범주 개수입니다.\n기대값은 다음과 같이 계산됩니다:\n\\[E_{ij} = \\frac{\\text{row total}_i \\times \\text{column total}_j}{\\text{grand total}}\\]\n카이제곱 검정에서 중요한 개념은 자유도(degree of freedom)입니다.\n이는 단순히 “얼마나 많은 독립적인 셀 정보를 비교할 수 있는가”를 의미하며, 다음과 같이 계산됩니다:\n\\[\\text{df} = (r - 1)(c - 1)\\]\n자유도가 높을수록 \\(\\chi^2\\) 분포는 더 부드러운 종 모양에 가까워지고, 검정의 민감도도 함께 증가합니다.\n한 가지 흥미로운 점은, 카이제곱 검정에서 단순한 제곱합(Euclidean distance)이 아니라, 각 항에 대해 기대값으로 나눈 제곱비율을 사용한다는 것입니다. 이는 기대 도수에 따라 관측치의 변동 폭이 다르다는 점을 반영하기 위함입니다. 희귀한 사건에서의 작은 차이는 큰 의미를 가질 수 있기 때문에 (c.f. 정보 엔트로피), 스케일을 조정한 통계량이 필요합니다.\n예시: 성별과 정치 참여의 독립성 검정\n\n\n\n\n참여함\n참여 안함\n합계\n\n\n\n\n남성\n550\n450\n1000\n\n\n여성\n600\n400\n1000\n\n\n합계\n1150\n850\n2000\n\n\n\n이 경우, 두 변수(성별, 참여여부)가 독립이라면\n각 셀의 기대값은 행합계 × 열합계 / 전체합계로 계산됩니다.\n\n남성-참여: \\(E = \\frac{1000 \\times 1150}{2000} = 575\\)\n남성-불참: \\(E = \\frac{1000 \\times 850}{2000} = 425\\)\n여성-참여: \\(E = 575\\)\n여성-불참: \\(E = 425\\)\n\n관측값과의 차이를 기반으로 \\(\\chi^2\\) 통계량을 계산하고,\n자유도(df=1)에 따른 임계값과 비교하면, 두 변수의 독립 여부를 판단할 수 있습니다.",
    "crumbs": [
      "Apps",
      "가설 점정 (Hypothesis Test)",
      "여러 집단을 한꺼번에 비교"
    ]
  },
  {
    "objectID": "4-1.html#단기-구조-분석",
    "href": "4-1.html#단기-구조-분석",
    "title": "외생성 (exogeneity) vs. 내생성 (endogeneity)",
    "section": "1 단기 구조 분석",
    "text": "1 단기 구조 분석\n장기적으로 두 변수 간에 공적분관계나 평균 회귀(mean-reversion) 구조가 존재할 수 있더라도,\n단기에서는 수많은 외생적 충격(external shocks), 정책 개입, 비대칭 정보, 기대의 변화 등으로 인해 두 변수 사이의 관계는 훨씬 더 복잡하고 변동성이 높습니다.\n따라서 단기 분석의 주요 목적은, 어떤 변수의 변화가 다른 변수의 단기적 변동을 얼마나 예측하거나 설명할 수 있는가? 이며, 이 과정에서 반드시 구분되어야 할 개념이 바로 외생성(exogeneity)과 내생성(endogeneity)입니다.",
    "crumbs": [
      "Apps",
      "데이터 활용 (Exploitation)",
      "외생성 (exogeneity) vs. 내생성 (endogeneity)"
    ]
  },
  {
    "objectID": "4-1.html#외생성exogeneity-변수는-설명자인가",
    "href": "4-1.html#외생성exogeneity-변수는-설명자인가",
    "title": "외생성 (exogeneity) vs. 내생성 (endogeneity)",
    "section": "2 외생성(Exogeneity): 변수는 “설명자”인가?",
    "text": "2 외생성(Exogeneity): 변수는 “설명자”인가?\n외생변수(exogenous variable)란, 주어진 모형의 구조나 오차항과 확률적으로 독립인 설명변수입니다.\n즉, \\(X\\)가 \\(Y\\)를 설명하는 모형\n\\[Y = \\beta_0 + \\beta_1 X + \\varepsilon\\]\n에서 \\(X\\)가 외생적이라는 말은 다음을 의미합니다:\n\\[\\mathrm{Cov}(X, \\varepsilon) = 0\\]\n이 조건이 충족되면, \\(\\beta_1\\)은 인과적 해석이 가능한 계수로 받아들여질 수 있으며,\n최소제곱추정량(OLS estimator)은 편향(bias) 없이 추정됩니다.",
    "crumbs": [
      "Apps",
      "데이터 활용 (Exploitation)",
      "외생성 (exogeneity) vs. 내생성 (endogeneity)"
    ]
  },
  {
    "objectID": "4-1.html#내생성endogeneity-설명자-속에-결과가-섞여-있다",
    "href": "4-1.html#내생성endogeneity-설명자-속에-결과가-섞여-있다",
    "title": "외생성 (exogeneity) vs. 내생성 (endogeneity)",
    "section": "3 내생성(Endogeneity): 설명자 속에 결과가 섞여 있다",
    "text": "3 내생성(Endogeneity): 설명자 속에 결과가 섞여 있다\n반대로, 만약 \\(\\mathrm{Cov}(X, \\varepsilon) \\neq 0\\)이라면,\n\\(X\\)는 내생적(endogenous)입니다. 이 경우, 회귀계수 \\(\\beta_1\\)은 다음의 이유로 편향되고 일관되지 않은 추정치를 산출합니다:\n\nOmitted variable bias: \\(X\\)와 \\(Y\\) 모두에 영향을 주는 제3의 변수 \\(Z\\)가 모형에 누락됨\nMeasurement error: \\(X\\)가 부정확하게 측정되었을 경우\nSimultaneity: \\(X\\)와 \\(Y\\)가 서로 동시에 영향을 미치는 경우 (동시 방정식 시스템)\nReverse causality: 사실은 \\(Y \\to X\\)의 방향성이 더 강한데, \\(X \\to Y\\)로 잘못 가정함\n\n예를 들어, 교육(\\(X\\))과 임금(\\(Y\\))의 관계를 회귀모형으로 분석할 때,\n교육 수준과 임금 수준 모두에 영향을 미치는 부모의 소득이나 지역적 요인이 빠졌다면, \\(X\\)는 내생적입니다.",
    "crumbs": [
      "Apps",
      "데이터 활용 (Exploitation)",
      "외생성 (exogeneity) vs. 내생성 (endogeneity)"
    ]
  },
  {
    "objectID": "4-1.html#내생성-문제의-해결-도구변수-instrumental-variable",
    "href": "4-1.html#내생성-문제의-해결-도구변수-instrumental-variable",
    "title": "외생성 (exogeneity) vs. 내생성 (endogeneity)",
    "section": "4 내생성 문제의 해결: 도구변수 (Instrumental Variable)",
    "text": "4 내생성 문제의 해결: 도구변수 (Instrumental Variable)\n통계학에서는 내생성 문제를 해결하기 위해 여러 방법이 사용되며, 가장 대표적인 것은 도구변수(instrumental variable, IV)입니다.\n도구변수 \\(Z\\)는 다음의 조건을 만족해야 합니다:\n\nRelevance: \\(\\mathrm{Cov}(Z, X) \\neq 0\\) (설명변수 \\(X\\)와 상관됨)\nExogeneity: \\(\\mathrm{Cov}(Z, \\varepsilon) = 0\\) (오차항과는 무관함)\n\n도구변수를 사용하면, 내생적 설명변수 \\(X\\)를 \\(Z\\)로 “대체하여”,\n편향되지 않은 계수 추정을 할 수 있습니다. 이는 2단계 최소제곱법(2SLS)을 통해 구현됩니다.",
    "crumbs": [
      "Apps",
      "데이터 활용 (Exploitation)",
      "외생성 (exogeneity) vs. 내생성 (endogeneity)"
    ]
  },
  {
    "objectID": "4-1.html#현실의-단기-구조는-대부분-내생적이다",
    "href": "4-1.html#현실의-단기-구조는-대부분-내생적이다",
    "title": "외생성 (exogeneity) vs. 내생성 (endogeneity)",
    "section": "5 현실의 단기 구조는 대부분 내생적이다",
    "text": "5 현실의 단기 구조는 대부분 내생적이다\n실제 경제 데이터에서 대부분의 설명변수는 정책적 반응, 기대의 변화, 시장 피드백 효과로 인해\n모형의 오차항과 상관될 가능성이 높습니다. 따라서 현실에서는 다음과 같은 대안이 논의됩니다:\n\nControl function approach: 오차항의 추정 값을 변수로 넣어 bias 조정\nPanel data fixed-effects: 관측 불가능한 고정 특성 제거\nDifference-in-differences: 시간 전후의 변화량 비교\n\n하지만 어떤 방법을 사용하더라도, 내생성의 존재를 명확히 인식하고 그로 인한 추정 오류를 보정하려는 시도가 통계적 추론의 전제입니다.\n\n\n\n\n\n\n\n\n\n개념\n정의\n수학적 조건\n결과\n\n\n\n\nExogeneity\n설명변수가 오차와 독립\n\\(\\mathrm{Cov}(X, \\varepsilon) = 0\\)\nOLS는 일관되고 편향 없음\n\n\nEndogeneity\n설명변수가 오차와 상관\n\\(\\mathrm{Cov}(X, \\varepsilon) \\neq 0\\)\nOLS는 편향되고 일관되지 않음\n\n\nIV Solution\n대체 변수 \\(Z\\) 사용\n\\(Z \\perp \\varepsilon\\), \\(Z\\) 관련성\n2SLS를 통해 추정 가능",
    "crumbs": [
      "Apps",
      "데이터 활용 (Exploitation)",
      "외생성 (exogeneity) vs. 내생성 (endogeneity)"
    ]
  },
  {
    "objectID": "2-1.html#수치로-요약하기-statistics",
    "href": "2-1.html#수치로-요약하기-statistics",
    "title": "1D Statistics and Plots",
    "section": "1 수치로 요약하기: Statistics",
    "text": "1 수치로 요약하기: Statistics\n\n1.1 중심 경향의 측정\n\n평균 (mean): 수익률의 총합을 관측치 수로 나눈 값. 그러나 극단값(outlier)에 민감합니다.\n중앙값 (median): 데이터를 정렬했을 때 중간에 위치한 값. 비대칭 분포에서 더 안정적인 대표값입니다.\n최빈값 (mode): 가장 자주 등장한 값. 분포의 패턴이 뚜렷한 경우 유용합니다.\n\n예: 고소득자의 존재로 인해 평균소득이 높아진 분포에서는 중앙값이 오히려 실제 중간소득층을 더 잘 대표합니다. 이처럼 소득분포는 대개 오른쪽으로 긴 꼬리를 가진 비대칭 분포를 보입니다.\n\n\n\n1.2 산포의 측정과 변동성의 해석\n\n표준편차 (standard deviation)는 평균으로부터 값들이 얼마나 퍼져 있는지를 나타냅니다. 분산의 제곱근입니다.\nRMS (Root Mean Square)는 값들의 크기를 평균적으로 평가하며, 평균이 0일 때에도 유용합니다.\n\n\\[\\text{RMS} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n x_i^2}, \\quad s = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2}\\]\n수익률이 평균 0에 가까워도 변동성이 크면 RMS는 여전히 크며, 이는 평균 수익률 (mean return)이 같아도 변동성(volatility)이 다르다는 점을 반영합니다.\n\n\n1.3 분포의 모양: 왜도와 첨도\n\n왜도 (skewness)는 분포의 비대칭성을 나타냅니다. 오른쪽 꼬리가 길면 양의 왜도, 왼쪽이 길면 음의 왜도입니다.\n첨도 (kurtosis)는 분포의 뾰족함과 꼬리의 두께를 설명합니다. 정규분포보다 꼬리가 두꺼우면 극단값의 발생 가능성이 높습니다.\n\n예를 들어, 시가총액이 작은 주식들(small size)의 수익률이 시가총액이 큰 주식들(big size)보다 평균적으로 더 높을 수 있습니다. 그러나 small size 주식의 수익률 분포가 더 큰 왜도(skewness)와 더 높은 첨도(kurtosis)를 가진다면, 해당 주식에 투자한 투자자는 높은 기대수익률과 함께 더 큰 변동성 및 극단적 손익 가능성을 동시에 감수해야 합니다.\n이러한 현상은 “more risk, more return”이라는 주류 금융경제학 이론의 핵심 철학과도 일치합니다. 특히 자산 가격 결정 이론에서는, 평균 수익률이 높은 자산은 일반적으로 더 큰 분산(variance), 더 긴 꼬리(tail), 더 뾰족한 분포(peakedness)를 동반한다고 가정합니다. 이는 왜도와 첨도가 단순한 통계량이 아니라, 자본시장에서 보상(risk premium)이 형성되는 구조를 이해하는 데도 중요한 역할을 함을 의미합니다.\n\n\n1.4 자유도와 통계적 추정의 정확성\n자유도(degree of freedom)는 통계적 추정에서 독립적인 정보가 얼마나 있는가를 수량화한 개념입니다.\n표준편차를 계산할 때, 전체 편차 합이 0이 되기 때문에 마지막 하나는 나머지로부터 결정됩니다. 따라서 \\(n\\)개의 편차 중 자유로운 값은 \\(n-1\\)개입니다. 이 때문에 분산 계산에서 분모가 \\(n\\)이 아닌 \\((n-1)\\)이 되는 것입니다.\n선형회귀에서는 자유도가 더욱 중요한 의미를 가집니다.\n\nT-통계량(T-statistic)은 회귀계수가 0이라는 귀무가설을 검정할 때 사용됩니다. 이때의 자유도는 \\(n - k - 1\\)입니다. 여기서 \\(k\\)는 독립변수의 수이며, 1은 상수항(intercept) 때문입니다.\n\\[t = \\frac{\\hat{\\beta}_j}{\\text{SE}(\\hat{\\beta}_j)}, \\quad \\text{with df} = n - k - 1\\]\nAdjusted R-squared (조정된 결정계수)도 모델의 설명력을 평가할 때 자유도를 고려하여 계산됩니다. 단순한 R-squared는 변수의 수를 늘릴수록 무조건 커지기 때문에, 다음과 같이 자유도로 보정합니다: \\[ \\bar{R}^2 = 1 - \\left(1 - R^2\\right) \\cdot \\frac{n - 1}{n - k - 1} \\]\n\n이는 변수의 수가 증가할수록 모델이 더 복잡해지지만, 자유도가 줄어드는 만큼 설명력에 대한 과도한 낙관을 억제하려는 구조입니다. 이처럼 자유도는 통계적 추정의 신뢰성, 검정의 엄밀함, 모델 선택의 합리성에 직결되는 핵심 개념입니다.",
    "crumbs": [
      "Apps",
      "데이터 탐색 (Exploration)",
      "1D Statistics and Plots"
    ]
  },
  {
    "objectID": "2-1.html#그림으로-보기-plots",
    "href": "2-1.html#그림으로-보기-plots",
    "title": "1D Statistics and Plots",
    "section": "2 그림으로 보기: Plots",
    "text": "2 그림으로 보기: Plots\n\n2.1 히스토그램과 밀도 해석\n히스토그램(histogram)은 데이터를 구간별로 나누고 각 구간의 빈도수를 막대그래프로 표현한 것입니다. 단순한 수치보다 분포의 모양을 직관적으로 파악할 수 있습니다.\n\n히스토그램은 확률 밀도(probability density)의 개념으로도 해석할 수 있습니다. 관측치가 많아질수록 히스토그램은 연속적인 확률 밀도 함수에 수렴하며, 특히 로그변환이나 비율 변화율(퍼센트 변화)을 적용하면 비대칭 구조를 완화할 수 있습니다.\n\n\n2.2 Boxplot: 중앙값, 사분위수, 이상값\n상자그림(Boxplot)은 데이터를 5개의 지점(최소, Q1, 중앙값, Q3, 최대)으로 요약합니다. 이상값은 박스 밖의 점으로 나타납니다.\n\nIQR(Interquartile Range)이 크면 변동성이 크다는 뜻입니다.\n박스가 한쪽으로 치우쳐 있으면 분포가 비대칭임을 나타냅니다.\n\n\nBoxplot은 특히 변수 간 비교에서 수익률의 위치와 분산을 동시에 시각화할 수 있는 장점이 있습니다.",
    "crumbs": [
      "Apps",
      "데이터 탐색 (Exploration)",
      "1D Statistics and Plots"
    ]
  },
  {
    "objectID": "index.html#preface",
    "href": "index.html#preface",
    "title": "AI 응용 선형대수학",
    "section": "",
    "text": "cover\n\n\n우리는 “외국인 직접투자가 증가하면 경제성장률도 상승할까?”\n\n모든 모형은 틀렸다. 다만, 몇몇 모형은 유용하다.\n\n이러한 인식이 통계학의 출발점이며, 합리적 사고를 위한 훈련의 첫 걸음입니다.",
    "crumbs": [
      "Apps",
      "AI 응용 선형대수학"
    ]
  },
  {
    "objectID": "목차 by NotebookLM.html",
    "href": "목차 by NotebookLM.html",
    "title": "1 Book Title: Practical Linear Algebra for Data Science (Example-Driven with Python)",
    "section": "",
    "text": "Brief overview of how linear algebra is the language of data and machine learning.\nIntroducing the practical, visualization-focused approach of the book.\n\n\n\n\n\n\n\nFamous Problem: Simple resource allocation or balancing a basic chemical equation, represented as a system of linear equations. (Similar to the introductory systems in Lay).\nKey Assumptions: Data can be represented as vectors in Rn. Relationships between data points or variables can be modeled by linear equations.\nPython Visualization: Plotting 2D/3D vectors. Visualizing lines and planes from simple equations. Showing the intersection point/line as the solution.\nInterpretation Summary: How vectors capture multi-dimensional data, and how systems of equations represent constraints or relationships between features/variables.\n\n\n\n\n\nFamous Problem: Solving for coefficients in a simple polynomial fitting problem (e.g., fitting a line or parabola to a few exact points). This leads to an Ax=b problem.\nKey Assumptions: The system Ax=b has a unique solution, infinite solutions, or no solution. Gaussian Elimination is a systematic way to transform the system.\nPython Visualization: Illustrating row operations on an augmented matrix. Showing the matrix A, vector b, and solving using numpy.linalg.solve. Visualizing the result (e.g., the fitted polynomial).\nInterpretation Summary: Understanding matrix A as representing the structure of the problem, x as the unknown parameters (e.g., coefficients), and b as the observed data. Gaussian Elimination finds x. Introduction of matrix operations like multiplication (relevant for Ax=b).\n\n\n\n\n\nFamous Problem: Understanding the set of all possible outputs of a linear model, or the set of inputs that result in zero output (null space).\nKey Assumptions: Data and results live within vector spaces. Subspaces represent important subsets (like the column space or null space of a matrix).\nPython Visualization: Visualizing the span of vectors in 2D/3D (showing subspaces). Illustrating column space (image) and null space (kernel) for simple matrices.\nInterpretation Summary: Connecting the column space of A to the possible values of Ax=b and the null space of A to the non-uniqueness of solutions. Introduction of concepts like basis and dimension.\n\n\n\n\n\nFamous Problem: Applying geometric transformations (scaling, rotation, shear) to data points or images. (Examples of transformations are in Lay).\nKey Assumptions: Linear transformations preserve vector addition and scalar multiplication. Every linear transformation can be represented by a matrix.\nPython Visualization: Applying transformation matrices to sets of 2D points (e.g., a square or a face) and visualizing the result. Visualizing 3D transformations if possible.\nInterpretation Summary: Understanding how matrices can linearly transform data, which is fundamental to neural networks and many other models.\n\n\n\n\n\n\n\n\nFamous Problem: Analyzing the long-term behavior of a system or process, like population distribution changes or Markov chains. (Eigenvectors show stable states).\nKey Assumptions: The matrix A represents the transition or transformation rules of the system. Eigenvectors represent directions that are only scaled by the transformation.\nPython Visualization: Visualizing eigenvectors (direction) and eigenvalues (magnitude of scaling) for a 2x2 matrix. Simulating the step-by-step application of a transition matrix (e.g., Lay’s Markov chain example) and showing convergence to the eigenvector.\nInterpretation Summary: Eigenvectors reveal the fundamental directions or components of a linear transformation, and eigenvalues tell us the factor by which variance is scaled in those directions. This is key to understanding PCA.\n\n\n\n\n\nFamous Problem: Image compression or reducing the dimensionality of a dataset while retaining most information (Principal Component Analysis).\nKey Assumptions: Any matrix can be decomposed into a product of three matrices (U, Σ, V^T). The singular values in Σ capture the “importance” of each dimension.\nPython Visualization: Performing SVD on a simple data matrix or grayscale image using numpy.linalg.svd. Showing the singular values. Reconstructing the matrix/image using only the top k singular values and vectors, visualizing the compressed result. Illustrating the four fundamental subspaces using SVD.\nInterpretation Summary: SVD provides a powerful way to understand a matrix’s structure, identify its rank, find orthogonal bases for the fundamental subspaces, and approximate the matrix with lower rank, directly enabling techniques like PCA for dimensionality reduction.\n\n\n\n\n\nFamous Problem: Analyzing the spread and correlation within a dataset, represented by the covariance matrix.\nKey Assumptions: The covariance matrix is symmetric and (often) positive semidefinite or positive definite. Its eigenvectors point in directions of maximum variance (principal components).\nPython Visualization: Calculating the covariance matrix for a 2D/3D dataset using numpy.cov. Visualizing the data scatter plot. Overlaying the eigenvectors of the covariance matrix scaled by the eigenvalues to show the principal axes of variance (linking back to Unit 2.1/2.2).\nInterpretation Summary: The covariance matrix summarizes feature relationships. Positive definiteness is a key property related to variance and optimization objectives. Eigenanalysis of the covariance matrix reveals the principal components.\n\n\n\n\n\n\n\n\nFamous Problem: Linear Regression: Finding the line (or plane) that best fits a set of data points. (The data points b are not exactly in the column space of A, so Ax=b is unsolvable).\nKey Assumptions: The “best fit” line/plane is the orthogonal projection of the data vector b onto the column space of the design matrix A. The error vector is orthogonal to the column space.\nPython Visualization: Plotting 2D data points (t, b). Visualizing the design matrix A and data vector b setup for linear regression. Calculating the projection p = Ax̂ using the normal equations (ATAx̂ = ATb). Plotting the resulting “best fit” line y = C + Dt. Visualizing the error vectors (b-p) as orthogonal to the column space of A.\nInterpretation Summary: Least squares provides a standard method to find model parameters when an exact solution doesn’t exist, minimizing the sum of squared errors. This is achieved by projecting the observed data onto the model’s prediction space. Orthogonality is the key geometric principle.\n\n\n\n\n\nFamous Problem: Improving numerical stability or simplifying calculations in linear models.\nKey Assumptions: Orthogonal bases simplify projection calculations. The Gram-Schmidt process can find an orthogonal basis.\nPython Visualization: Demonstrating the Gram-Schmidt process on a set of linearly independent vectors and visualizing the resulting orthogonal vectors. Comparing the ease of calculating a projection onto a subspace using a standard basis vs. an orthogonal basis.\nInterpretation Summary: Orthogonal bases provide a computationally advantageous way to represent vector spaces and perform projections, improving the reliability of algorithms in practice.\n\n\n\n\n\nFamous Problem: Solving very large systems of linear equations that arise from complex models or massive datasets (e.g., in finite element methods or network analysis). (Direct methods like Gaussian Elimination can be too slow or memory-intensive).\nKey Assumptions: Iterative methods approach the true solution Ax=b through successive approximations [Strang 7.4].\nPython Visualization: Implementing a simple iterative method (e.g., Jacobi or Gauss-Seidel, or gradient descent for Ax=b). Visualizing the convergence of the solution vector over iterations.\nInterpretation Summary: For very large problems, iterative methods provide a practical way to find approximate solutions to linear systems, essential in fields like numerical simulation and large-scale optimization (which underlies many machine learning algorithms).\n\n\n\n\n\n\nBriefly discuss advanced topics (e.g., tensors, optimization details, advanced factorizations).\nEncourage further exploration of linear algebra’s role in specific ML algorithms.\n\nThis structure directly addresses your requirements:\n\nStrang-based structure: It builds upon the Parts (Basic, Unsupervised, Supervised) derived from the Strang TOC concepts.\nLay Examples: It identifies areas where concepts and examples from the Lay excerpts can be integrated (e.g., systems of equations, transformations, eigenvalues/Markov chains, SVD/PCA, projections/least squares, curve fitting).\nPer-Unit Structure: Each suggested unit follows the “Famous Problem -&gt; Key Assumptions -&gt; Python -&gt; Interpretation” flow.\nFocus: The problems, visualizations, and interpretations are framed with a practical, data-science/ML application focus.\nFormat: It is provided in English Markdown.\n\nThis outline provides a solid framework for a book that teaches linear algebra through the lens of its practical applications in data science, leveraging specific examples and concepts found in the provided Lay book excerpts."
  },
  {
    "objectID": "목차 by NotebookLM.html#introduction-why-linear-algebra-for-data",
    "href": "목차 by NotebookLM.html#introduction-why-linear-algebra-for-data",
    "title": "1 Book Title: Practical Linear Algebra for Data Science (Example-Driven with Python)",
    "section": "",
    "text": "Brief overview of how linear algebra is the language of data and machine learning.\nIntroducing the practical, visualization-focused approach of the book."
  },
  {
    "objectID": "목차 by NotebookLM.html#part-1-basic-tools-and-geometry-building-blocks",
    "href": "목차 by NotebookLM.html#part-1-basic-tools-and-geometry-building-blocks",
    "title": "1 Book Title: Practical Linear Algebra for Data Science (Example-Driven with Python)",
    "section": "",
    "text": "Famous Problem: Simple resource allocation or balancing a basic chemical equation, represented as a system of linear equations. (Similar to the introductory systems in Lay).\nKey Assumptions: Data can be represented as vectors in Rn. Relationships between data points or variables can be modeled by linear equations.\nPython Visualization: Plotting 2D/3D vectors. Visualizing lines and planes from simple equations. Showing the intersection point/line as the solution.\nInterpretation Summary: How vectors capture multi-dimensional data, and how systems of equations represent constraints or relationships between features/variables.\n\n\n\n\n\nFamous Problem: Solving for coefficients in a simple polynomial fitting problem (e.g., fitting a line or parabola to a few exact points). This leads to an Ax=b problem.\nKey Assumptions: The system Ax=b has a unique solution, infinite solutions, or no solution. Gaussian Elimination is a systematic way to transform the system.\nPython Visualization: Illustrating row operations on an augmented matrix. Showing the matrix A, vector b, and solving using numpy.linalg.solve. Visualizing the result (e.g., the fitted polynomial).\nInterpretation Summary: Understanding matrix A as representing the structure of the problem, x as the unknown parameters (e.g., coefficients), and b as the observed data. Gaussian Elimination finds x. Introduction of matrix operations like multiplication (relevant for Ax=b).\n\n\n\n\n\nFamous Problem: Understanding the set of all possible outputs of a linear model, or the set of inputs that result in zero output (null space).\nKey Assumptions: Data and results live within vector spaces. Subspaces represent important subsets (like the column space or null space of a matrix).\nPython Visualization: Visualizing the span of vectors in 2D/3D (showing subspaces). Illustrating column space (image) and null space (kernel) for simple matrices.\nInterpretation Summary: Connecting the column space of A to the possible values of Ax=b and the null space of A to the non-uniqueness of solutions. Introduction of concepts like basis and dimension.\n\n\n\n\n\nFamous Problem: Applying geometric transformations (scaling, rotation, shear) to data points or images. (Examples of transformations are in Lay).\nKey Assumptions: Linear transformations preserve vector addition and scalar multiplication. Every linear transformation can be represented by a matrix.\nPython Visualization: Applying transformation matrices to sets of 2D points (e.g., a square or a face) and visualizing the result. Visualizing 3D transformations if possible.\nInterpretation Summary: Understanding how matrices can linearly transform data, which is fundamental to neural networks and many other models."
  },
  {
    "objectID": "목차 by NotebookLM.html#part-2-analyzing-data-structure-unsupervised-learning",
    "href": "목차 by NotebookLM.html#part-2-analyzing-data-structure-unsupervised-learning",
    "title": "1 Book Title: Practical Linear Algebra for Data Science (Example-Driven with Python)",
    "section": "",
    "text": "Famous Problem: Analyzing the long-term behavior of a system or process, like population distribution changes or Markov chains. (Eigenvectors show stable states).\nKey Assumptions: The matrix A represents the transition or transformation rules of the system. Eigenvectors represent directions that are only scaled by the transformation.\nPython Visualization: Visualizing eigenvectors (direction) and eigenvalues (magnitude of scaling) for a 2x2 matrix. Simulating the step-by-step application of a transition matrix (e.g., Lay’s Markov chain example) and showing convergence to the eigenvector.\nInterpretation Summary: Eigenvectors reveal the fundamental directions or components of a linear transformation, and eigenvalues tell us the factor by which variance is scaled in those directions. This is key to understanding PCA.\n\n\n\n\n\nFamous Problem: Image compression or reducing the dimensionality of a dataset while retaining most information (Principal Component Analysis).\nKey Assumptions: Any matrix can be decomposed into a product of three matrices (U, Σ, V^T). The singular values in Σ capture the “importance” of each dimension.\nPython Visualization: Performing SVD on a simple data matrix or grayscale image using numpy.linalg.svd. Showing the singular values. Reconstructing the matrix/image using only the top k singular values and vectors, visualizing the compressed result. Illustrating the four fundamental subspaces using SVD.\nInterpretation Summary: SVD provides a powerful way to understand a matrix’s structure, identify its rank, find orthogonal bases for the fundamental subspaces, and approximate the matrix with lower rank, directly enabling techniques like PCA for dimensionality reduction.\n\n\n\n\n\nFamous Problem: Analyzing the spread and correlation within a dataset, represented by the covariance matrix.\nKey Assumptions: The covariance matrix is symmetric and (often) positive semidefinite or positive definite. Its eigenvectors point in directions of maximum variance (principal components).\nPython Visualization: Calculating the covariance matrix for a 2D/3D dataset using numpy.cov. Visualizing the data scatter plot. Overlaying the eigenvectors of the covariance matrix scaled by the eigenvalues to show the principal axes of variance (linking back to Unit 2.1/2.2).\nInterpretation Summary: The covariance matrix summarizes feature relationships. Positive definiteness is a key property related to variance and optimization objectives. Eigenanalysis of the covariance matrix reveals the principal components."
  },
  {
    "objectID": "목차 by NotebookLM.html#part-3-modeling-data-relationships-supervised-learning",
    "href": "목차 by NotebookLM.html#part-3-modeling-data-relationships-supervised-learning",
    "title": "1 Book Title: Practical Linear Algebra for Data Science (Example-Driven with Python)",
    "section": "",
    "text": "Famous Problem: Linear Regression: Finding the line (or plane) that best fits a set of data points. (The data points b are not exactly in the column space of A, so Ax=b is unsolvable).\nKey Assumptions: The “best fit” line/plane is the orthogonal projection of the data vector b onto the column space of the design matrix A. The error vector is orthogonal to the column space.\nPython Visualization: Plotting 2D data points (t, b). Visualizing the design matrix A and data vector b setup for linear regression. Calculating the projection p = Ax̂ using the normal equations (ATAx̂ = ATb). Plotting the resulting “best fit” line y = C + Dt. Visualizing the error vectors (b-p) as orthogonal to the column space of A.\nInterpretation Summary: Least squares provides a standard method to find model parameters when an exact solution doesn’t exist, minimizing the sum of squared errors. This is achieved by projecting the observed data onto the model’s prediction space. Orthogonality is the key geometric principle.\n\n\n\n\n\nFamous Problem: Improving numerical stability or simplifying calculations in linear models.\nKey Assumptions: Orthogonal bases simplify projection calculations. The Gram-Schmidt process can find an orthogonal basis.\nPython Visualization: Demonstrating the Gram-Schmidt process on a set of linearly independent vectors and visualizing the resulting orthogonal vectors. Comparing the ease of calculating a projection onto a subspace using a standard basis vs. an orthogonal basis.\nInterpretation Summary: Orthogonal bases provide a computationally advantageous way to represent vector spaces and perform projections, improving the reliability of algorithms in practice.\n\n\n\n\n\nFamous Problem: Solving very large systems of linear equations that arise from complex models or massive datasets (e.g., in finite element methods or network analysis). (Direct methods like Gaussian Elimination can be too slow or memory-intensive).\nKey Assumptions: Iterative methods approach the true solution Ax=b through successive approximations [Strang 7.4].\nPython Visualization: Implementing a simple iterative method (e.g., Jacobi or Gauss-Seidel, or gradient descent for Ax=b). Visualizing the convergence of the solution vector over iterations.\nInterpretation Summary: For very large problems, iterative methods provide a practical way to find approximate solutions to linear systems, essential in fields like numerical simulation and large-scale optimization (which underlies many machine learning algorithms)."
  },
  {
    "objectID": "목차 by NotebookLM.html#conclusion-where-to-go-next",
    "href": "목차 by NotebookLM.html#conclusion-where-to-go-next",
    "title": "1 Book Title: Practical Linear Algebra for Data Science (Example-Driven with Python)",
    "section": "",
    "text": "Briefly discuss advanced topics (e.g., tensors, optimization details, advanced factorizations).\nEncourage further exploration of linear algebra’s role in specific ML algorithms.\n\nThis structure directly addresses your requirements:\n\nStrang-based structure: It builds upon the Parts (Basic, Unsupervised, Supervised) derived from the Strang TOC concepts.\nLay Examples: It identifies areas where concepts and examples from the Lay excerpts can be integrated (e.g., systems of equations, transformations, eigenvalues/Markov chains, SVD/PCA, projections/least squares, curve fitting).\nPer-Unit Structure: Each suggested unit follows the “Famous Problem -&gt; Key Assumptions -&gt; Python -&gt; Interpretation” flow.\nFocus: The problems, visualizations, and interpretations are framed with a practical, data-science/ML application focus.\nFormat: It is provided in English Markdown.\n\nThis outline provides a solid framework for a book that teaches linear algebra through the lens of its practical applications in data science, leveraging specific examples and concepts found in the provided Lay book excerpts."
  },
  {
    "objectID": "8-Finite-State Markov Chains.html",
    "href": "8-Finite-State Markov Chains.html",
    "title": "[GitSAM](../index.html)",
    "section": "",
    "text": "A Markov chain is a model for movement between discrete states.\nBut this model only makes sense when the Markov Property holds — that is:\n\n(Memoryless) The future depends only on the present, not on the past.\n\nFormally, for states \\(X_0, X_1, \\dots, X_n\\) in a discrete-time stochastic process:\n\\[\\mathbb{P}(X_{n+1} = x \\mid X_n = x_n, X_{n-1} = x_{n-1}, \\dots, X_0 = x_0) = \\mathbb{P}(X_{n+1} = x \\mid X_n = x_n)\\]\nThis assumption is not universal. It must be justified by the nature of the system being modeled.\n\nMarkov property is not about the system itself, but about how you model information and relevance.\n\n\n\n0.1 When the Markov Property Fails\n\nHuman Learning Behavior A student’s future performance depends not just on their current grade, but also on how long and how intensely they studied in the past.\n→ Long-term memory → violates the Markov property.\nDisease Progression\nIn medical diagnosis, the probability of developing complications may depend on how long the patient has been ill — not just the current stage of disease.\n→ Duration-sensitive states → non-Markovian.\nPassive Investment Based on Long-Term History An investor builds a portfolio by analyzing a firm’s extended earnings history, sector trends, and macroeconomic business cycles to predict future return. Decisions at time \\(t\\) are conditioned on a long series of past financial states: \\(\\mathbb{P}(\\text{return}_{t+1} \\mid \\text{history}_{\\leq t})\\) → Past-dependent inference violates the Markov property.\n\n\n\n0.2 When the Markov Property Holds (or approximately holds)\n\nActive Investment Based on Present Signal An investor makes decisions based on current financial signals, market sentiment, or recent events — such as a product launch or policy shift. In this case, the future expected return depends only on the current observable state: \\(\\mathbb{P}(\\text{return}_{t+1} \\mid \\text{state}_t)\\) → If the uncertainty horizon is short, and the system resets quickly, the process is approximately Markovian.\nInventory Replenishment Models\nIf a warehouse is monitored daily, and decisions are made based on current stock levels only, then future inventory is conditionally independent of the past given the present.\n→ Well-controlled, memoryless policy → approximates a Markov chain.\nWeb Page Navigation (PageRank)\nThe likelihood of a user jumping from one page to another depends only on the current page, not the full click history.\n→ Modeled as a Markov chain with transition matrix from current to next page.\n\n\nHence, before using a Markov model, we must ask:\n\n“Does the system forget its past once the present is known?”\n\nOnly then can transition probabilities (e.g., \\(P(i \\to j)\\)) fully describe the process.\nOnce this assumption is accepted, we can begin to explore structure within the state space — and the first structural question is:\n\n“Which states communicate with each other?”\n\nThis leads us to our first concept: Communicating States.\n\n\n\n\nReuseCC BY-NC-SA 4.0"
  },
  {
    "objectID": "1.1-LP-Geometric and Simplex method.html",
    "href": "1.1-LP-Geometric and Simplex method.html",
    "title": "[GitSAM](../index.html)",
    "section": "",
    "text": "A factory produces two products, A and B.\n\nEach unit of product A requires 3 hours of machine time, and product B requires 2 hours. The total available machine time per day is 180 hours.\nProduct A requires 2 kg of raw material per unit, and product B requires 3 kg. The total available raw material is 160 kg.\nProfit per unit is $50 for product A and $40 for product B.\n\nGoal: Determine the production quantities of A and B that maximize profit without exceeding the resource constraints.\n\n\n\nVariables:\nLet\n\\(x\\) = units of product A\n\\(y\\) = units of product B\nObjective function:\nMaximize \\[Z = 50x + 40y\\]\nSubject to the constraints:\n\\[\\begin{cases} 3x + 2y \\leq 180 & \\text{(machine time)} \\\\ 2x + 3y \\leq 160 & \\text{(raw material)} \\\\ x \\geq 0,\\quad y \\geq 0 & \\text{(non-negativity)} \\end{cases}\\]"
  },
  {
    "objectID": "1.1-LP-Geometric and Simplex method.html#production-planning-problem",
    "href": "1.1-LP-Geometric and Simplex method.html#production-planning-problem",
    "title": "[GitSAM](../index.html)",
    "section": "",
    "text": "A factory produces two products, A and B.\n\nEach unit of product A requires 3 hours of machine time, and product B requires 2 hours. The total available machine time per day is 180 hours.\nProduct A requires 2 kg of raw material per unit, and product B requires 3 kg. The total available raw material is 160 kg.\nProfit per unit is $50 for product A and $40 for product B.\n\nGoal: Determine the production quantities of A and B that maximize profit without exceeding the resource constraints.\n\n\n\nVariables:\nLet\n\\(x\\) = units of product A\n\\(y\\) = units of product B\nObjective function:\nMaximize \\[Z = 50x + 40y\\]\nSubject to the constraints:\n\\[\\begin{cases} 3x + 2y \\leq 180 & \\text{(machine time)} \\\\ 2x + 3y \\leq 160 & \\text{(raw material)} \\\\ x \\geq 0,\\quad y \\geq 0 & \\text{(non-negativity)} \\end{cases}\\]"
  },
  {
    "objectID": "1.1-LP-Geometric and Simplex method.html#solve-with-python-and-interpret",
    "href": "1.1-LP-Geometric and Simplex method.html#solve-with-python-and-interpret",
    "title": "[GitSAM](../index.html)",
    "section": "2 Solve with Python and Interpret",
    "text": "2 Solve with Python and Interpret\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import linprog\n\n# Objective function coefficients (we minimize -Z for maximization)\nc = [-50, -40]\n\n# Inequality constraints (Ax &lt;= b)\nA = [\n    [3, 2],   # Machine time\n    [2, 3]    # Raw material\n]\nb = [180, 160]\n\n# Variable bounds\nx_bounds = (0, None)\ny_bounds = (0, None)\n\n# Solve using linprog\nres = linprog(c, A_ub=A, b_ub=b, bounds=[x_bounds, y_bounds], method='highs')\n\n# Extract results\nx_opt, y_opt = res.x\nz_opt = -res.fun  # remember we minimized -Z\n\n# Create a grid for plotting\nx = np.linspace(0, 80, 400)\ny1 = (180 - 3 * x) / 2  # from 3x + 2y &lt;= 180\ny2 = (160 - 2 * x) / 3  # from 2x + 3y &lt;= 160\n\n# Plotting\nplt.figure(figsize=(8, 6))\nplt.plot(x, y1, label=r'$3x + 2y \\leq 180$')\nplt.plot(x, y2, label=r'$2x + 3y \\leq 160$')\nplt.xlim(0, 80)\nplt.ylim(0, 80)\n\n# Fill feasible region\ny1_clip = np.minimum(y1, y2)\ny1_clip = np.where((y1_clip &gt;= 0) & (y1 &gt;= 0) & (y2 &gt;= 0), y1_clip, np.nan)\nplt.fill_between(x, 0, y1_clip, where=~np.isnan(y1_clip), color='gray', alpha=0.3)\n\n# Plot optimal point\nplt.plot(x_opt, y_opt, 'ro', label='Optimal Solution')\nplt.text(x_opt + 1, y_opt, f'({x_opt:.2f}, {y_opt:.2f})\\nProfit = ${z_opt:.2f}', color='red')\n\nplt.xlabel('Product A (x)')\nplt.ylabel('Product B (y)')\nplt.title('Feasible Region and Optimal Production Plan')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\nx_opt, y_opt, z_opt\n최적해:\n\n제품 A 생산량 \\(x = 44.00\\) 단위\n제품 B 생산량 \\(y = 24.00\\) 단위\n총 이익 \\(Z = 3160.00\\)\n\n해석:\n\n주어진 기계 시간 및 원재료 제약을 모두 고려했을 때, 해당 생산량 조합이 가능한 해 중에서 이익을 최대화합니다.\n그림에서 회색 영역은 feasible region이며, 붉은 점이 최적 생산량을 나타냅니다.\n두 제약조건이 교차하는 꼭짓점에서 최적해가 발생하므로, 이 문제는 다각형(polytope) 구조의 고전적인 LP 문제입니다."
  },
  {
    "objectID": "0-preface.html",
    "href": "0-preface.html",
    "title": "[GitSAM](../index.html)",
    "section": "",
    "text": "9 Optimization: Linear Programming - Duality\n8 Geometry of linear spaces - The Platonic Solids - Polytopes\n1 Linear Equations: Linear Models in Economics and Engineering - Systems of linear equations - Applications of Linear systems - Linear Models in Business, Science, and Engineering\n2 Matrix Algebra : Computer Models in Aircraft Design - The Leontief Input-Output Model - Applications to Computer Graphics\n3 Determinants: Random Paths and Distortion - Properties of Determinants, Volume\n4 Linear Spaces: Control Systems - Rank-Nullity theorem - Applications to Difference Equations - Applications to Markov Chains\n5 Eigenvectors - Dynamic systems and Spotted Owls - Applications to Differential equations\n10 Finite-State Markov Chains : Google’s PageRank - Steady-state vector\n6 Orthogonality and Least Squares : GPS Navigation - Orthogonal Projection\n7 Symmetric matrices and Quadratic Forms : Multichannel Image Processing - SVD\n\n0.1 Applied Linear Algebra: Practical Modules\n\n9 Linear Programming Optimization and Linear Programming\n  Linear inequality problems in economics, resource allocation, and engineering\n  • Duality and economic interpretation\n8 Geometric Structures of Linear Spaces\n  Visualizing algebra through Platonic solids and polytopes\n  • Foundations of convexity and feasibility\n1 Linear Equations\n  Economic input-output analysis and physical system simulations\n  • Business, Science, and Engineering applications\n2 Matrix Algebra in Computation and Graphics\n  How matrices govern structural transformations\n  • Leontief Model and Computer-Aided Design\n3 Determinants and Volume-Based Intuition\n  Orientation, distortion, and random paths in multi-dimensional spaces\n  • From row operations to volume calculation\n4 Linear Spaces\n  Abstract structure behind control systems and recursive dynamics\n  • Rank-Nullity Theorem\n  • Difference equations and Markov chains\n5 Eigenvectors in Dynamical Systems\n  Growth, decay, and stability in nature and engineering\n  • Differential equations and population models\n10 Finite-State Markov Chains and PageRank\n  How Google ranks the internet using steady-state vectors\n6 Orthogonality and Least Squares\n  From projections to GPS: minimizing errors in systems\n7 Symmetric Matrices and Quadratic Forms\n  Multichannel image processing and data compression\n  • Singular Value Decomposition (SVD)\n\n\nMathematical Modelling\nObjective: Explain the complex real-world system and Predict the future behavior of the system.\nProcess: - State the Problem - Clarify the problem (5W1H): problem? why (objective)? who (observer)? where (area)? what (system)? when (dynamics)? how (criteria)? - Simplify the problem: simplify the problem enough to make it mathematically tractable, by making critical assumptions and by naming and identifying the variables. - Formulate the problem: express the problem in a systematic way. - Solve the formulated problem : derive the mathematical conclusion - Interpret the solved problem : derive the meaningful conclusion - Test the conclusion by making a prediction\n\n\n\n\nReuseCC BY-NC-SA 4.0"
  },
  {
    "objectID": "0-preface (advanced).html",
    "href": "0-preface (advanced).html",
    "title": "[GitSAM](../index.html)",
    "section": "",
    "text": "주제: 선형 공간 안에 벡터 (방향성, 크기)들의 관계 (연산)\n\n이런 걸 배울 수 있어요\n\n행렬로 나타내고 그림으로 보는 선형 연립 방정식\n데이터 테이블 (행렬)과 데이터 (벡터)\n평행사변형과 선형공간\n미분방정식에서 고유벡터의 의미 (축척)\n선형회귀모형에서 수직이 아닌 방향\n최적화이론의 목적함수와 정부호행렬\n여러가지 행렬 분해법 활용\n군과 선형사상\n\nIntroduction to Linear Algebra (6th edition): Gilbert Strang in MIT OpenCourseWare (MIT OCW) ![[book-strang.png]]\n\n0.1 커리큘럼\n선형연립방정식에서 출발해 선형공간과 고유값으로 확장하고, 이어서 내적공간·분해정리(SVD, QR) 및 최소제곱법·힐버트 공간 등 특수한 응용을 다룬 뒤, 마지막으로 군(群)과 추상구조 등 일반화 된 개념을 소개하며 전체 학습을 마무리합니다.\n\n\n0.2 (선형연립방정식 해석) \\(X\\boldsymbol{\\beta} = \\boldsymbol{y}\\)\n\n행렬 표현과 행렬의 연산법칙\nGeometry of Linear Equations:\n\nGaussian Elimination (가우스 소거법), Row-reduced Echelon Form\nPivot (피봇), Pivot Column (피봇 열): 행렬의 랭크(rank)와 직결되며, 해의 개수 및 유·무결정성에 영향을 준다.\nRank–Nullity Theorem (계수-퇴화차수 정리)와 4개의 부분공간 (열공간(Column space), 행공간(Row space), 영공간(Null space), 왼쪽 영공간(Left null space))\n\n치환 (Permutation)과 대칭행렬 (Symmetric matrix)\n\n행렬식 (Determinant)이 결정하는 것\n\n선형공간의 차원: 초등 행렬 (Elementary matrix)\n특이행렬(singular matrix)? 선형변환이 “부피(volume)를 0으로 만든다”\n비특이행렬(non-singular matrix): 역행렬 (Inverse matrix)이 존재\n\nLU factorization: 정방행렬 𝐴를 두 개의 삼각행렬로 분해해서 선형계를 빨리 풀기\n\n필요 시 치환행렬(Permutation matrix)로 부분 피벗팅(Partial Pivoting)을 고려해 PA=LU 형태로 분해\nCholesky decomposition: 정방행렬이 양정부호인 경우는 더 빠르게.\n\n\n\n\n\n0.3 (선형공간의 구조와 분해) Structures of Linear space\n\n선형 공간 (Linear space = Vector space)\n\n집합이 선형적 연산 (즉, 덧셈과 스칼라곱)에 대해 닫혀 있음: 덧셈에 대한 가환군 (Abelian Group)에 크기 늘리기 연산자 (스칼라곱, scalar multiplication)을 추가한 공간\n선형 공간: 선형적 벡터 공간, 선형적 함수 공간\n선형결합 (Linear combination), 선형독립 (linear independence), 부분공간(subspace), 기저(basis), 차원(dimension)\n\n평행사변형에서의 대각선: 고유벡터(Eigenvector)와 고유값 (Eigenvalue)\n\n케일리-해밀턴 정리 (Cayley-Hamilton Theorem): 행렬의 특성 방정식 (Characteristic equation)\n고윳값분해(Eigendecomposition), 행렬의 대각화 (Diagonalization)\n선형미분방정식 (Linear Differential Equations)과 지수행렬 (A t)\n마크코프체인 모형 (Markov-Chain model): 전이행렬(transition matrix)에 의한 steady-state vector(정상분포)를 고유벡터로 해석\n\n직사각형에서의 피타고라스 정리 (Pythagorean theorem): 수직 (perpendicularity)의 개념\n\n내적 연산자 (Inner product operator)가 있는 내적 공간 (Inner Product Space)\n내적에서 유도되는 거리(길이) 개념인 놈(Norm)\n직교여공간(Orthogonal complement): 해 공간과 여공간의 관계\n\n직교기저 분해정리 (Spectral Theorem)\n\n표준기저(Standard Basis)를 정규직교기저(Orthonormal Basis)로 만드는 과정\nQR decomposition: 직사각형 행렬 𝐴를 직교(orthonormal) 행렬 𝑄와 상삼각행렬(upper triangular matrix) 𝑅로 분해\n\nGram-Schmidt Algorithm 을 활용하여 Q, R 추출\n\nSVD (Singular Value Decomposition): 직사각형 행렬 𝐴를 정규직교 정사각행렬들과 직사각 대각행렬로 분해.\n\n선형회귀모형 (Linear regression model)에서 최소제곱법 (Least Squares method)\n\n정규방정식(Normal equation)과 직교사영(Orthogonal Projection)\n\n힐버트 공간(Hilbert space)과 푸리에 급수(Fourier Series)\n\n내적공간의 구조가 무한 차원으로 확장되는 예시\n기함수/짝함수 분해, 편의상 \\([-\\pi,\\pi]\\) 구간에서의 직교기저\n\nBilinear form as a local volume form\n\n국소적 선형공간에서의 표준적 부피 형식 (Volume form)과 텐서 (Tensor)\nPositive definite kernel (양의정부호 커널), Dual space (쌍대공간), Optimization problems (최적화 문제들)\n편미분방정식(PDE), 미분기하학 등 고급 응용과도 연결\n\n\n\n\n0.4 (군과 구조 개념) Group and structures\n\n대수적 공간 (Algebraic space): Space(공간) = Set(집합) + Structure(구조)\n\n직합(Direct Sum, \\(\\oplus\\)): 두 개 이상의 선형 공간(또는 부분공간 subspace)을 하나의 더 큰 벡터 공간으로 묶는 연산.\n몫공간 (Quotient Space, \\(V/W\\)): 벡터 공간 V에서 부분공간 W를 나누어 새로운 공간을 정의하는 개념. 몫공간 V/W의 원소는 여공간(coset) \\(v+W\\) 로 표현됨. 예를 들어, \\(\\mathbb{R}^3\\)에서 \\(W = \\operatorname{Span} \\{(1,0,0)\\}\\) 라고 하면, ,\\(\\mathbb{R}^3 / W\\)는 원점을 지나면서 x-축과 평행한 평면들의 공간.\n벡터 공간을 나누는 서로 다른 방법: 직합과 몫공간\n\n\\(U \\oplus W\\)이면, \\(V/W \\cong U\\). 즉, 몫공간을 취하면 남은 부분이 원래 공간의 다른 부분과 동형이 됨.\n\n\n대수적 군 (Group) : 공간의 분류\n\n정사각형, 직사각형, 마름모, 평행사변형: 평행이동·회전·대칭 등 기하학적 변환을 “군” 관점에서 볼 수도 있음\n몫공간 (Quotient space),\n일반선형군 (General linear group, GL(n)): 행렬 곱셈이 가능한 가역 정방행렬들의 집합. 역행렬이 존재하는 정방행렬들의 집합. 행렬식이 0이 아닌 모든 정방행렬\n\n선형공간 내 벡터들에 벡터 내적연산을 추가해도 벡터들이 속한 선형공간은 가환군이지만, 선형공간 내 행렬들에 행렬 곱셈을 추가하면 행렬들이 속한 선형공간은 비가환군(Non-Abelian Group, )\nGL(n,R) (실수 계수 정방행렬의 군)\n유니터리 군 (Unitary group, U(n)): 복소수 계수 n×n 행렬 중에서, \\(U'U=I\\) 를 만족하는 (즉, 켤레전치가 역행렬) 유니터리 행렬(unitary matrix)의 집합. 행렬의 원소인 벡터의 길이(norm)와 각도(inner product)를 보존하는 변환. 회전, 반사 등을 포함하는 복소수 변환들 (행렬군).\n\n실수 행렬 버전이 직교군 (Orthogonal group, O(n)),\n\n\n\n구조 불변 변환 (Morphism):\n\n“구조 불변”이란 특정 성질(길이·각도·부피·위상 등)을 그대로 보존하는 변환을 의미하며, 군과 범주의 핵심 개념이다.\n선형사상(Linear map)\n동형사상(Isomorphism, same)\n준동형사상(Homomorphism, similar)\nRank–Nullity Theorem as the first isomorphism theorem (*Fundamental theorem on homomorphisms)* in Noether’s isomorphism theorems\n\n\n\n\n\n\nReuseCC BY-NC-SA 4.0"
  },
  {
    "objectID": "1-Linear Programming Problem.html",
    "href": "1-Linear Programming Problem.html",
    "title": "[GitSAM](../index.html)",
    "section": "",
    "text": "수학적 최적화를 ‘프로그래밍’이라고 부르는 것은 컴퓨터 프로그래밍과 직접적인 관련은 없지만, 수학적 최적화 문제를 해결하기 위한 체계적인 ’계획’ 또는 ’절차’를 의미합니다. 특히, 2차 세계대전 이후 조지 단치그(George Dantzig)가 선형 계획법 (linear programming)을 개발하면서 프로그래밍이란 용어가 널리 사용되었습니다. 당시 미국 국방부에서 군사 작전, 훈련, 물자 수송 등의 계획을 수립하는 데 이 방법이 활용되었습니다.\n\n0.1 기원: George Dantzig와 베를린 공수 작전 (The Berlin Airlift)\nGeorge Dantzig는 제2차 세계대전 직후 미 공군에서 근무하던 중 선형계획법(Linear Programming)을 개발하게 되었습니다. 이 개념이 탄생하게 된 역사적 계기 중 하나는 바로 베를린 공수 작전(1948–1949)이었습니다.\n\n전쟁 이후 소련이 서베를린에 대한 육로 봉쇄를 단행하자, 미국과 영국을 비롯한 서방 연합군은 항공편을 통해 식량과 연료, 생필품을 수송하기로 결정합니다.\n이 작전은 매일 수백 대의 항공기를 운용해야 했고, 제한된 비행기 수, 활주로, 연료, 조종사, 시간이라는 여러 제약 조건 속에서 효율적으로 물자를 운반해야 하는 상황이었습니다.\n당시 국방부에서 일하던 Dantzig는 이 복잡한 자원 배분 문제를 수학적으로 최적화할 수 없을까라는 요청을 받게 되었고, 이 문제를 선형 함수의 최적화 문제로 수학적으로 형식화하면서 선형계획법이 등장하게 된 것입니다.\n\n\n\n0.2 Key Application Areas of Linear Programming\n\n\n\n\n\n\n\nDomain\nExamples\n\n\n\n\nLogistics and Transportation\nVehicle routing, airline scheduling, supply chain optimization\n\n\nOperations Research\nResource allocation, production planning, facility location\n\n\nEconomics\nLeontief Input-Output Model, utility maximization, shadow pricing\n\n\nFinance\nPortfolio optimization (basic LP or LP-relaxed), risk minimization\n\n\nEnergy Systems\nGrid optimization, power generation and dispatch\n\n\nManufacturing\nCutting stock problem, workforce scheduling, inventory management\n\n\nTelecommunications\nBandwidth allocation, network flow optimization\n\n\nMilitary and Defense\nMission planning, logistics under constraint\n\n\nHealth and Medicine\nOptimal allocation of medical resources, treatment planning\n\n\nAI and ML (Modern)\nLinear SVMs, constrained learning problems, LP relaxations for ILPs\n\n\n\n\n\n0.3 Core Idea of Linear Programming\nGoal:\nMaximize or minimize a linear objective function (e.g., profit, cost, delivery volume)\nSubject to a system of linear equality and inequality constraints (e.g., capacity, time, resource limits)\nMathematically:\n\\[\\text{maximize (or minimize) } \\mathbf{c}^T \\mathbf{x} \\quad \\text{subject to} \\quad A \\mathbf{x} \\leq \\mathbf{b},\\quad \\mathbf{x} \\geq 0\\]\n\n\n\n\nReuseCC BY-NC-SA 4.0"
  },
  {
    "objectID": "1.2-Duality.html",
    "href": "1.2-Duality.html",
    "title": "[GitSAM](../index.html)",
    "section": "",
    "text": "In a canonical linear programming problem, the primal (maximization) problem is associated with its dual (minimization) problem.\n\n\nWe revisit the same production planning problem from section 1.1. This time, instead of maximizing profit directly by choosing production quantities, we ask a different question:\nWhat is the minimum total cost one would have to pay per unit of resource (machine hours and raw material) to cover the required profits per product?\nThis alternative formulation is called the dual problem of the original (primal) linear programming problem.\n\n\n\nLet\n\\(u\\) = price per unit of machine time\n\\(v\\) = price per unit of raw material\nObjective:\nMinimize the total cost of resources:\n\\[W = 180u + 160v\\]\nSubject to (each product must be worth at least its profit):\n\\[\\begin{cases} 3u + 2v \\geq 50 & \\text{(product A)} \\\\ 2u + 3v \\geq 40 & \\text{(product B)} \\\\ u \\geq 0,\\quad v \\geq 0 & \\text{(non-negativity)} \\end{cases}\\]\n\n\n\nSolution to the dual:\n\n\\(u = 14.00\\)\n\\(v = 4.00\\)\nMinimum total cost \\(W = 3160.00\\)\n\nInterpretation:\n\nThis result confirms the duality theorem: the maximum profit from the primal equals the minimum resource cost in the dual.\nThe shadow prices (dual variables) represent the marginal value of each resource:\n\n\\(u = 14\\) means each additional unit of machine time increases potential profit by $14.\n\\(v = 4\\) means each additional kg of raw material increases potential profit by $4.\n\nThese values are also known as dual prices or shadow prices, and they help in resource pricing, sensitivity analysis, and economic interpretation."
  },
  {
    "objectID": "1.2-Duality.html#duality",
    "href": "1.2-Duality.html#duality",
    "title": "[GitSAM](../index.html)",
    "section": "",
    "text": "In a canonical linear programming problem, the primal (maximization) problem is associated with its dual (minimization) problem.\n\n\nWe revisit the same production planning problem from section 1.1. This time, instead of maximizing profit directly by choosing production quantities, we ask a different question:\nWhat is the minimum total cost one would have to pay per unit of resource (machine hours and raw material) to cover the required profits per product?\nThis alternative formulation is called the dual problem of the original (primal) linear programming problem.\n\n\n\nLet\n\\(u\\) = price per unit of machine time\n\\(v\\) = price per unit of raw material\nObjective:\nMinimize the total cost of resources:\n\\[W = 180u + 160v\\]\nSubject to (each product must be worth at least its profit):\n\\[\\begin{cases} 3u + 2v \\geq 50 & \\text{(product A)} \\\\ 2u + 3v \\geq 40 & \\text{(product B)} \\\\ u \\geq 0,\\quad v \\geq 0 & \\text{(non-negativity)} \\end{cases}\\]\n\n\n\nSolution to the dual:\n\n\\(u = 14.00\\)\n\\(v = 4.00\\)\nMinimum total cost \\(W = 3160.00\\)\n\nInterpretation:\n\nThis result confirms the duality theorem: the maximum profit from the primal equals the minimum resource cost in the dual.\nThe shadow prices (dual variables) represent the marginal value of each resource:\n\n\\(u = 14\\) means each additional unit of machine time increases potential profit by $14.\n\\(v = 4\\) means each additional kg of raw material increases potential profit by $4.\n\nThese values are also known as dual prices or shadow prices, and they help in resource pricing, sensitivity analysis, and economic interpretation."
  },
  {
    "objectID": "1.2-Duality.html#the-duality-theorem",
    "href": "1.2-Duality.html#the-duality-theorem",
    "title": "[GitSAM](../index.html)",
    "section": "2 The Duality Theorem",
    "text": "2 The Duality Theorem\nLet the primal linear program be:\nMaximize\n\\[Z = \\mathbf{c}^T \\mathbf{x}\\]\nSubject to\n\\[A \\mathbf{x} \\leq \\mathbf{b}, \\quad \\mathbf{x} \\geq 0\\]\nIts dual is:\nMinimize\n\\[W = \\mathbf{b}^T \\mathbf{y}\\]\nSubject to\n\\[A^T \\mathbf{y} \\geq \\mathbf{c}, \\quad \\mathbf{y} \\geq 0\\]\nThen the Duality Theorem states:\nIf either the primal or the dual problem has an optimal solution, then so does the other, and the optimal values are equal: \\[\\max_{\\mathbf{x}} \\{\\mathbf{c}^T \\mathbf{x} \\mid A\\mathbf{x} \\leq \\mathbf{b}, \\ \\mathbf{x} \\geq 0 \\} = \\min_{\\mathbf{y}} \\{\\mathbf{b}^T \\mathbf{y} \\mid A^T\\mathbf{y} \\geq \\mathbf{c}, \\ \\mathbf{y} \\geq 0 \\}\\]\nIn our example:\n\nPrimal maximum profit \\(Z = 3160.00\\)\nDual minimum cost \\(W = 3160.00\\)\n→ Duality theorem holds exactly."
  },
  {
    "objectID": "1.2-Duality.html#complementary-slackness",
    "href": "1.2-Duality.html#complementary-slackness",
    "title": "[GitSAM](../index.html)",
    "section": "3 Complementary Slackness",
    "text": "3 Complementary Slackness\n\n\n\n\n\n\n\nPrimal 문제\nDual 문제\n\n\n\n\nMaximize \\(Z = \\mathbf{c}^T \\mathbf{x}\\) Subject to: \\(A \\mathbf{x} \\leq \\mathbf{b}, \\quad \\mathbf{x} \\geq 0\\)\nMinimize \\(W = \\mathbf{b}^T \\mathbf{y}\\) Subject to: \\(A^T \\mathbf{y} \\geq \\mathbf{c}, \\quad \\mathbf{y} \\geq 0\\)\n\n\n\nComplementary Slackness는 Primal 해와 Dual 해가 최적일 때 반드시 성립하는 조건입니다. 이 조건은 다음과 같은 형태로 표현됩니다.\n\n각 Primal 제약조건에 대해: \\(y_i \\cdot (b_i - (A\\mathbf{x})_i) = 0\\)\n\n즉, Dual 변수 \\(y_i\\)가 양수이면, 해당 Primal 제약조건은 딱 맞게 작동해야 한다 (등호 성립).\n\n각 Dual 제약조건에 대해: \\(x_j \\cdot ((A^T \\mathbf{y})_j - c_j) = 0\\)\n\n즉, Primal 변수 \\(x_j\\)가 양수이면, 해당 Dual 제약조건은 딱 맞게 작동해야 한다.\n\n\n\nPrimal에서 slack이 있으면 해당 Dual 변수는 0이다.\nDual에서 slack이 있으면 해당 Primal 변수는 0이다.\n\n우리 문제의 경우,\n\nPrimal에서 두 제약조건 모두 등호로 성립 → Dual 변수 \\(u\\), \\(v\\)는 둘 다 양수\nPrimal의 두 변수 \\(x\\), \\(y\\)도 양수 → Dual의 두 제약조건도 등호 성립\n\n→ 따라서 Complementary Slackness 조건이 모두 만족됨."
  },
  {
    "objectID": "8.1-Communicating States.html",
    "href": "8.1-Communicating States.html",
    "title": "[GitSAM](../index.html)",
    "section": "",
    "text": "“A state is not truly reachable unless it can reach back — fairness lies in mutual paths.”"
  },
  {
    "objectID": "8.1-Communicating States.html#relation",
    "href": "8.1-Communicating States.html#relation",
    "title": "[GitSAM](../index.html)",
    "section": "1 Relation",
    "text": "1 Relation\n\n1.1 Non-equivalence relation as a very common but unfair relation\n\n“Asymmetry reveals power, not understanding.”\n\nIn many social, economic, or logical structures, relations are:\n\nNot reflexive: I might not understand myself.\nNot symmetric: If I understand you, it doesn’t mean you understand me.\nNot transitive: Even if I understand you, and you understand Sam, I may not understand Sam.\n\nSuch relations are directional, asymmetric, and often hierarchical, reflecting power, ignorance, or miscommunication — thus, structurally unfair.\n\n\n\n1.2 Equivalence relation as a very special but fair relation\n\n“True fairness begins when I understand you, and you understand me.”\n\nIn contrast, an equivalence relation ensures mutuality, balance, and internal coherence. It defines a structure where all members of a group communicate equally and symmetrically.\n\n1.2.1 Logical example\n\nReflexivity: I understand myself.\nSymmetry: If I understand you, you understand me.\nTransitivity: If I understand you, and you understand Sam, then I understand Sam.\n\n\n\n1.2.2 Algebraic example: Congruence modulo 3\n\nReflexive: 0 ≡ 0, 1 ≡ 1, 2 ≡ 2 mod 3\nSymmetric: 1 + 2 ≡ 0 ≡ 2 + 1 mod 3\nTransitive: If 0 ≡ 3 and 3 ≡ 6, then 0 ≡ 6 mod 3\n\nNote: Group structure implies closure and invertibility under a binary operation, which yields equivalence relations on cosets or orbits — not necessarily between individual elements.\nCongruence modulo 3 partitions ℕ into equivalence classes (e.g. 0 ≡ 3 ≡ 6, 1 ≡ 4 ≡ 7, etc.)."
  },
  {
    "objectID": "8.1-Communicating States.html#communication-in-markov-chains",
    "href": "8.1-Communicating States.html#communication-in-markov-chains",
    "title": "[GitSAM](../index.html)",
    "section": "2 Communication in Markov Chains",
    "text": "2 Communication in Markov Chains\n\n“Communication is not just connection, but mutual reachability across time.”\n\nDefinition: In a finite-state Markov chain, state \\(i\\) communicates with state \\(j\\) if there exists some \\(n \\geq 0\\) such that:\n\n\\(P^n(i, j) &gt; 0\\), and\n\\(P^m(j, i) &gt; 0\\) for some \\(m\\)\n\nThis communication relation satisfies:\n\nReflexivity: Each state eventually returns to itself\nSymmetry: If \\(i\\) can reach \\(j\\) and \\(j\\) can reach \\(i\\)\nTransitivity: If \\(i \\leftrightarrow j\\) and \\(j \\leftrightarrow k\\), then \\(i \\leftrightarrow k\\)\n\nTherefore, communication is an equivalence relation on the state space.\nThe state space is partitioned into communication classes — mutually reachable subsets that determine long-term dynamics.\n\n\n2.1 Cross-sectional Fairness: Equivalence through Group Action\n\n“In an Abelian world, no action is dominant — every action is equally valid in every direction.”\n\nIn abstract algebra, group actions define structural fairness by letting group elements “act” on a set. If a group \\(G\\) acts on a set \\(X\\), we say:\n\\[x \\sim y \\quad \\text{if there exists } g \\in G \\text{ such that } g \\cdot x = y\\]\nThis relation is:\n\nReflexive (\\(e \\cdot x = x\\))\nSymmetric (if \\(g \\cdot x = y\\), then \\(g^{-1} \\cdot y = x\\))\nTransitive (composition of group actions)\n\nIn particular, Abelian groups (where \\(a + b = b + a\\)) such as \\((\\mathbb{Z}_n, +)\\) reinforce this symmetry.\nEach orbit under a group action forms a class of equivalent configurations — just as communication classes in Markov chains represent mutually reachable states. Thus, no element has a privileged direction of action over another within each orbit — a kind of “structural justice” within class, even if there may be inequality across different orbits.\n\n\n\n2.2 Time-Sensitive Fairness: 조삼모사(朝三暮四)\n\n“To those with finite lives, timing reveals the meaning of fairness.”\n\nIn the fable 조삼모사 (three in the morning, four in the evening), a monkey protests that receiving three chestnuts in the morning and four in the evening is not the same as four in the morning and three in the evening — despite the total being seven in both.\nThe monkey is often mocked as foolish, but in fact may be rational: For mortals with finite lives, time matters. Receiving earlier is better than later, because future utility is uncertain and discounted.\nJust as in discounted utility models in economics or stochastic dominance in finance, the timing of allocation affects perceived fairness — even when totals are equal.\nHence:\n\nFairness should be not only structural (symmetry, equivalence), but also temporal\nEven in Markov chains or stochastic systems, when transitions occur can matter as much as whether they occur"
  }
]