[
  {
    "objectID": "yield_curve.html",
    "href": "yield_curve.html",
    "title": "Yield curve",
    "section": "",
    "text": "채권 가격 (bond price)과 시장 금리(yield rate, 채권 수익률)는 역의 관계(inverse relationship)\n\n금리가 오르면 채권 가격은 하락하고,\n금리가 내리면 채권 가격은 상승한다.\n\n왜?\n\n시장 금리가 상승하면 기존의 낮은 금리를 가진 채권의 상대적 매력이 떨어지므로 가격이 하락하고,\n시장 금리가 하락하면 기존의 높은 금리를 가진 채권이 더 매력적으로 보이므로 가격이 상승한다.\n\n채권의 현재 가치(PV)를 수식으로 표현하면 다음과 같다.\n\\[\nP = \\sum_{t=1}^{T} \\frac{C}{(1 + r)^t} + \\frac{F}{(1 + r)^T}\n\\]\n여기서,\n- \\(P\\) : 채권 가격 (Present Value)\n- \\(C\\) : 매년 지급되는 쿠폰 이자 (Coupon Payment)\n- \\(F\\) : 만기 시 원금(Face Value)\n- \\(r\\) : 시장 금리 (Yield Rate)\n- \\(T\\) : 잔존 만기\n시장금리(\\(r\\))가 증가하면 할인율이 커져서 내가 가진 채권의 현재 가치(\\(P\\))가 낮아짐\n\n\n가정:\n\n채권의 액면가: 1,000달러\n쿠폰 이자율: 5% (연간)\n만기: 3년\n현재 시장 금리: 5% → 2% (3% 하락)\n\n경기불황 때문에 기준금리가 내려가서 시장 금리가 하락하면,\n새로 발행되는 2% 금리의 채권보다 기존 5%의 쿠폰을 지급하는 채권이 더 매력적으로 보이므로,\n기존 채권의 가격이 상승한다.\n\n\n\n채권 금리는 단순히 하나의 고정된 값이 아니라, 만기에 따라 다르게 형성됨.\n\n단기 채권(Short-term bonds, 1년 이하): 단기 금리는 중앙은행의 정책금리(예: 연준의 FFR, 한국은행의 기준금리)와 직접적으로 연결됨.\n중기 채권(Mid-term bonds, 2~10년): 경제 성장률, 기대 인플레이션, 신용 위험 등에 영향을 받음.\n장기 채권(Long-term bonds, 10년 이상): 장기 기대 인플레이션, 국가 신용, 경기 사이클 등에 따라 수익률이 결정됨.\n\n\n\n\n채권은 듀레이션(Duration)이 길수록 금리 변화에 대한 가격 민감도가 커진다.\n- 장기 채권: 금리 변화에 더 민감\n- 단기 채권: 금리 변화에 덜 민감\n\n\n\nTerm Structure는 채권 시장에서 만기(term)에 따라 형성되는 금리 구조\nTerm Structure는 주로 Yield Curve(수익률 곡선)의 형태로 시각화된다.\n\n\n\n\n금리 상승 (Yield 상승) 예상 → 채권 가격 하락 → 기존 장기 채권 보유자는 손해. 기존 장기 채권을 매도하고, 새로운 금리가 반영된 채권을 매수. 기준금리 상승때문에 단기 채권은 괜찮음.\n금리 하락 (Yield 하락) 에상 → 채권 가격 상승 → 기존 장기 채권 보유자는 이득. 기존 채권을 보유하여 가격 상승에 따른 자본 이득(capital gain) 확보. 장기 채권을 보유하는 것이 유리.",
    "crumbs": [
      "교육",
      "금융",
      "Yield curve"
    ]
  },
  {
    "objectID": "yield_curve.html#채권-가격과-시장-금리의-관계",
    "href": "yield_curve.html#채권-가격과-시장-금리의-관계",
    "title": "Yield curve",
    "section": "",
    "text": "채권 가격 (bond price)과 시장 금리(yield rate, 채권 수익률)는 역의 관계(inverse relationship)\n\n금리가 오르면 채권 가격은 하락하고,\n금리가 내리면 채권 가격은 상승한다.\n\n왜?\n\n시장 금리가 상승하면 기존의 낮은 금리를 가진 채권의 상대적 매력이 떨어지므로 가격이 하락하고,\n시장 금리가 하락하면 기존의 높은 금리를 가진 채권이 더 매력적으로 보이므로 가격이 상승한다.\n\n채권의 현재 가치(PV)를 수식으로 표현하면 다음과 같다.\n\\[\nP = \\sum_{t=1}^{T} \\frac{C}{(1 + r)^t} + \\frac{F}{(1 + r)^T}\n\\]\n여기서,\n- \\(P\\) : 채권 가격 (Present Value)\n- \\(C\\) : 매년 지급되는 쿠폰 이자 (Coupon Payment)\n- \\(F\\) : 만기 시 원금(Face Value)\n- \\(r\\) : 시장 금리 (Yield Rate)\n- \\(T\\) : 잔존 만기\n시장금리(\\(r\\))가 증가하면 할인율이 커져서 내가 가진 채권의 현재 가치(\\(P\\))가 낮아짐\n\n\n가정:\n\n채권의 액면가: 1,000달러\n쿠폰 이자율: 5% (연간)\n만기: 3년\n현재 시장 금리: 5% → 2% (3% 하락)\n\n경기불황 때문에 기준금리가 내려가서 시장 금리가 하락하면,\n새로 발행되는 2% 금리의 채권보다 기존 5%의 쿠폰을 지급하는 채권이 더 매력적으로 보이므로,\n기존 채권의 가격이 상승한다.\n\n\n\n채권 금리는 단순히 하나의 고정된 값이 아니라, 만기에 따라 다르게 형성됨.\n\n단기 채권(Short-term bonds, 1년 이하): 단기 금리는 중앙은행의 정책금리(예: 연준의 FFR, 한국은행의 기준금리)와 직접적으로 연결됨.\n중기 채권(Mid-term bonds, 2~10년): 경제 성장률, 기대 인플레이션, 신용 위험 등에 영향을 받음.\n장기 채권(Long-term bonds, 10년 이상): 장기 기대 인플레이션, 국가 신용, 경기 사이클 등에 따라 수익률이 결정됨.\n\n\n\n\n채권은 듀레이션(Duration)이 길수록 금리 변화에 대한 가격 민감도가 커진다.\n- 장기 채권: 금리 변화에 더 민감\n- 단기 채권: 금리 변화에 덜 민감\n\n\n\nTerm Structure는 채권 시장에서 만기(term)에 따라 형성되는 금리 구조\nTerm Structure는 주로 Yield Curve(수익률 곡선)의 형태로 시각화된다.\n\n\n\n\n금리 상승 (Yield 상승) 예상 → 채권 가격 하락 → 기존 장기 채권 보유자는 손해. 기존 장기 채권을 매도하고, 새로운 금리가 반영된 채권을 매수. 기준금리 상승때문에 단기 채권은 괜찮음.\n금리 하락 (Yield 하락) 에상 → 채권 가격 상승 → 기존 장기 채권 보유자는 이득. 기존 채권을 보유하여 가격 상승에 따른 자본 이득(capital gain) 확보. 장기 채권을 보유하는 것이 유리.",
    "crumbs": [
      "교육",
      "금융",
      "Yield curve"
    ]
  },
  {
    "objectID": "yield_curve.html#yield-curve수익률-곡선",
    "href": "yield_curve.html#yield-curve수익률-곡선",
    "title": "Yield curve",
    "section": "Yield Curve(수익률 곡선)",
    "text": "Yield Curve(수익률 곡선)\nYield Curve는 만기별 수익률을 연결한 곡선으로, 주요 형태는 3가지:\n\n정상적인 형태(Normal Yield Curve): 장기 금리가 단기 금리보다 높음 → 경제 성장이 기대되는 경우\n역전된 형태(Inverted Yield Curve): 장기 금리가 단기 금리보다 낮음 → 경기 침체 신호\n평평한 형태(Flat Yield Curve): 단기와 장기 금리 차이가 거의 없음 → 경기 전환기\n\n이렇게 다양한 형태가 나타나는 이유를 설명하는 이론들에는, 금리 기대이론(Expectations Hypothesis), 유동성 프리미엄 이론(Liquidity Premium Theory), 시장 분할 이론(Market Segmentation Theory) 등이 있음.\nYield Curve(수익률 곡선)는 본질적으로 \\((\\text{만기}, \\text{yield})\\) 쌍에 대한 Conditional Expectation Model을 적용한 결과라고 볼 수 있다.\n채권 시장에서 다양한 만기의 국채(예: 1년, 2년, 5년, 10년, 30년 등)의 수익률 데이터를 수집하면, 기본적으로 (만기, yield) 쌍의 산점도(scatter plot)를 얻을 수 있다.\n이후, Yield Curve는 이러한 데이터를 조건부 기대값 모형 (Conditional Expectation model)을 사용하여 스무딩(Smoothing)하거나 추정(Fitting)한 것이라고 볼 수 있다.\n수익률 곡선을 추정하는 대표적인 방법으로 다음과 같은 모델들이 있다.\n\nPolynomial Regression\n\n가장 기본적인 방법은 2차 또는 3차 다항식을 사용하여 스무딩한 곡선을 추정.\n\\[\n\\mathbb{E}[Y | T] = \\beta_0 + \\beta_1 T + \\beta_2 T^2 + \\beta_3 T^3 + \\epsilon\n\\] 여기서:\n\n\\(T\\)는 채권의 만기,\n\\(Y\\)는 수익률(Yield),\n\\(\\mathbb{E}[Y | T]\\)는 조건부 기대값.\n\n\n\n\nSpline Regression\n\nCubic Spline 또는 B-spline을 사용하여 여러 구간에서 스무딩된 Yield Curve를 만듬.\n\n\n\nNelson-Siegel & Svensson model\n\n실무에서 많이 사용하는 Nelson-Siegel Model의 기본 식 \\[\nY(T) = \\beta_0 + \\beta_1 \\frac{1 - e^{-T/\\tau}}{T/\\tau} + \\beta_2 \\left(\\frac{1 - e^{-T/\\tau}}{T/\\tau} - e^{-T/\\tau} \\right)\n\\]\n여기서:\n\n\\(\\beta_0, \\beta_1, \\beta_2\\)는 모델의 파라미터\n\\(\\tau\\)는 시간 척도 조정 파라미터\n\\(T\\)는 만기 (term)\n\n\n\n\nGaussian Process, Neural Networks\n\nGaussian Process Regression (GPR) 또는 딥러닝 모델(Neural Networks)을 활용하여 Yield Curve를 추정하는 방법",
    "crumbs": [
      "교육",
      "금융",
      "Yield curve"
    ]
  },
  {
    "objectID": "yield_curve.html#code-scatter-plot-yield-curve",
    "href": "yield_curve.html#code-scatter-plot-yield-curve",
    "title": "Yield curve",
    "section": "Code: Scatter Plot + Yield Curve",
    "text": "Code: Scatter Plot + Yield Curve\n(만기, 수익률) 데이터 산점도를 그린 후, 스무딩된 Yield Curve를 적용.\n\n\nCode\n# Yield Curve(수익률 곡선)**는 본질적으로 (term,yield) 쌍에 대한 Conditional Expectation Model을 적용한 결과\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\n\n# 예제 데이터 (만기, 수익률)\nmaturity = np.array([1, 2, 3, 5, 7, 10, 20, 30])  # 만기 (years)\nyield_rates = np.array([3.2, 3.4, 3.5, 3.7, 3.8, 4.0, 4.2, 4.3])  # 수익률 (%)\n\n# Nelson-Siegel 모델 함수 정의\ndef nelson_siegel(T, beta0, beta1, beta2, tau):\n    return beta0 + beta1 * (1 - np.exp(-T/tau)) / (T/tau) + beta2 * ((1 - np.exp(-T/tau)) / (T/tau) - np.exp(-T/tau))\n\n# 초기값 설정 및 최적화\npopt, _ = curve_fit(nelson_siegel, maturity, yield_rates, p0=[4, -1, 1, 2])\n\n# 스무딩된 곡선 생성\nT_fit = np.linspace(0.5, 30, 100)  # 연속적인 만기 값\nY_fit = nelson_siegel(T_fit, *popt)\n\n# 산점도 및 수익률 곡선 그래프\nplt.figure(figsize=(10, 6))\nplt.scatter(maturity, yield_rates, color='blue', label=\"Observed Data (Scatter)\")\nplt.plot(T_fit, Y_fit, color='red', linestyle='-', label=\"Fitted Yield Curve (Nelson-Siegel)\")\nplt.xlabel(\"Maturity (Years)\")\nplt.ylabel(\"Yield (%)\")\nplt.title(\"Yield Curve: Scatter Plot with Nelson-Siegel Fit\")\nplt.legend()\nplt.grid(True)\nplt.show()",
    "crumbs": [
      "교육",
      "금융",
      "Yield curve"
    ]
  },
  {
    "objectID": "pareto_index.html",
    "href": "pareto_index.html",
    "title": "Modeling Wealth Distribution",
    "section": "",
    "text": "The Pareto distribution has long been used to model wealth or income distributions, starting from the seminal work of Vilfredo Pareto in the late 19th century (Pareto 1964). One of the most intuitive ways to visualize the inequality within such a distribution is through the Lorenz curve, introduced by Lorenz in 1905 (Lorenz 1905). The Lorenz curve describes the cumulative proportion of total wealth (or resources) held by the bottom fraction of the population. Closely related to the Lorenz curve is the Gini coefficient, which provides a scalar measure of inequality (Gini 1912).\nThis document first derives the Lorenz curve for a Pareto distribution, then demonstrates how to estimate the Pareto index (shape parameter) from Lorenz curve observations. We additionally relate the Pareto distribution to Constant Relative Risk Aversion (CRRA) utility function, highlighting the same shared structure in a differential equation that governs both.",
    "crumbs": [
      "교육",
      "수학",
      "Modeling Wealth Distribution"
    ]
  },
  {
    "objectID": "pareto_index.html#introduction",
    "href": "pareto_index.html#introduction",
    "title": "Modeling Wealth Distribution",
    "section": "",
    "text": "The Pareto distribution has long been used to model wealth or income distributions, starting from the seminal work of Vilfredo Pareto in the late 19th century (Pareto 1964). One of the most intuitive ways to visualize the inequality within such a distribution is through the Lorenz curve, introduced by Lorenz in 1905 (Lorenz 1905). The Lorenz curve describes the cumulative proportion of total wealth (or resources) held by the bottom fraction of the population. Closely related to the Lorenz curve is the Gini coefficient, which provides a scalar measure of inequality (Gini 1912).\nThis document first derives the Lorenz curve for a Pareto distribution, then demonstrates how to estimate the Pareto index (shape parameter) from Lorenz curve observations. We additionally relate the Pareto distribution to Constant Relative Risk Aversion (CRRA) utility function, highlighting the same shared structure in a differential equation that governs both.",
    "crumbs": [
      "교육",
      "수학",
      "Modeling Wealth Distribution"
    ]
  },
  {
    "objectID": "pareto_index.html#deriving-the-lorenz-curve-of-a-pareto-distribution",
    "href": "pareto_index.html#deriving-the-lorenz-curve-of-a-pareto-distribution",
    "title": "Modeling Wealth Distribution",
    "section": "2. Deriving the Lorenz Curve of a Pareto Distribution",
    "text": "2. Deriving the Lorenz Curve of a Pareto Distribution\n\n2.1. Definition of the Lorenz Curve\nFor a continuous distribution of wealth (X), the Lorenz curve \\(L(p)\\) is defined as the fraction of total wealth owned by the bottom \\(p\\) fraction of the population:\n\\[\nL(p) \\;=\\; \\frac{\\int_{x_m}^{x(p)} x\\,f(x)\\,dx}{\\int_{x_m}^{\\infty} x\\,f(x)\\,dx},\n\\]\nwhere\n\n\\(p = F\\bigl(x(p)\\bigr)\\) is the cumulative proportion of individuals with wealth below \\(x(p)\\),\nThe numerator represents the cumulative wealth of the bottom \\(p\\) fraction,\nThe denominator represents the total wealth in the system, given by the expected value of \\(X\\) over its support.\n\n\n\n2.2. Pareto Distribution\nA Pareto distribution with shape parameter \\(\\alpha&gt;0\\) and scale parameter \\(x_m&gt;0\\) is defined by the PDF\n\\[\nf(x) \\;=\\; \\alpha\\,\\frac{x_m^\\alpha}{x^{\\alpha+1}},\n\\quad x \\ge x_m,\n\\]\nand the corresponding CDF\n\\[\nF(x) \\;=\\; 1 \\;-\\;\\Bigl(\\tfrac{x_m}{x}\\Bigr)^\\alpha,\n\\quad x \\ge x_m.\n\\]\nEquivalently, for \\(0 &lt; p &lt; 1\\), the quantile \\(x(p)\\) satisfying \\(F\\bigl(x(p)\\bigr)=p\\) is\n\\[\np \\;=\\; 1 \\;-\\;\\Bigl(\\tfrac{x_m}{x(p)}\\Bigr)^\\alpha\n\\;\\;\\Longleftrightarrow\\;\\;\nx(p) \\;=\\; \\frac{x_m}{\\bigl(1 - p\\bigr)^{1/\\alpha}}.\n\\]\n\n\n2.3. Total Wealth\nLet the total wealth be \\(W_{\\text{total}} = E[X]\\), the expected value of \\(X\\). Substituting the PDF of the Pareto distribution into the definition of expectation,\n\\[\nE[X]\n\\;=\\; \\int_{x_m}^{\\infty} x\\,f(x)\\,dx\n\\;=\\; \\int_{x_m}^{\\infty} x \\,\\alpha\\,\\frac{x_m^\\alpha}{x^{\\alpha+1}}\\,dx\n\\;=\\; \\alpha\\,x_m^\\alpha \\int_{x_m}^{\\infty} x^{-\\alpha}\\,dx.\n\\]\nFor \\(\\alpha&gt;1\\), the improper integral converges and we obtain\n\\[\nW_{\\text{total}}\n\\;=\\; E[X]\n\\;=\\; \\frac{\\alpha\\,x_m}{\\alpha - 1}.\n\\]\n\n\n2.4. Cumulative Wealth for the Bottom \\(p\\) Fraction\nThe cumulative wealth held by the bottom \\(p\\) fraction is\n\\[\nW(p)\n\\;=\\;\\int_{x_m}^{x(p)} x\\,f(x)\\,dx\n\\;=\\;\\alpha\\,x_m^\\alpha \\int_{x_m}^{x(p)} x^{-\\alpha}\\,dx\n\\;=\\;\\alpha\\,x_m^\\alpha\n\\Bigl[\\frac{x^{-\\alpha+1}}{-\\alpha+1}\\Bigr]_{x_m}^{x(p)}.\n\\]\nSimplifying,\n\\[\nW(p)\n\\;=\\; \\frac{\\alpha\\,x_m}{\\alpha - 1}\n\\;\\Bigl(1 - \\bigl(\\tfrac{x_m}{x(p)}\\bigr)^{\\alpha - 1}\\Bigr).\n\\]\n\n\n2.5. Lorenz Curve for the Pareto Distribution\nBy definition,\n\\[\nL(p)\n\\;=\\; \\frac{W(p)}{W_{\\text{total}}}\n\\;=\\; \\frac{\\frac{\\alpha\\,x_m}{\\alpha - 1}\\,\\Bigl(1 - \\bigl(\\tfrac{x_m}{x(p)}\\bigr)^{\\alpha - 1}\\Bigr)}\n            {\\frac{\\alpha\\,x_m}{\\alpha - 1}}\n\\;=\\; 1 - \\bigl(1 - p\\bigr)^{\\frac{\\alpha - 1}{\\alpha}}.\n\\]\nHence, for a Pareto distribution, the Lorenz curve is\n\\[\nL(p) \\;=\\; 1 \\;-\\; \\bigl(1 - p\\bigr)^{\\tfrac{\\alpha - 1}{\\alpha}}.\n\\]\n\nIf \\(\\alpha \\gg 1\\), the distribution is more equal, and \\(L(p)\\) is closer to the 45-degree line of perfect equality.\n\nIf \\(\\alpha\\) is only slightly larger than 1, the distribution is more unequal, with significant concentration of wealth in the upper tail.",
    "crumbs": [
      "교육",
      "수학",
      "Modeling Wealth Distribution"
    ]
  },
  {
    "objectID": "pareto_index.html#estimating-the-pareto-index-from-the-lorenz-curve",
    "href": "pareto_index.html#estimating-the-pareto-index-from-the-lorenz-curve",
    "title": "Modeling Wealth Distribution",
    "section": "3. Estimating the Pareto Index from the Lorenz Curve",
    "text": "3. Estimating the Pareto Index from the Lorenz Curve\nSuppose empirical data or external studies indicate specific points \\((p, L(p))\\) on the Lorenz curve. We can use\n\\[\nL(p) \\;=\\; 1 - (1 - p)^{\\frac{\\alpha - 1}{\\alpha}}\n\\]\nto solve numerically for \\(\\alpha\\). Commonly cited examples:\nSeveral Empirical Illustrations\n\nPareto 80:20 Rule: \\(p=0.80\\), \\(L(p)=0.20\\)\n\\(\\Rightarrow\\) \\(\\alpha \\approx \\frac{\\ln(4)}{\\ln(5)} \\approx 1.16\\).\nPareto 90:10 Rule: \\(p=0.90\\), \\(L(p)=0.10\\)\n\\(\\Rightarrow\\) \\(\\alpha \\approx 1.05\\).\nU.S. Stock Market: According to a report by Axios (2024), the top 10% own about 93% of total equity wealth, implying \\(p=0.90\\) and \\(L(p)=0.07\\). Solving yields \\(\\alpha \\approx 1.03\\).\n\nCredit Suisse Global Wealth Report: In 2013, it was reported that the top 1% control about 50% of global wealth, which implies \\(p=0.99\\) and \\(L(p)=0.50\\). Solving gives \\(\\alpha \\approx 1.18\\). Additionally, the top 10% were said to own about 85% of global wealth (\\(p=0.90\\), \\(L(p)=0.15\\)), giving \\(\\alpha \\approx 1.08\\). Comparing such estimates across years (e.g., 2013 vs. 2020) can reveal the time dynamics of the global wealth distribution (Credit Suisse 2013, 2020).",
    "crumbs": [
      "교육",
      "수학",
      "Modeling Wealth Distribution"
    ]
  },
  {
    "objectID": "pareto_index.html#the-gini-coefficient",
    "href": "pareto_index.html#the-gini-coefficient",
    "title": "Modeling Wealth Distribution",
    "section": "4. The Gini Coefficient",
    "text": "4. The Gini Coefficient\nThe Gini coefficient is a measure of wealth or income inequality that is closely related to the Lorenz curve. The Gini coefficient is defined as the ratio of the area between the Lorenz curve and the 45-degree equality line to the total area under the 45-degree line. Mathematically, the Gini coefficient ( G ) is given by:\n\\[\nG = 1 - 2 \\int_0^1 L(p) \\, dp.\n\\]\nSubstituting the Lorenz curve for a Pareto distribution:\n\\[\nL(p) = 1 - (1 - p)^{\\frac{\\alpha - 1}{\\alpha}},\n\\]\nwe obtain:\n\\[\nG = 1 - 2 \\int_0^1 \\big[1 - (1 - p)^{\\frac{\\alpha - 1}{\\alpha}}\\big] \\, dp = \\frac{1}{2\\alpha - 1}.\n\\]\nThus, for a Pareto distribution, the Gini coefficient is:\n\\[\nG = \\frac{1}{2\\alpha - 1}, \\quad \\text{for } \\alpha &gt; \\frac{1}{2}.\n\\]\nSome features of the Gini coefficient:\n\nAs \\(\\alpha \\to 1^+\\), the Gini coefficient approaches 1, indicating extreme inequality (a few individuals hold nearly all the wealth).\nAs \\(\\alpha \\to \\infty\\), the Gini coefficient approaches 0, indicating perfect equality.\nFor typical empirical values of \\(\\alpha\\) in wealth distributions (e.g., 1.1 to 1.8), the Gini coefficient ranges from 0.83 to 0.38, reflecting significant inequality\nRelative measure: \\(G\\) compares the distribution to perfect equality, but does not capture absolute differences.\n\nNon‐additivity: One cannot simply average the Gini coefficients of subpopulations to obtain an overall Gini coefficient.\n\nSensitivity: The Gini coefficient is sensitive to changes in the middle of the distribution, but less so at the tails.",
    "crumbs": [
      "교육",
      "수학",
      "Modeling Wealth Distribution"
    ]
  },
  {
    "objectID": "pareto_index.html#pareto-distribution-and-crra-utility",
    "href": "pareto_index.html#pareto-distribution-and-crra-utility",
    "title": "Modeling Wealth Distribution",
    "section": "5. Pareto Distribution and CRRA Utility",
    "text": "5. Pareto Distribution and CRRA Utility\nIn microeconomic theory, a widely used utility specification is the Constant Relative Risk Aversion (CRRA) form (Arrow et al. 1974; Pratt 1978). The CRRA utility function \\(u(x)\\) satisfies\n\\[\n-\\frac{x\\,u''(x)}{u'(x)} \\;=\\; \\gamma,\n\\]\nwhere \\(\\gamma&gt;0\\) is the coefficient of relative risk aversion. Solving for \\(u(x)\\) under boundary conditions such as \\(u(1)=0\\) yields:\n\nIf \\(\\gamma=1\\), \\(u(x)=\\ln x\\). (using the L’hopital’s Rule)\nIf \\(\\gamma\\neq 1\\), \\(u(x)=\\frac{x^{\\,1-\\gamma}-1}{\\,1-\\gamma\\,}\\).\n\nLarger \\(\\gamma\\) indicates higher risk aversion, while \\(\\gamma=0\\) corresponds to risk neutrality (\\(u(x)=x\\)).\nA Pareto PDF can also be derived from a differential equation with similar form. If \\(f(x)\\) is the PDF of a Pareto random variable on \\(x\\ge x_m&gt;0\\), one can write:\n\\[\n-\\frac{x\\,f'(x)}{\\,f(x)\\!}\\;=\\;(1+\\alpha)\\,x_m^{\\alpha},\n\\]\nwhich likewise has a “power‐law” solution structure. Thus, Pareto distributions and CRRA utilities each emerge from a linear differential equation of analogous form, underscoring a conceptual parallel in how “power‐type” functional solutions can appear in both economic choice models (through marginal utility) and in heavy‐tailed probability distributions.\nFurthermore, in mainstream economic theory, marginal utility \\(u'(x)\\) is assumed to be strictly positive, and \\(u''(x)\\) typically negative (diminishing marginal utility). In probability theory, any valid PDF \\(f(x)\\) must be positive, and for heavy‐tailed distributions like Pareto, \\(f(x)\\) decreases for large \\(x\\). These parallels lead to a one‐to‐one analogy between certain types of declining utilities and distributions whose density functions also decline in \\(x\\).\n\nRemark: There is a well‐known relationship via logarithmic transforms: if \\(X\\) is Pareto(\\(x_m,\\alpha\\)), then \\(Y=\\ln(X/x_m)\\) is exponentially distributed with rate \\(\\alpha\\). This exponential distribution also arises from a first‐order linear differential equation, reinforcing these structural similarities.",
    "crumbs": [
      "교육",
      "수학",
      "Modeling Wealth Distribution"
    ]
  },
  {
    "objectID": "pareto_index.html#conclusion",
    "href": "pareto_index.html#conclusion",
    "title": "Modeling Wealth Distribution",
    "section": "6. Conclusion",
    "text": "6. Conclusion\nBecause the Pareto distribution has only two parameters (\\(x_m\\) and \\(\\alpha\\)), even minimal distributional data—such as “the bottom 80% own 20% of total wealth”—enables one to solve directly for the Pareto index \\(\\alpha\\). This simplicity makes the Pareto distribution a convenient model or approximation for global wealth distribution, although in practice the estimated \\(\\alpha\\) can vary greatly depending on the dataset, sampling, and specific segment of the population observed.\nIn wealth and income distribution analysis, pairing empirical Lorenz curves with Pareto modeling remains a powerful—if simplified—approach to gauging inequality. For both theoretical and practical reasons, it continues to be integral in economic research, policy discussions, and broader studies of social welfare. Meanwhile, connections to CRRA utility function illustrate that core economic principles and certain types of heavy‐tailed probabilistic behavior can share similar mathematical underpinnings.",
    "crumbs": [
      "교육",
      "수학",
      "Modeling Wealth Distribution"
    ]
  },
  {
    "objectID": "indicator_growth.html",
    "href": "indicator_growth.html",
    "title": "경제성장 대표지표?",
    "section": "",
    "text": "중앙은행이 대기업(‘고래’ 기업)들에게만 몰빵으로 현금 유동성을 공급하고, 이 기업들은 새로운 회사를 설립하고 그 자금을 투자에 사용한다. 투자된 돈이 중위 소득 상승과 소비로 연결되지 않고 다시 주식시장으로 유입되어 주가가 상승하면, 명목 GDP(Nominal GDP)는 증가할 수 있다. 하지만 이러한 명목 GDP 증가는 실제 경제 성장(real growth)을 반영하는 것일까?\n\n\n\n현금 유동성 공급으로 새롭게 만들어진 기업들이 소비자물가지수(CPI) 산정에 포함되지 않는 사업(B2B, 금융, AI, 데이터센터 등)을 한다면, CPI는 거의 변하지 않을 가능성이 매우 높다. 결과적으로, 명목 GDP 증가가 실질 GDP(Real GDP) 증가로 보이는 착시가 발생할 수 있다. 즉, 통계적으로 경제가 성장한 것처럼 보일 수 있지만, 실질적으로 대다수의 생활 수준은 나아지지 않아졌을 가능성이 매우 높다.\n\n\n\n공리주의 사회에서 경제 성장의 핵심 목표는 단순한 GDP 증가가 아니라 대다수 국민의 실질적 생활 수준 향상이다. 이를 더 잘 반영하는 지표는 Real GDP per capita 보다 Median of Real hourly Wage (시간당 실질 임금의 중위값) 이 될 수 있다.\n\nReal GDP per capita는 평균적인 경제 생산량 증가를 보여주지만, 실제로 국민 개개인의 소득 분배를 고려하지 않는다.\n\nReal Median hourly Wage는 국민 대다수가 체감하는 실질 소득의 변화를 나타내므로, 보다 정확한 경제 성장 지표가 될 수 있다.\n\n만약 명목 GDP가 증가하지만 시간당 중위임금이 정체되거나 하락한다면, 이는 대기업과 자산가 중심의 성장일 뿐, 일반 국민들에게 혜택이 돌아가지 않는 성장일 가능성이 크다.\n\n\n\n\n경제 성장의 지속 가능성을 예측하는 데 있어, 현재 사용되는 Natural GDP Growth Rate(자연 경제 성장률) 모델은 구조적인 문제를 가질 수 있다.\n- 이런 모델들은 과거 데이터를 기반으로 경제 성장률을 예측하지만, 금융시장 및 소비자 행동의 비선형적 변화를 제대로 반영하지 못한다.\n- 반면 Consumer Sentiment Index(소비자 심리 지수) 는 실시간으로 소비자들의 기대와 시장 심리를 측정한다.\n- 즉, 미래의 경제 성장 가능성을 예측하는 데 있어, 소비자들이 체감하는 경제 신뢰도와 구매 의향이 더 실질적인 지표가 될 수 있다.\n\n\n\n\n명목 GDP와 실질 GDP만으로 경제 성장을 평가하는 것은 한계가 있다.\n\nReal Median Wage가 상승하지 않는다면, GDP 성장은 일부 계층에만 혜택을 주는 비대칭적 성장일 가능성이 크다.\n경제 전망을 판단할 때, Consumer Sentiment Index와 같은 체감 지표가 전통적인 성장률 모델보다 더 현실적인 통찰을 제공할 수 있다.\n\n결국, 우리가 고민해야 할 것은 단순한 GDP 증가가 아니라 ’국민 개개인이 체감하는 실질적 경제 성장’이다.",
    "crumbs": [
      "교육",
      "경제",
      "경제성장 대표지표?"
    ]
  },
  {
    "objectID": "indicator_growth.html#real-gdp-vs.-real-median-hourly-wage",
    "href": "indicator_growth.html#real-gdp-vs.-real-median-hourly-wage",
    "title": "경제성장 대표지표?",
    "section": "",
    "text": "중앙은행이 대기업(‘고래’ 기업)들에게만 몰빵으로 현금 유동성을 공급하고, 이 기업들은 새로운 회사를 설립하고 그 자금을 투자에 사용한다. 투자된 돈이 중위 소득 상승과 소비로 연결되지 않고 다시 주식시장으로 유입되어 주가가 상승하면, 명목 GDP(Nominal GDP)는 증가할 수 있다. 하지만 이러한 명목 GDP 증가는 실제 경제 성장(real growth)을 반영하는 것일까?\n\n\n\n현금 유동성 공급으로 새롭게 만들어진 기업들이 소비자물가지수(CPI) 산정에 포함되지 않는 사업(B2B, 금융, AI, 데이터센터 등)을 한다면, CPI는 거의 변하지 않을 가능성이 매우 높다. 결과적으로, 명목 GDP 증가가 실질 GDP(Real GDP) 증가로 보이는 착시가 발생할 수 있다. 즉, 통계적으로 경제가 성장한 것처럼 보일 수 있지만, 실질적으로 대다수의 생활 수준은 나아지지 않아졌을 가능성이 매우 높다.\n\n\n\n공리주의 사회에서 경제 성장의 핵심 목표는 단순한 GDP 증가가 아니라 대다수 국민의 실질적 생활 수준 향상이다. 이를 더 잘 반영하는 지표는 Real GDP per capita 보다 Median of Real hourly Wage (시간당 실질 임금의 중위값) 이 될 수 있다.\n\nReal GDP per capita는 평균적인 경제 생산량 증가를 보여주지만, 실제로 국민 개개인의 소득 분배를 고려하지 않는다.\n\nReal Median hourly Wage는 국민 대다수가 체감하는 실질 소득의 변화를 나타내므로, 보다 정확한 경제 성장 지표가 될 수 있다.\n\n만약 명목 GDP가 증가하지만 시간당 중위임금이 정체되거나 하락한다면, 이는 대기업과 자산가 중심의 성장일 뿐, 일반 국민들에게 혜택이 돌아가지 않는 성장일 가능성이 크다.\n\n\n\n\n경제 성장의 지속 가능성을 예측하는 데 있어, 현재 사용되는 Natural GDP Growth Rate(자연 경제 성장률) 모델은 구조적인 문제를 가질 수 있다.\n- 이런 모델들은 과거 데이터를 기반으로 경제 성장률을 예측하지만, 금융시장 및 소비자 행동의 비선형적 변화를 제대로 반영하지 못한다.\n- 반면 Consumer Sentiment Index(소비자 심리 지수) 는 실시간으로 소비자들의 기대와 시장 심리를 측정한다.\n- 즉, 미래의 경제 성장 가능성을 예측하는 데 있어, 소비자들이 체감하는 경제 신뢰도와 구매 의향이 더 실질적인 지표가 될 수 있다.\n\n\n\n\n명목 GDP와 실질 GDP만으로 경제 성장을 평가하는 것은 한계가 있다.\n\nReal Median Wage가 상승하지 않는다면, GDP 성장은 일부 계층에만 혜택을 주는 비대칭적 성장일 가능성이 크다.\n경제 전망을 판단할 때, Consumer Sentiment Index와 같은 체감 지표가 전통적인 성장률 모델보다 더 현실적인 통찰을 제공할 수 있다.\n\n결국, 우리가 고민해야 할 것은 단순한 GDP 증가가 아니라 ’국민 개개인이 체감하는 실질적 경제 성장’이다.",
    "crumbs": [
      "교육",
      "경제",
      "경제성장 대표지표?"
    ]
  },
  {
    "objectID": "indicator_growth.html#부록-gdp-성장률의-한계와-대안적-경제지표",
    "href": "indicator_growth.html#부록-gdp-성장률의-한계와-대안적-경제지표",
    "title": "경제성장 대표지표?",
    "section": "부록: GDP 성장률의 한계와 대안적 경제지표",
    "text": "부록: GDP 성장률의 한계와 대안적 경제지표\n\nGDP 성장률의 측정과 한계\nGross Domestic Product(GDP)는 전통적으로 국가 경제 성장을 나타내는 핵심 지표로 활용되어 왔다. 그러나 최근 학계에서는 GDP 중심의 성장률 평가가 실제 국민의 삶의 질을 정확히 반영하지 못할 수 있다는 우려가 꾸준히 제기되고 있다(Kubiszewski et al., 2013; Stiglitz et al., 2009). 특히 금융 및 자산시장 중심의 성장 패턴이 강해질수록, GDP가 증가함에도 불구하고 소득 격차가 벌어지고 중산층의 생활 수준은 오히려 하락할 수 있다(Piketty, 2014).\n\nStiglitz, Sen, and Fitoussi (2009) 는 ’GDP가 단순한 생산물의 양적 증가만을 측정할 뿐, 불평등 심화와 실제 국민들이 체감하는 경제적 안녕(well-being)을 제대로 반영하지 못한다’는 문제를 공식적으로 제기한 바 있다. 이들은 GDP의 한계를 극복할 새로운 경제지표가 필요하다고 주장했다.\nCoyle (2014) 역시 GDP 수치 자체가 금융시장의 과열 현상으로 인해 실질적 경제 생산성 및 개개인의 복지 향상과 크게 괴리될 가능성을 지적했다. 그는 특히 금융시장의 투자 활동이 명목 GDP를 상승시키지만, 이러한 상승이 곧 일반 대중의 삶의 질 향상과 연결되지 않을 수 있음을 명시했다.\n\n\n\n대안적 경제지표로서의 실질 중위임금(Real Median Wage)\n경제학자들은 경제 성과 측정의 대안으로 소득분포의 중위값(median) 또는 중간층의 실질 소득 변화에 주목할 필요가 있다고 강조한다(Saez & Zucman, 2016; Chetty et al., 2017). 특히 중위임금(Median Wage)은 경제 성장이 국민 개개인의 생활 수준에 실제로 기여하는지를 명확히 나타내는 핵심적 지표로 주목받는다.\n\nChetty et al. (2017) 의 연구에 따르면, 미국 경제에서 GDP의 꾸준한 상승에도 불구하고 최근 30년간 실질 중위 소득이 정체된 사례가 발견되었으며, 이는 GDP 중심의 경제 성장이 반드시 대다수의 국민에게 이득을 가져다주지 않는다는 사실을 보여준다. 이 연구는 중위 소득을 핵심적 지표로 삼아 경제 정책의 우선순위를 재조정할 필요가 있다고 제안한다.\nSaez and Zucman (2016) 은 소득불평등의 증가가 GDP 성장률과 무관하게 진행되며, 중위 소득의 정체 또는 감소가 발생하면 경제 성장은 지속가능성을 잃게 된다고 경고한다. 그들은 중위 소득과 실질 생활 수준을 함께 고려하는 새로운 정책 프레임워크의 필요성을 강조한다.\n\n\n\n경제적 기대지표: Consumer Sentiment Index의 우월성\n경제 성장의 미래 전망을 평가할 때, 일반적으로 Central Bank 및 국제기구는 자연 경제 성장률(natural GDP rate)과 같은 구조적 모형 기반의 지표를 선호해왔다. 하지만 최근 연구들은 구조적 모형이 금융시장과 실물경제의 복잡한 상호작용을 잘 반영하지 못할 가능성이 있다고 지적한다(Sims, 2010; Coibion & Gorodnichenko, 2015). 이에 따라 소비자심리지수(Consumer Sentiment Index)가 경제 전망의 보다 정확한 선행지표로 주목받기 시작했다.\n\nCoibion and Gorodnichenko (2015) 는 소비자심리지수가 GDP 성장률, 특히 실질 소비지출의 미래 경로를 잘 예측하는 능력을 가졌음을 실증적으로 증명했다. 이 연구는 특히 불확실성이 높은 시기일수록 소비자심리지수가 공식적인 성장률 모형보다 경제 예측력에서 뛰어남을 보였다.\nSims (2010) 는 구조적 경제 예측 모형이 과거 데이터의 패턴에 과도하게 의존하여 금융 위기와 같은 비선형적 충격을 놓치는 경향이 있다고 지적했다. 반면 소비자 기대감은 그러한 경제적 충격을 보다 신속히 반영하며, 실물경제의 미래 변화에 대한 중요한 통찰력을 제공한다.\n\n\n\n시사점\n위 연구들의 공통된 결론은 명확하다. 전통적 GDP 성장률 측정 방식은 경제 성장과 국민 생활 수준 향상을 반드시 보장하지 않으며, 때로는 현실과 심각한 괴리를 일으킬 수 있다. 실질 중위임금(Real Median Wage)과 시간당 실질 중위임금 (Hourly Real Median Wage)은 국민들의 실제 생활수준 개선 여부를 평가하는 데 있어 GDP보다 더욱 정확한 지표이며, 미래 경제에 대한 소비자의 심리를 측정하는 소비자심리지수는 구조적 예측모델보다 현실적 통찰력을 제공한다.\n경제 정책은 GDP라는 숫자 증가에 매몰되지 않고, 개개인의 실질 생활수준과 소비자의 체감적 경제 기대감을 더 정확히 반영하는 지표 중심으로 전환해야 한다.",
    "crumbs": [
      "교육",
      "경제",
      "경제성장 대표지표?"
    ]
  },
  {
    "objectID": "indicator_growth.html#참고문헌",
    "href": "indicator_growth.html#참고문헌",
    "title": "경제성장 대표지표?",
    "section": "참고문헌",
    "text": "참고문헌\n\nChetty, R., Grusky, D., Hell, M., Hendren, N., Manduca, R., & Narang, J. (2017). The fading American dream: Trends in absolute income mobility since 1940. Science, 356(6336), 398-406.\nCoibion, O., & Gorodnichenko, Y. (2015). Information Rigidity and the Expectations Formation Process: A Simple Framework and New Facts. American Economic Review, 105(8), 2644-2678.\nCoyle, D. (2014). GDP: A brief but affectionate history. Princeton University Press.\nKubiszewski, I., Costanza, R., Franco, C., Lawn, P., Talberth, J., Jackson, T., & Aylmer, C. (2013). Beyond GDP: Measuring and achieving global genuine progress. Ecological Economics, 93, 57-68.\nPiketty, T. (2014). Capital in the Twenty-First Century. Harvard University Press.\nSaez, E., & Zucman, G. (2016). Wealth inequality in the United States since 1913: Evidence from capitalized income tax data. Quarterly Journal of Economics, 131(2), 519-578.\nSims, C. A. (2010). Rational Inattention and Monetary Economics. Handbook of Monetary Economics, 3, 155-181.\nStiglitz, J., Sen, A., & Fitoussi, J. P. (2009). Report by the commission on the measurement of economic performance and social progress. Paris: Commission on the Measurement of Economic Performance and Social Progress.",
    "crumbs": [
      "교육",
      "경제",
      "경제성장 대표지표?"
    ]
  },
  {
    "objectID": "finance_linear.html",
    "href": "finance_linear.html",
    "title": "Linear Models of Finance",
    "section": "",
    "text": "The failure to rationally explain the excess returns for dominant firms reflects not merely model misspecification but a systematic failure to capture the fundamental nature of contemporary capitalism, where returns increasingly flow not as compensation for risk-bearing but as rents extracted through structural market advantages. This insight challenges the core assumptions of traditional asset management, which relies heavily on models like the Capital Asset Pricing Model (CAPM) or multi-factor frameworks to guide investment decisions. In an post-2008 era of quantitative easing (QE), ETF dominance, and mega-cap firms leveraging network effects, regulatory capture, and institutional protection to secure outsized profits, these models lead to suboptimal portfolio allocations. This study delves into the implications of this disconnect, critiques the limitations of conventional approaches, and proposes a data-driven alternative using stochastic dominance to better align asset management with the realities of modern markets.\nThe evolution of portfolio optimization and asset pricing reflects a tension between theory and empirics:\n\nMarkowitz (1952): Introduced mean-variance optimization, assuming normal returns and quadratic utility—elegant but empirically fragile.\nCAPM (1964): Linked returns to market risk, but anomalies (e.g., size, value) prompted multi-factor models.\nFama-French (1992): Added factors, yet struggled with momentum and low-volatility effects.\nBehavioral Finance: Addressed inefficiencies but lacked a cohesive framework.\n\nc.f. Tested Linear Models\n\nChallenging Linear Factor Models\n\nFactor Selection: Highlight multicollinearity (mkt_excess, macro_bm) and limited explanatory power.\n\nLasso Regression: Apply regularization to select predictors for FF 10 industry portfolio excess returns.\n\n\nChallenging Betting Against Beta\n\nOut-of-Sample Performance: Optimal assets have betas near 1, not high or low (SIC 10 portfolios).\n\n\nChallenging the traditional Mean-Variance weight Optimization\n\nExtreme Weights: Unconstrained MV yields impractical results; constraints (e.g., no short sales) added.\n\nPassive vs. Active: Passive value-weighted portfolios outperform MV with lower costs.\n\n\nChallenging the Parametric Mean-Variance Weight Optimization\n\nBayesian update : Black and Litterman (1992)\nMomentum and Size Tilting: Brandt et al. (2009) approach underperforms passive benchmarks.",
    "crumbs": [
      "교육",
      "금융",
      "Linear Models of Finance"
    ]
  },
  {
    "objectID": "finance_linear.html#overview",
    "href": "finance_linear.html#overview",
    "title": "Linear Models of Finance",
    "section": "",
    "text": "The failure to rationally explain the excess returns for dominant firms reflects not merely model misspecification but a systematic failure to capture the fundamental nature of contemporary capitalism, where returns increasingly flow not as compensation for risk-bearing but as rents extracted through structural market advantages. This insight challenges the core assumptions of traditional asset management, which relies heavily on models like the Capital Asset Pricing Model (CAPM) or multi-factor frameworks to guide investment decisions. In an post-2008 era of quantitative easing (QE), ETF dominance, and mega-cap firms leveraging network effects, regulatory capture, and institutional protection to secure outsized profits, these models lead to suboptimal portfolio allocations. This study delves into the implications of this disconnect, critiques the limitations of conventional approaches, and proposes a data-driven alternative using stochastic dominance to better align asset management with the realities of modern markets.\nThe evolution of portfolio optimization and asset pricing reflects a tension between theory and empirics:\n\nMarkowitz (1952): Introduced mean-variance optimization, assuming normal returns and quadratic utility—elegant but empirically fragile.\nCAPM (1964): Linked returns to market risk, but anomalies (e.g., size, value) prompted multi-factor models.\nFama-French (1992): Added factors, yet struggled with momentum and low-volatility effects.\nBehavioral Finance: Addressed inefficiencies but lacked a cohesive framework.\n\nc.f. Tested Linear Models\n\nChallenging Linear Factor Models\n\nFactor Selection: Highlight multicollinearity (mkt_excess, macro_bm) and limited explanatory power.\n\nLasso Regression: Apply regularization to select predictors for FF 10 industry portfolio excess returns.\n\n\nChallenging Betting Against Beta\n\nOut-of-Sample Performance: Optimal assets have betas near 1, not high or low (SIC 10 portfolios).\n\n\nChallenging the traditional Mean-Variance weight Optimization\n\nExtreme Weights: Unconstrained MV yields impractical results; constraints (e.g., no short sales) added.\n\nPassive vs. Active: Passive value-weighted portfolios outperform MV with lower costs.\n\n\nChallenging the Parametric Mean-Variance Weight Optimization\n\nBayesian update : Black and Litterman (1992)\nMomentum and Size Tilting: Brandt et al. (2009) approach underperforms passive benchmarks.",
    "crumbs": [
      "교육",
      "금융",
      "Linear Models of Finance"
    ]
  },
  {
    "objectID": "finance_linear.html#dual-approaches-to-asset-management",
    "href": "finance_linear.html#dual-approaches-to-asset-management",
    "title": "Linear Models of Finance",
    "section": "Dual Approaches to Asset Management",
    "text": "Dual Approaches to Asset Management\nIn economics, a limited resource allocation involves relative price vectors (\\(P\\)) and quantity vectors (\\(Q\\)), often constrained by a structural inner product:\n\\[P \\cdot Q = \\text{constant}\\]\nWith a convex or concave scalar function of \\(P\\) or \\(Q\\) which are complete and compact Euclidean spaces, this structure enables convex optimization tools. Duality in convex optimization posits that solving for \\(Q\\) given \\(P\\) (primal problem) or \\(P\\) given \\(Q\\) (dual problem) yields equivalent results under linear constraints, reflecting the Hahn-Banach theorem’s guarantee of consistent linear functionals.\n\nAsset Weight Optimization:\n\nGoal: Given \\(P\\) (joint distribution of returns), find \\(Q\\) (asset weights) to optimize an objective (e.g., maximize Sharpe ratio).\nCritique: Modern Portfolio Theory (MPT) assumes elliptical distributions and time-separable utility (e.g., CRRA), which are unrealistic. Non-stationary or fat-tailed distributions (e.g., Cauchy) render moment estimates unreliable, and utility may not reflect diminishing marginal returns for future wealth.\n\nAsset Pricing Optimization:\n\nGoal: Given \\(Q\\) (future cash flows), find \\(P\\) (state prices) to satisfy no-arbitrage conditions via the Euler equation.\nCritique: If risk-return tradeoffs break down (as with mega-caps), linear factor models derived from local approximations fail, undermining theoretical predictions.",
    "crumbs": [
      "교육",
      "금융",
      "Linear Models of Finance"
    ]
  },
  {
    "objectID": "finance_linear.html#limitations-of-linear-asset-pricing-models",
    "href": "finance_linear.html#limitations-of-linear-asset-pricing-models",
    "title": "Linear Models of Finance",
    "section": "Limitations of Linear Asset Pricing Models",
    "text": "Limitations of Linear Asset Pricing Models\nTraditional asset pricing theories operate on the fundamental premise that expected returns directly compensate for systematic risk exposure. However, the empirical reality of top-performing mega-cap stocks presents a profound contradiction: these entities often display disproportionate returns relative to their risk profiles. Consider Apple, which despite having relatively high beta compared to other mega-caps, has generated returns that consistently exceed what would be predicted by its systematic risk exposure alone.\n\nStatistical Limitations\nThe conventional approach to addressing anomalies involves extending traditional asset pricing models by incorporating additional factors. These extensions represent a fundamentally inadequate methodology as they merely perpetuate the linear structure of existing frameworks while appending explanatory variables. The standard multi-factor model takes the form:\n\\[E(R_i) = R_f + \\beta_{i,1}F_1 + \\beta_{i,2}F_2 + ... + \\beta_{i,n}F_n + \\alpha_i\\]\nWhere \\(E(R_i)\\) represents expected returns, \\(R_f\\) is the risk-free rate, \\(\\beta_{i,j}\\) are factor loadings, and \\(F_j\\) are risk factors. The proposed extensions for capturing phenomena such as network effects, winner-take-all dynamics, and regulatory capture simply add terms to this equation.\nThis approach has several critical flaws related to both model specification and parameter estimation.\nLinear additive models inherently assume factor orthogonality, normal distribution of returns, and stable risk premiums—assumptions violated by the complex, non-linear relationships between institutional protection, market power, and returns. These models rely on the core statistical assumptions:\n\n\\(E(\\varepsilon_i) = 0\\) (zero-mean residuals)\n\\(Cov(F_j, \\varepsilon_i) = 0\\) (factors uncorrelated with residuals)\n\\(Cov(F_j, F_k) = 0\\) for \\(j \\neq k\\) (orthogonal factors)\n\\(\\varepsilon_i \\sim N(0, \\sigma^2)\\) (normally distributed residuals)\n\nYet empirical tests would consistently reject these assumptions for mega-cap returns. A particularly problematic issue is multicollinearity between factors. When adding variables such as institutional protection, regulatory capture, and network effects to explain mega-cap performance, these factors often exhibit high correlation amongst themselves. This multicollinearity leads to unstable coefficient estimates, inflated standard errors, and ultimately, unreliable inference. For instance, a firm’s market power is often highly correlated with its regulatory influence, making it difficult to disentangle their individual effects.\nMoreover, adding qualitative variables like “network effects” to statistical models is particularly vulnerable to p-hacking, as these constructs can be operationalized in numerous ways. The improvement in in-sample fitting (\\(R^2\\)) often comes at the expense of out-of-sample prediction accuracy—a fundamental manifestation of the bias-variance tradeoff in mean squared error:\n\\[MSE(prediction) = Bias^2 + Variance + Irreducible\\_Error\\]\nAdding complex factors to capture mega-cap outperformance typically increases model complexity, which may reduce in-sample bias but simultaneously increases estimation variance. This increased variance occurs because more complex models with additional parameters require more data for stable estimation and are more prone to overfitting noise rather than signal. This relationship is particularly problematic for stock returns, which violate ergodicity assumptions necessary for consistent parameter estimation.\nFurthermore, traditional frameworks remain epistemologically constrained by their reliance on backward-looking statistical relationships. Consider the standard time-series approach to estimating factor models:\n\\[R_{it} = \\alpha_i + \\beta_{i,MKT}R_{mt} + \\beta_{i,SMB}SMB_t + \\beta_{i,HML}HML_t + \\varepsilon_{it}\\]\nThis specification assumes parameter stability over time (\\(\\beta_{i,j,t} = \\beta_{i,j,t+1}\\)), an assumption violated when institutional dynamics create structural breaks. Microsoft’s evolving regulatory landscape demonstrates this challenge—its systematic risk profile has fundamentally changed as its market and political power have increased, rendering historical beta estimates increasingly irrelevant for forward-looking predictions.\n\n\nThe Inverse Relationship Between Risk and Return for Dominant Firms\nThe conventional understanding of systematic risk requires fundamental reconceptualization. The Capital Asset Pricing Model posits:\n\\[E(R_i) = R_f + \\beta_i(E(R_m) - R_f)\\]\nWhere \\(\\beta_i = \\frac{Cov(R_i, R_m)}{Var(R_m)} = \\rho_{i,m}\\frac{\\sigma_i}{\\sigma_m}\\) measures systematic risk exposure. This can be decomposed into correlation (\\(\\rho_{i,m}\\)) and relative volatility components (\\(\\frac{\\sigma_i}{\\sigma_m}\\)).\nAn empirical analysis of market data reveals a critical paradox for mega-cap stocks versus non-mega-cap stocks. Both categories typically exhibit similar correlation coefficients with the market (\\(\\rho_{i,m} \\approx 1\\)), indicating strong co-movement with overall market trends. Similarly, their expected returns (\\(E(R_i)\\)) are often comparable or even favor mega-caps. However, the crucial distinction emerges in their volatility profiles: mega-cap stocks consistently demonstrate lower idiosyncratic volatility (\\(\\sigma_i^{mega} &lt; \\sigma_i^{non-mega}\\)) due to their institutional protection, diversified revenue streams, and market power.\nThis creates a fundamental contradiction in the risk-return paradigm. When calculating the Sharpe ratio or other risk-adjusted return metrics:\n\\[Sharpe_i = \\frac{E(R_i) - R_f}{\\sigma_i}\\]\nMega-cap stocks consistently outperform on a risk-adjusted basis. This superior risk-adjusted performance cannot be reconciled with traditional asset pricing theory, which predicts that lower risk should be associated with lower returns. The empirical reality suggests the opposite for market dominant stocks:\n\\[\\frac{E(R_i^{mega}) - R_f}{\\sigma_i^{mega}} &gt; \\frac{E(R_i^{non-mega}) - R_f}{\\sigma_i^{non-mega}}\\]\nDespite comparable raw returns and market correlations, the risk-adjusted outperformance of mega-cap stocks directly contradicts the foundational risk-return trade-off of asset pricing theory. Amazon exemplifies this phenomenon—its volatility has decreased relative to smaller competitors as its market dominance has increased, yet its returns have remained exceptional, generating superior risk-adjusted performance that cannot be explained by traditional models.\n\n\nMarket Efficiency and Structural Advantages\nMarket efficiency assumptions underlying traditional models presuppose:\n\\[E[R_{i,t+1} | \\Omega_t] := E_t[R_{i,t+1}]\\]\nWhere \\(\\Omega_t\\) represents the information set available at time \\(t\\), and \\(E_t[\\cdot]\\) denotes expectation conditional on information at time \\(t\\). However, the persistence of abnormal returns for dominant firms suggests either:\n\nInformation about institutional advantages is not fully incorporated in prices\nThese advantages create structural market inefficiencies that cannot be arbitraged away\n\nGoogle’s digital advertising dominance illustrates these dynamics. Its network effects create increasing returns to scale that can be modeled as:\n\\[Profit_t = f(MarketShare_{t-1})^{\\gamma}\\]\nWhere \\(\\gamma &gt; 1\\) indicates increasing returns to scale. Traditional asset pricing models assume competitive markets where \\(\\gamma \\leq 1\\), making them structurally incapable of capturing the valuation implications of dominant market positions.\nThe fundamental inadequacy of traditional models becomes evident when examining their predictive accuracy for top market-cap stocks. A comparison of realized returns versus CAPM-predicted returns for top 10 stocks from 2010-2020 shows systematic underestimation:\n\\[\\alpha_i:= E(R_i) - [R_f + \\beta_i(R_m - R_f)] &gt; 0 \\text{ for top 10 stocks}\\]\nThis persistent alpha cannot be explained as compensation for omitted risk factors, as these firms typically enjoy reduced risk through institutional protection and market dominance.\nThis theoretical crisis demands not incremental additions to existing models but a comprehensive reconceptualization of capital market dynamics. The failure to accurately predict returns for dominant firms reflects not merely model misspecification but a systematic failure to capture the fundamental nature of contemporary capitalism, where returns increasingly flow not as compensation for risk-bearing but as rents extracted through structural market advantages.\nA more appropriate specification might acknowledge the non-linear relationship between institutional position and returns:\n\\[E(R_i) = R_f + \\beta_i(R_m - R_f) + g(MarketPower_i, InstitutionalProtection_i)\\]\nWhere \\(g(\\cdot)\\) is a non-linear function capturing the complex relationship between market power, institutional protection, and returns—a relationship that fundamentally contradicts the risk-return paradigm underlying traditional asset pricing theory.",
    "crumbs": [
      "교육",
      "금융",
      "Linear Models of Finance"
    ]
  },
  {
    "objectID": "finance_linear.html#practical-asset-management",
    "href": "finance_linear.html#practical-asset-management",
    "title": "Linear Models of Finance",
    "section": "Practical Asset Management",
    "text": "Practical Asset Management\n\nImplications of Model Failure for Asset Management\nAsset management involves making investment decisions to achieve financial objectives, such as maximizing returns or minimizing risk, typically by constructing portfolios based on expected returns and risk estimates derived from asset pricing models. However, the inadequacy of traditional models when applied to dominant firms—those “superstar” companies that capture disproportionate market value—creates significant practical challenges:\n\nBreakdown of Market Efficiency Assumptions\nTraditional models assume markets are efficient, with asset prices reflecting all available information. Yet, dominant firms consistently generate abnormal returns that persist over time, suggesting inefficiencies tied to structural advantages—like brand loyalty or data monopolies—that markets fail to fully price in. For asset managers, this means traditional models underestimate the value of these firms, leading to underweighting in portfolios.\nInverse Risk-Return Dynamics\nModels like the CAPM posit a linear relationship where higher risk (measured as beta) yields higher expected returns. For dominant firms, however, the opposite often holds: their entrenched positions reduce idiosyncratic risk—through diversified revenue streams or regulatory buffers—while delivering superior returns. This inversion misleads asset managers into overestimating the risk of mega-cap stocks and underestimating their return potential, skewing portfolio optimization.\nFactor Instability and Omission\nMulti-factor models, such as those based on Fama-French factors, depend on stable relationships between risk factors (e.g., size, value) and returns. Yet, the market power of dominant firms evolves dynamically—through technological innovation or policy shifts—causing factor loadings to fluctuate. Moreover, these models lack factors that explicitly capture rents from structural advantages, leaving asset managers with incomplete tools to assess true risk-adjusted performance.\nInability to Model Non-Linear Effects\nThe linear structure of traditional models cannot accommodate the non-linear dynamics of contemporary capitalism, such as increasing returns to scale or tipping points in market dominance. For example, a firm like Amazon benefits from self-reinforcing network effects that amplify its returns beyond what risk-based models predict, rendering traditional allocations ineffective.\n\nThese shortcomings result in portfolios that miss out on the concentrated returns of dominant firms, which increasingly drive market performance. Asset managers relying on outdated frameworks risk underperformance in a landscape where structural advantages, not risk exposure, dictate success.\n\n\nA Data-Driven Alternative: Stochastic Dominance\nTo address these challenges, asset management must shift toward empirical, data-driven approaches that eschew the restrictive assumptions of traditional models. One such method is stochastic dominance, which evaluates assets by comparing their entire return distributions rather than relying on summary statistics like mean and variance. This approach offers a practical way to identify investments that align with the empirical realities of contemporary markets.\n\nHow It Works\n\nFirst-Order Stochastic Dominance (FSD): Asset A dominates Asset B if its cumulative distribution function (CDF) lies below B’s for all return levels, meaning A offers higher returns across all outcomes.\n\nSecond-Order Stochastic Dominance (SSD): For risk-averse investors, A dominates B if the area under A’s CDF (its integral) is less than B’s up to any point, indicating a better risk-return tradeoff.\n\nAdvantages\n\nRobustness: Unlike mean-variance optimization, stochastic dominance requires no assumptions about return distributions (e.g., normality), making it suitable for markets with fat tails or skewness—common in mega-cap stock performance.\n\nEmpirical Focus: By analyzing historical return distributions, it captures the real-world impact of structural advantages, such as the persistent outperformance of firms like Apple or Microsoft.\n\nFlexibility: It accommodates diverse investor risk preferences without specifying a utility function, broadening its applicability.\n\nPractical Implementation\nAsset managers can apply stochastic dominance by:\n\nHistorical Simulations: Using past return data to estimate CDFs and calculate the likelihood of one asset dominating another.\n\nResampling: Employing techniques like bootstrapping to test dominance under varying market conditions, enhancing robustness.\n\nChallenges\n\nData Requirements: Accurate CDF estimation demands extensive historical data, which may be scarce for newer firms or asset classes.\n\nNon-Stationarity: Return distributions shift over time—due to regulatory changes or competitive disruptions—limiting the reliability of historical dominance.\n\nPortfolio Complexity: Extending stochastic dominance to multi-asset portfolios is computationally intensive, often requiring simplifications or heuristic rules.\n\n\nDespite these hurdles, stochastic dominance provides a rigorous framework that sidesteps the theoretical pitfalls of traditional models. It allows asset managers to focus on observable performance, directly addressing the rent extraction that defines dominant firms.\n\n\nComplementary Tools\nWhile stochastic dominance serves as a strong foundation, asset managers can bolster their strategies with additional tools to capture the nuances of contemporary markets:\n\nFundamental Analysis\nDetailed assessments of a firm’s financials, competitive moat (e.g., patents, scale), and industry trends can reveal structural advantages missed by quantitative models. For instance, analyzing Tesla’s innovation pipeline or Alphabet’s data ecosystem offers insights into their sustained dominance.\nMachine Learning\nAdvanced algorithms can detect non-linear patterns in returns or predict shifts in market power, though they require careful tuning to avoid overfitting and ensure interpretability.\nLiquidity Considerations\nMega-cap stocks typically offer high liquidity, reducing transaction costs and making them practical choices for large portfolios—a factor stochastic dominance alone might not prioritize.\n\nMoreover, the concentration of returns in a few dominant firms suggests that a focused investment strategy—overweighting current mega-caps or scouting emerging leaders—may outperform broad diversification. However, identifying future dominants demands foresight into technological, regulatory, and economic trends, adding complexity to the process.\n\n\nConclusion\nIn a world where returns stem from structural market advantages rather than risk-bearing, traditional asset pricing models fail to guide effective asset management. Their inability to predict the performance of dominant firms—rooted in flawed assumptions about efficiency, risk, and linearity—leads to misallocated portfolios that undervalue mega-cap opportunities. A data-driven approach using stochastic dominance offers a compelling alternative, leveraging empirical return distributions to identify superior investments without theoretical baggage. While challenges like data demands and non-stationarity persist, combining this method with fundamental analysis and modern tools equips asset managers to navigate contemporary capitalism. By embracing flexibility and empiricism, asset management can better capture the rents that define today’s markets, delivering superior outcomes in an era of concentrated dominance.",
    "crumbs": [
      "교육",
      "금융",
      "Linear Models of Finance"
    ]
  },
  {
    "objectID": "finance_asset_puzzle.html",
    "href": "finance_asset_puzzle.html",
    "title": "Asset Premium Puzzles",
    "section": "",
    "text": "Since the 1980s, financial economists have grappled with two so-called “puzzles” in asset pricing theory: the Equity Premium Puzzle (EPP) and the Risk-Free Rate Puzzle (RFRP). These puzzles refer to the pronounced gap between theoretical predictions derived from standard rational-expectations models with representative-agent utility maximization and the empirical observations on asset returns and risk-free rates. In particular, Mehra and Prescott (1985) and Weil (1989) spotlighted the disparity between observed excess returns on equities and the relatively tame variability of aggregate consumption, labeling the phenomenon a “puzzle” under the canonical equilibrium framework.\nBeneath both puzzles lies a single “master equation,” namely the Euler equation, which in its most general form states:\n\\[\n\\mathbb{E}_t \\big[m_t R_{t+1}\\big] = 1,\n\\]\nwhere \\(m_t\\) (the Stochastic Discount Factor) is closely tied to the marginal utility of consumption, and \\(R_{t+1}\\) represents the return on various assets. The equity premium and risk-free rate each follow from this broad formulation, yet empirical magnitudes diverge significantly from conventional theoretical predictions.",
    "crumbs": [
      "교육",
      "금융",
      "Asset Premium Puzzles"
    ]
  },
  {
    "objectID": "finance_asset_puzzle.html#introduction",
    "href": "finance_asset_puzzle.html#introduction",
    "title": "Asset Premium Puzzles",
    "section": "",
    "text": "Since the 1980s, financial economists have grappled with two so-called “puzzles” in asset pricing theory: the Equity Premium Puzzle (EPP) and the Risk-Free Rate Puzzle (RFRP). These puzzles refer to the pronounced gap between theoretical predictions derived from standard rational-expectations models with representative-agent utility maximization and the empirical observations on asset returns and risk-free rates. In particular, Mehra and Prescott (1985) and Weil (1989) spotlighted the disparity between observed excess returns on equities and the relatively tame variability of aggregate consumption, labeling the phenomenon a “puzzle” under the canonical equilibrium framework.\nBeneath both puzzles lies a single “master equation,” namely the Euler equation, which in its most general form states:\n\\[\n\\mathbb{E}_t \\big[m_t R_{t+1}\\big] = 1,\n\\]\nwhere \\(m_t\\) (the Stochastic Discount Factor) is closely tied to the marginal utility of consumption, and \\(R_{t+1}\\) represents the return on various assets. The equity premium and risk-free rate each follow from this broad formulation, yet empirical magnitudes diverge significantly from conventional theoretical predictions.",
    "crumbs": [
      "교육",
      "금융",
      "Asset Premium Puzzles"
    ]
  },
  {
    "objectID": "finance_asset_puzzle.html#theoretical-framework",
    "href": "finance_asset_puzzle.html#theoretical-framework",
    "title": "Asset Premium Puzzles",
    "section": "Theoretical Framework",
    "text": "Theoretical Framework\n\nEquity Premium Puzzle\nThe seminal work of Lucas (1978) and later Mehra and Prescott (1985) established the Equity Premium Puzzle within a representative-agent, general-equilibrium asset pricing model. The typical consumer (or representative agent) maximizes expected utility over time, commonly assumed to be CRRA (Constant Relative Risk Aversion):\n\\[\nU(C_t) = \\frac{C_t^{1-\\gamma}}{1-\\gamma}, \\quad \\gamma &gt; 0, \\,\\gamma \\neq 1,\n\\]\nwhere \\(\\gamma\\) is the coefficient of relative risk aversion (RRA). From the consumer’s intertemporal optimization, an Euler equation emerges:\n\\[\n\\mathbb{E}_t \\Big[\\beta \\Big(\\frac{C_{t+1}}{C_t}\\Big)^{-\\gamma} \\big(R_{e,t+1} - R_{f,t+1}\\big)\\Big] = 0,\n\\]\nwhere:\n\n\\(\\beta\\): subjective time discount factor (\\(0 &lt; \\beta &lt; 1\\)),\n\n\\(C_{t+1} / C_t\\): consumption growth rate,\n\n\\(R_{e,t+1}\\): equity return,\n\n\\(R_{f,t+1}\\): risk-free asset return.\n\nUnder some approximations (e.g., log-linearization of consumption growth), Mehra and Prescott (1985) famously derived:\n\\[\n\\mathbb{E}[R_e - R_f] \\approx \\gamma \\,\\sigma_c^2,\n\\]\nwhere \\(\\sigma_c^2\\) is the variance of consumption growth. If the variance of consumption growth is small (on the order of 1%–2% annually), the observed 6%–8% annual equity premium can only be reconciled by positing implausibly high risk aversion coefficients (often above 20). Since typical estimates of \\(\\gamma\\) in microeconomic or macroeconomic studies hover around 1–5, this gap forms the crux of the EPP.\n\n\nRisk-Free Rate Puzzle\nA closely related conundrum is the Risk-Free Rate Puzzle, initially highlighted by Weil (1989). Under the same CRRA framework and rational expectations, the Euler equation for the risk-free asset implies:\n\\[\n1 = \\mathbb{E}_t \\Big[\\beta \\Big(\\frac{C_{t+1}}{C_t}\\Big)^{-\\gamma} R_{f,t+1}\\Big].\n\\]\nApproximating in logs,\n\\[\nr_f \\approx \\delta + \\gamma \\,g_c - \\frac{1}{2}\\,\\gamma\\,(\\gamma + 1)\\,\\sigma_c^2,\n\\]\nwhere:\n\n\\(r_f = \\ln(R_f)\\): the log of the risk-free rate,\n\n\\(\\delta = -\\ln(\\beta)\\): time preference rate,\n\n\\(g_c = \\mathbb{E}[\\ln(C_{t+1}/C_t)]\\): average consumption growth rate.\n\nEmpirically, long-run real risk-free rates are typically in the 1%–3% range, whereas the above equation might predict rates of 4%–8% given plausible values for \\(\\gamma\\), \\(\\delta\\), and \\(g_c\\). Again, the severe mismatch between model forecasts and observed data has led researchers to classify it as a “puzzle.”\n\n\nCritical Assumptions\nTo preserve tractability, the standard model assumes:\n\nA representative agent — but who truly “represents” the market?\n\nTime-separability of the utility function — ensuring that period utilities add linearly over time.\n\nGlobal concavity of CRRA utility — guaranteeing diminishing marginal utility at all consumption levels.\n\nWhile these assumptions yield elegant closed-form solutions, they may excessively simplify real-world heterogeneity. Crucially, when the economy scales up over time, CRRA utility remains well-defined, but in practice this might obscure the role of vastly different consumption paths across distinct wealth brackets.",
    "crumbs": [
      "교육",
      "금융",
      "Asset Premium Puzzles"
    ]
  },
  {
    "objectID": "finance_asset_puzzle.html#critical-perspective",
    "href": "finance_asset_puzzle.html#critical-perspective",
    "title": "Asset Premium Puzzles",
    "section": "Critical Perspective",
    "text": "Critical Perspective\n\nRethinking ‘Rational Expectations’\nTraditionally, Rational Expectations is seen as a condition that agents use all available information efficiently, forming unbiased forecasts. However, from a purely mathematical standpoint, “expectations” and “covariances” are simply operators for dealing with means and correlations of random variables. Such operators—particularly bilinear forms and inner products—require specific algebraic properties (linearity, symmetry, Cauchy–Schwarz inequality, etc.). While these simplifications can be powerful in physics or engineering, in economics they might be overly restrictive when applied to highly heterogeneous populations and institutions.\n\n\nThe Euler Equation\nBy construction, the Euler condition \\(\\mathbb{E}_t [m_{t+1} R_{t+1}] = 1\\) is mathematically akin to an inner product on a probability space. This yields a focus on second moments (variance, covariance) and often leads to elliptical distribution assumptions (e.g., normal, Student-\\(t\\)). Real-world wealth distributions and market participation, however, may be far from elliptical in their statistical properties—especially when only a small fraction of the population holds the majority of risky assets.\n\n\nImplications of Globally Concave CRRA Utility\nCRRA utility, with its global concavity, implies a declining marginal utility as consumption grows. If aggregate consumption (\\(C_t\\)) trends upward over time, the ratio of marginal utilities \\(\\bigl[u'(C_{t+1}) / u'(C_t)\\bigr]\\) naturally declines, ensuring an inverse relationship between the SDF (\\(m_{t+1}\\)) and any asset (or variable) with a long-term growth trend. In equity markets, returns \\(R_{t+1}\\) also tend to grow over time, so \\(m_t\\) and \\(R_{t+1}\\) end up negatively correlated by construction.\nIf one then chooses a suitably volatile variable (with sufficient high variance) to stand in for \\(m_{t+1}\\), one can reconcile observed excess returns with the theoretical predictions—effectively defusing the puzzle. In that sense, the puzzle may be an artifact of incomplete modeling of real-world heterogeneity.",
    "crumbs": [
      "교육",
      "금융",
      "Asset Premium Puzzles"
    ]
  },
  {
    "objectID": "finance_asset_puzzle.html#explaining-asset-premiums",
    "href": "finance_asset_puzzle.html#explaining-asset-premiums",
    "title": "Asset Premium Puzzles",
    "section": "Explaining Asset Premiums",
    "text": "Explaining Asset Premiums\n\nThrough Market Heterogeneity\nRather than focusing exclusively on heterogeneous preferences (e.g., habit formation, behavioral biases), the perspective here is that heterogeneous economic environments—in particular, vast differences in wealth and market participation—play a decisive role:\n\nConcentrated Stock Market Participation\n\nA small fraction of wealthy households hold the majority of equity. Their risk attitudes and consumption patterns have disproportionate influence on asset prices.\n\nWealth and Consumption Inequality\n\nHigh-net-worth individuals exhibit markedly different consumption patterns from the average household, more closely aligning with equity market fluctuations.\n\n\nFrom this viewpoint, the high observed equity premium is not a puzzle at all once one acknowledges that a very small sub-population—namely, the extremely wealthy—holds large, volatile wealth positions that effectively determine marginal prices in the stock market. According to Federal Reserve data (Board 2025), 93% of households’ stock market wealth (though not 93% of total market capitalization) belongs to the wealthiest 10%. This implies that most aggregate equity risk and returns accrue to a relatively narrow stratum. For the “representative household,” which holds only about 7% of equity, stock price movements have minimal impact on its marginal utility of consumption. Hence, standard representative-agent formulations fail to capture the truly relevant marginal investor.\nEmpirical evidence from the following studies supports this line of reasoning:\n\nBasak and Cuoco (1998): Demonstrates that limited stock market participation elevates risk premia and depresses risk-free rates.\n\nYogo (2006): Shows that consumption by wealthy households closely tracks equity returns, reinforcing the link between high-end consumption dynamics and asset prices.\n\nGomes and Michaelides (2008): Connects growing income inequality with stock market participation patterns, resulting in rising equity premia.\n\nLettau, Ludvigson, and Ma (2019): Shows that a single macroeconomic factor tied to capital share (reflecting wealthy shareholders’ consumption) can explain a broad range of cross-sectional stock return premia.\n\n\n\nLimitations\nDespite offering a plausible explanation, this heterogeneity-based view faces practical hurdles:\n\nLow frequency data: Due to infrequent reporting on high-net-worth wealth, applying short-term no-arbitrage principles in cross-sectional asset pricing is problematic.\n\nDifficulty of reconciling EMH: Efficient Markets Hypothesis (EMH) posits that arbitrage opportunities vanish quickly, but wealth data often lack the granularity or frequency to confirm this.\n\nLong-run identification: Changes in upper-tier wealth or consumption may be valid for a long-run SDF (\\(m\\)), yet verifying this for short-run asset pricing remains challenging.",
    "crumbs": [
      "교육",
      "금융",
      "Asset Premium Puzzles"
    ]
  },
  {
    "objectID": "finance_asset_puzzle.html#conclusion",
    "href": "finance_asset_puzzle.html#conclusion",
    "title": "Asset Premium Puzzles",
    "section": "Conclusion",
    "text": "Conclusion\nThe Equity Premium Puzzle and Risk-Free Rate Puzzle have dominated discussions in asset pricing for decades. However, labeling them as genuine “puzzles” may reflect an artifact of restrictive models that hinge on a single representative agent, uniform preferences, and high-level assumptions about consumption growth. By introducing heterogeneous market participation, particularly the reality that a small fraction of wealthy agents holds the lion’s share of risky assets, one finds that what appears to be a puzzle for the average consumer is, in fact, quite explicable among those who actually drive stock prices.\nIn short, when empirical ownership and wealth concentration data are properly accounted for, the puzzling gaps between theory and observation can diminish or disappear. The challenge remains to integrate heterogeneous agent frameworks with accurate micro-level data on wealth and consumption in order to provide a more comprehensive understanding of asset prices—an endeavor that holds promise for reconciling the so-called “puzzles” with empirical reality.",
    "crumbs": [
      "교육",
      "금융",
      "Asset Premium Puzzles"
    ]
  },
  {
    "objectID": "correlation_crypto.html",
    "href": "correlation_crypto.html",
    "title": "Correlation in Cryptos",
    "section": "",
    "text": "Abstract: 2025년 3월 현재, 시가총액이 크거나 투자자들에게 인기가 많은 주요 암호화폐(popular cryptocurrencies)를 선정하여 지난 1년간의 상관관계를 분석하였다. 대부분의 암호화폐 투자자들은 이러한 주요 암호화폐에 집중적으로 투자하는 경향이 있다. 한편, 암호화폐 자산에 대한 투자자의 평균 투자 기간은 단기(short-term)로, 일반적으로 1개월에서 3개월 사이에 해당한다. 이에 따라 본 연구에서는 데이터의 관측 빈도(observation frequency)를 일간(daily) 단위로 설정하고, 30일, 60일, 90일의 롤링 윈도우(rolling window)를 적용하여 주요 암호화폐 수익률의 선형 상관계수(Pearson’s coefficient)를 분석하였다. 이러한 분석은 변동성 헤징(volatility hedging)을 고려한 분산 투자(diversified investment) 전략 수립에 도움이 될 수 있다. 예를 들어, 일정한 투자 금액(예: 1억 원)을 주요 암호화폐 자산군 내에서 어떻게 배분할지 결정하는 데 있어, 상관계수 분석 결과가 투자 비중 조정에 유용한 정보를 제공할 것으로 기대된다.",
    "crumbs": [
      "교육",
      "금융",
      "Correlation in Cryptos"
    ]
  },
  {
    "objectID": "correlation_crypto.html#서론",
    "href": "correlation_crypto.html#서론",
    "title": "Correlation in Cryptos",
    "section": "서론",
    "text": "서론\n비트코인(BTC)의 가격 및 수익률은, 단기적으로 다음과 같은 관계를 보여왔다.\n\nNDX (나스닥 100 지수)와 강한 양의 상관관계 (Nasdaq 2024, 2023),\nDXY (미국 달러 지수)와 강한 음의 상관관계 (Coindesk 2023; Coinglass 2023). 만약 비트코인 가격이 달러 가격과 장기적으로도 반대 방향으로 움직인다면, 이는 비트코인이 인플레이션 헤지 자산으로 간주될 수도 있을 가능성을 나타낸다 (Dyhrberg 2021).\n금 가격 (GOLD), 국내 실질 총생산량 (GDP) 과의 상관관계는 불명확하거나 간접적인 것으로 알려져 있음 (Cointelegraph 2023; Cryptoslate 2022).\n\n역사적 사례\n\n2020년 COVID-19 위기 이후 BTC와 NDX의 상관관계가 강화됨 (Nasdaq 2020),\n2022년 5월 연방준비제도(Fed)의 금리 인상 발표 당시, BTC와 NDX 모두 하락.\n2023년 비트코인 빙하기 기간 동안 BTC와 NDX의 상관관계 변화 분석 필요.\n2024년 3월 비트코인 ETF가 출시하여 기관 투자자 참여가 증가와 함께께 BTC과 NDX의 coupling이 심해짐.\n\n\n주요 암호화폐 목록 및 카테고리\n\n\n\n\n\n\n\n\n암호화폐 (Cryptocurrency)\n심볼 (Ticker)\n카테고리 (Category)\n\n\n\n\n비트코인 (Bitcoin)\nBTC/USD\nLayer 1\n\n\n이더리움 (Ethereum)\nETH/USD\nLayer 1, Smart Contract\n\n\n테더 (Tether)\nUSDT/USD\nStablecoin\n\n\n리플 (XRP)\nXRP/USD\nPayment Network\n\n\n솔라나 (Solana)\nSOL/USD\nLayer 1\n\n\n체인링크 (Chainlink)\nLINK/USD\nOracle\n\n\n온도 (Ondo)\nONDO/USD\nReal-World Asset (RWA)\n\n\n카르다노 (Cardano)\nADA/USD\nLayer 1\n\n\n트론 (Tron)\nTRX/USD\nLayer 1\n\n\n도지코인 (Dogecoin)\nDOGE/USD\nMeme Coin\n\n\n\n\n\n암호화폐 관련 정보 제공 매체 리뷰\n\n시세 데이터 (Price Data): 실시간 및 과거 가격 변동, 거래량(volume) 등\n\nCoinMarketCap\nCoinGecko\n\n온체인 데이터 (On-Chain Data): 거래량, 지갑 주소 변화, 네트워크 활성도 등\n\nGlassnode\nIntoTheBlock\n\n시장 분석 (Market Analysis): 전문가 및 AI 기반 분석 리포트\n\nMessari\nCryptoQuant\n\n뉴스 및 이벤트 (News & Events): 프로젝트 업데이트, 규제 변화 등\n\nCoinDesk\nThe Block\n\n소셜 미디어 분석 (Social Media Analysis): 트위터(X), 레딧(Reddit) 등에서의 커뮤니티 반응\n\nLunarCrush\nSantiment",
    "crumbs": [
      "교육",
      "금융",
      "Correlation in Cryptos"
    ]
  },
  {
    "objectID": "correlation_crypto.html#데이터-분석",
    "href": "correlation_crypto.html#데이터-분석",
    "title": "Correlation in Cryptos",
    "section": "데이터 분석",
    "text": "데이터 분석\n\n데이터\n\n데이터 소스: CCXT\n데이터 기간: 2024년 3월 1일 - 2025년 2월 28일\n데이터 빈도 (Data Frequency): 일간(Daily)\n분석 대상 암호화폐:\n\nBTC/USD, ETH/USD, USDT/USD, XRP/USD, SOL/USD, LINK/USD, ONDO/USD, ADA/USD, TRX/USD, DOGE/USD\n\n롤링 윈도우 크기 (Rolling Window Size): 30일, 60일, 90일\n\n\n\n분석 방법\n\n암호화폐의 일간 수익률(daily return)을 계산.\n각 롤링 윈도우 크기(30, 60, 90일)에 대해 롤링 상관 행렬(rolling correlation matrix)을 계산.\n평균 상관계수(mean of rolling correlation matrix)를 도출하여 암호화폐 간의 관계를 분석.\n\n\n\nCode\n# 분석 결과 (Results)\n\n# 여러 거래소에서 지원하는 거래쌍을 확인\n\nimport ccxt\nimport pandas as pd\n\n# 주요 암호화폐 목록\nTICKER_COIN = ['BTC/USDT', 'ETH/USDT', 'USDT/USD', 'XRP/USDT', 'SOL/USDT', 'LINK/USDT', 'ONDO/USDT', 'ADA/USDT', 'TRX/USDT', 'DOGE/USDT']\n\n# 지원하는 거래소 목록\nexchanges = ['binance', 'kraken', 'bitfinex', 'poloniex']\n\n# 각 거래소에서 지원하는 거래쌍 확인\nfor exchange_id in exchanges:\n    exchange = getattr(ccxt, exchange_id)()\n    markets = exchange.load_markets()\n    supported_pairs = [pair for pair in TICKER_COIN if pair in markets]\n    print(f\"{exchange_id} supports: {supported_pairs}\")\n\n# 주요 암호화폐 목록\nbinance_tickers = ['BTC/USDT', 'ETH/USDT', 'XRP/USDT', 'SOL/USDT', 'LINK/USDT', 'ADA/USDT', 'TRX/USDT', 'DOGE/USDT']\nkraken_tickers = ['USDT/USD']\npoloniex_tickers = ['ONDO/USDT']\n\n# 데이터 기간 설정\nSTART_DATE = '2024-03-01'\nEND_DATE = '2025-02-28'\n\n# 거래소 설정\nbinance = ccxt.binance()\nkraken = ccxt.kraken()\npoloniex = ccxt.poloniex()\n\n# 데이터 불러오기 함수\ndef fetch_crypto_data(exchange, tickers, start, end):\n    data = {}\n    start_timestamp = exchange.parse8601(f'{start}T00:00:00Z')\n    end_timestamp = exchange.parse8601(f'{end}T00:00:00Z')\n    for ticker in tickers:\n        try:\n            ohlcv = exchange.fetch_ohlcv(ticker, '1d', since=start_timestamp, limit=1000)\n            df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n            df.set_index('timestamp', inplace=True)\n            data[ticker] = df['close']\n        except Exception as e:\n            print(f\"Error fetching {ticker} from {exchange.id}: {e}\")\n    return pd.DataFrame(data)\n\n# 데이터 불러오기\nbinance_data = fetch_crypto_data(binance, binance_tickers, START_DATE, END_DATE)\nkraken_data = fetch_crypto_data(kraken, kraken_tickers, START_DATE, END_DATE)\npoloniex_data = fetch_crypto_data(poloniex, poloniex_tickers, START_DATE, END_DATE)\n\n# 모든 데이터를 하나의 DataFrame으로 병합\ncrypto_prices = pd.concat([binance_data, kraken_data, poloniex_data], axis=1)\n\n# 1) 일간 수익률 계산\ndef compute_returns(price_data: pd.DataFrame) -&gt; pd.DataFrame:\n    return price_data.pct_change().dropna(how='all')\n\ncrypto_returns = compute_returns(crypto_prices)\n\n# 2) 롤링 상관계수 계산\ndef rolling_correlation(returns: pd.DataFrame, window: int) -&gt; pd.DataFrame:\n    \"\"\"\n    returns: (date x tickers) DataFrame\n    window:  rolling window size (days)\n    \n    returns.rolling(window).corr() 결과는\n      - MultiIndex (date, ticker1)\n      - columns = ticker2\n    형태를 가집니다.\n    \"\"\"\n    corr_rolling = returns.rolling(window).corr()\n    return corr_rolling\n\n# 3) 날짜별 상관행렬을 모아서 평균 상관행렬을 산출\ndef average_correlation_matrix(returns: pd.DataFrame, window: int) -&gt; pd.DataFrame:\n    \"\"\"\n    - returns.rolling(window).corr() 결과를 사용\n    - 각 날짜별 (티커 x 티커) 상관행렬을 합산 후, 날짜 개수로 나누어 평균\n    \"\"\"\n    corr_rolling = rolling_correlation(returns, window)\n    \n    # MultiIndex에서 날짜(level=0) 목록을 추출\n    unique_dates = corr_rolling.index.get_level_values(0).unique()\n    tickers = returns.columns\n    \n    # 상관행렬 누적 합을 위한 (티커 x 티커) 형태의 빈 DataFrame\n    sum_matrix = pd.DataFrame(0.0, index=tickers, columns=tickers)\n    count = 0\n    \n    for date in unique_dates:\n        # (ticker1 x ticker2) 형태를 얻기 위해 xs(date, level=0)\n        date_corr = corr_rolling.xs(date, level=0)\n        # date_corr.index = ticker1, date_corr.columns = ticker2\n        \n        # 혹시 일부 티커에 대한 데이터가 누락되었을 경우를 대비하여 reindex\n        date_corr = date_corr.reindex(index=tickers, columns=tickers)\n        \n        # 날짜별 상관행렬(N x N)을 모두 누적\n        if date_corr.notna().all().all():\n            sum_matrix += date_corr.fillna(0.0)\n            count += 1\n    \n    # 평균 계산 (count가 0이 되지 않는다고 가정)\n    mean_matrix = sum_matrix / count\n    \n    return mean_matrix\n\n# 4) 롤링 상관계수 평균 계산\nrolling_corr_results = {}\nfor window in [30, 60, 90]:\n    mean_corr_matrix = average_correlation_matrix(crypto_returns, window)\n    rolling_corr_results[window] = mean_corr_matrix\n\n\nbinance supports: ['BTC/USDT', 'ETH/USDT', 'XRP/USDT', 'SOL/USDT', 'LINK/USDT', 'ADA/USDT', 'TRX/USDT', 'DOGE/USDT']\nkraken supports: ['BTC/USDT', 'ETH/USDT', 'USDT/USD', 'XRP/USDT', 'SOL/USDT', 'LINK/USDT', 'ADA/USDT', 'DOGE/USDT']\nbitfinex supports: ['BTC/USDT', 'ETH/USDT', 'USDT/USD', 'XRP/USDT', 'SOL/USDT', 'LINK/USDT', 'ADA/USDT', 'TRX/USDT', 'DOGE/USDT']\npoloniex supports: ['BTC/USDT', 'ETH/USDT', 'XRP/USDT', 'SOL/USDT', 'LINK/USDT', 'ONDO/USDT', 'ADA/USDT', 'TRX/USDT', 'DOGE/USDT']\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# 모든 행과 열이 출력되도록 설정\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.expand_frame_repr', False)\n\n# 결과 출력 및 시각화\nfor window, result in rolling_corr_results.items():\n    # 상관 행렬을 DataFrame으로 변환\n    result_df = result.dropna(how='all')\n    print(f\"\\n[Window = {window} days] Mean Correlation Matrix\\n\", result_df)\n    \n    # 히트맵 시각화\n    plt.figure(figsize=(10, 8))\n    \n    # 대각선 요소를 마스킹\n    mask = np.triu(np.ones(result_df.shape, dtype=bool))\n    \n    sns.heatmap(result_df, annot=True, cmap='coolwarm', center=0, mask=mask)\n    plt.title(f'Mean Rolling Correlation Matrix (Window Size: {window} days)')\n    plt.show()\n\n\n\n[Window = 30 days] Mean Correlation Matrix\n            BTC/USDT  ETH/USDT  XRP/USDT  SOL/USDT  LINK/USDT  ADA/USDT  TRX/USDT  DOGE/USDT  USDT/USD  ONDO/USDT\nBTC/USDT   1.000000  0.799026  0.571408  0.742676   0.685758  0.705122  0.382635   0.813508  0.453062   0.349196\nETH/USDT   0.799026  1.000000  0.560926  0.704027   0.738149  0.719166  0.380325   0.747406  0.287540   0.379395\nXRP/USDT   0.571408  0.560926  1.000000  0.555322   0.571433  0.654386  0.316579   0.593454  0.168919   0.231349\nSOL/USDT   0.742676  0.704027  0.555322  1.000000   0.674792  0.684847  0.319929   0.695521  0.256255   0.302730\nLINK/USDT  0.685758  0.738149  0.571433  0.674792   1.000000  0.758198  0.312524   0.670449  0.235560   0.416411\nADA/USDT   0.705122  0.719166  0.654386  0.684847   0.758198  1.000000  0.414416   0.725032  0.259014   0.326216\nTRX/USDT   0.382635  0.380325  0.316579  0.319929   0.312524  0.414416  1.000000   0.367991  0.185106   0.113894\nDOGE/USDT  0.813508  0.747406  0.593454  0.695521   0.670449  0.725032  0.367991   1.000000  0.316094   0.348878\nUSDT/USD   0.453062  0.287540  0.168919  0.256255   0.235560  0.259014  0.185106   0.316094  1.000000   0.239797\nONDO/USDT  0.349196  0.379395  0.231349  0.302730   0.416411  0.326216  0.113894   0.348878  0.239797   1.000000\n\n\n\n\n\n\n\n\n\n\n[Window = 60 days] Mean Correlation Matrix\n            BTC/USDT  ETH/USDT  XRP/USDT  SOL/USDT  LINK/USDT  ADA/USDT  TRX/USDT  DOGE/USDT  USDT/USD  ONDO/USDT\nBTC/USDT   1.000000  0.795085  0.519163  0.740575   0.665564  0.688028  0.317917   0.814903  0.443900   0.343188\nETH/USDT   0.795085  1.000000  0.500095  0.688051   0.711334  0.691409  0.307365   0.727155  0.292622   0.368091\nXRP/USDT   0.519163  0.500095  1.000000  0.506107   0.539207  0.632774  0.270189   0.542379  0.132053   0.203709\nSOL/USDT   0.740575  0.688051  0.506107  1.000000   0.652499  0.667691  0.272871   0.687130  0.238814   0.282382\nLINK/USDT  0.665564  0.711334  0.539207  0.652499   1.000000  0.742219  0.258079   0.641447  0.219591   0.397010\nADA/USDT   0.688028  0.691409  0.632774  0.667691   0.742219  1.000000  0.367565   0.708010  0.239892   0.307549\nTRX/USDT   0.317917  0.307365  0.270189  0.272871   0.258079  0.367565  1.000000   0.306786  0.154952   0.087516\nDOGE/USDT  0.814903  0.727155  0.542379  0.687130   0.641447  0.708010  0.306786   1.000000  0.313379   0.329382\nUSDT/USD   0.443900  0.292622  0.132053  0.238814   0.219591  0.239892  0.154952   0.313379  1.000000   0.239723\nONDO/USDT  0.343188  0.368091  0.203709  0.282382   0.397010  0.307549  0.087516   0.329382  0.239723   1.000000\n\n\n\n\n\n\n\n\n\n\n[Window = 90 days] Mean Correlation Matrix\n            BTC/USDT  ETH/USDT  XRP/USDT  SOL/USDT  LINK/USDT  ADA/USDT  TRX/USDT  DOGE/USDT  USDT/USD  ONDO/USDT\nBTC/USDT   1.000000  0.793500  0.477055  0.742082   0.647763  0.679408  0.271705   0.811226  0.430280   0.340976\nETH/USDT   0.793500  1.000000  0.453537  0.688858   0.698125  0.672898  0.245730   0.709919  0.286290   0.366230\nXRP/USDT   0.477055  0.453537  1.000000  0.459386   0.506985  0.611727  0.234191   0.497992  0.104102   0.192324\nSOL/USDT   0.742082  0.688858  0.459386  1.000000   0.635791  0.653999  0.251416   0.684731  0.236916   0.277513\nLINK/USDT  0.647763  0.698125  0.506985  0.635791   1.000000  0.726979  0.217451   0.614228  0.193642   0.386709\nADA/USDT   0.679408  0.672898  0.611727  0.653999   0.726979  1.000000  0.330674   0.693851  0.225266   0.298088\nTRX/USDT   0.271705  0.245730  0.234191  0.251416   0.217451  0.330674  1.000000   0.249737  0.129630   0.080927\nDOGE/USDT  0.811226  0.709919  0.497992  0.684731   0.614228  0.693851  0.249737   1.000000  0.301838   0.321657\nUSDT/USD   0.430280  0.286290  0.104102  0.236916   0.193642  0.225266  0.129630   0.301838  1.000000   0.242524\nONDO/USDT  0.340976  0.366230  0.192324  0.277513   0.386709  0.298088  0.080927   0.321657  0.242524   1.000000",
    "crumbs": [
      "교육",
      "금융",
      "Correlation in Cryptos"
    ]
  },
  {
    "objectID": "correlation_crypto.html#토론-discussions",
    "href": "correlation_crypto.html#토론-discussions",
    "title": "Correlation in Cryptos",
    "section": "토론 (Discussions)",
    "text": "토론 (Discussions)\n\n\n\n\n\n\nImportant\n\n\n\n변수들의 관찰 주기 (단기? 장기?)에 따라 또는 관찰 시기 (10년전? 지금?)에 따라 변수들 간의 선형관계는 유지되지 않을 수 있습니다. 2025년 현재 비트코인 (BTC) 가격은 시장 심리, 규제 변화, 기술적 요인 등에 크게 영향을 받고 있습니다.\n\n\n\n상관계수가 낮은 암호화폐 자산 조합을 식별하고, 헤지 투자 전략을 논의.\n특정 암호화폐 간의 높은 상관관계가 나타나는 이유 및 그에 따른 리스크 분석.\n\n2024년 3월부터 2025년 2월까지 암호화폐 시장에 큰 영향을 미친 주요 변화 시기와 원인:\n\n비트코인 반감기 (2024년 4월 20일): 비트코인 채굴 보상이 6.25 BTC에서 3.125 BTC로 절반으로 감소. 이는 비트코인의 공급 감소로 이어져 가격에 상승 압력을 가함.\n비트코인 ETF 자금 유입 증가 (2024년 10월): 비트코인 ETF로의 지속적인 자금 유입이 관찰됨. 10월까지 ETF 투자자들이 총 345,200 BTC(200억 달러 이상의 가치)를 매입.\n트럼프의 대통령 당선 (2024년 11월): 도널드 트럼프가 “암호화폐 대통령”이 되겠다는 공약을 내세우며 당선됨. 이는 암호화폐 시장에 대한 긍정적인 기대감을 불러일으킴.\nEU의 암호화폐 시장 규제(MiCA) 전면 시행 (2024년 12월 30일): 유럽연합에서 암호화폐 시장 규제(MiCA)가 전면 시행됨. 이로 인해 EU 전역에서 암호화폐 서비스 제공업체들에 대한 통일된 규제 프레임워크가 적용되기 시작.\n트럼프의 암호화폐 정책 발표 (2025년 1월): 트럼프 대통령이 취임 후 미국을 “암호화폐의 수도”로 만들겠다는 계획을 발표함. 여기에는 비트코인 전략적 비축 등의 아이디어가 포함됨.\n\nstars and bins에서 위의 변화 시기가 bins 역할을 한다는 가정하여, 기간 stars을 다음과 같이 나누어 상관관계를 conditional 해 본다.\n\n20224년 3월 1일 (관측기간 시작일) - 2024년 4월 20일\n2024년 4월 21일 - 2024년 9월 30일\n2024년 10월 1일 - 2024년 11월 5일\n2024년 11월 6일 - 2024년 12월 31일\n2025년 1월 1일 - 2025년 2월 28일 (관측기간 종료일)\n\n변화를 \\(Z\\)로 표기했다면, covariance decomposition formula에 의해,\n(추후 계속)",
    "crumbs": [
      "교육",
      "금융",
      "Correlation in Cryptos"
    ]
  },
  {
    "objectID": "correlation_crypto.html#부록-conditioning-theorems-in-probability-theory",
    "href": "correlation_crypto.html#부록-conditioning-theorems-in-probability-theory",
    "title": "Correlation in Cryptos",
    "section": "부록: Conditioning Theorems in Probability Theory",
    "text": "부록: Conditioning Theorems in Probability Theory\n\nAdam’s Law: Smoothing Property of Conditional Expectation\nAlso known as the Law of Total Expectation or Law of Iterated Expectations.\nIf \\(X\\) is a random variable whose expectation \\(E(X)\\) is defined and \\(Z\\) is any random variable defined on the same probability space, then: \\[E(X) = E(E(X|Z)).\\]\nA conditional expectation can be viewed as a Radon–Nikodym derivative, making the tower property a direct consequence of the chain rule for conditional expectations.\nA special discrete case: If \\(\\{Z_{i}\\}\\) is a finite or countable partition of the sample space, then: \\[E(X) = \\sum_{i} E(X \\mid Z_i)P(Z_i).\\]\n\n\nEve’s Law: Variance Decomposition Formula\nKnown as the Conditional Variance Formula or the Law of Iterated Variances.\nIf \\(X\\) and \\(Y\\) are random variables defined on the same probability space, and if \\(Y\\) has finite variance, then:\n\\[\\operatorname{Var}(Y)=E[\\operatorname{Var}(Y\\mid X)]+\\operatorname{Var}(E[Y\\mid X]).\\]\nThis is a special case of the covariance decomposition formula.\nApplications\n\n분산 = “(조건부 분산)의 평균” + “(조건부 평균)의 분산”\nAnalysis of Variance (ANOVA): Variability in \\(Y\\) splits into an “unexplained” within-group variance and an “explained” between-group variance. The F-test examines if the explained variance is significantly large, indicating a meaningful effect of \\(X\\) on \\(Y\\).\nLinear Regression Models: The proportion of explained variance is measured as \\(R^2\\). For simple linear regression (single predictor), \\(R^2\\) equals the squared Pearson correlation coefficient between \\(X\\) and \\(Y\\).\nMachine Learning and Bayesian Inference: In many Bayesian and ensemble methods, one decomposes prediction uncertainty via the law of total variance. For a Bayesian neural network with random parameters \\(\\theta\\): \\(\\operatorname {Var} (Y)=\\operatorname {E} {\\bigl [}\\operatorname {Var} (Y\\mid \\theta ){\\bigr ]}+\\operatorname {Var} {\\bigl (}\\operatorname {E} [Y\\mid \\theta ]{\\bigr )}\\) often referred to as “aleatoric” (within-model) vs. “epistemic” (between-model) uncertainty\nInformation Theory: For jointly Gaussian \\((X,Y)\\), the fraction \\(\\operatorname {Var} (\\operatorname {E} [Y\\mid X])/\\operatorname {Var} (Y)\\) relates directly to the mutual information \\(I(Y;X)\\). In non-Gaussian settings, a high explained-variance ratio still indicates significant information about Y contained in X\n\nExample 1 (Exam Scores): Suppose students’ exam scores vary between two classrooms. The variance of all scores (\\(Y\\)) can be decomposed into the variance within classrooms (unexplained) and the variance between classroom averages (explained), reflecting differences in teaching quality or resources.\nExample 2 (Mixture of Two Gaussians): Consider \\(Y\\) as a mixture of two normal distributions, where the mixing distribution is Bernoulli with parameter \\(p\\). Suppose:\n\\[Y \\mid (X=0) \\sim N(\\mu_0, \\sigma_0^2), \\quad Y \\mid (X=1) \\sim N(\\mu_1, \\sigma_1^2).\\]\nThen the law of total variance gives:\n\\[\\operatorname{Var}(Y) = p\\sigma_1^2 + (1-p)\\sigma_0^2 + p(1-p)(\\mu_1 - \\mu_0)^2.\\]\n\n\nCovariance Decomposition Formula\nKnown as the Law of Total Covariance or Conditional Covariance Formula.\nIf \\(X\\), \\(Y\\), and \\(Z\\) are random variables defined on the same probability space, with finite covariance between \\(X\\) and \\(Y\\), then:\n\\[\\operatorname{cov}(X,Y)=E[\\operatorname{cov}(X,Y\\mid Z)]+\\operatorname{cov}(E[X\\mid Z],E[Y\\mid Z]).\\]\nThis relationship is a particular instance of the general Law of Total Cumulance and is crucial for analyzing dependencies among variables conditioned on a third variable or groupings.\n\n\nBias-Variance Decomposition of MSE\nKey: The Bias-Variance Decomposition emphasizes the trade-off between making \\(\\hat{Y}\\) reliably close to its own expected value (low variance) and aligning that expected value with the true target \\(\\mathbb{E}[Y]\\) (low bias).\nIn many estimation or prediction settings, we have a random outcome \\(Y\\) and an estimator (or model prediction) \\(\\hat{Y}\\). The Mean Squared Error (MSE) of \\(\\hat{Y}\\) is:\n\\[\n\\mathrm{MSE}(\\hat{Y})\n= \\mathbb{E}\\bigl[(Y - \\hat{Y})^2\\bigr].\n\\]\nIf we decompose \\(\\hat{Y}\\) around its expected value, we can split MSE into a bias term and a variance term (plus any irreducible noise in certain contexts). Formally, assume\n\\[\n\\hat{Y} = f(X) + \\text{estimation noise},\n\\quad\nY = f(X) + \\varepsilon,\n\\]\nwhere \\(\\varepsilon\\) is an irreducible error term with mean zero (e.g., observational or inherent noise). Then:\n\\[\n\\begin{aligned}\n\\mathrm{MSE}(\\hat{Y})\n&= \\mathbb{E}\\bigl[(Y - \\hat{Y})^2\\bigr]\\\\\n&= \\underbrace{\\mathbb{E}\\Bigl[\\bigl(\\hat{Y} - \\mathbb{E}[\\hat{Y}]\\bigr)^2\\Bigr]}_{\\text{Variance}(\\hat{Y})}\n\\;+\\;\n\\underbrace{\\Bigl(\\mathbb{E}[\\hat{Y}] \\;-\\; \\mathbb{E}[Y]\\Bigr)^2}_{\\text{Bias}(\\hat{Y})^2}\n\\;+\\;\n\\underbrace{\\mathbb{E}\\bigl[\\varepsilon^2\\bigr]}_{\\text{Irreducible noise}},\n\\end{aligned}\n\\]\nwhere:\n\nVariance: \\(\\text{Var}(\\hat{Y})\\) represents how much \\(\\hat{Y}\\) fluctuates around its own mean.\nBias: \\(\\text{Bias}(\\hat{Y})^2\\) measures how far \\(\\hat{Y}\\) (on average) deviates from the true mean \\(\\mathbb{E}[Y]\\).\nIrreducible noise: \\(\\mathbb{E}[\\varepsilon^2]\\) is the part of the error that cannot be reduced by any estimator.\n\nIn a strictly theoretical sense (when \\(\\varepsilon\\) is embedded in \\(Y\\)), one often writes the Bias-Variance decomposition as:\n\\[\n\\mathrm{MSE}(\\hat{Y})\n= \\underbrace{\\mathrm{Var}(\\hat{Y})}_{\\text{Variance term}}\n\\;+\\;\n\\underbrace{\\mathrm{Bias}(\\hat{Y})^2}_{\\text{Bias term}}.\n\\]\nHere, if there is additional irreducible noise, it appears as a separate constant term. This decomposition closely aligns with the Law of Total Variance (Eve’s Law) in the sense that the total mean squared difference can be split into a “spread around the estimator’s mean” plus the “squared difference of that mean from the true value,” mirroring how variance itself decomposes into conditional components.",
    "crumbs": [
      "교육",
      "금융",
      "Correlation in Cryptos"
    ]
  },
  {
    "objectID": "finance_existence.html",
    "href": "finance_existence.html",
    "title": "Rethinking Asset Existence",
    "section": "",
    "text": "0.1 Conventional View: Financial Assets as Risk-Reward Instruments\nIn classical asset pricing theory, financial assets are presumed to exist to reward investors for bearing risk. Through mechanisms such as arbitrage and diversification, idiosyncratic risk is mitigated, and equilibrium returns are determined by each asset’s exposure to systematic risk. This framework justifies the existence of any asset through the promise of a fair, risk-adjusted return. Asset prices thus reflect not market power or concentration, but the compensated cost of risk.\nThis view embeds a philosophical logic of fairness: that market participants are compensated proportionally for risk undertaken, and that capital flows dynamically toward efficiency. Such a model assumes fluid reallocations, reversibility of state transitions, and the dominance of fundamentals over flow mechanics.\n\n\n0.2 TBTF View: Financial Assets as Power-Concentrating Instruments\nIn contrast, the TBTF framework rejects this equilibrium-based justification. It posits that some financial assets (especially in the public secondary market) persist not because of their risk-return characteristics, but because they are capital sinks—entities with entrenched market dominance, self-reinforcing narratives, and structural insulation from volatility.\nThese assets do not compensate for uncertainty—they absorb uncertainty by offering investors a sense of safety rooted in scale, liquidity, and systemic centrality. TBTF assets act as narrative anchors in uncertain markets: not because they offer greater upside, but because they symbolize continuity, legitimacy, and collective default.\nThis redefines the rationale for capital allocation: from return-maximization to structure-maximization. Investors increasingly allocate not to “the best opportunities”, but to “the safest dominant incumbents”. The performance of TBTF strategies arises not from risk pricing, but from market structure engineering—index inclusion, ETF flows, regulatory inertia, and the psychology of too-big-to-fail.\n\n\n0.3 Philosophical and Mathematical Justification\nThis philosophical inversion has deep implications:\n\nExistential Justification: Financial assets exist not to reward risk-takers but to retain power. Their performance is a function of initial endowment, not market responsiveness.\nStatistical Structure: The return-generating process resembles a non-ergodic, non-reversible Markov chain, where top and exit states become quasi-absorbing.\nMathematical Inertia: TBTF assets exhibit second-moment stationarity, even when first moments shift. This aligns with the capital inertia hypothesis: structure is stable, returns are residual.\nNarrative Economics: Echoing Shiller and Gennaioli–Shleifer, capital allocation is driven less by fundamentals than by narratives of dominance, size, and safety.\n\n\n\n0.4 Implication for Market Theory\nThe TBTF perspective challenges the canonical assumptions of:\n\nArbitrage efficiency: capital does not equalize returns across assets\nRisk-reward equilibrium: structural power trumps systematic beta\nDynamic reallocation: capital flows become locked, not optimized\nAllocative neutrality: flow-based reinforcements create concentration spirals\n\nIn this view, financial markets do not price risk—they price dominance. The empirical success of TBTF strategies is not an anomaly; it is a manifestation of structural inertia, capital path dependence, and the self-fulfilling logic of institutional safety.",
    "crumbs": [
      "교육",
      "금융",
      "Rethinking Asset Existence"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GitSAM 교육자료",
    "section": "",
    "text": "GitSAM 교육\n\n구체적이고 명확한 사고로 미래를 여는 리더 (leader)를 위한 교육\n\n현대 사회에서는 고위직 엘리트들이 공적인 자리에서 “모든 수사는 법과 절차에 따라 공정하게 진행됩니다.”와 같이 당연한 원칙만을 반복하며, 구체적인 정보를 회피하는 발언이 빈번합니다. 이러한 항진명제식 발언은 정보 비대칭을 이용한 책임 회피의 전략으로, 국민들에게 불투명한 소통을 강요합니다. GitSAM 학원은 이러한 사회적 모호함과 회피의 태도를 극복하고, 학생들이 문제를 단순히 풀어내는 것을 넘어 문제의 본질을 해석하고, 정의하며, 창의적인 해결책을 도출하는 사고력을 갖추도록 하는 교육을 지향합니다.\nGitSAM 교육은 전통적인 암기 위주의 수업 방식을 탈피하여, 현실 문제를 직접 경험하고 분석하는 실습 중심의 융합 교육을 제공합니다. 교수·박사 등 전문 멘토들이 학생 한 사람 한 사람의 사고 흐름을 꼼꼼히 지도하며, 주가, 소득, 날씨 등 실제 데이터를 바탕으로 문제를 모델링하고 해석하는 방법을 체득하게 합니다. 이로써 단기적인 점수 향상을 넘어, 장기적으로 사회의 복잡한 문제를 스스로 정의하고 해결할 수 있는 인재로 거듭날 수 있도록 돕습니다.\n특히 GitSAM은 최신 기술을 적극 활용합니다. Python, Desmos 등의 도구로 수학과 과학의 추상적 개념을 시각화하고, 실전 응용 모형을 통해 산업 현장에서 활용되는 모델들을 구체적으로 이해하도록 돕습니다. 또한, 앞으로 도입될 AI와 블록체인 기반의 평가 시스템은 학생들의 사고 과정을 정량적, 객관적으로 기록하고 평가하여, 창의성, 논리성, 윤리적 책임감을 강화할 예정입니다.\nGitSAM에서는 대수학, 기하학, 미적분학, 확률통계학을 비롯해, 고전역학, 상대성이론, 양자역학 등 심도 있는 과학 교육과 화폐금융론, 재무론, 리더십, 거시·미시경제학, 그리고 근대 문학에 이르기까지 다양한 분야를 융합하여 교육합니다. 이를 통해 학생들은 여러 학문이 하나의 통합된 사고 도구로 작용함을 깨닫고, 복합적 문제 해결 능력과 리더십을 기르게 됩니다.\nGitSAM의 교육 철학은 “복잡한 세상은 수학적 언어로 구체화할 수 있으며, 현실 문제를 융합적으로 해결하기 위해서는 명확한 사고와 구체적인 실행력이 필요하다”는 데 있습니다. 저희는 단순한 암기가 아니라, 문제를 해석하고 해결하는 능력을 통해, 미래 사회에서 지속 가능한 ’지적 자산’을 쌓는 것이 진정한 교육의 목표라고 믿습니다.\n이와 같이 GitSAM은 기존의 입시 중심 교육이나 모호한 책임 회피 발언과는 달리,\n구체적 근거와 명확한 사고를 바탕으로, 학생들이 스스로 세상을 해석하고 변화시킬 수 있는 힘을 기르도록 돕는 혁신적 교육 프로그램을 운영하고 있습니다.\n앞으로도 GitSAM은 혁신적 강의 방식, 융합 교육 프로그램, AI를 활용한 Prompt-based Evaluation 시스템 및 학습 과정을 블록체인에 저장하는 Proof of Thinking 시스템을 통해, 학생들이 단기적 성적 향상을 넘어, 장기적으로 사회에 기여할 수 있는 인재로 성장할 수 있도록 지속적인 노력을 기울일 것입니다.",
    "crumbs": [
      "교육",
      "GitSAM 교육자료"
    ]
  },
  {
    "objectID": "labor_decoupling.html",
    "href": "labor_decoupling.html",
    "title": "노동: 기여 vs. 분배",
    "section": "",
    "text": "Neoclassical Growth Model(신고전적 성장 모형)은 경제성장의 핵심 지표로 총생산량(Gross Domestic Product, GDP)을 중시한다. 이 모형에서는 생산의 핵심 요소를 다음과 같이 세 가지로 구분한다 (Solow 1956).\n\n총 노동량(Labor Input, \\(L\\))\n\n총 자본량(Capital Input, \\(K\\))\n\n노동증강기술(Labor-Augmenting Technology, \\(A\\))\n\n그러나 자본량(\\(K\\))은 직접적으로 측정하기 어려우며, 일반적으로 Perpetual Inventory Method(영속적 재고법)과 같은 방식을 활용하여 추정한다 (Hall and Jones 1999). 이에 더하여, 노동증강기술(\\(A\\))은 더욱 측정이 어려운 정성적 요소이기 때문에, 많은 연구에서는 Constant Returns to Scale(규모에 대한 수익 불변)을 가정한 Cobb-Douglas 생산함수를 이용하고, 추정 과정에서 발생하는 오차항(Error Term)을 기술진보(\\(A\\))로 대체하기도 한다 (Barro 1991). 이러한 방식이 현실을 얼마나 정확하게 반영하는지는 논란의 여지가 크다.\n모형(Model)은 본질적으로 이론적 가설이며, 경제성장에 대한 다양한 견해를 반영할 수 있는 도구이기 때문에, 각자의 가설과 주장을 펼치는 것은 학문적으로 허용될 수 있다. 그러나 문제의 본질은 해당 모형이 현실을 얼마나 정확하게 설명하고 예측할 수 있는지에 있다. 현실을 반영하지 못하고, 지속적으로 예측에 실패하는 경제성장 모형은 실질적으로 활용 가치가 떨어질 수밖에 없다 (Stiglitz 2017).\n경제성장과 관련하여 실용적인 측면에서 가장 중요한 변수는 총 노동량(\\(L\\))이라고 볼 수 있다. 이는 비교적 객관적인 측정이 가능하며, 경제의 실질적 변화를 분석하는 데 유용하다. 이러한 맥락에서 노동 생산성(Productivity)과 노동 분배성(Distributivity)라는 두 가지 핵심 개념을 살펴볼 필요가 있다.",
    "crumbs": [
      "교육",
      "경제",
      "노동: 기여 vs. 분배"
    ]
  },
  {
    "objectID": "labor_decoupling.html#서론",
    "href": "labor_decoupling.html#서론",
    "title": "노동: 기여 vs. 분배",
    "section": "",
    "text": "Neoclassical Growth Model(신고전적 성장 모형)은 경제성장의 핵심 지표로 총생산량(Gross Domestic Product, GDP)을 중시한다. 이 모형에서는 생산의 핵심 요소를 다음과 같이 세 가지로 구분한다 (Solow 1956).\n\n총 노동량(Labor Input, \\(L\\))\n\n총 자본량(Capital Input, \\(K\\))\n\n노동증강기술(Labor-Augmenting Technology, \\(A\\))\n\n그러나 자본량(\\(K\\))은 직접적으로 측정하기 어려우며, 일반적으로 Perpetual Inventory Method(영속적 재고법)과 같은 방식을 활용하여 추정한다 (Hall and Jones 1999). 이에 더하여, 노동증강기술(\\(A\\))은 더욱 측정이 어려운 정성적 요소이기 때문에, 많은 연구에서는 Constant Returns to Scale(규모에 대한 수익 불변)을 가정한 Cobb-Douglas 생산함수를 이용하고, 추정 과정에서 발생하는 오차항(Error Term)을 기술진보(\\(A\\))로 대체하기도 한다 (Barro 1991). 이러한 방식이 현실을 얼마나 정확하게 반영하는지는 논란의 여지가 크다.\n모형(Model)은 본질적으로 이론적 가설이며, 경제성장에 대한 다양한 견해를 반영할 수 있는 도구이기 때문에, 각자의 가설과 주장을 펼치는 것은 학문적으로 허용될 수 있다. 그러나 문제의 본질은 해당 모형이 현실을 얼마나 정확하게 설명하고 예측할 수 있는지에 있다. 현실을 반영하지 못하고, 지속적으로 예측에 실패하는 경제성장 모형은 실질적으로 활용 가치가 떨어질 수밖에 없다 (Stiglitz 2017).\n경제성장과 관련하여 실용적인 측면에서 가장 중요한 변수는 총 노동량(\\(L\\))이라고 볼 수 있다. 이는 비교적 객관적인 측정이 가능하며, 경제의 실질적 변화를 분석하는 데 유용하다. 이러한 맥락에서 노동 생산성(Productivity)과 노동 분배성(Distributivity)라는 두 가지 핵심 개념을 살펴볼 필요가 있다.",
    "crumbs": [
      "교육",
      "경제",
      "노동: 기여 vs. 분배"
    ]
  },
  {
    "objectID": "labor_decoupling.html#본론",
    "href": "labor_decoupling.html#본론",
    "title": "노동: 기여 vs. 분배",
    "section": "본론",
    "text": "본론\n\n1. 노동생산성(Labor Productivity)의 변화\n노동생산성은 단위 노동량당 산출량을 의미하며, 다음과 같이 정의할 수 있다 (Acemoglu et al. 2014).\n\\[\n\\text{노동생산성} = \\frac{Y_t}{L_t}\n\\]\n여기서 \\(Y_t\\)는 총생산량(GDP)의 대표적 대리 변수(proxy)이며, \\(L_t\\)는 총 노동량을 나타낸다. 각각의 변수는 다음과 같이 측정하였다.\n\n총생산량 Proxy (\\(Y_t\\)): Real Gross Domestic Product per Capita\n\n장기적으로 볼 때, 총생산량은 갈수록 증가하는 추세를 보였다.\n\n총노동량 Proxy (\\(L_t\\)): Hours Worked by Full-Time and Part-Time Employees\n\n총 노동시간\n이 역시 갈수록 증가하는 경향을 보였다.\n\n\n이로부터 계산한 노동생산성(\\(Y_t/L_t\\)) 역시 시간에 따라 증가하는 함수임을 확인할 수 있다. 즉, 경제가 성장함에 따라 노동자 1인당 산출하는 생산량은 꾸준히 증가해 왔다.\n\n\n2. 노동분배성(Labor Distributivity)의 변화\n노동생산성이 증가하면, 일반적으로 노동자에게 돌아가는 보상 역시 증가해야 한다는 것이 경제적 정의(economic fairness)와 균형적 성장(balanced growth)의 핵심 원칙이다 (Piketty 2014). 그렇다면 노동 분배성(Labor Distributivity) 역시 시간이 갈수록 증가하였을까?\n이를 확인하기 위해, 노동자의 시간당 실질 중위임금 (Real Median hourly Wage) 을 대리 변수로 활용하였다.\n\n노동에 분배된 총량 Proxy (\\(X_t\\)): Employed Full-Time: Median Usual Weekly Real Earnings\n\n장기적으로 증가하는 경향을 보였으나, 변동성이 존재하였다.\n\n노동분배성 Proxy (\\(\\frac{X_t}{L_t}\\))\n\nReal Median hourly Wage = median real wage per hour\n노동생산성이 증가하는 것과는 대조적으로, 노동자에게 분배된 시간당 소득의 중위값 (\\(X_t/L_t\\))은 오히려 갈수록 감소하는 경향을 보였다.\n\n\n이를 시각적으로 명확히 비교하기 위해, 노동분배성을 \\(\\frac{X_t}{L_t} \\times 100\\)으로 스케일링하여 그래프로 나타냈다.\nFRED Graph (1980년 이후 노동생산성과 노동분배성 비교)\n그래프를 살펴보면, 1980년대 이후 노동생산성과 노동분배성 사이의 격차가 점점 더 커지고 있음을 확인할 수 있다. 이는 무엇을 의미하는가? 노동생산성이 증가함에도 불구하고, 노동자에게 돌아가는 보상의 증가 속도가 이에 미치지 못하고 있다는 점을 시사한다. 다시 말해, “노동생산성과 노동자 보상의 분리 현상(decoupling)”이 지속적으로 심화되고 있음을 보여준다 (Stansbury and Summers 2020). 이러한 현상은 장기적으로 경제적 불평등(economic inequality)을 악화시키는 주요 원인 중 하나로 작용할 수 있다 (Stiglitz 2012).\n\n\n3. 노동분배성 감소의 원인\n노동분배성(Labor distributivity)과 총노동소득분배율 (Labor share)의 감소에 대한 경제학적 분석은 다양한 요인을 고려해야 하지만, 주요한 원인으로 다음 두 가지가 지적된다.\n\nAI 기술 진보(Technological Progress)\n\nOECD의 분석에 따르면, 인공지능(AI)과 같은 첨단 기술의 발전은 특정 고숙련 노동자(high-skilled workers)에게는 유리하게 작용하지만, 그렇지 않은 노동자들에게는 불리한 영향을 미칠 수 있다 (OECD 2018). 이는 노동시장 내 임금 불평등(wage inequality)을 심화시키는 요인으로 작용한다.\n\n선진국 주도형 세계화(Globalization)\n\n생산 공정의 해외 이전(offshoring)과 국제 무역의 확대는 저임금 노동력을 활용한 생산 방식을 증가시켜, 선진국 내 노동자의 소득 증가율을 둔화시키는 결과를 초래할 수 있다 (Autor, Dorn, and Hanson 2013).\n\n\n\n\n4. 노동참여율(Labor Force Participation Rate)의 변화\n또한, 노동소득 분배율 감소와 노동시장 변화를 분석하기 위해, 노동참여율(Labor Force Participation Rate)을 함께 고려할 필요가 있다. 이를 위해 Current Population Survey (CPS)에서 조사한 Labor Force Participation Rate를 분석하였다.\n\nCOVID-19 팬데믹이 발생한 2020년에는 노동참여율이 일시적으로 급락했으나, 이후 빠르게 정상 수준으로 회복되었다 (Coibion, Gorodnichenko, and Weber 2020).\n\n장기적으로는 완만한 하락세를 보이고 있으며, 이는 인구 고령화(demographic aging) 등의 요인과도 관련이 있을 것으로 추정된다 (Krueger 2017).",
    "crumbs": [
      "교육",
      "경제",
      "노동: 기여 vs. 분배"
    ]
  },
  {
    "objectID": "labor_decoupling.html#결론",
    "href": "labor_decoupling.html#결론",
    "title": "노동: 기여 vs. 분배",
    "section": "결론",
    "text": "결론\n생산에 대한 노동의 기여와 노동자 보상의 분리 현상(decoupling)은 실증적 데이터에 의해 명확히 확인된다. 노동생산성이 지속적으로 증가함에도 불구하고, 노동자에게 분배되는 소득 상대적으로 감소해 왔다 (Karabarbounis and Neiman 2014). 단위 노동량당 노동자에게 분배되는 소득도 감소해 왔다. 경제 전문가들은 이러한 현상의 주요 원인으로 AI 기술 진보와 선진국 주도형 세계화를 지목하고 있으며, 이는 장기적으로 노동시장 내 불평등 심화 및 경제적 불안정성을 초래할 가능성이 크다 (Milanovic 2016). 향후 연구에서는 노동분배성 (Labor distributivity) 회복을 위한 제도적 개선책을 추가적으로 검토할 필요가 있다.",
    "crumbs": [
      "교육",
      "경제",
      "노동: 기여 vs. 분배"
    ]
  },
  {
    "objectID": "regressions.html",
    "href": "regressions.html",
    "title": "Geometry of error space",
    "section": "",
    "text": "This short study is designed to explain the structural difference between Ordinary Least Squares (OLS) and the Generalized Method of Moments (GMM) from a geometric and mathematical standpoint, emphasizing their foundations in inner product spaces, with an analogy to Einstein’s Field Equations in general relativity. This analogy would be powerful not only for deep mathematical understanding but also for teaching estimation theory with geometric and physical intuition.",
    "crumbs": [
      "교육",
      "수학",
      "Geometry of error space"
    ]
  },
  {
    "objectID": "regressions.html#ols",
    "href": "regressions.html#ols",
    "title": "Geometry of error space",
    "section": "1. OLS",
    "text": "1. OLS\n\nOrthogonal Projection in Euclidean Space\nOLS solves the following problem:\n\\[\n\\hat{\\beta}_{OLS} = \\arg\\min_\\beta \\| y - X\\beta \\|_2^2 = (X^\\top X)^{-1} X^\\top y\n\\]\n\nThis corresponds to projecting \\(y\\) orthogonally onto the column space of \\(X\\).\nThe residual \\(\\varepsilon = y - X\\hat{\\beta}\\) satisfies:\n\n\\[\nX^\\top \\varepsilon = 0\n\\]\n\n\nInner Product and Geometry\n\nThe \\(L^2\\) norm used in OLS is induced by the standard Euclidean inner product:\n\n\\[\n\\langle u, v \\rangle = u^\\top v\n\\]\n\nThe distance function becomes:\n\n\\[\n\\| u \\|_2 = \\sqrt{u^\\top u}\n\\]\n\nThe set of parameter values yielding equal error defines a level set (isocurve):\n\n\\[\n\\{ \\beta \\mid \\| y - X\\beta \\|_2^2 = c \\} \\Rightarrow \\text{spheres in parameter space}\n\\]\n\nThis reflects Pythagorean geometry — the isocurves are circles (in 2D), spheres (in 3D), or hyperspheres in higher dimensions.",
    "crumbs": [
      "교육",
      "수학",
      "Geometry of error space"
    ]
  },
  {
    "objectID": "regressions.html#gmm",
    "href": "regressions.html#gmm",
    "title": "Geometry of error space",
    "section": "2. GMM",
    "text": "2. GMM\n\nWeighted Projection via Positive Definite Kernel\nGMM generalizes the idea by allowing estimation over a broader space defined by arbitrary moment conditions:\n\\[\n\\hat{\\theta}_{GMM} = \\arg\\min_\\theta \\left[ \\bar{g}_n(\\theta)^\\top W \\bar{g}_n(\\theta) \\right]\n\\]\nwhere:\n\n\\(\\bar{g}_n(\\theta) = \\frac{1}{n} \\sum_{i=1}^n g(Z_i, \\theta)\\)\n\\(W\\) is a positive definite weighting matrix, often an estimate of the optimal variance-covariance structure\n\n\n\nGeneralized Inner Product and Geometry\n\nGMM defines a new inner product:\n\n\\[\n\\langle u, v \\rangle_W = u^\\top W v \\quad \\text{with } W \\succ 0\n\\]\n\nThe corresponding norm is:\n\n\\[\n\\| u \\|_W = \\sqrt{u^\\top W u}\n\\]\n\nThe isocurves of the GMM objective:\n\n\\[\n\\{ \\theta \\mid \\bar{g}_n(\\theta)^\\top W \\bar{g}_n(\\theta) = c \\} \\Rightarrow \\text{ellipsoids in parameter space}\n\\]\nThus, unlike OLS, GMM does not treat all directions equally; the weighting matrix \\(W\\) skews the geometry so that errors in some directions are penalized more.",
    "crumbs": [
      "교육",
      "수학",
      "Geometry of error space"
    ]
  },
  {
    "objectID": "regressions.html#visual-and-geometric-summary",
    "href": "regressions.html#visual-and-geometric-summary",
    "title": "Geometry of error space",
    "section": "3. Visual and Geometric Summary",
    "text": "3. Visual and Geometric Summary\n\n\n\n\n\n\n\n\nConcept\nOLS\nGMM\n\n\n\n\nInner product\n\\(\\langle u, v \\rangle\\)\n\\(\\langle u, v \\rangle_W = u^\\top W v\\)\n\n\nNorm\nEuclidean (\\(L^2\\))\nMahalanobis-like (\\(W\\)-norm)\n\n\nIsocurve shape\nCircle / Sphere\nEllipse / Ellipsoid\n\n\nGeometry\nUniform in all directions\nAnisotropic (weighted directions)\n\n\n\n\nOLS minimizes error under the geometry of circles.\nGMM minimizes error under the geometry of ellipses.",
    "crumbs": [
      "교육",
      "수학",
      "Geometry of error space"
    ]
  },
  {
    "objectID": "regressions.html#einstein-field-equations-and-gmm-structural-analogy",
    "href": "regressions.html#einstein-field-equations-and-gmm-structural-analogy",
    "title": "Geometry of error space",
    "section": "4. Einstein Field Equations and GMM: Structural Analogy",
    "text": "4. Einstein Field Equations and GMM: Structural Analogy\nEinstein’s Field Equations (EFE) in general relativity:\n\\[\nG_{\\mu\\nu} = R_{\\mu\\nu} - \\frac{1}{2} R g_{\\mu\\nu} = 8\\pi T_{\\mu\\nu}\n\\]\nwhere:\n\n\\(T_{\\mu\\nu}\\): Stress-energy tensor, representing energy and matter (matter-energy distribution)\n\\(G_{\\mu\\nu}\\): Einstein tensor, encoding the curvature of spacetime (curvature)\n\n\\(R_{\\mu\\nu}\\): Ricci curvature tensor\n\\(R\\): Scalar curvature\n\\(g_{\\mu\\nu}\\): Metric tensor, defining the inner product in spacetime and governing geodesics\n\n\n\nAdditional Explanation\nIn EFE, the metric tensor \\(g_{\\mu\\nu}\\) defines how distances and angles are measured — it is the analogue of a positive definite kernel in GMM. The field equations determine how the geometry (curvature) of spacetime reacts to matter and energy. In this sense, spacetime is optimized or shaped in response to external inputs, just as GMM shapes its estimation space based on the kernel \\(W\\) and moment functions.\nThe level sets in general relativity are often visualized as lightcones — the surface separating causal influence from spacelike separation. Geometrically, a lightcone can be interpreted as the degenerate case of a conic section, where the quadric form\n\\[\nQ(x) = x^\\top g_{\\mu\\nu} x = 0\n\\]\nresults in a pair of intersecting lines: this represents all null (light-like) directions emanating from a point. These are the boundary cases between time-like and space-like intervals, analogous to the way ellipsoids in GMM collapse into degenerate forms under singular kernel matrices.\nThus, in both GMM and EFE, the shape and degeneracy of level sets encode deep information about the underlying structure — whether it is a statistical model or the geometry of spacetime.\n\n\nAnalogy to Estimation Frameworks\n\n\n\n\n\n\n\n\n\nFeature\nOLS\nGMM\nEFE (Physics)\n\n\n\n\nSpace\nEuclidean\nKernel-defined\nCurved spacetime\n\n\nInner product\n\\(I\\)\n\\(W\\) (kernel)\n\\(g_{\\mu\\nu}\\) (metric)\n\n\nProjection\nOrthogonal\nWeighted / Generalized\nEnergy-curvature balance\n\n\nLevel sets\nCircles\nEllipses\nLightcones / geodesics\n\n\n\nIn all three cases, the key structure-defining object (\\(I\\), \\(W\\), or \\(g_{\\mu\\nu}\\)) defines how vectors are compared, how error or curvature is measured, and how optimization or balance occurs.",
    "crumbs": [
      "교육",
      "수학",
      "Geometry of error space"
    ]
  },
  {
    "objectID": "regressions.html#conclusion",
    "href": "regressions.html#conclusion",
    "title": "Geometry of error space",
    "section": "5. Conclusion",
    "text": "5. Conclusion\n\nOLS and GMM are not just estimation techniques, but geometric procedures grounded in inner product space theory.\nOLS relies on Euclidean projection, yielding circular/spherical symmetry.\nGMM generalizes the space through a positive definite kernel, yielding elliptical contours and emphasizing certain directions.\nThis parallels how general relativity defines geometry via the metric tensor, adapting the very notion of measurement to the structure of the system.",
    "crumbs": [
      "교육",
      "수학",
      "Geometry of error space"
    ]
  },
  {
    "objectID": "regressions.html#visuals",
    "href": "regressions.html#visuals",
    "title": "Geometry of error space",
    "section": "Visuals",
    "text": "Visuals\n\nOLS vs GMM : 동일한 선형 회귀 구조에서도 서로 다른 projection\n설정 요약:\n\n데이터 생성:\n\n\\(y = \\beta_0 + \\beta_1 x + \\varepsilon\\)\n잡음 \\(\\varepsilon\\)는 \\(x\\)의 값이 커질수록 분산이 커지는 이질적(heteroskedastic) 형태로 생성\n\nOLS:\n\n모든 관측값을 동일한 중요도로 간주하여 잔차 제곱합을 최소화. 즉, 평균적인 방향으로 선을 맞춤.\nOLS는 전체 데이터를 균등하게 반영하며, 잡음이 큰 부분에서도 기울기가 영향을 받음.\n\nGMM:\n\n잔차의 분산(또는 신뢰도)에 따라 가중치 positive definite weighting를 달리 부여. 이 경우 분산의 역수를 사용하여 노이즈가 적은 관측값을 더 신뢰하도록 추정.\nGMM은 잡음이 작은 구간(왼쪽)의 패턴에 더 많은 가중치를 부여하여 기울기가 더 가파르게 추정\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulate data with heteroskedastic noise (to favor GMM adjustment)\nnp.random.seed(0)\nn = 100\nx = np.linspace(0, 10, n)\nX = np.vstack([np.ones(n), x]).T\n\n# True model\nbeta_true = np.array([1, 2])\n# Heteroskedastic noise: variance increases with x\nnoise_std = 0.5 + 1.5 * (x / x.max())  # ranges from 0.5 to 2.0\ny = X @ beta_true + np.random.normal(0, noise_std)\n\n# OLS estimation\nbeta_ols = np.linalg.inv(X.T @ X) @ X.T @ y\ny_hat_ols = X @ beta_ols\n\n# GMM weighting: inverse of variance (precision weighting)\nW = np.diag(1 / noise_std**2)\n\n# GMM estimation (optimal weighting under heteroskedasticity)\nXTWX = X.T @ W @ X\nXTWy = X.T @ W @ y\nbeta_gmm = np.linalg.inv(XTWX) @ XTWy\ny_hat_gmm = X @ beta_gmm\n\n# Plot with aspect ratio 1:1\nplt.figure(figsize=(8, 8))\nplt.scatter(x, y, color='lightgray', label='Observed data')\nplt.plot(x, y_hat_ols, label='OLS projection', color='blue', linewidth=2)\nplt.plot(x, y_hat_gmm, label='GMM projection (precision-weighted)', color='red', linestyle='--', linewidth=2)\nplt.title(\"OLS vs GMM Projection with Heteroskedastic Data\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.axis('equal')  # Set equal aspect ratio\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nobjective function (quadratic form)의 level set\n참고: GMM의 목적함수는 원래부터 정규화(normalized)되어 있는 반면, OLS의 목적함수는 정규화 없이 나타나는 일반적 2차형식이므로, 두 방법을 엄밀히 비교하려면 OLS도 동일한 형태로 정규화해 주어야 함.\n\n\n\n\n\n\n\n\n\n방법\n목적함수 형태\n거리해석\nLevel set 형태\n\n\n\n\nOLS (일반형)\n\\((y - X\\beta)^\\top (y - X\\beta)\\)\nEuclidean norm\n타원 (elliptical)\n\n\nOLS (정규화형)\n\\((\\beta - \\hat{\\beta})^\\top (X^\\top X)^{-1} (\\beta - \\hat{\\beta})\\)\nMahalanobis norm\n구형 (spherical)\n\n\nGMM\n\\(\\bar{g}(\\theta)^\\top W \\bar{g}(\\theta)\\)\nMahalanobis norm\n타원 또는 구형\n\n\n\n\n\nCode\n# Z-score normalization of x to improve XtX condition\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nx_normalized = scaler.fit_transform(x.reshape(-1, 1)).flatten()\nX_normalized = np.vstack([np.ones(n), x_normalized]).T\n\n# Recalculate OLS and GMM using normalized X\nbeta_ols_norm = np.linalg.inv(X_normalized.T @ X_normalized) @ X_normalized.T @ y\ny_hat_ols_norm = X_normalized @ beta_ols_norm\n\n# GMM estimation with same W\nXTWX_norm = X_normalized.T @ W @ X_normalized\nXTWy_norm = X_normalized.T @ W @ y\nbeta_gmm_norm = np.linalg.inv(XTWX_norm) @ XTWy_norm\ny_hat_gmm_norm = X_normalized @ beta_gmm_norm\n\n# New grid around beta_ols_norm\nb0_vals = np.linspace(beta_ols_norm[0] - 1, beta_ols_norm[0] + 1, 100)\nb1_vals = np.linspace(beta_ols_norm[1] - 1, beta_ols_norm[1] + 1, 100)\nB0, B1 = np.meshgrid(b0_vals, b1_vals)\nB_flat = np.vstack([B0.ravel(), B1.ravel()])\n\n# Normalized OLS objective (Mahalanobis)\nXtX_inv_norm = np.linalg.inv(X_normalized.T @ X_normalized)\ndelta_norm = B_flat - beta_ols_norm[:, None]\nJ_ols_normalized = np.einsum('ji,jk,ki-&gt;i', delta_norm, XtX_inv_norm, delta_norm).reshape(B0.shape)\n\n# GMM objective with normalized X\nJ_gmm_norm = []\nfor i in range(B_flat.shape[1]):\n    r = y - X_normalized @ B_flat[:, i]\n    obj = r.T @ W @ r\n    J_gmm_norm.append(obj)\nJ_gmm_norm = np.array(J_gmm_norm).reshape(B0.shape)\n\n# Plotting\nfig, axs = plt.subplots(1, 2, figsize=(14, 6))\n\n# Normalized OLS (should be spherical)\ncs1 = axs[0].contour(B0, B1, J_ols_normalized, levels=20, cmap='Blues')\naxs[0].plot(beta_ols_norm[0], beta_ols_norm[1], 'bo', label='OLS solution')\naxs[0].set_title(\"OLS (Normalized X): Spherical Level Sets\")\naxs[0].set_xlabel(r\"$\\beta_0$\")\naxs[0].set_ylabel(r\"$\\beta_1$\")\naxs[0].axis('equal')\naxs[0].legend()\naxs[0].grid(True)\n\n# GMM with normalized X\ncs2 = axs[1].contour(B0, B1, J_gmm_norm, levels=20, cmap='Reds')\naxs[1].plot(beta_gmm_norm[0], beta_gmm_norm[1], 'ro', label='GMM solution')\naxs[1].set_title(\"GMM (Normalized X): Ellipsoidal Level Sets\")\naxs[1].set_xlabel(r\"$\\beta_0$\")\naxs[1].set_ylabel(r\"$\\beta_1$\")\naxs[1].axis('equal')\naxs[1].legend()\naxs[1].grid(True)\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "교육",
      "수학",
      "Geometry of error space"
    ]
  }
]