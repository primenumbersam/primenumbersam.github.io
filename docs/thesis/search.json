[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Too Big to Fail?",
    "section": "",
    "text": "Too big to fail? More like too big to be good, too big for justice, and too big to not exploit.\n\n\nStructural Market Distortions and Return Asymmetry\nPost-2008 policies have induced persistent capital flows toward dominant firms while allowing unproductive small-cap firms to survive via liquidity support, resulting in asymmetric return distributions and distorted risk–return profiles.\nThe TBTF Strategy as a Diagnostic Tool\nA simple portfolio of the top 10 market-cap stocks outperforms traditional benchmarks across multiple performance metrics. However, its success reflects structural capital lock-in and systemic inefficiencies rather than superior risk pricing or fundamentals.\nA Paradox of Optimality and Inefficiency\nThe TBTF strategy remains optimal as long as inefficiencies persist. If it fails, it may signal a return to efficient markets. Thus, investors profit from distortions that are socially and economically suboptimal.\nImplications for Asset Pricing and Market Design\nThe findings call for new asset pricing models that incorporate rank-based capital dynamics, asymmetric transition probabilities, and non-ergodic behavior—moving beyond equilibrium-based risk-return tradeoffs.",
    "crumbs": [
      "교육",
      "Too Big to Fail?"
    ]
  },
  {
    "objectID": "index.html#key-highlights",
    "href": "index.html#key-highlights",
    "title": "Too Big to Fail?",
    "section": "",
    "text": "Too big to fail? More like too big to be good, too big for justice, and too big to not exploit.\n\n\nStructural Market Distortions and Return Asymmetry\nPost-2008 policies have induced persistent capital flows toward dominant firms while allowing unproductive small-cap firms to survive via liquidity support, resulting in asymmetric return distributions and distorted risk–return profiles.\nThe TBTF Strategy as a Diagnostic Tool\nA simple portfolio of the top 10 market-cap stocks outperforms traditional benchmarks across multiple performance metrics. However, its success reflects structural capital lock-in and systemic inefficiencies rather than superior risk pricing or fundamentals.\nA Paradox of Optimality and Inefficiency\nThe TBTF strategy remains optimal as long as inefficiencies persist. If it fails, it may signal a return to efficient markets. Thus, investors profit from distortions that are socially and economically suboptimal.\nImplications for Asset Pricing and Market Design\nThe findings call for new asset pricing models that incorporate rank-based capital dynamics, asymmetric transition probabilities, and non-ergodic behavior—moving beyond equilibrium-based risk-return tradeoffs.",
    "crumbs": [
      "교육",
      "Too Big to Fail?"
    ]
  },
  {
    "objectID": "4_8_implementation.html",
    "href": "4_8_implementation.html",
    "title": "08 Implementation",
    "section": "",
    "text": "This section summarizes the technical implementation details of the TBTF strategy, including programming environment, file structure, and reproducibility instructions. The entire empirical workflow is based on Quarto and organized into modular .qmd documents with embedded Python code blocks. This ensures seamless rendering of figures and tables alongside text, and supports reproducible research.",
    "crumbs": [
      "교육",
      "Model",
      "08 Implementation"
    ]
  },
  {
    "objectID": "4_8_implementation.html#implementation-notes",
    "href": "4_8_implementation.html#implementation-notes",
    "title": "08 Implementation",
    "section": "",
    "text": "This section summarizes the technical implementation details of the TBTF strategy, including programming environment, file structure, and reproducibility instructions. The entire empirical workflow is based on Quarto and organized into modular .qmd documents with embedded Python code blocks. This ensures seamless rendering of figures and tables alongside text, and supports reproducible research.",
    "crumbs": [
      "교육",
      "Model",
      "08 Implementation"
    ]
  },
  {
    "objectID": "4_8_implementation.html#programming-environment",
    "href": "4_8_implementation.html#programming-environment",
    "title": "08 Implementation",
    "section": "Programming Environment",
    "text": "Programming Environment\nAll analysis was conducted using Python 3.x. Core packages used include:\n\nData Processing: pandas, numpy\nStatistical Modeling: statsmodels, scipy, sklearn\nVisualization: matplotlib, seaborn, plotly\nRendering: quarto, jupyter, ipykernel\n\nAdditional utilities like joblib and multiprocessing were used for robustness tests and performance acceleration.",
    "crumbs": [
      "교육",
      "Model",
      "08 Implementation"
    ]
  },
  {
    "objectID": "4_8_implementation.html#document-structure",
    "href": "4_8_implementation.html#document-structure",
    "title": "08 Implementation",
    "section": "Document Structure",
    "text": "Document Structure\nThe analysis is broken into modular .qmd files, each corresponding to a specific stage of the empirical framework. These files are rendered either as part of the full manuscript or as standalone reports.\n\n\n\n\n\n\n\n\nQMD File\nDescription\nRelated Section\n\n\n\n\n02_data.qmd\nLoad and clean CRSP & FF data, construct derived variables\nSection 2 – Data\n\n\n03_ff_compare.qmd\nFF size decile analysis, Sharpe dynamics comparison\nSection 3 – FF Benchmark\n\n\n04_structure.qmd\nCapital convexity, mixture modeling, Markov transition structure\nSection 4 – Structure\n\n\n05_strategy.qmd\nTBTF logic: What–How–When framework, weight estimation\nSection 5 – Strategy\n\n\n06_performance.qmd\nPerformance comparison: distribution, price level, metrics\nSection 6 – Performance\n\n\n07_robustness.qmd\nRobustness checks: size, frequency, weights, sample splits\nSection 7 – Robustness\n\n\n\nAll figures and tables are embedded directly within these .qmd documents using Quarto-native fig-cap and tbl-cap options.",
    "crumbs": [
      "교육",
      "Model",
      "08 Implementation"
    ]
  },
  {
    "objectID": "4_8_implementation.html#output-artifacts",
    "href": "4_8_implementation.html#output-artifacts",
    "title": "08 Implementation",
    "section": "Output Artifacts",
    "text": "Output Artifacts\nThe results are organized in structured folders and linked automatically to each section:\n\nfigs/: Figures (e.g., PNG/HTML via matplotlib or plotly)\ntables/: Summary tables and evaluation metrics (CSV or HTML)\n\nEach artifact is generated at render time and versioned with appropriate suffixes. Example:\n\nsharpe_curve_s10_b10.png, return_dist_tbtf_post2010.png\nperformance_summary_pre2010.csv, weight_table_exp_fit.html",
    "crumbs": [
      "교육",
      "Model",
      "08 Implementation"
    ]
  },
  {
    "objectID": "4_8_implementation.html#reproducibility",
    "href": "4_8_implementation.html#reproducibility",
    "title": "08 Implementation",
    "section": "Reproducibility",
    "text": "Reproducibility\nTo reproduce the full pipeline:\n\nClone the project repository and install dependencies:\n\npip install -r requirements.txt\n\nDownload CRSP/Compustat data via WRDS (institutional access required)\nEdit configuration or .qmd arguments if needed\nRender the full manuscript or individual parts:\n\nquarto render\nOptional automation scripts using make, snakemake, or papermill are available for batch processing.",
    "crumbs": [
      "교육",
      "Model",
      "08 Implementation"
    ]
  },
  {
    "objectID": "4_8_implementation.html#appendix",
    "href": "4_8_implementation.html#appendix",
    "title": "08 Implementation",
    "section": "Appendix",
    "text": "Appendix\n\n\nrequirements.txt\npandas&gt;=1.5\nnumpy&gt;=1.22\nscikit-learn&gt;=1.3\nmatplotlib&gt;=3.6\nseaborn&gt;=0.12\nstatsmodels&gt;=0.13\nscipy&gt;=1.10\nplotly&gt;=5.10\nquarto&gt;=1.3",
    "crumbs": [
      "교육",
      "Model",
      "08 Implementation"
    ]
  },
  {
    "objectID": "4_6_performance.html",
    "href": "4_6_performance.html",
    "title": "06 Performance",
    "section": "",
    "text": "Code\nfrom scipy.stats import skew, kurtosis\nimport numpy as np\nimport pandas as pd\n\nimport sqlite3\ntbtf = sqlite3.connect(database=\"../../tbtf.sqlite\")\n\n#cursor = tbtf.cursor()\n#cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n#print(cursor.fetchall())\n\n#crsp = pd.read_sql_query(\n#  sql=\"SELECT * FROM crsp\",\n#  con=tbtf,\n#  parse_dates={\"date\"}\n#)\n\nff3 = pd.read_sql_query(\n  sql=\"SELECT * FROM ff3\",\n  con=tbtf,\n  parse_dates={\"date\"}\n)\n\npre_index = pd.read_sql_query(\n  sql=\"SELECT * FROM pre_index\",\n  con=tbtf,\n  parse_dates={\"date\"}\n)\n\npost_index = pd.read_sql_query(\n  sql=\"SELECT * FROM post_index\",\n  con=tbtf,\n  parse_dates={\"date\"}\n)\n\npost_ff = pd.read_sql_query(\n  sql=\"SELECT * FROM post_ff\",\n  con=tbtf,\n  parse_dates={\"date\"}\n)\n\ndef evaluate_performance(returns, eta=3, p=0.01, periods_per_year=12):\n    \"\"\"\n    주어진 초과수익률(returns) 시계열에 대해 다양한 성과 지표를 계산합니다.\n    returns는 이미 risk-free rate이 차감된 초과수익률(ret_excess) 시계열이라고 가정합니다.\n    \n    Parameters:\n      - returns: pandas Series 또는 numpy array 형태의 초과수익률\n      - eta: CRRA 계수 (default: 3)\n      - p: Omega ratio 계산 시 기준 수익률 threshold (default: 0.01)\n      - periods_per_year: 연간 관측 기간 수 (예: 월간이면 12, 6M이면 2, 연간이면 1)\n      \n    Returns:\n      - metrics: dict 형태로 각 성과 지표를 반환\n    \"\"\"\n    def ann_return(r):\n        return (1 + r).prod()**(periods_per_year / len(r)) - 1\n\n    def ann_vol(r):\n        return r.std() * np.sqrt(periods_per_year)\n\n    def sharpe(r):\n        return r.mean() / r.std() * np.sqrt(periods_per_year)\n\n    def sortino(r):\n        downside = r[r &lt; 0]\n        # 만약 downside의 표준편차가 0인 경우, 평균이 양수면 무한대로 처리\n        if downside.std() &gt; 0:\n            return r.mean() / downside.std() * np.sqrt(periods_per_year)\n        else:\n            return np.inf if r.mean() &gt; 0 else 0\n\n    def omega(r):\n        gains = r[r &gt; p] - p\n        losses = p - r[r &lt; p]\n        return gains.sum() / losses.sum() if losses.sum() &gt; 0 else np.nan\n\n    def max_dd(r):\n        cum = (1 + r).cumprod()\n        dd = (cum - cum.cummax()) / cum.cummax()\n        return dd.min()\n\n    def calmar(r):\n        a = ann_return(r)\n        m = max_dd(r)\n        return a / abs(m) if m &lt; 0 else np.nan\n\n    def expected_crra(r):\n        x = 1 + r\n        if eta == 1:\n            return np.log(x).mean()\n        else:\n            return ((x**(1 - eta) - 1) / (1 - eta)).mean()\n\n    def fisher_skew(r):\n        return skew(r, bias=False)\n\n    def pearson_skew(r):\n        mu, med, std = r.mean(), r.median(), r.std()\n        return 3 * (mu - med) / std if std &gt; 0 else np.nan\n\n    def ex_kurt(r):\n        return kurtosis(r, fisher=True, bias=False)\n\n    metrics = {\n        'Annualized Return': ann_return(returns),\n        'Annualized Volatility': ann_vol(returns),\n        'Sharpe Ratio': sharpe(returns),\n        'Sortino Ratio': sortino(returns),\n        'Omega Ratio': omega(returns),\n        'Max Drawdown': max_dd(returns),\n        'Calmar Ratio': calmar(returns),\n        'Expected CRRA': expected_crra(returns),\n        'Fisher Skewness': fisher_skew(returns),\n        'Pearson Skewness': pearson_skew(returns),\n        'Excess Kurtosis': ex_kurt(returns),\n    }\n    return metrics\n\n# 예시 사용:\n# 만약 재조정 주기가 '6M'이면, periods_per_year=2, '12M'이면 1, '1M'이면 12 등을 전달.\n# 예: monthly returns 시, evaluate_performance(returns, periods_per_year=12)\n\n# 벤치마크 포트폴리오 들의 Excess return (i.e. return - r_f) 계산 \ndef subtract_rf_from_df(df, ff3, date_col='date'):\n    \"\"\"\n    df: 날짜와 수익률 데이터를 포함하는 DataFrame. \n        날짜 컬럼 이름은 기본적으로 'date'로 가정합니다.\n    ff3: ff3 데이터프레임, 'date'와 'r_f' 컬럼을 포함해야 합니다.\n    date_col: 날짜 컬럼의 이름 (기본값 'date').\n    \n    df의 모든 숫자형 수익률 컬럼(날짜 컬럼 제외)에서 ff3의 r_f 값을 해당 날짜에 맞게 차감합니다.\n    \"\"\"\n    # 날짜 컬럼을 datetime으로 변환\n    df[date_col] = pd.to_datetime(df[date_col])\n    ff3['date'] = pd.to_datetime(ff3['date'])\n    \n    # ff3의 r_f 컬럼만 추출 후, 날짜 기준으로 병합\n    merged = df.merge(ff3[[date_col, 'r_f']], on=date_col, how='left')\n    \n    # 날짜와 r_f를 제외한 모든 숫자형 컬럼에 대해 r_f를 차감\n    numeric_cols = merged.select_dtypes(include='number').columns.tolist()\n    # 만약 날짜 컬럼이 숫자형으로 인식될 경우, 제거\n    if date_col in numeric_cols:\n        numeric_cols.remove(date_col)\n    if 'r_f' in numeric_cols:\n        numeric_cols.remove('r_f')\n        \n    for col in numeric_cols:\n        merged[col] = merged[col] - merged['r_f']\n        \n    # r_f 컬럼은 제거\n    return merged.drop(columns='r_f')\n\npre_index_excess  = subtract_rf_from_df(pre_index, ff3, date_col='date')\npost_index_excess = subtract_rf_from_df(post_index, ff3, date_col='date')\npost_ff_excess    = subtract_rf_from_df(post_ff, ff3, date_col='date')\n\n\nA core pillar of traditional asset pricing theory posits that investors are compensated for bearing systematic risk. Higher expected returns are assumed to reflect higher exposures to non-diversifiable shocks. This logic extends into the literature linking market power to expected return: dominant firms with pricing power and barriers to entry may generate higher profit margins, which are viewed as justifying higher long-run returns, albeit at the cost of greater cyclicality or factor sensitivity.\nHowever, the Too Big to Fail (TBTF) framework challenges this equilibrium-based view by demonstrating that the structural dominance of mega-cap firms yields not only comparable or higher average returns, but also consistently lower volatility. Our empirical analysis reveals that:\n\nThe top-ranked firms (e.g., top 10% by market cap) consistently generate higher median returns with narrower return distributions, virtually eliminating downside outliers post-2010.\n\n\n\nCode\n# 성과 지표 평가\nperformance = evaluate_performance(pre_index_excess['^NDX'], eta=3, p=0.01)\nperformance\n\n\n\n\nCode\nplot_return_distribution(tbtf_returns, market_returns, period='pre-2010')\n\n\n\n\nCode\nplot_return_distribution(tbtf_returns, market_returns, period='post-2010')\n\n\n\n\nCode\nplot_price_level(tbtf_returns, market_returns, start_date='2000-01-01', end_date='2009-12-31')\n\n\n\n\nCode\nplot_price_level(tbtf_returns, market_returns, start_date='2010-01-01', end_date='2023-12-31')\n\n\nThis figure compares the monthly return distribution of the TBTF portfolio and the market-wide index (e.g., SPY or VTI) during the post-2010 period. TBTF returns are notably less dispersed and exhibit a thicker concentration around the mean.\nIn this context, we propose a reframed interpretation:\n\nMarket power no longer demands a risk premium—it precludes the risk itself.\n\nTo formalize this intuition, we treat: - log(capshare) as a proxy for forward-looking return potential, reflecting capital markets’ implicit confidence in dominant firms, - and percentile-based rank (e.g., within top-𝑛 assets) as a proxy for structural market power.\nThe estimated relationship between rank and log(capshare) is strongly convex, best approximated by an exponential function. This convexity implies that small changes in rank yield disproportionate shifts in capital allocation, consistent with a winner-takes-most dynamic.\nWhile prior studies (e.g., Hou et al., 2015; Bai et al., 2019) posit that higher expected returns arise as compensation for firm-level productivity or intangible capital risk, our results suggest an alternative mechanism: persistent capital lock-in due to passive flows, platform effects, and institutional mandates.\nThis reorientation invites a broader philosophical reframing:\nTBTF stocks do not merely command a premium for bearing risk—they accumulate rents by removing risk through their institutional embeddedness.\nIn contrast to traditional convex pricing models where market competition disciplines excess returns, the TBTF structure implies a regime of quasi-feudal rent extraction, where market power ensures return, not through risk, but through insulation.",
    "crumbs": [
      "교육",
      "Model",
      "06 Performance"
    ]
  },
  {
    "objectID": "4_6_performance.html#tbtf-interpretation-market-power-return-and-volatility",
    "href": "4_6_performance.html#tbtf-interpretation-market-power-return-and-volatility",
    "title": "06 Performance",
    "section": "",
    "text": "Code\nfrom scipy.stats import skew, kurtosis\nimport numpy as np\nimport pandas as pd\n\nimport sqlite3\ntbtf = sqlite3.connect(database=\"../../tbtf.sqlite\")\n\n#cursor = tbtf.cursor()\n#cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n#print(cursor.fetchall())\n\n#crsp = pd.read_sql_query(\n#  sql=\"SELECT * FROM crsp\",\n#  con=tbtf,\n#  parse_dates={\"date\"}\n#)\n\nff3 = pd.read_sql_query(\n  sql=\"SELECT * FROM ff3\",\n  con=tbtf,\n  parse_dates={\"date\"}\n)\n\npre_index = pd.read_sql_query(\n  sql=\"SELECT * FROM pre_index\",\n  con=tbtf,\n  parse_dates={\"date\"}\n)\n\npost_index = pd.read_sql_query(\n  sql=\"SELECT * FROM post_index\",\n  con=tbtf,\n  parse_dates={\"date\"}\n)\n\npost_ff = pd.read_sql_query(\n  sql=\"SELECT * FROM post_ff\",\n  con=tbtf,\n  parse_dates={\"date\"}\n)\n\ndef evaluate_performance(returns, eta=3, p=0.01, periods_per_year=12):\n    \"\"\"\n    주어진 초과수익률(returns) 시계열에 대해 다양한 성과 지표를 계산합니다.\n    returns는 이미 risk-free rate이 차감된 초과수익률(ret_excess) 시계열이라고 가정합니다.\n    \n    Parameters:\n      - returns: pandas Series 또는 numpy array 형태의 초과수익률\n      - eta: CRRA 계수 (default: 3)\n      - p: Omega ratio 계산 시 기준 수익률 threshold (default: 0.01)\n      - periods_per_year: 연간 관측 기간 수 (예: 월간이면 12, 6M이면 2, 연간이면 1)\n      \n    Returns:\n      - metrics: dict 형태로 각 성과 지표를 반환\n    \"\"\"\n    def ann_return(r):\n        return (1 + r).prod()**(periods_per_year / len(r)) - 1\n\n    def ann_vol(r):\n        return r.std() * np.sqrt(periods_per_year)\n\n    def sharpe(r):\n        return r.mean() / r.std() * np.sqrt(periods_per_year)\n\n    def sortino(r):\n        downside = r[r &lt; 0]\n        # 만약 downside의 표준편차가 0인 경우, 평균이 양수면 무한대로 처리\n        if downside.std() &gt; 0:\n            return r.mean() / downside.std() * np.sqrt(periods_per_year)\n        else:\n            return np.inf if r.mean() &gt; 0 else 0\n\n    def omega(r):\n        gains = r[r &gt; p] - p\n        losses = p - r[r &lt; p]\n        return gains.sum() / losses.sum() if losses.sum() &gt; 0 else np.nan\n\n    def max_dd(r):\n        cum = (1 + r).cumprod()\n        dd = (cum - cum.cummax()) / cum.cummax()\n        return dd.min()\n\n    def calmar(r):\n        a = ann_return(r)\n        m = max_dd(r)\n        return a / abs(m) if m &lt; 0 else np.nan\n\n    def expected_crra(r):\n        x = 1 + r\n        if eta == 1:\n            return np.log(x).mean()\n        else:\n            return ((x**(1 - eta) - 1) / (1 - eta)).mean()\n\n    def fisher_skew(r):\n        return skew(r, bias=False)\n\n    def pearson_skew(r):\n        mu, med, std = r.mean(), r.median(), r.std()\n        return 3 * (mu - med) / std if std &gt; 0 else np.nan\n\n    def ex_kurt(r):\n        return kurtosis(r, fisher=True, bias=False)\n\n    metrics = {\n        'Annualized Return': ann_return(returns),\n        'Annualized Volatility': ann_vol(returns),\n        'Sharpe Ratio': sharpe(returns),\n        'Sortino Ratio': sortino(returns),\n        'Omega Ratio': omega(returns),\n        'Max Drawdown': max_dd(returns),\n        'Calmar Ratio': calmar(returns),\n        'Expected CRRA': expected_crra(returns),\n        'Fisher Skewness': fisher_skew(returns),\n        'Pearson Skewness': pearson_skew(returns),\n        'Excess Kurtosis': ex_kurt(returns),\n    }\n    return metrics\n\n# 예시 사용:\n# 만약 재조정 주기가 '6M'이면, periods_per_year=2, '12M'이면 1, '1M'이면 12 등을 전달.\n# 예: monthly returns 시, evaluate_performance(returns, periods_per_year=12)\n\n# 벤치마크 포트폴리오 들의 Excess return (i.e. return - r_f) 계산 \ndef subtract_rf_from_df(df, ff3, date_col='date'):\n    \"\"\"\n    df: 날짜와 수익률 데이터를 포함하는 DataFrame. \n        날짜 컬럼 이름은 기본적으로 'date'로 가정합니다.\n    ff3: ff3 데이터프레임, 'date'와 'r_f' 컬럼을 포함해야 합니다.\n    date_col: 날짜 컬럼의 이름 (기본값 'date').\n    \n    df의 모든 숫자형 수익률 컬럼(날짜 컬럼 제외)에서 ff3의 r_f 값을 해당 날짜에 맞게 차감합니다.\n    \"\"\"\n    # 날짜 컬럼을 datetime으로 변환\n    df[date_col] = pd.to_datetime(df[date_col])\n    ff3['date'] = pd.to_datetime(ff3['date'])\n    \n    # ff3의 r_f 컬럼만 추출 후, 날짜 기준으로 병합\n    merged = df.merge(ff3[[date_col, 'r_f']], on=date_col, how='left')\n    \n    # 날짜와 r_f를 제외한 모든 숫자형 컬럼에 대해 r_f를 차감\n    numeric_cols = merged.select_dtypes(include='number').columns.tolist()\n    # 만약 날짜 컬럼이 숫자형으로 인식될 경우, 제거\n    if date_col in numeric_cols:\n        numeric_cols.remove(date_col)\n    if 'r_f' in numeric_cols:\n        numeric_cols.remove('r_f')\n        \n    for col in numeric_cols:\n        merged[col] = merged[col] - merged['r_f']\n        \n    # r_f 컬럼은 제거\n    return merged.drop(columns='r_f')\n\npre_index_excess  = subtract_rf_from_df(pre_index, ff3, date_col='date')\npost_index_excess = subtract_rf_from_df(post_index, ff3, date_col='date')\npost_ff_excess    = subtract_rf_from_df(post_ff, ff3, date_col='date')\n\n\nA core pillar of traditional asset pricing theory posits that investors are compensated for bearing systematic risk. Higher expected returns are assumed to reflect higher exposures to non-diversifiable shocks. This logic extends into the literature linking market power to expected return: dominant firms with pricing power and barriers to entry may generate higher profit margins, which are viewed as justifying higher long-run returns, albeit at the cost of greater cyclicality or factor sensitivity.\nHowever, the Too Big to Fail (TBTF) framework challenges this equilibrium-based view by demonstrating that the structural dominance of mega-cap firms yields not only comparable or higher average returns, but also consistently lower volatility. Our empirical analysis reveals that:\n\nThe top-ranked firms (e.g., top 10% by market cap) consistently generate higher median returns with narrower return distributions, virtually eliminating downside outliers post-2010.\n\n\n\nCode\n# 성과 지표 평가\nperformance = evaluate_performance(pre_index_excess['^NDX'], eta=3, p=0.01)\nperformance\n\n\n\n\nCode\nplot_return_distribution(tbtf_returns, market_returns, period='pre-2010')\n\n\n\n\nCode\nplot_return_distribution(tbtf_returns, market_returns, period='post-2010')\n\n\n\n\nCode\nplot_price_level(tbtf_returns, market_returns, start_date='2000-01-01', end_date='2009-12-31')\n\n\n\n\nCode\nplot_price_level(tbtf_returns, market_returns, start_date='2010-01-01', end_date='2023-12-31')\n\n\nThis figure compares the monthly return distribution of the TBTF portfolio and the market-wide index (e.g., SPY or VTI) during the post-2010 period. TBTF returns are notably less dispersed and exhibit a thicker concentration around the mean.\nIn this context, we propose a reframed interpretation:\n\nMarket power no longer demands a risk premium—it precludes the risk itself.\n\nTo formalize this intuition, we treat: - log(capshare) as a proxy for forward-looking return potential, reflecting capital markets’ implicit confidence in dominant firms, - and percentile-based rank (e.g., within top-𝑛 assets) as a proxy for structural market power.\nThe estimated relationship between rank and log(capshare) is strongly convex, best approximated by an exponential function. This convexity implies that small changes in rank yield disproportionate shifts in capital allocation, consistent with a winner-takes-most dynamic.\nWhile prior studies (e.g., Hou et al., 2015; Bai et al., 2019) posit that higher expected returns arise as compensation for firm-level productivity or intangible capital risk, our results suggest an alternative mechanism: persistent capital lock-in due to passive flows, platform effects, and institutional mandates.\nThis reorientation invites a broader philosophical reframing:\nTBTF stocks do not merely command a premium for bearing risk—they accumulate rents by removing risk through their institutional embeddedness.\nIn contrast to traditional convex pricing models where market competition disciplines excess returns, the TBTF structure implies a regime of quasi-feudal rent extraction, where market power ensures return, not through risk, but through insulation.",
    "crumbs": [
      "교육",
      "Model",
      "06 Performance"
    ]
  },
  {
    "objectID": "4_6_performance.html#risk-adjusted-performance-metrics",
    "href": "4_6_performance.html#risk-adjusted-performance-metrics",
    "title": "06 Performance",
    "section": "Risk-Adjusted Performance Metrics",
    "text": "Risk-Adjusted Performance Metrics\nWe evaluate the TBTF strategy against standard benchmarks using several key performance metrics: Sharpe ratio, Sortino ratio, Omega ratio, maximum drawdown, and CRRA utility. All metrics are computed over the out-of-sample post-2010 period.\n\n\nCode\nplot_return_distribution(tbtf_returns, market_returns, period='pre-2010')\n\n\n\n\nCode\nplot_return_distribution(tbtf_returns, market_returns, period='post-2010')\n\n\n\n\nCode\nplot_price_level(tbtf_returns, market_returns, start_date='2000-01-01', end_date='2009-12-31')\n\n\n\n\nCode\nplot_price_level(tbtf_returns, market_returns, start_date='2010-01-01', end_date='2023-12-31')\n\n\nThe TBTF portfolio consistently outperforms across all dimensions, with both higher average returns and lower downside risk exposure. Notably, its Omega ratio remains significantly above 1 for low thresholds, reflecting extremely thin left tails.",
    "crumbs": [
      "교육",
      "Model",
      "06 Performance"
    ]
  },
  {
    "objectID": "4_6_performance.html#clarification-on-distributional-decomposition",
    "href": "4_6_performance.html#clarification-on-distributional-decomposition",
    "title": "06 Performance",
    "section": "Clarification on Distributional Decomposition",
    "text": "Clarification on Distributional Decomposition\nAlthough the intuition behind TBTF’s superior return profile is deeply tied to changes in distributional dynamics, we emphasize that the Gaussian Mixture decomposition, transition matrix estimation, and stationary state analysis are fully addressed in Section 4. Readers seeking formal modeling of return asymmetry, regime persistence, or long-run capital concentration should refer to that section.\nIn this performance section, we instead emphasize practical backtest results and the empirical coherence of TBTF’s return superiority, without over-relying on latent structure estimation.",
    "crumbs": [
      "교육",
      "Model",
      "06 Performance"
    ]
  },
  {
    "objectID": "4_4_structure.html",
    "href": "4_4_structure.html",
    "title": "04 Structure",
    "section": "",
    "text": "This section decomposes the structural mechanisms underlying the TBTF strategy across two economic regimes—pre-2010 and post-2010. By comparing these periods, we identify changes in distributional composition, capital concentration, and rank mobility that reflect broader shifts in financial market dynamics following systemic interventions and structural innovation—each of which challenges conventional assumptions in asset pricing and market efficiency.\n\n\nCode\n# CRSP dataframe import\nimport pandas as pd\nimport sqlite3\ntbtf = sqlite3.connect(database=\"../../tbtf.sqlite\")\n\n#cursor = tbtf.cursor()\n#cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n#print(cursor.fetchall())\n\ncrsp = pd.read_sql_query(\n  sql=\"SELECT * FROM crsp\",\n  con=tbtf,\n  parse_dates={\"date\"}\n)\ncrsp.head()\n\n\n\n\n\n\n\nIs the market portfolio return better explained as a mixture of distinct structural groups, such as top decile vs. the rest, and how have their mixture weights changed across regimes?\n\n\n\n\n\nWe define two return-generating groups:\n\nGroup A: Top 10% stocks by market cap (state = 10)\nGroup B: All other listed stocks (states &lt; 10)\n\nAt each month \\(t\\), compute value-weighted returns:\n\\[\nR_t^{\\text{market}} = w_t \\cdot R_t^{(10)} + (1 - w_t) \\cdot R_t^{(&lt;10)}\n\\]\nwhere \\(w_t\\) is the capital share of Group A—i.e., the mixture weight.\n\n\n\n\nThe empirical results from the mixture decomposition provide compelling evidence of a structural shift in how market returns are generated and distributed. Unlike traditional interpretations that focus on regime switching (e.g., bull vs. bear), our analysis reveals a persistent cross-sectional asymmetry—notably between the top decile and the remaining 90% of firms by market capitalization.\n\n\nCode\n# df_mix\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 1. Value-Weighted Return 계산 함수 정의\ndef value_weighted_return(df, value_col='mktcap_lag', return_col='ret'):\n    weighted = df[return_col] * df[value_col]\n    total_weight = df[value_col].sum()\n    return weighted.sum() / total_weight if total_weight &gt; 0 else np.nan\n\n# 2. Group별 수익률 계산 및 mixture 분해\n# 결과 저장용 리스트\nresults = []\n\n# 월별 반복\nfor date, group in crsp.groupby('date'):\n    top_group = group[group['state'] == 10]\n    rest_group = group[(group['state'] &lt; 10) & (group['state'] &gt; 0)]\n\n    if len(top_group) == 0 or len(rest_group) == 0:\n        continue\n\n    # 개별 그룹의 value-weighted return 계산\n    r_top = value_weighted_return(top_group)\n    r_rest = value_weighted_return(rest_group)\n    \n    # 전체 시장 포트폴리오 value-weighted return 계산\n    r_total = value_weighted_return(group[group['state'] &gt; 0])\n\n    # mixture weight (자본 비중)\n    w_top = top_group['mktcap_lag'].sum() / group[group['state'] &gt; 0]['mktcap_lag'].sum()\n\n    results.append({\n        'date': date,\n        'r_top': r_top,\n        'r_rest': r_rest,\n        'r_total': r_total,\n        'w_top': w_top,\n        'w_rest': 1 - w_top,\n        'r_predicted': w_top * r_top + (1 - w_top) * r_rest\n    })\n\ndf_mix = pd.DataFrame(results).sort_values('date')\n\ndf_mix.head()\n\n\n\n\n\n\nCode\n# Time-series Capital Weight of Top 10% (State = 10)\nplt.plot(df_mix['date'], df_mix['w_top'])\nplt.title('Time-series Capital Weight of Top 10% (State = 10)')\nplt.xlabel('Date')\nplt.ylabel('Top 10% Capital Weight')\nplt.show()\n\n\nThe time-series evolution of \\(w_t\\)—the value-weighted capital share of the top 10% stocks—offers compelling evidence of persistent and intensifying capital lock-in.\n\nBetween 1996 and 2001, \\(w_t\\) rose rapidly from below 0.75 to a peak of approximately 0.88, reflecting the early stages of mega-cap dominance during the dot-com boom.\nAfter peaking, it declined gradually, reaching just below 0.80 by 2006, indicating a temporary rebalancing of capital across ranks.\nFrom 2006 to 2009, \\(w_t\\) climbed again, stabilizing around 0.83 during the financial crisis—a period marked by concentrated flight to safety and the early impact of Federal Reserve interventions.\nA modest decline occurred post-crisis, with \\(w_t\\) dipping to around 0.78 in 2011.\nSince then, however, the capital share of the top 10% has exhibited near-monotonic increases with remarkably low volatility, except for a brief interruption during the COVID-19 shock in 2020.\nBy the end of 2023, \\(w_t\\) had reached levels exceeding 0.85, with persistent upward momentum and minimal fluctuations.\n\nThis structural pattern indicates not merely high concentration but capital inertia—a state where capital ceases to reallocate dynamically and instead becomes entrenched within a quasi-permanent elite. Capital flows no longer reflect dynamic responses to fundamentals or risk signals, but instead conform to structural entrenchment supported by indexation, ETF flows, and policy-driven yield compression. Capital lock-in is not only persistent but self-reinforcing, reflecting a transition from competitive allocation to institutionalized dominance.\nc.f. Compared to Fama-French’s traditional use of NYSE-only breakpoints, our percentile-based method—spanning NYSE + Nasdaq + AMEX—captures the true cross-sectional concentration of market power, including modern platform firms (e.g., AAPL, MSFT, NVDA) that disproportionately shape returns in the post-crisis era.\n\n\n\n\n\nCode\n# Unconditional Return Distribution: Actual vs. Estimated Mixture\n# GMM의 간소화 버전\" 또는 \"semi-parametric mixture modeling\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import norm\n\n# 샘플 수\nobs = len(df_mix)\n\n# 각 그룹의 수익률 평균 및 표준편차\nr_top_mean = df_mix['r_top'].mean()\nr_top_std = df_mix['r_top'].std()\nr_rest_mean = df_mix['r_rest'].mean()\nr_rest_std = df_mix['r_rest'].std()\nw_top_mean = df_mix['w_top'].mean()\n\n# 실제 및 mixture 수익률\nr_total = df_mix['r_total']\nr_predicted = df_mix['r_predicted']\n\n# 히스토그램과 PDF x축 범위\nxmin = min(r_total.min(), r_predicted.min())\nxmax = max(r_total.max(), r_predicted.max())\nx = np.linspace(xmin, xmax, 1000)\n\n# PDF 계산\npdf_top = norm.pdf(x, loc=r_top_mean, scale=r_top_std)\npdf_rest = norm.pdf(x, loc=r_rest_mean, scale=r_rest_std)\npdf_mix = w_top_mean * pdf_top + (1 - w_top_mean) * pdf_rest\n\n# Plot\nplt.figure(figsize=(10, 6))\nbins = np.linspace(xmin, xmax, 50)\n\n# 실제 수익률 히스토그램\nplt.hist(r_total, bins=bins, alpha=0.3, label='Actual Total Return', color='steelblue', density=True)\n\n# 각 컴포넌트 PDF\nplt.plot(x, pdf_top, linestyle='-', color='red', label='Top 10% Estimated PDF')\nplt.plot(x, pdf_rest, linestyle='-', color='blue', label='Bottom 90% Estimated PDF')\n\n# 혼합 정규 분포 PDF\nplt.plot(x, pdf_mix, linestyle='--', color='black', linewidth=2, label='Mixture PDF (Estimated)')\n\nplt.title(\"Unconditional Return Distribution: Actual vs. Estimated Mixture\")\nplt.xlabel(\"Monthly Return\")\nplt.ylabel(\"Density\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\nCode\n# Unconditional Return Distribution by Period and Capital Rank Group\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# 요약 통계 (Pre-2010 vs Post-2010 비교 등)\ndf_mix['period'] = np.where(df_mix['date'] &lt; '2010-01-01', 'Pre-2010', 'Post-2010')\n\n# 데이터 재구조화: long format\ndf_long = pd.melt(\n    df_mix,\n    id_vars=['date', 'period'],\n    value_vars=['r_top', 'r_rest'],\n    var_name='Group',\n    value_name='Monthly Return'\n)\n\n# Group 이름 정리\ndf_long['Group'] = df_long['Group'].replace({\n    'r_top': 'Top 10%',\n    'r_rest': 'Bottom 90%'\n})\n\n# Boxplot\nplt.figure(figsize=(10, 6))\nsns.boxplot(\n    data=df_long,\n    x='period',\n    y='Monthly Return',\n    hue='Group',\n    palette='Set2'\n)\nplt.title('Unconditional Return Distribution by Period and Capital Rank Group')\nplt.axhline(0, color='gray', linestyle='--', linewidth=1)\nplt.ylabel('Monthly Return')\nplt.xlabel('Period')\nplt.legend(title='Group', loc='lower left')\nplt.tight_layout()\nplt.show()\n\n\n\nBoxplot visualizations reveal that in the post-2010 regime:\n\nThe top decile (r_top) has higher median returns and a narrower interquartile range than the bottom 90% (r_rest).\nOutliers on the downside virtually disappear for r_top, while they persist—especially on both tails—for r_rest.\nThe return distribution of r_top becomes increasingly compact, upward-shifted, and volatility-suppressed, resembling a managed financial product rather than a random equity basket.\n\nThese patterns imply that risk-adjusted return asymmetry is not just about mean differences, but about structural volatility suppression and downside risk elimination at the top.\n\n\n\n\n\n\nCode\n# GMM (k=2) for Top 10% and for Remining 90% \nimport os\nos.environ['LOKY_MAX_CPU_COUNT'] = '4'  # 사용하려는 코어 수를 직접 지정\n\nfrom sklearn.mixture import GaussianMixture\nimport seaborn as sns\n\n# 각 그룹의 return 시리즈\ntop_returns = df_mix['r_top'].dropna().values.reshape(-1, 1)\nrest_returns = df_mix['r_rest'].dropna().values.reshape(-1, 1)\n\n# GMM 적합 함수\ndef fit_gmm(data, n_components=2):\n    gmm = GaussianMixture(n_components=n_components, random_state=42)\n    gmm.fit(data)\n    return gmm\n\ngmm_top = fit_gmm(top_returns, n_components=2)\ngmm_rest = fit_gmm(rest_returns, n_components=2)\n\ndef print_gmm_summary(gmm, label):\n    print(f\"\\nGMM summary for {label}\")\n    for i, (w, mu, var) in enumerate(zip(gmm.weights_, gmm.means_, gmm.covariances_)):\n        print(f\" Component {i+1}: weight={w:.3f}, mean={mu[0]:.4f}, std={np.sqrt(var[0][0]):.4f}\")\n\nprint_gmm_summary(gmm_top, 'Top 10%')\nprint_gmm_summary(gmm_rest, 'Remaining 90%')\n\n\nThe Gaussian Mixture Models (GMMs) estimated separately on r_top and r_rest uncover two key structural regimes:\n\nBoth groups exhibit bimodal structure, with one cluster capturing low-mean, high-volatility returns, and the other positive-mean, low-volatility returns.\nHowever, for the top decile, the low-return regime has a lower weight and tighter variance, suggesting that even when shocks occur, they are attenuated within this elite group.\nIn contrast, the bottom 90% remains exposed to broader volatility regimes, maintaining a larger spread of possible outcomes.\n\n\n\n\n\n\nCode\n# k Selection by AIC vs. BIC\nfrom sklearn.mixture import GaussianMixture\nimport numpy as np\n\nX = df_mix['r_top'].dropna().values.reshape(-1, 1)\n\naic_scores = []\nbic_scores = []\nks = range(1, 21)\n\nfor k in ks:\n    gmm = GaussianMixture(n_components=k, random_state=42)\n    gmm.fit(X)\n    aic_scores.append(gmm.aic(X))\n    bic_scores.append(gmm.bic(X))\n\n# Plot AIC vs BIC to visualize optimal k\nplt.plot(ks, aic_scores, label='AIC')\nplt.plot(ks, bic_scores, label='BIC')\nplt.xlabel('Number of Components (k)')\nplt.ylabel('Information Criterion')\nplt.legend()\nplt.title('Model Selection: AIC vs BIC')\nplt.grid(True)\nplt.show()\n\n\nTo test whether the top decile itself contains further latent substructures, we estimate GMMs with increasing number of components (\\(k = 1, 2, ..., 20\\)). The result is striking:\n\nBoth AIC and BIC increase monotonically in \\(k\\), indicating that no additional latent components significantly improve model fit.\nThe most parsimonious and interpretable solution is obtained at \\(k = 2\\) or even \\(k = 1\\), reinforcing the view that a single dominant structural cluster governs return generation at the top.\n\nThis suggests that return asymmetry may not stem from overlapping regimes or transitory factors, but rather from the structural dominance of a statistically persistent subgroup of mega-cap firms.\n\n\n\n\nTogether, these results imply that:\n\nThe top-decile portfolio behaves like a capital sink—absorbing flows while insulating itself from market volatility.\nMarket return is no longer the outcome of dispersed competition, but rather converges to a structurally locked elite, supported by policy, passive flows, and indexation mechanics.\nThis undermines the classical view of a dynamic, risk-compensated market and instead points toward structural rent extraction by the few.\n\n\n\n\n\n\n\n\nHas capital become more concentrated in the top-ranked firms over time?\n\n\n\n\n\nCross-sectional regression at each \\(t\\):\n\\[\n\\text{CapShare}_{i,t} = \\alpha_t + \\beta_t \\cdot \\text{Rank}_i + \\gamma_t \\cdot \\text{Rank}_i^2 + \\varepsilon_{i,t}\n\\]\nwhere \\(\\text{Rank}_i\\) is percentile-based with state 1 = bottom 10%, …, state 10 = top 10%.\nThe convexity of capital distribution is captured by \\(\\gamma_t\\):\n\n\\(\\gamma_t &gt; 0\\) implies increasing convexity, or accelerated capital share at the top.\n\nTime-series \\(\\{\\gamma_t\\}\\) is visualized to show evolution of capital inequality.\n\n\n\n\n\n\nCode\n# Time-Series of Capital Share Convexity\nimport statsmodels.api as sm\n\n# 1. 각 날짜별로 state별 mktcap 합산 → CapShare_i 생성\ndef compute_capshare(df):\n    df = df.copy()\n    cap_by_state = df.groupby('state')['mktcap'].sum()\n    total_cap = cap_by_state.sum()\n    df['cap_share'] = df['state'].map(cap_by_state / total_cap)\n    return df\n\n# 2. cross-sectional regression에서 사용될 state별 요약 데이터 생성\ndef compute_quadratic_coefficients(df):\n    results = []\n    for date, group in df.groupby('date'):\n        # 유효한 state만 사용 (state 0은 제외)\n        group = group[group['state'] &gt; 0].copy()\n        cap_by_state = group.groupby('state')['mktcap'].sum()\n        total_cap = cap_by_state.sum()\n        capshare = cap_by_state / total_cap\n\n        # x = state (1~10), y = capshare\n        x = capshare.index.values\n        y = capshare.values\n\n        X = np.column_stack([x, x**2])\n        X = sm.add_constant(X)\n        model = sm.OLS(y, X).fit()\n\n        results.append({\n            'date': date,\n            'alpha': model.params[0],\n            'beta': model.params[1],\n            'gamma': model.params[2],\n            'r_squared': model.rsquared\n        })\n\n    return pd.DataFrame(results)\n\n# 실제 데이터가 존재한다고 가정하고 이후 분석 및 시각화 준비\n# 아래는 시각화 함수만 준비\ndef plot_gamma_time_series(gamma_df):\n    plt.figure(figsize=(12, 6))\n    plt.plot(gamma_df['date'], gamma_df['gamma'], label='Gamma (Quadratic Term)', color='darkred')\n\n    # 기준선들\n    plt.axhline(0, color='gray', linestyle='--', linewidth=1)\n    plt.axvline(pd.to_datetime(\"2010-01-01\"), color='black', linestyle='--', linewidth=1.2, label='2010 Breakpoint')\n    plt.title('Time-Series of Capital Share Convexity (γₜ)')\n    plt.xlabel('Date')\n    plt.ylabel('Gamma Coefficient (Convexity)')\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()\n\ncrsp_filtered = crsp[crsp['state'] &gt; 0].copy()\n\ngamma_df = compute_quadratic_coefficients(crsp_filtered)\n\nplot_gamma_time_series(gamma_df)\n\n\nThe time-series of the capital share convexity coefficient, \\(\\gamma_t\\), estimated via cross-sectional quadratic regressions, reveals a striking and persistent structure.\n\n\nCode\ngamma_df.drop('date', axis=1).describe()\n\n\nAcross the full sample period (1996–2023), \\(\\gamma_t\\) remains consistently positive and tightly bounded, with:\n\nA mean value of approximately 0.0186,\n\nA narrow interquartile range: [0.0181, 0.0192],\n\nA low standard deviation of 0.0007, and\n\nR-squared values averaging 0.67, indicating robust fit quality across time.\n\nThis remarkable temporal consistency suggests that the top ranks in the market-cap distribution have maintained a structurally convex advantage—absorbing disproportionately higher capital relative to their rank in nearly every month over the past three decades.\nA closer examination shows that: - Prior to 2010, there was mild downward drift in \\(\\gamma_t\\), consistent with episodic rebalancing and potential capital redistribution (e.g., during the post-dotcom or pre-GFC phase). - However, post-2010, \\(\\gamma_t\\) exhibits a slight but stable upward trend, indicating reinforced convexity—a steeper capital concentration curve among the top-ranked firms.\nThis empirical curvature echoes the shape of a discrete Lorenz curve, where \\(\\gamma_t\\) can be interpreted as a “capital inequality curvature index”. In this framework, increases in \\(\\gamma_t\\) correspond to a more extreme dominance of top ranks, consistent with both the capital lock-in and transition persistence mechanisms documented elsewhere in this study.\nImportantly, the tight dispersion and monotonic stabilization of \\(\\gamma_t\\) provide indirect but compelling support for the out-of-sample consistency of capital-weight-based strategies. The fact that the quadratic relationship between rank and capshare is not only convex but also time-invariant implies that a convex weighting scheme calibrated in-sample may remain optimal (or near-optimal) out-of-sample.\nThis further reinforces the central hypothesis of the TBTF framework: top capital positions are not merely episodic outcomes but persistent structural anchors in modern financial markets.\n\n\n\n\n\n\n\nIs capital mobility declining, with firms increasingly locked in to top positions?\n\n\n\n\n\nDefine states:\n\nState 10: Top 10% (≥ 90th percentile)\nStates 6–9: Middle 40% (50–90)\nStates 1–5: Bottom 50%\nState 0: Delisted or unranked\n\nFrom the panel data, estimate empirical first-order transition matrix:\n\\[\nP_{ij} = \\Pr(\\text{state}_{t+1} = j \\mid \\text{state}_t = i)\n\\]\nCompute:\n\nDiagonal elements: Retention probability (persistence)\nUpward vs. downward transitions: Asymmetry in mobility\nStationary distribution: Long-run capital allocation across states\n\n\n\n\n\nThe estimated transition matrix and its long-run stationary distribution reveal clear signs of capital lock-in, irreversibility, and structural mobility collapse in the U.S. equity market.\n\n\nCode\n# transition_matrix, stationary_series\n\n# Step 1: Ensure 'state' and 'state_lag' are present and valid\ncrsp_filtered = crsp[(crsp['state'].notnull()) & (crsp['state_lag'].notnull())].copy()\n\n# Step 2: Compute empirical transition matrix\ntransition_counts = (\n    crsp_filtered\n    .groupby(['state_lag', 'state'])\n    .size()\n    .unstack(fill_value=0)\n    .sort_index()\n    .reindex(columns=range(11), fill_value=0)\n)\n\n# Step 3: Convert counts to probabilities\ntransition_matrix = transition_counts.div(transition_counts.sum(axis=1), axis=0)\n\n# Step 4: Compute stationary distribution\n# Method: Use eigen decomposition for the transpose of the transition matrix\neigvals, eigvecs = np.linalg.eig(transition_matrix.T.values)\nstationary = np.real(eigvecs[:, np.isclose(eigvals, 1)])\nstationary = stationary[:, 0]\nstationary_dist = stationary / stationary.sum()\n\n# Prepare final transition matrix and stationary distribution\ntransition_matrix.index.name = 'From State'\ntransition_matrix.columns.name = 'To State'\nstationary_series = pd.Series(stationary_dist, index=transition_matrix.columns, name=\"Stationary Dist.\")\n\n\n# Markov Transition Matrix Heatmap (States 0–10)\nimport seaborn as sns\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(\n    transition_matrix,\n    annot=True,\n    fmt=\".2f\",\n    cmap=\"YlGnBu\",\n    linewidths=0.5,\n    cbar_kws={'label': 'Transition Probability'},\n    xticklabels=[f\"To {i}\" for i in transition_matrix.columns],\n    yticklabels=[f\"From {i}\" for i in transition_matrix.index]\n)\nplt.title(\"Markov Transition Matrix Heatmap (States 0–10)\")\nplt.xlabel(\"Next Period State\")\nplt.ylabel(\"Current Period State\")\nplt.tight_layout()\nplt.show()\n\n\nFirst, the persistence of the top decile group (State = 10) is striking. The monthly self-transition probability \\(P_{10,10}\\) reaches 0.972, the highest among all states, implying an expected duration of stay exceeding 36 months:\n\\[\n\\mathbb{E}[\\text{Duration} \\mid \\text{State } i] = \\frac{1}{1 - P_{ii}}\n\\]\nThis result supports the characterization of the top decile as a quasi-absorbing state—once firms enter, they tend to remain for extended periods, creating a long-term incumbency advantage.\nConversely, states 4 and 5 show the lowest diagonal entries (both ≈ 0.79), yielding the shortest expected durations. This suggests that firms in these mid-level ranks are most prone to reclassification—either moving up, down, or out—indicating local instability near the center of the capital distribution.\nDespite expectations that delisting risk (i.e., transition to State 0) would dominate in the lower ranks, the empirical matrix reveals that direct exits from States 1–10 are essentially zero. This implies that delisting or market exit rarely occurs from within the ranked universe. Instead, new listings (from State 0) enter most frequently into State 3, resulting in a pronounced inverted U-shape in the transition row from State 0.\n\n\nCode\n# Long-Run Stationary Distribution by State (Excl. Exit)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# State 1부터 10까지의 stationary probability\nstationary_series_nonzero = stationary_series.iloc[1:]\n\n# Plot\nplt.figure(figsize=(8, 5))\nsns.barplot(\n    x=stationary_series_nonzero.index.astype(str),\n    y=stationary_series_nonzero.values,\n    color='steelblue',\n    edgecolor='black'\n)\n\n# Annotate each bar with its value\nfor i, val in enumerate(stationary_series_nonzero.values):\n    plt.text(i, val + 0.001, f\"{val:.3f}\", ha='center', va='bottom', fontsize=10)\n\n# Plot labels and formatting\nplt.title(\"Long-Run Stationary Distribution by State (Excl. Exit)\", fontsize=14)\nplt.xlabel(\"State (Market Cap Rank)\", fontsize=12)\nplt.ylabel(\"Stationary Probability\", fontsize=12)\nplt.xticks(rotation=0)\nplt.ylim(0, stationary_series_nonzero.max() + 0.01)\nplt.tight_layout()\nplt.show()\n\n\nSecond, the stationary distribution—the long-run probability of occupying each state—exhibits a Pareto-like decay. The highest steady-state probability is in State 1 (≈ 16.6%), and declines monotonically through State 10 (≈ 7.9%), forming a shape reminiscent of the density of a Pareto distribution or a discrete inverse-power law:\n\nWhile State 1 represents the largest share of firms at any point in time, this is not reflective of capital dominance.\nRather, it suggests that the system structurally regenerates the bottom group, maintaining a large base of small-cap firms with limited upward mobility.\n\nThis result aligns with prior findings in household wealth studies—such as those using FRED data—where the top 1% accumulates disproportionate wealth, but the modal position of individuals lies in the bottom or middle deciles. Our firm-level Markov model complements this view by adding monthly transition granularity and dynamic persistence structure, which cannot be captured by static wealth shares alone.\nTaken together, the findings indicate that capital mobility is increasingly non-reversible and stratified, violating assumptions of ergodicity, symmetry, and competitive reallocation often found in representative-agent equilibrium models.\nThe U.S. equity market, viewed through the lens of Markovian state transitions, behaves less like a competitive allocation engine and more like a stratified hierarchy, where entry is rare, exits are asymmetric, and the top functions as a structural sink for capital.\n\n\n\n\n\n\n\n\n\n\n\n\nStructural Channel\nMechanism\nInterpretation\n\n\n\n\nMixture\nReturn as weighted sum of 2 subgroups\nTop decile increasingly drives aggregate performance\n\n\nConvexity\nCapital share as function of rank\nLorenz-type concentration intensifies over time\n\n\nTransition\nMarkov dynamics of rank-state mobility\nTop and Exit states are quasi-absorbing, path-dependent\n\n\n\nIntegrated Interpretation:\n\nTop-decile persistence reveals a form of quasi-absorption, where firms not only rise to the top but stay there disproportionately longer — effectively becoming “Too Big to Exit.”\nMiddle-rank volatility suggests structural fragility, with firms unable to stabilize or ascend — reflecting a “Fragile Middle” unable to convert mobility into permanence.\nLower-rank saturation shows pseudo-stationarity, where exit is more likely than upward mobility — a symptom of “regenerative failure” in the bottom half of the market.\nThe stationary distribution over rank states closely resembles a Pareto-like decay, echoing wealth inequality in household distributions and reinforcing the idea that capital mobility in financial markets has collapsed into hierarchical entrenchment.",
    "crumbs": [
      "교육",
      "Model",
      "04 Structure"
    ]
  },
  {
    "objectID": "4_4_structure.html#mixture-distribution-decomposition",
    "href": "4_4_structure.html#mixture-distribution-decomposition",
    "title": "04 Structure",
    "section": "",
    "text": "Is the market portfolio return better explained as a mixture of distinct structural groups, such as top decile vs. the rest, and how have their mixture weights changed across regimes?\n\n\n\n\n\nWe define two return-generating groups:\n\nGroup A: Top 10% stocks by market cap (state = 10)\nGroup B: All other listed stocks (states &lt; 10)\n\nAt each month \\(t\\), compute value-weighted returns:\n\\[\nR_t^{\\text{market}} = w_t \\cdot R_t^{(10)} + (1 - w_t) \\cdot R_t^{(&lt;10)}\n\\]\nwhere \\(w_t\\) is the capital share of Group A—i.e., the mixture weight.\n\n\n\n\nThe empirical results from the mixture decomposition provide compelling evidence of a structural shift in how market returns are generated and distributed. Unlike traditional interpretations that focus on regime switching (e.g., bull vs. bear), our analysis reveals a persistent cross-sectional asymmetry—notably between the top decile and the remaining 90% of firms by market capitalization.\n\n\nCode\n# df_mix\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 1. Value-Weighted Return 계산 함수 정의\ndef value_weighted_return(df, value_col='mktcap_lag', return_col='ret'):\n    weighted = df[return_col] * df[value_col]\n    total_weight = df[value_col].sum()\n    return weighted.sum() / total_weight if total_weight &gt; 0 else np.nan\n\n# 2. Group별 수익률 계산 및 mixture 분해\n# 결과 저장용 리스트\nresults = []\n\n# 월별 반복\nfor date, group in crsp.groupby('date'):\n    top_group = group[group['state'] == 10]\n    rest_group = group[(group['state'] &lt; 10) & (group['state'] &gt; 0)]\n\n    if len(top_group) == 0 or len(rest_group) == 0:\n        continue\n\n    # 개별 그룹의 value-weighted return 계산\n    r_top = value_weighted_return(top_group)\n    r_rest = value_weighted_return(rest_group)\n    \n    # 전체 시장 포트폴리오 value-weighted return 계산\n    r_total = value_weighted_return(group[group['state'] &gt; 0])\n\n    # mixture weight (자본 비중)\n    w_top = top_group['mktcap_lag'].sum() / group[group['state'] &gt; 0]['mktcap_lag'].sum()\n\n    results.append({\n        'date': date,\n        'r_top': r_top,\n        'r_rest': r_rest,\n        'r_total': r_total,\n        'w_top': w_top,\n        'w_rest': 1 - w_top,\n        'r_predicted': w_top * r_top + (1 - w_top) * r_rest\n    })\n\ndf_mix = pd.DataFrame(results).sort_values('date')\n\ndf_mix.head()\n\n\n\n\n\n\nCode\n# Time-series Capital Weight of Top 10% (State = 10)\nplt.plot(df_mix['date'], df_mix['w_top'])\nplt.title('Time-series Capital Weight of Top 10% (State = 10)')\nplt.xlabel('Date')\nplt.ylabel('Top 10% Capital Weight')\nplt.show()\n\n\nThe time-series evolution of \\(w_t\\)—the value-weighted capital share of the top 10% stocks—offers compelling evidence of persistent and intensifying capital lock-in.\n\nBetween 1996 and 2001, \\(w_t\\) rose rapidly from below 0.75 to a peak of approximately 0.88, reflecting the early stages of mega-cap dominance during the dot-com boom.\nAfter peaking, it declined gradually, reaching just below 0.80 by 2006, indicating a temporary rebalancing of capital across ranks.\nFrom 2006 to 2009, \\(w_t\\) climbed again, stabilizing around 0.83 during the financial crisis—a period marked by concentrated flight to safety and the early impact of Federal Reserve interventions.\nA modest decline occurred post-crisis, with \\(w_t\\) dipping to around 0.78 in 2011.\nSince then, however, the capital share of the top 10% has exhibited near-monotonic increases with remarkably low volatility, except for a brief interruption during the COVID-19 shock in 2020.\nBy the end of 2023, \\(w_t\\) had reached levels exceeding 0.85, with persistent upward momentum and minimal fluctuations.\n\nThis structural pattern indicates not merely high concentration but capital inertia—a state where capital ceases to reallocate dynamically and instead becomes entrenched within a quasi-permanent elite. Capital flows no longer reflect dynamic responses to fundamentals or risk signals, but instead conform to structural entrenchment supported by indexation, ETF flows, and policy-driven yield compression. Capital lock-in is not only persistent but self-reinforcing, reflecting a transition from competitive allocation to institutionalized dominance.\nc.f. Compared to Fama-French’s traditional use of NYSE-only breakpoints, our percentile-based method—spanning NYSE + Nasdaq + AMEX—captures the true cross-sectional concentration of market power, including modern platform firms (e.g., AAPL, MSFT, NVDA) that disproportionately shape returns in the post-crisis era.\n\n\n\n\n\nCode\n# Unconditional Return Distribution: Actual vs. Estimated Mixture\n# GMM의 간소화 버전\" 또는 \"semi-parametric mixture modeling\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import norm\n\n# 샘플 수\nobs = len(df_mix)\n\n# 각 그룹의 수익률 평균 및 표준편차\nr_top_mean = df_mix['r_top'].mean()\nr_top_std = df_mix['r_top'].std()\nr_rest_mean = df_mix['r_rest'].mean()\nr_rest_std = df_mix['r_rest'].std()\nw_top_mean = df_mix['w_top'].mean()\n\n# 실제 및 mixture 수익률\nr_total = df_mix['r_total']\nr_predicted = df_mix['r_predicted']\n\n# 히스토그램과 PDF x축 범위\nxmin = min(r_total.min(), r_predicted.min())\nxmax = max(r_total.max(), r_predicted.max())\nx = np.linspace(xmin, xmax, 1000)\n\n# PDF 계산\npdf_top = norm.pdf(x, loc=r_top_mean, scale=r_top_std)\npdf_rest = norm.pdf(x, loc=r_rest_mean, scale=r_rest_std)\npdf_mix = w_top_mean * pdf_top + (1 - w_top_mean) * pdf_rest\n\n# Plot\nplt.figure(figsize=(10, 6))\nbins = np.linspace(xmin, xmax, 50)\n\n# 실제 수익률 히스토그램\nplt.hist(r_total, bins=bins, alpha=0.3, label='Actual Total Return', color='steelblue', density=True)\n\n# 각 컴포넌트 PDF\nplt.plot(x, pdf_top, linestyle='-', color='red', label='Top 10% Estimated PDF')\nplt.plot(x, pdf_rest, linestyle='-', color='blue', label='Bottom 90% Estimated PDF')\n\n# 혼합 정규 분포 PDF\nplt.plot(x, pdf_mix, linestyle='--', color='black', linewidth=2, label='Mixture PDF (Estimated)')\n\nplt.title(\"Unconditional Return Distribution: Actual vs. Estimated Mixture\")\nplt.xlabel(\"Monthly Return\")\nplt.ylabel(\"Density\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\nCode\n# Unconditional Return Distribution by Period and Capital Rank Group\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# 요약 통계 (Pre-2010 vs Post-2010 비교 등)\ndf_mix['period'] = np.where(df_mix['date'] &lt; '2010-01-01', 'Pre-2010', 'Post-2010')\n\n# 데이터 재구조화: long format\ndf_long = pd.melt(\n    df_mix,\n    id_vars=['date', 'period'],\n    value_vars=['r_top', 'r_rest'],\n    var_name='Group',\n    value_name='Monthly Return'\n)\n\n# Group 이름 정리\ndf_long['Group'] = df_long['Group'].replace({\n    'r_top': 'Top 10%',\n    'r_rest': 'Bottom 90%'\n})\n\n# Boxplot\nplt.figure(figsize=(10, 6))\nsns.boxplot(\n    data=df_long,\n    x='period',\n    y='Monthly Return',\n    hue='Group',\n    palette='Set2'\n)\nplt.title('Unconditional Return Distribution by Period and Capital Rank Group')\nplt.axhline(0, color='gray', linestyle='--', linewidth=1)\nplt.ylabel('Monthly Return')\nplt.xlabel('Period')\nplt.legend(title='Group', loc='lower left')\nplt.tight_layout()\nplt.show()\n\n\n\nBoxplot visualizations reveal that in the post-2010 regime:\n\nThe top decile (r_top) has higher median returns and a narrower interquartile range than the bottom 90% (r_rest).\nOutliers on the downside virtually disappear for r_top, while they persist—especially on both tails—for r_rest.\nThe return distribution of r_top becomes increasingly compact, upward-shifted, and volatility-suppressed, resembling a managed financial product rather than a random equity basket.\n\nThese patterns imply that risk-adjusted return asymmetry is not just about mean differences, but about structural volatility suppression and downside risk elimination at the top.\n\n\n\n\n\n\nCode\n# GMM (k=2) for Top 10% and for Remining 90% \nimport os\nos.environ['LOKY_MAX_CPU_COUNT'] = '4'  # 사용하려는 코어 수를 직접 지정\n\nfrom sklearn.mixture import GaussianMixture\nimport seaborn as sns\n\n# 각 그룹의 return 시리즈\ntop_returns = df_mix['r_top'].dropna().values.reshape(-1, 1)\nrest_returns = df_mix['r_rest'].dropna().values.reshape(-1, 1)\n\n# GMM 적합 함수\ndef fit_gmm(data, n_components=2):\n    gmm = GaussianMixture(n_components=n_components, random_state=42)\n    gmm.fit(data)\n    return gmm\n\ngmm_top = fit_gmm(top_returns, n_components=2)\ngmm_rest = fit_gmm(rest_returns, n_components=2)\n\ndef print_gmm_summary(gmm, label):\n    print(f\"\\nGMM summary for {label}\")\n    for i, (w, mu, var) in enumerate(zip(gmm.weights_, gmm.means_, gmm.covariances_)):\n        print(f\" Component {i+1}: weight={w:.3f}, mean={mu[0]:.4f}, std={np.sqrt(var[0][0]):.4f}\")\n\nprint_gmm_summary(gmm_top, 'Top 10%')\nprint_gmm_summary(gmm_rest, 'Remaining 90%')\n\n\nThe Gaussian Mixture Models (GMMs) estimated separately on r_top and r_rest uncover two key structural regimes:\n\nBoth groups exhibit bimodal structure, with one cluster capturing low-mean, high-volatility returns, and the other positive-mean, low-volatility returns.\nHowever, for the top decile, the low-return regime has a lower weight and tighter variance, suggesting that even when shocks occur, they are attenuated within this elite group.\nIn contrast, the bottom 90% remains exposed to broader volatility regimes, maintaining a larger spread of possible outcomes.\n\n\n\n\n\n\nCode\n# k Selection by AIC vs. BIC\nfrom sklearn.mixture import GaussianMixture\nimport numpy as np\n\nX = df_mix['r_top'].dropna().values.reshape(-1, 1)\n\naic_scores = []\nbic_scores = []\nks = range(1, 21)\n\nfor k in ks:\n    gmm = GaussianMixture(n_components=k, random_state=42)\n    gmm.fit(X)\n    aic_scores.append(gmm.aic(X))\n    bic_scores.append(gmm.bic(X))\n\n# Plot AIC vs BIC to visualize optimal k\nplt.plot(ks, aic_scores, label='AIC')\nplt.plot(ks, bic_scores, label='BIC')\nplt.xlabel('Number of Components (k)')\nplt.ylabel('Information Criterion')\nplt.legend()\nplt.title('Model Selection: AIC vs BIC')\nplt.grid(True)\nplt.show()\n\n\nTo test whether the top decile itself contains further latent substructures, we estimate GMMs with increasing number of components (\\(k = 1, 2, ..., 20\\)). The result is striking:\n\nBoth AIC and BIC increase monotonically in \\(k\\), indicating that no additional latent components significantly improve model fit.\nThe most parsimonious and interpretable solution is obtained at \\(k = 2\\) or even \\(k = 1\\), reinforcing the view that a single dominant structural cluster governs return generation at the top.\n\nThis suggests that return asymmetry may not stem from overlapping regimes or transitory factors, but rather from the structural dominance of a statistically persistent subgroup of mega-cap firms.\n\n\n\n\nTogether, these results imply that:\n\nThe top-decile portfolio behaves like a capital sink—absorbing flows while insulating itself from market volatility.\nMarket return is no longer the outcome of dispersed competition, but rather converges to a structurally locked elite, supported by policy, passive flows, and indexation mechanics.\nThis undermines the classical view of a dynamic, risk-compensated market and instead points toward structural rent extraction by the few.",
    "crumbs": [
      "교육",
      "Model",
      "04 Structure"
    ]
  },
  {
    "objectID": "4_4_structure.html#capital-share-convexity-quadratic-regression-evolution",
    "href": "4_4_structure.html#capital-share-convexity-quadratic-regression-evolution",
    "title": "04 Structure",
    "section": "",
    "text": "Has capital become more concentrated in the top-ranked firms over time?\n\n\n\n\n\nCross-sectional regression at each \\(t\\):\n\\[\n\\text{CapShare}_{i,t} = \\alpha_t + \\beta_t \\cdot \\text{Rank}_i + \\gamma_t \\cdot \\text{Rank}_i^2 + \\varepsilon_{i,t}\n\\]\nwhere \\(\\text{Rank}_i\\) is percentile-based with state 1 = bottom 10%, …, state 10 = top 10%.\nThe convexity of capital distribution is captured by \\(\\gamma_t\\):\n\n\\(\\gamma_t &gt; 0\\) implies increasing convexity, or accelerated capital share at the top.\n\nTime-series \\(\\{\\gamma_t\\}\\) is visualized to show evolution of capital inequality.\n\n\n\n\n\n\nCode\n# Time-Series of Capital Share Convexity\nimport statsmodels.api as sm\n\n# 1. 각 날짜별로 state별 mktcap 합산 → CapShare_i 생성\ndef compute_capshare(df):\n    df = df.copy()\n    cap_by_state = df.groupby('state')['mktcap'].sum()\n    total_cap = cap_by_state.sum()\n    df['cap_share'] = df['state'].map(cap_by_state / total_cap)\n    return df\n\n# 2. cross-sectional regression에서 사용될 state별 요약 데이터 생성\ndef compute_quadratic_coefficients(df):\n    results = []\n    for date, group in df.groupby('date'):\n        # 유효한 state만 사용 (state 0은 제외)\n        group = group[group['state'] &gt; 0].copy()\n        cap_by_state = group.groupby('state')['mktcap'].sum()\n        total_cap = cap_by_state.sum()\n        capshare = cap_by_state / total_cap\n\n        # x = state (1~10), y = capshare\n        x = capshare.index.values\n        y = capshare.values\n\n        X = np.column_stack([x, x**2])\n        X = sm.add_constant(X)\n        model = sm.OLS(y, X).fit()\n\n        results.append({\n            'date': date,\n            'alpha': model.params[0],\n            'beta': model.params[1],\n            'gamma': model.params[2],\n            'r_squared': model.rsquared\n        })\n\n    return pd.DataFrame(results)\n\n# 실제 데이터가 존재한다고 가정하고 이후 분석 및 시각화 준비\n# 아래는 시각화 함수만 준비\ndef plot_gamma_time_series(gamma_df):\n    plt.figure(figsize=(12, 6))\n    plt.plot(gamma_df['date'], gamma_df['gamma'], label='Gamma (Quadratic Term)', color='darkred')\n\n    # 기준선들\n    plt.axhline(0, color='gray', linestyle='--', linewidth=1)\n    plt.axvline(pd.to_datetime(\"2010-01-01\"), color='black', linestyle='--', linewidth=1.2, label='2010 Breakpoint')\n    plt.title('Time-Series of Capital Share Convexity (γₜ)')\n    plt.xlabel('Date')\n    plt.ylabel('Gamma Coefficient (Convexity)')\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()\n\ncrsp_filtered = crsp[crsp['state'] &gt; 0].copy()\n\ngamma_df = compute_quadratic_coefficients(crsp_filtered)\n\nplot_gamma_time_series(gamma_df)\n\n\nThe time-series of the capital share convexity coefficient, \\(\\gamma_t\\), estimated via cross-sectional quadratic regressions, reveals a striking and persistent structure.\n\n\nCode\ngamma_df.drop('date', axis=1).describe()\n\n\nAcross the full sample period (1996–2023), \\(\\gamma_t\\) remains consistently positive and tightly bounded, with:\n\nA mean value of approximately 0.0186,\n\nA narrow interquartile range: [0.0181, 0.0192],\n\nA low standard deviation of 0.0007, and\n\nR-squared values averaging 0.67, indicating robust fit quality across time.\n\nThis remarkable temporal consistency suggests that the top ranks in the market-cap distribution have maintained a structurally convex advantage—absorbing disproportionately higher capital relative to their rank in nearly every month over the past three decades.\nA closer examination shows that: - Prior to 2010, there was mild downward drift in \\(\\gamma_t\\), consistent with episodic rebalancing and potential capital redistribution (e.g., during the post-dotcom or pre-GFC phase). - However, post-2010, \\(\\gamma_t\\) exhibits a slight but stable upward trend, indicating reinforced convexity—a steeper capital concentration curve among the top-ranked firms.\nThis empirical curvature echoes the shape of a discrete Lorenz curve, where \\(\\gamma_t\\) can be interpreted as a “capital inequality curvature index”. In this framework, increases in \\(\\gamma_t\\) correspond to a more extreme dominance of top ranks, consistent with both the capital lock-in and transition persistence mechanisms documented elsewhere in this study.\nImportantly, the tight dispersion and monotonic stabilization of \\(\\gamma_t\\) provide indirect but compelling support for the out-of-sample consistency of capital-weight-based strategies. The fact that the quadratic relationship between rank and capshare is not only convex but also time-invariant implies that a convex weighting scheme calibrated in-sample may remain optimal (or near-optimal) out-of-sample.\nThis further reinforces the central hypothesis of the TBTF framework: top capital positions are not merely episodic outcomes but persistent structural anchors in modern financial markets.",
    "crumbs": [
      "교육",
      "Model",
      "04 Structure"
    ]
  },
  {
    "objectID": "4_4_structure.html#persistence-and-lock-in-dynamics",
    "href": "4_4_structure.html#persistence-and-lock-in-dynamics",
    "title": "04 Structure",
    "section": "",
    "text": "Is capital mobility declining, with firms increasingly locked in to top positions?\n\n\n\n\n\nDefine states:\n\nState 10: Top 10% (≥ 90th percentile)\nStates 6–9: Middle 40% (50–90)\nStates 1–5: Bottom 50%\nState 0: Delisted or unranked\n\nFrom the panel data, estimate empirical first-order transition matrix:\n\\[\nP_{ij} = \\Pr(\\text{state}_{t+1} = j \\mid \\text{state}_t = i)\n\\]\nCompute:\n\nDiagonal elements: Retention probability (persistence)\nUpward vs. downward transitions: Asymmetry in mobility\nStationary distribution: Long-run capital allocation across states\n\n\n\n\n\nThe estimated transition matrix and its long-run stationary distribution reveal clear signs of capital lock-in, irreversibility, and structural mobility collapse in the U.S. equity market.\n\n\nCode\n# transition_matrix, stationary_series\n\n# Step 1: Ensure 'state' and 'state_lag' are present and valid\ncrsp_filtered = crsp[(crsp['state'].notnull()) & (crsp['state_lag'].notnull())].copy()\n\n# Step 2: Compute empirical transition matrix\ntransition_counts = (\n    crsp_filtered\n    .groupby(['state_lag', 'state'])\n    .size()\n    .unstack(fill_value=0)\n    .sort_index()\n    .reindex(columns=range(11), fill_value=0)\n)\n\n# Step 3: Convert counts to probabilities\ntransition_matrix = transition_counts.div(transition_counts.sum(axis=1), axis=0)\n\n# Step 4: Compute stationary distribution\n# Method: Use eigen decomposition for the transpose of the transition matrix\neigvals, eigvecs = np.linalg.eig(transition_matrix.T.values)\nstationary = np.real(eigvecs[:, np.isclose(eigvals, 1)])\nstationary = stationary[:, 0]\nstationary_dist = stationary / stationary.sum()\n\n# Prepare final transition matrix and stationary distribution\ntransition_matrix.index.name = 'From State'\ntransition_matrix.columns.name = 'To State'\nstationary_series = pd.Series(stationary_dist, index=transition_matrix.columns, name=\"Stationary Dist.\")\n\n\n# Markov Transition Matrix Heatmap (States 0–10)\nimport seaborn as sns\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(\n    transition_matrix,\n    annot=True,\n    fmt=\".2f\",\n    cmap=\"YlGnBu\",\n    linewidths=0.5,\n    cbar_kws={'label': 'Transition Probability'},\n    xticklabels=[f\"To {i}\" for i in transition_matrix.columns],\n    yticklabels=[f\"From {i}\" for i in transition_matrix.index]\n)\nplt.title(\"Markov Transition Matrix Heatmap (States 0–10)\")\nplt.xlabel(\"Next Period State\")\nplt.ylabel(\"Current Period State\")\nplt.tight_layout()\nplt.show()\n\n\nFirst, the persistence of the top decile group (State = 10) is striking. The monthly self-transition probability \\(P_{10,10}\\) reaches 0.972, the highest among all states, implying an expected duration of stay exceeding 36 months:\n\\[\n\\mathbb{E}[\\text{Duration} \\mid \\text{State } i] = \\frac{1}{1 - P_{ii}}\n\\]\nThis result supports the characterization of the top decile as a quasi-absorbing state—once firms enter, they tend to remain for extended periods, creating a long-term incumbency advantage.\nConversely, states 4 and 5 show the lowest diagonal entries (both ≈ 0.79), yielding the shortest expected durations. This suggests that firms in these mid-level ranks are most prone to reclassification—either moving up, down, or out—indicating local instability near the center of the capital distribution.\nDespite expectations that delisting risk (i.e., transition to State 0) would dominate in the lower ranks, the empirical matrix reveals that direct exits from States 1–10 are essentially zero. This implies that delisting or market exit rarely occurs from within the ranked universe. Instead, new listings (from State 0) enter most frequently into State 3, resulting in a pronounced inverted U-shape in the transition row from State 0.\n\n\nCode\n# Long-Run Stationary Distribution by State (Excl. Exit)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# State 1부터 10까지의 stationary probability\nstationary_series_nonzero = stationary_series.iloc[1:]\n\n# Plot\nplt.figure(figsize=(8, 5))\nsns.barplot(\n    x=stationary_series_nonzero.index.astype(str),\n    y=stationary_series_nonzero.values,\n    color='steelblue',\n    edgecolor='black'\n)\n\n# Annotate each bar with its value\nfor i, val in enumerate(stationary_series_nonzero.values):\n    plt.text(i, val + 0.001, f\"{val:.3f}\", ha='center', va='bottom', fontsize=10)\n\n# Plot labels and formatting\nplt.title(\"Long-Run Stationary Distribution by State (Excl. Exit)\", fontsize=14)\nplt.xlabel(\"State (Market Cap Rank)\", fontsize=12)\nplt.ylabel(\"Stationary Probability\", fontsize=12)\nplt.xticks(rotation=0)\nplt.ylim(0, stationary_series_nonzero.max() + 0.01)\nplt.tight_layout()\nplt.show()\n\n\nSecond, the stationary distribution—the long-run probability of occupying each state—exhibits a Pareto-like decay. The highest steady-state probability is in State 1 (≈ 16.6%), and declines monotonically through State 10 (≈ 7.9%), forming a shape reminiscent of the density of a Pareto distribution or a discrete inverse-power law:\n\nWhile State 1 represents the largest share of firms at any point in time, this is not reflective of capital dominance.\nRather, it suggests that the system structurally regenerates the bottom group, maintaining a large base of small-cap firms with limited upward mobility.\n\nThis result aligns with prior findings in household wealth studies—such as those using FRED data—where the top 1% accumulates disproportionate wealth, but the modal position of individuals lies in the bottom or middle deciles. Our firm-level Markov model complements this view by adding monthly transition granularity and dynamic persistence structure, which cannot be captured by static wealth shares alone.\nTaken together, the findings indicate that capital mobility is increasingly non-reversible and stratified, violating assumptions of ergodicity, symmetry, and competitive reallocation often found in representative-agent equilibrium models.\nThe U.S. equity market, viewed through the lens of Markovian state transitions, behaves less like a competitive allocation engine and more like a stratified hierarchy, where entry is rare, exits are asymmetric, and the top functions as a structural sink for capital.",
    "crumbs": [
      "교육",
      "Model",
      "04 Structure"
    ]
  },
  {
    "objectID": "4_4_structure.html#summary-table",
    "href": "4_4_structure.html#summary-table",
    "title": "04 Structure",
    "section": "",
    "text": "Structural Channel\nMechanism\nInterpretation\n\n\n\n\nMixture\nReturn as weighted sum of 2 subgroups\nTop decile increasingly drives aggregate performance\n\n\nConvexity\nCapital share as function of rank\nLorenz-type concentration intensifies over time\n\n\nTransition\nMarkov dynamics of rank-state mobility\nTop and Exit states are quasi-absorbing, path-dependent\n\n\n\nIntegrated Interpretation:\n\nTop-decile persistence reveals a form of quasi-absorption, where firms not only rise to the top but stay there disproportionately longer — effectively becoming “Too Big to Exit.”\nMiddle-rank volatility suggests structural fragility, with firms unable to stabilize or ascend — reflecting a “Fragile Middle” unable to convert mobility into permanence.\nLower-rank saturation shows pseudo-stationarity, where exit is more likely than upward mobility — a symptom of “regenerative failure” in the bottom half of the market.\nThe stationary distribution over rank states closely resembles a Pareto-like decay, echoing wealth inequality in household distributions and reinforcing the idea that capital mobility in financial markets has collapsed into hierarchical entrenchment.",
    "crumbs": [
      "교육",
      "Model",
      "04 Structure"
    ]
  },
  {
    "objectID": "4_2_data.html",
    "href": "4_2_data.html",
    "title": "02 Data",
    "section": "",
    "text": "This section describes the data used to construct and evaluate the TBTF portfolio strategy. We utilize multiple sources to enable a comprehensive analysis of market structure, return distributions, and benchmark comparisons. The primary dataset is drawn from CRSP, with supplemental reference portfolios and index returns from Fama-French and Yahoo Finance, respectively.\n\n\nWe use the Center for Research in Security Prices (CRSP) monthly stock files from January 1996 to December 2023. The CRSP universe includes all listed common stocks on the NYSE, Nasdaq, and AMEX exchanges. Key variables include:\n\nPERMNO: Unique stock identifier\n\nDate: Monthly observation date\n\nMarket Capitalization: Shares outstanding × price\n\nprimaryexch : primary exchange\nSIC Code: Optional industry classification\n\n\n\nUnlike fixed-income instruments, public equities derive their economic value primarily from their tradeability. While dividends contribute to realized returns, they are secondary to a stock’s market capitalization path, which reflects its ability to attract and retain capital under uncertainty.\nWe therefore adopt a market cap–based valuation approach, using capitalization time series as the core proxy for equity value and flow dynamics.\n\n\n\nWhen a stock is delisted:\n\nIts market capitalization becomes zero, and its final gross return is set to 0, implying net return of –1.\n\n\\[ R_{\\text{gross}} = 0, \\quad R_{\\text{net}} = -1 \\]\nThis reflects the reality that delisted equities become practically untradeable and lose all resale value for investors. In contrast to fixed-income instruments, equities have no residual recovery claim.\n\n\n\n\nNewly listed stocks (IPOs) are initially placed in an external state (state 0).\nFrom their first full month after IPO:\n\nTheir market capitalization is ranked within the universe.\nThey are assigned to a decile group (state 1–10) based on market cap percentile.\n\n\n\n\n\nTo avoid survivorship bias:\n\nWe retain all stocks, including those that were delisted during the sample period.\nThis prevents the overestimation of returns and misrepresentation of capital mobility that arises from survivor-only datasets.\n\n\n\n\nFor transition matrix estimation and mixture modeling (Section 4), we define the following percentile-based state mapping:\n\n\n\n\n\n\n\n\nState\nMarket Cap Percentile Range\nDescription\n\n\n\n\n10\n90%–100%\nTop 10% (largest-cap stocks)\n\n\n9\n80%–90%\n\n\n\n…\n…\n\n\n\n1\n0%–10%\nBottom 10% (smallest-cap stocks)\n\n\n0\nExternal\nDelisted stocks (absorbing state) or IPOs before full inclusion\n\n\n\nThis unconventional but analytically convenient mapping allows for intuitive modeling of:\n\nUpward mobility as a transition to higher state numbers\nCapital lock-in at the top (persistence in State 10)\nExit fragility for lower states (transition to State 0)\n\n\n\n\nThis mapping enables a semi-absorbing, non-reversible Markov transition model, where:\n\nDelisted stocks enter the absorbing state (0) permanently.\nStocks that remain listed transition among states 1–10, based on their market cap percentile ranks at each time \\(t\\).\nNew entrants (IPOs) begin in state 0 and join the ranked universe from their second month onward.\n\nThis design forms the empirical backbone for transition matrix analysis, mixture decomposition, and long-run mobility asymmetry in Section 4.\n\n\n\n\n\n\nFrom the Ken French Data Library, we collect the following datasets:\n\nME10 Decile Portfolios: Used to compare return characteristics between small-cap (s_10) and large-cap (b_10) portfolios.\nME × PRIOR Portfolios: Cross-sorted on size (ME) and prior 12-month return momentum. We focus on:\n\nME5 PRIOR4: Large size, high momentum\nME4 PRIOR3: Mid size, moderate momentum\n\n\nThese portfolios are value-weighted, rebalanced annually using NYSE breakpoints, and provided as monthly returns.\n\n\n\nTo compare TBTF performance with market-traded investment vehicles, we incorporate the following index and ETF data:\n\nPre-2010 Indices (from Yahoo Finance):\n\n^NDX (Nasdaq 100 Index)\n^DJI (Dow Jones Industrial Average)\n\nPost-2010 ETFs:\n\nVTI: Vanguard Total Stock Market ETF (inception: 2001)\nSPY: SPDR S&P 500 ETF (inception: 1993)\nQQQ: Invesco Nasdaq-100 ETF (inception: 1999)\nDIA: SPDR Dow Jones Industrial Average ETF (inception: 1998)\n\n\nThe inclusion of these instruments enables direct comparison with passive investment strategies commonly available to retail and institutional investors.",
    "crumbs": [
      "교육",
      "Model",
      "02 Data"
    ]
  },
  {
    "objectID": "4_2_data.html#primary-data-crsp-monthly-stock-file",
    "href": "4_2_data.html#primary-data-crsp-monthly-stock-file",
    "title": "02 Data",
    "section": "",
    "text": "We use the Center for Research in Security Prices (CRSP) monthly stock files from January 1996 to December 2023. The CRSP universe includes all listed common stocks on the NYSE, Nasdaq, and AMEX exchanges. Key variables include:\n\nPERMNO: Unique stock identifier\n\nDate: Monthly observation date\n\nMarket Capitalization: Shares outstanding × price\n\nprimaryexch : primary exchange\nSIC Code: Optional industry classification\n\n\n\nUnlike fixed-income instruments, public equities derive their economic value primarily from their tradeability. While dividends contribute to realized returns, they are secondary to a stock’s market capitalization path, which reflects its ability to attract and retain capital under uncertainty.\nWe therefore adopt a market cap–based valuation approach, using capitalization time series as the core proxy for equity value and flow dynamics.\n\n\n\nWhen a stock is delisted:\n\nIts market capitalization becomes zero, and its final gross return is set to 0, implying net return of –1.\n\n\\[ R_{\\text{gross}} = 0, \\quad R_{\\text{net}} = -1 \\]\nThis reflects the reality that delisted equities become practically untradeable and lose all resale value for investors. In contrast to fixed-income instruments, equities have no residual recovery claim.\n\n\n\n\nNewly listed stocks (IPOs) are initially placed in an external state (state 0).\nFrom their first full month after IPO:\n\nTheir market capitalization is ranked within the universe.\nThey are assigned to a decile group (state 1–10) based on market cap percentile.\n\n\n\n\n\nTo avoid survivorship bias:\n\nWe retain all stocks, including those that were delisted during the sample period.\nThis prevents the overestimation of returns and misrepresentation of capital mobility that arises from survivor-only datasets.\n\n\n\n\nFor transition matrix estimation and mixture modeling (Section 4), we define the following percentile-based state mapping:\n\n\n\n\n\n\n\n\nState\nMarket Cap Percentile Range\nDescription\n\n\n\n\n10\n90%–100%\nTop 10% (largest-cap stocks)\n\n\n9\n80%–90%\n\n\n\n…\n…\n\n\n\n1\n0%–10%\nBottom 10% (smallest-cap stocks)\n\n\n0\nExternal\nDelisted stocks (absorbing state) or IPOs before full inclusion\n\n\n\nThis unconventional but analytically convenient mapping allows for intuitive modeling of:\n\nUpward mobility as a transition to higher state numbers\nCapital lock-in at the top (persistence in State 10)\nExit fragility for lower states (transition to State 0)\n\n\n\n\nThis mapping enables a semi-absorbing, non-reversible Markov transition model, where:\n\nDelisted stocks enter the absorbing state (0) permanently.\nStocks that remain listed transition among states 1–10, based on their market cap percentile ranks at each time \\(t\\).\nNew entrants (IPOs) begin in state 0 and join the ranked universe from their second month onward.\n\nThis design forms the empirical backbone for transition matrix analysis, mixture decomposition, and long-run mobility asymmetry in Section 4.",
    "crumbs": [
      "교육",
      "Model",
      "02 Data"
    ]
  },
  {
    "objectID": "4_2_data.html#secondary-data-sources",
    "href": "4_2_data.html#secondary-data-sources",
    "title": "02 Data",
    "section": "",
    "text": "From the Ken French Data Library, we collect the following datasets:\n\nME10 Decile Portfolios: Used to compare return characteristics between small-cap (s_10) and large-cap (b_10) portfolios.\nME × PRIOR Portfolios: Cross-sorted on size (ME) and prior 12-month return momentum. We focus on:\n\nME5 PRIOR4: Large size, high momentum\nME4 PRIOR3: Mid size, moderate momentum\n\n\nThese portfolios are value-weighted, rebalanced annually using NYSE breakpoints, and provided as monthly returns.\n\n\n\nTo compare TBTF performance with market-traded investment vehicles, we incorporate the following index and ETF data:\n\nPre-2010 Indices (from Yahoo Finance):\n\n^NDX (Nasdaq 100 Index)\n^DJI (Dow Jones Industrial Average)\n\nPost-2010 ETFs:\n\nVTI: Vanguard Total Stock Market ETF (inception: 2001)\nSPY: SPDR S&P 500 ETF (inception: 1993)\nQQQ: Invesco Nasdaq-100 ETF (inception: 1999)\nDIA: SPDR Dow Jones Industrial Average ETF (inception: 1998)\n\n\nThe inclusion of these instruments enables direct comparison with passive investment strategies commonly available to retail and institutional investors.",
    "crumbs": [
      "교육",
      "Model",
      "02 Data"
    ]
  },
  {
    "objectID": "4_2_data.html#appendix",
    "href": "4_2_data.html#appendix",
    "title": "02 Data",
    "section": "Appendix",
    "text": "Appendix\n\n\nData Availability and Frequency\n\n\n\n\n\n\n\n\n\nDataset\nRaw data Frequency\nPeriod Covered\nSource\n\n\n\n\nFama-French ME Decile\nMonthly\n1963–2023\nKen French Data Library\n\n\nCRSP Monthly Stock File\nMonthly\n1996–2023\nWRDS/CRSP\n\n\nFama-French ME × PRIOR\nMonthly\n2010–2023\nKen French Data Library\n\n\nDIA, QQQ, SPY, VTI (ETFs)\nDaily\n2010–2023\nYahoo Finance\n\n\nDJIA, NDX (Price Indices)\nDaily\n1996–2009\nYahoo Finance\n\n\n\n\nPooled Panel data\n\nFama-French data library\nFRED database\n\nPanel data\n\nCRSP from WRDS\nYahoo Finance",
    "crumbs": [
      "교육",
      "Model",
      "02 Data"
    ]
  },
  {
    "objectID": "3_2_history_nyse.html",
    "href": "3_2_history_nyse.html",
    "title": "History of NYSE ME",
    "section": "",
    "text": "The NYSE continues to exhibit substantial market capitalization concentration. Since 2010 — and more sharply since 2016 — the very largest firms have distanced themselves even from the rest of the top 5%, highlighting the structural importance of tail dynamics in financial markets. Any realistic asset pricing model must account for this persistent and extreme upper-tail asymmetry.\nCode\nimport pandas as pd\nimport pandas_datareader.data as pdr\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning) # FutureWarning 제거\n\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\n# Define time period\nstart_date = \"1996-01-01\"\nend_date = \"2024-12-31\"\n\n# Fetch Fama-French ME_Breakpoints data (NYSE percentile breakpoints)\nbreakpoints_raw = pdr.DataReader(\n    name=\"ME_Breakpoints\",\n    data_source=\"famafrench\",\n    start=start_date,\n    end=end_date\n)[0]\n\n# Extract percentile labels from column names (e.g., \"(0, 5)\" -&gt; \"5\")\ndef extract_upper_bound(label):\n    if isinstance(label, str) and \"(\" in label:\n        try:\n            return str(int(label.split(\",\")[1].replace(\")\", \"\").strip()))\n        except Exception:\n            return label\n    elif isinstance(label, tuple):\n        return str(label[1])\n    return str(label)\n\n# Rename columns to only use upper percentile values\ncolumns_to_rename = {col: extract_upper_bound(col) for col in breakpoints_raw.columns if col != 'Count'}\nbreakpoints = breakpoints_raw.rename(columns=columns_to_rename)\n\n# Normalize ME values by number of firms (Count) to get \"per firm\" values\nfor col in breakpoints.columns:\n    if col != 'Count':\n        breakpoints[col] = breakpoints[col] / breakpoints['Count']\n\n# Print average firm count over the period\navg_count = int(breakpoints['Count'].mean())\nprint(f\"Average number of NYSE firms from {start_date} to {end_date}: {avg_count}\")\n\n\nAverage number of NYSE firms from 1996-01-01 to 2024-12-31: 1386",
    "crumbs": [
      "교육",
      "History",
      "History of NYSE ME"
    ]
  },
  {
    "objectID": "3_2_history_nyse.html#nyse-market-equity-breakpoints-long-term-dynamics",
    "href": "3_2_history_nyse.html#nyse-market-equity-breakpoints-long-term-dynamics",
    "title": "History of NYSE ME",
    "section": "NYSE Market Equity Breakpoints: Long-Term Dynamics",
    "text": "NYSE Market Equity Breakpoints: Long-Term Dynamics\nThe Fama-French dataset ME_Breakpoints provides monthly percentile breakpoints for market equity (ME), computed only from NYSE stocks. These breakpoints span from the 5th to the 100th percentile and are calculated based on market capitalization (price times shares outstanding, in millions of USD) at month-end. Importantly, closed-end funds and REITs are excluded, and only firms with CRSP share codes 10 or 11 and valid price/share data are included.\nThis file investigates the evolution of market concentration in the NYSE based on these ME breakpoints, emphasizing the dynamics in the upper tail of the distribution, particularly the top 5% of firms.",
    "crumbs": [
      "교육",
      "History",
      "History of NYSE ME"
    ]
  },
  {
    "objectID": "3_2_history_nyse.html#breakpoint-time-series-per-firm-19962024",
    "href": "3_2_history_nyse.html#breakpoint-time-series-per-firm-19962024",
    "title": "History of NYSE ME",
    "section": "Breakpoint Time Series per Firm (1996–2024)",
    "text": "Breakpoint Time Series per Firm (1996–2024)\nWe plot the NYSE ME breakpoints divided by the number of firms each month (“per firm”) from 1996 to 2024. The results reveal two distinct phases:\n\nPre-2010: A cyclical pattern dominates, consistent with broader economic expansions and contractions. For instance, the 2000–2001 tech bubble and the 2008 global financial crisis exhibit clear signals of expansion and collapse.\nPost-2010: A structural break is visible. Especially since 2016, the average ME per firm in the top percentile (100%) exhibits a sharp and persistent upward trend.\n\nThis long-term trend implies a sustained capital lock-in within a small number of mega-cap firms, increasingly distanced from the rest of the NYSE universe.\n\n\nCode\n# Plot selected percentile breakpoints over time\nselected_percentiles = ['80', '85', '90', '95', '100']\nbreakpoints[selected_percentiles].plot(figsize=(10, 5))\n\nplt.legend(title='Percentile')\nplt.ylabel('Market Equity (in millions) per firm')\nplt.title('NYSE ME Breakpoints (Per Firm Basis)')\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "교육",
      "History",
      "History of NYSE ME"
    ]
  },
  {
    "objectID": "3_2_history_nyse.html#cross-sectional-concentration-at-the-tail",
    "href": "3_2_history_nyse.html#cross-sectional-concentration-at-the-tail",
    "title": "History of NYSE ME",
    "section": "Cross-Sectional Concentration at the Tail",
    "text": "Cross-Sectional Concentration at the Tail\nTo better understand the shape of the right tail, we visualize the percentile distribution at the most recent observation (2024-12). The result is striking: while the ME per firm grows gradually between percentiles 5 to 95, a dramatic jump occurs at the 100th percentile.\nThis highlights that the concentration of market value within the top 5% is extreme, and the very last percentile alone contains firms with ME per firm often an order of magnitude greater than those in the 95th percentile.\n\n\nCode\ndef plot_breakpoints_at_end(df, count_col='Count', start_pct=0, end_pct=None, title_suffix=''):\n    \"\"\"\n    Plot breakpoints at the last available date.\n    \n    Parameters:\n        df: DataFrame with percentile columns and 'Count'\n        count_col: name of the column representing number of firms (default: 'Count')\n        start_pct: starting index for column slice (e.g., -20 for top 20 percentiles)\n        end_pct: ending index for column slice (default: None means till the end)\n        title_suffix: string appended to plot title\n    \"\"\"\n    # Select data at last date\n    last_row = df.tail(1).drop(columns=[count_col])\n    \n    # Slice desired percentile columns\n    selected_columns = last_row.columns[start_pct:end_pct]\n    y_data = pd.to_numeric(last_row[selected_columns].values.flatten(), errors='coerce')\n    \n    # Plot\n    plt.figure(figsize=(8, 4))\n    plt.plot(selected_columns, y_data, marker='o')\n    plt.xticks(rotation=45, ha='right')\n    plt.xlabel('Percentile')\n    plt.ylabel('ME Breakpoints (in millions per firm)')\n    plt.title(f'NYSE ME Breakpoints at {df.index[-1]} {title_suffix}')\n    plt.tight_layout()\n    plt.grid(True)\n    plt.show()\n\n# 전체 percentile 구간 시각화\nplot_breakpoints_at_end(breakpoints, start_pct=0, title_suffix='(Full Range)')\n\n# 상위 3개 빼고 시각화 (5~90)\nplot_breakpoints_at_end(breakpoints, start_pct=-20, end_pct=-2, title_suffix='(upto Top 20 Percentiles)')\n\n# 가장 극단적인 상위 3개만 (90, 95, 100 만)\nplot_breakpoints_at_end(breakpoints, start_pct=-3, title_suffix='(Top 3 Percentiles)')",
    "crumbs": [
      "교육",
      "History",
      "History of NYSE ME"
    ]
  },
  {
    "objectID": "3_2_history_nyse.html#time-series-of-the-tail-ratio-100th-95th-percentile",
    "href": "3_2_history_nyse.html#time-series-of-the-tail-ratio-100th-95th-percentile",
    "title": "History of NYSE ME",
    "section": "Time Series of the Tail Ratio: 100th / 95th Percentile",
    "text": "Time Series of the Tail Ratio: 100th / 95th Percentile\nTo quantify tail concentration dynamics over time, we construct a monthly time series of the ME per firm ratio between the 100th and 95th percentiles. This ratio serves as a tail index for how dominant the very largest firms are, even among the elite.\nThe time series reveals the following:\n\n1996–2001: Rapid escalation during the dot-com boom, with the ratio peaking above 20.\n2003–2008: Stabilization around ~13.\n2009: A brief post-crisis surge back above 18.\n2010–2016: A sharp decline and plateau near 7, indicating relative equality among top-tier firms.\nPost-2016: A gradual resurgence in the ratio, reflecting renewed concentration at the very top.\n\n\n\nCode\n# Calculate the ME per firm ratio: 100th / 95th percentile\nratio_series = breakpoints['100'] / breakpoints['95']\n\n# Convert PeriodIndex to DatetimeIndex for plotting\nratio_series.index = ratio_series.index.to_timestamp()\n\n# Plot both raw and log-transformed ratio\nplt.figure(figsize=(12, 5))\n\nax = plt.gca()\nax.xaxis.set_major_locator(mdates.YearLocator(2))\nax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\nax.tick_params(axis='x', rotation=45) # x축 눈금 회전 추가\n\n# Raw ratio\nplt.plot(ratio_series.index, ratio_series, marker='o')\nplt.title('ME per Firm Ratio: 100th / 95th Percentile')\nplt.xlabel('Date')\nplt.ylabel('Ratio (ME[100] / ME[95])')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Calculate the ME per firm ratio: 95th / 50th percentile\nratio_series = breakpoints['95'] / breakpoints['50']\n\n# Convert PeriodIndex to DatetimeIndex for plotting\nratio_series.index = ratio_series.index.to_timestamp()\n\n# Plot both raw and log-transformed ratio\nplt.figure(figsize=(12, 5))\n\n# Raw ratio\nplt.plot(ratio_series.index, ratio_series, marker='o')\nplt.title('ME per Firm Ratio: 95th / 50th Percentile')\nplt.xlabel('Date')\nplt.ylabel('Ratio (ME[95] / ME[50])')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "교육",
      "History",
      "History of NYSE ME"
    ]
  },
  {
    "objectID": "3_2_history_nyse.html#broader-context-nyse-and-the-top-5",
    "href": "3_2_history_nyse.html#broader-context-nyse-and-the-top-5",
    "title": "History of NYSE ME",
    "section": "Broader Context: NYSE and the Top 5%",
    "text": "Broader Context: NYSE and the Top 5%\nIt is critical to underscore that Fama-French breakpoints are calculated using only NYSE stocks. Despite the rise of Nasdaq dominance in recent decades, the NYSE remains the foundation for constructing breakpoints in academic asset pricing.\nThe breakpoints for:\n\nMarket Equity (ME): monthly, based on NYSE stocks with viable price and share data.\nBook-to-Market (BE/ME): annually, using BE from t-1 and ME from December of t-1.\nPrior 2–12 month Return: monthly, requiring CRSP price and return data.\n\nThese indicators, especially in the upper tail, are overwhelmingly driven by the top 5% of NYSE firms — roughly 60–70 firms. These companies exert outsized influence on asset pricing, portfolio construction, and market dynamics.",
    "crumbs": [
      "교육",
      "History",
      "History of NYSE ME"
    ]
  },
  {
    "objectID": "2_lit_review.html",
    "href": "2_lit_review.html",
    "title": "Literature Review",
    "section": "",
    "text": "Traditional financial economics posits that expected returns serve as compensation for bearing systematic risk. This principle underpins several foundational models: - The Capital Asset Pricing Model (CAPM) asserts that expected excess returns are proportional to market beta. - The Fama-French Three- and Five-Factor Models extend CAPM by incorporating size, value, profitability, and investment factors (Fama and French 1993; - Fama and French 2015). - The Betting Against Beta (BAB) framework suggests that low-beta assets may outperform on a risk-adjusted basis (Frazzini and Pedersen 2014). - Mean-Variance Optimization, introduced by Markowitz (1952), assumes that returns are normally distributed and fully explained by their mean and covariance structure.\nThese models rest on assumptions of frictionless markets, convex opportunity sets, homogeneous beliefs, and the absence of arbitrage opportunities.\n\n\n\nUtilizing CRSP monthly data, Fama-French industry portfolios, and ETF benchmarks across pre- and post-2010 periods, this study revisits and challenges these foundational models.\n\n\n\nMulti-Factor Model with Fama-MacBeth Regression (Fama and MacBeth 1973): When applied to industry portfolios, it reveals large, persistent unexplained alphas (Fama and French 1997) while the market risk premium becomes statistically insignificant. Asness et al. (2018) argue that the size premium persists when controlling for quality factors, suggesting the reversal may stem from low-quality small-cap stocks. Additionally, Hou, Xue, and Zhang (2020) find that many documented anomalies, including size-related ones, do not hold up under rigorous testing, while mitigated if carefully controlling microcap stocks and using value-weighted returns. For example, excess returns of FF industry portfolios are regressed on macro and financial factors, revealing multicollinearity among predictors like market excess returns, value spreads, and volatility indicators, leading to unstable coefficients and weak out-of-sample explanatory power.\n\nLasso regularization confirms model sparsity, with only a few predictors consistently selected, often exhibiting inconsistent signs (Kozak, Nagel, and Santosh 2020).\n\n\n\n\n\nThe BAB Strategy posits that high-beta assets underperform on a risk-adjusted basis (Frazzini and Pedersen 2014).\nEmpirical tests using SIC 10 portfolios reveal that optimal Sharpe ratios are achieved by portfolios with betas close to 1, rather than low or high values. The relationship between beta and performance is either flat or non-monotonic, suggesting that risk exposure is not systematically mispriced but structurally inert.\n\n\n\n\n\nUnconstrained Mean-Variance (MV) Optimization yields extreme and highly unstable weights, leading to high turnover and poor out-of-sample performance (DeMiguel, Garlappi, and Uppal 2009).\nImposing constraints (e.g., no short-selling) enhances stability but reduces performance below passive benchmarks. Passive value-weighted portfolios consistently outperform MV portfolios, even when optimized using historical data.\n\n\n\n\n\nBlack and Litterman (1992) combines CAPM equilibrium with investor views via Bayesian updating.\n\nBrandt, Santa-Clara, and Valkanov (2009) incorporate momentum and size signals into dynamic allocation, but our empirical tests reveal no consistent outperformance over ETF benchmarks post-2010, suffering from estimation error and overfitting in regimes characterized by unstable return distributions.\n\n\n\n\n\nThe underperformance of theoretically sound but empirically fragile strategies suggests capital markets reward position within a segmented system rather than risk.\n\nLarge-cap firms are insulated from downward transitions, mirroring the ‘superstar’ effect where small quality differences yield large market outcomes, as described by Rosen (Rosen 1981). In banking, Gandhi and Lustig (2015) find large banks have lower risk-adjusted returns, indicative of implicit government guarantees in adverse scenarios.\n\nSmall-cap firms increasingly comprise structurally inefficient “zombie” entities sustained by liquidity support (Acharya et al. 2024).\nReturn distributions no longer reflect efficient risk pricing but are shaped by arbitrage-limited capital flows, ETF-induced inflows, and policy distortions (Glosten, Li, and Zhang 2021; Shleifer and Vishny 1997).\nReturn distributions reflect arbitrage-limited capital flows, ETF-induced inflows, and policy distortions, with Jiang, Vayanos, and Zheng (2020) showing passive investing disproportionately benefits large firms, exacerbating concentration. This aligns with Autor et al. (2020), linking market power concentration to declining labor share, highlighting structural market dynamics.\n\nThese findings motivate rank-based strategies like TBTF, capitalizing on structural capital dynamics rather than informational edges.\n\n\n\nThis study extends literature on asset pricing anomalies, building on Hou, Xue, and Zhang (2020). regarding anomaly robustness. It introduces Markov-based transition models for capital lock-in, aligning with rising concentration insights from Autor et al. (2020). It offers low-turnover, high-performance portfolio strategies and critiques ETF-driven distortions and post-QE concentration, reflecting real-world finance breakdowns under structural stress.\nThis study builds upon and extends several strands of the literature:\n\nAsset Pricing Anomalies: It demonstrates new forms of structural deviation not explained by established factors (Hou, Xue, and Zhang 2020)\nMarket Structure and Capital Mobility: It introduces Markov-based transition models to analyze capital lock-in .\nPortfolio Strategy Design: It offers a low-turnover, high-performance alternative to optimization-based approaches.\nMarket Efficiency Critique: It provides empirical support for critiques of ETF-driven distortions and post-QE market concentration (Autor et al. 2020; Jiang, Vayanos, and Zheng 2020).\n\nThe TBTF strategy does not reject modern finance but reflects its real-world breakdown under structural stress. Its continued success raises fundamental questions about what financial markets price and for whose benefit.",
    "crumbs": [
      "교육",
      "Literature Review"
    ]
  },
  {
    "objectID": "2_lit_review.html#classical-theories-of-risk-based-asset-pricing",
    "href": "2_lit_review.html#classical-theories-of-risk-based-asset-pricing",
    "title": "Literature Review",
    "section": "",
    "text": "Traditional financial economics posits that expected returns serve as compensation for bearing systematic risk. This principle underpins several foundational models: - The Capital Asset Pricing Model (CAPM) asserts that expected excess returns are proportional to market beta. - The Fama-French Three- and Five-Factor Models extend CAPM by incorporating size, value, profitability, and investment factors (Fama and French 1993; - Fama and French 2015). - The Betting Against Beta (BAB) framework suggests that low-beta assets may outperform on a risk-adjusted basis (Frazzini and Pedersen 2014). - Mean-Variance Optimization, introduced by Markowitz (1952), assumes that returns are normally distributed and fully explained by their mean and covariance structure.\nThese models rest on assumptions of frictionless markets, convex opportunity sets, homogeneous beliefs, and the absence of arbitrage opportunities.",
    "crumbs": [
      "교육",
      "Literature Review"
    ]
  },
  {
    "objectID": "2_lit_review.html#empirical-challenges-to-risk-based-models",
    "href": "2_lit_review.html#empirical-challenges-to-risk-based-models",
    "title": "Literature Review",
    "section": "",
    "text": "Utilizing CRSP monthly data, Fama-French industry portfolios, and ETF benchmarks across pre- and post-2010 periods, this study revisits and challenges these foundational models.\n\n\n\nMulti-Factor Model with Fama-MacBeth Regression (Fama and MacBeth 1973): When applied to industry portfolios, it reveals large, persistent unexplained alphas (Fama and French 1997) while the market risk premium becomes statistically insignificant. Asness et al. (2018) argue that the size premium persists when controlling for quality factors, suggesting the reversal may stem from low-quality small-cap stocks. Additionally, Hou, Xue, and Zhang (2020) find that many documented anomalies, including size-related ones, do not hold up under rigorous testing, while mitigated if carefully controlling microcap stocks and using value-weighted returns. For example, excess returns of FF industry portfolios are regressed on macro and financial factors, revealing multicollinearity among predictors like market excess returns, value spreads, and volatility indicators, leading to unstable coefficients and weak out-of-sample explanatory power.\n\nLasso regularization confirms model sparsity, with only a few predictors consistently selected, often exhibiting inconsistent signs (Kozak, Nagel, and Santosh 2020).\n\n\n\n\n\nThe BAB Strategy posits that high-beta assets underperform on a risk-adjusted basis (Frazzini and Pedersen 2014).\nEmpirical tests using SIC 10 portfolios reveal that optimal Sharpe ratios are achieved by portfolios with betas close to 1, rather than low or high values. The relationship between beta and performance is either flat or non-monotonic, suggesting that risk exposure is not systematically mispriced but structurally inert.\n\n\n\n\n\nUnconstrained Mean-Variance (MV) Optimization yields extreme and highly unstable weights, leading to high turnover and poor out-of-sample performance (DeMiguel, Garlappi, and Uppal 2009).\nImposing constraints (e.g., no short-selling) enhances stability but reduces performance below passive benchmarks. Passive value-weighted portfolios consistently outperform MV portfolios, even when optimized using historical data.\n\n\n\n\n\nBlack and Litterman (1992) combines CAPM equilibrium with investor views via Bayesian updating.\n\nBrandt, Santa-Clara, and Valkanov (2009) incorporate momentum and size signals into dynamic allocation, but our empirical tests reveal no consistent outperformance over ETF benchmarks post-2010, suffering from estimation error and overfitting in regimes characterized by unstable return distributions.",
    "crumbs": [
      "교육",
      "Literature Review"
    ]
  },
  {
    "objectID": "2_lit_review.html#structural-interpretation-and-motivation-for-tbtf",
    "href": "2_lit_review.html#structural-interpretation-and-motivation-for-tbtf",
    "title": "Literature Review",
    "section": "",
    "text": "The underperformance of theoretically sound but empirically fragile strategies suggests capital markets reward position within a segmented system rather than risk.\n\nLarge-cap firms are insulated from downward transitions, mirroring the ‘superstar’ effect where small quality differences yield large market outcomes, as described by Rosen (Rosen 1981). In banking, Gandhi and Lustig (2015) find large banks have lower risk-adjusted returns, indicative of implicit government guarantees in adverse scenarios.\n\nSmall-cap firms increasingly comprise structurally inefficient “zombie” entities sustained by liquidity support (Acharya et al. 2024).\nReturn distributions no longer reflect efficient risk pricing but are shaped by arbitrage-limited capital flows, ETF-induced inflows, and policy distortions (Glosten, Li, and Zhang 2021; Shleifer and Vishny 1997).\nReturn distributions reflect arbitrage-limited capital flows, ETF-induced inflows, and policy distortions, with Jiang, Vayanos, and Zheng (2020) showing passive investing disproportionately benefits large firms, exacerbating concentration. This aligns with Autor et al. (2020), linking market power concentration to declining labor share, highlighting structural market dynamics.\n\nThese findings motivate rank-based strategies like TBTF, capitalizing on structural capital dynamics rather than informational edges.",
    "crumbs": [
      "교육",
      "Literature Review"
    ]
  },
  {
    "objectID": "2_lit_review.html#contributions-to-the-literature",
    "href": "2_lit_review.html#contributions-to-the-literature",
    "title": "Literature Review",
    "section": "",
    "text": "This study extends literature on asset pricing anomalies, building on Hou, Xue, and Zhang (2020). regarding anomaly robustness. It introduces Markov-based transition models for capital lock-in, aligning with rising concentration insights from Autor et al. (2020). It offers low-turnover, high-performance portfolio strategies and critiques ETF-driven distortions and post-QE concentration, reflecting real-world finance breakdowns under structural stress.\nThis study builds upon and extends several strands of the literature:\n\nAsset Pricing Anomalies: It demonstrates new forms of structural deviation not explained by established factors (Hou, Xue, and Zhang 2020)\nMarket Structure and Capital Mobility: It introduces Markov-based transition models to analyze capital lock-in .\nPortfolio Strategy Design: It offers a low-turnover, high-performance alternative to optimization-based approaches.\nMarket Efficiency Critique: It provides empirical support for critiques of ETF-driven distortions and post-QE market concentration (Autor et al. 2020; Jiang, Vayanos, and Zheng 2020).\n\nThe TBTF strategy does not reject modern finance but reflects its real-world breakdown under structural stress. Its continued success raises fundamental questions about what financial markets price and for whose benefit.",
    "crumbs": [
      "교육",
      "Literature Review"
    ]
  },
  {
    "objectID": "1_intro.html",
    "href": "1_intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Over the past two decades, U.S. financial markets have undergone a series of profound structural transformations. These changes—accelerated by post-2008 Federal Reserve interventions, the rise of passive investing, and the dominance of a few mega-cap firms—have gradually undermined the foundational assumptions of neoclassical asset pricing models. Most notably, the classical assumptions of global convexity, representative agents, and arbitrage-free pricing appear increasingly misaligned with empirical realities.\nContemporary market dynamics no longer conform to the canonical vision of systematic risk being priced efficiently. Instead, capital appears to flow disproportionately toward a small set of dominant firms, driven not by marginal productivity or risk-adjusted returns, but by network effects, ETF-driven capital flows, and central bank distortions. These flows exhibit characteristics more consistent with structural rent allocation than with traditional risk-based compensation.\n\n\n\nThis paper challenges the notion that financial markets primarily reward investors for bearing systematic risk. Instead, it proposes that post-crisis capital markets have evolved into a system of rent-based pricing, in which certain firms absorb a growing share of capital inflows due to structural advantage rather than economic fundamentals. This environment favors firms that are “too big to fail” (TBTF)—not because they are intrinsically more productive or efficient, but because the market structure allows them to persist at the top through passive flows and low volatility.\nThe study investigates whether stock return distributions in the U.S. stock market now resemble a time-varying mixture of heterogeneous assets under arbitrage-limited dynamics, as opposed to a homogeneous no-arbitrage equilibrium. The market appears to be governed less by efficient risk sharing and more by path-dependent capital accumulation, leading to a form of structural capital lock-in and asymmetric mobility across capital ranks.\n\n\n\nThe study is guided by a set of empirical regularities and theoretical conjectures:\n\nReturn Distributions have become increasingly asymmetric and exhibit fat tails concentrated in top-cap stocks.\nCapital Concentration is not only extreme in level but persistent in share and rank.\nTransition Dynamics across size ranks show decreasing upward mobility and increasing lock-in duration for top-cap stocks.\nMarket Efficiency is being eroded by capital polarization, in which capital increasingly accumulates at the top while the rest of the distribution remains stagnant.\nETFs and QE Policies act as reinforcing mechanisms, feeding capital into already dominant firms, thereby reducing entropy and distorting allocative efficiency.\n\n\n\n\nImportantly, the distortion of allocative efficiency is not limited to the top end of the market. At the lower end of the capitalization spectrum, another structural problem emerges: the survival and persistence of zombie firms—unproductive small-cap companies that continue to operate despite being economically non-viable.\nFueled by prolonged low interest rates, government credit guarantees, and liquidity injections, these firms are shielded from market discipline and allowed to persist. As a result, the small-cap group no longer reflects a competitive set of risky but high-upside firms; rather, it includes a disproportionate share of structurally inefficient entities whose survival biases the return distribution downward and increases tail risk.\nThis zombification effect is likely to depress the average excess return and Sharpe ratio of the small-cap segment, while simultaneously increasing the variance and skewness of returns. Though this paper does not directly model zombie firm identification, the empirical pattern—lower mean, higher volatility, greater downside asymmetry in small-cap returns—is consistent with this interpretation.\nThus, the efficiency erosion in capital markets is dual-channel:\n- Top-heavy concentration reinforces structural dominance and allocative inertia.\n- Bottom-heavy zombification distorts the return-generating process and weakens the innovation–destruction cycle.\n\n\n\nWithin this environment, the study evaluates a simple yet powerful investment strategy: the TBTF portfolio, which selects the 10 largest U.S. stocks by market capitalization from the full CRSP universe (NYSE, Nasdaq, AMEX), applies a quadratic weighting scheme based on rank, and rebalances at fixed intervals (e.g., monthly).\nEmpirical results show that this strategy has delivered exceptional risk-adjusted performance, particularly after 2010. The strategy outperforms major indices and alternative benchmarks across multiple metrics, including Sharpe ratio, Sortino ratio, Omega ratio, and maximum drawdown—while maintaining low turnover.\nYet the strategy’s success raises a paradox. It is “sadly optimal”:\n- If the strategy continues to succeed, it signals a persistent mispricing and a further decline in market efficiency.\n- If the strategy fails, it suggests a return to competitive capital allocation and improved economic welfare.\nIn either case, it remains optimal from an individual investor’s standpoint, but potentially damaging from a societal perspective.\n\n\n\nA key innovation in this study is the use of percentile-based Markov transition matrices and stationary capital share distributions to analyze capital mobility. The findings indicate that:\n\nTop decile firms exhibit extremely high persistence, acting as near-absorbing states.\nLower decile firms display high volatility and minimal upward transition probability.\nThe stationary distribution of capital shares shows growing convexity over time, resembling a structural Lorenz curve.\nMobility inequality, measured via entropy and upward/downward transition asymmetry, is worsening.\n\nThese features mirror social stratification, where economic agents (firms, in this case) become locked into hierarchical capital classes, resulting in reduced dynamism and allocative efficiency.\n\n\n\nThe findings challenge the mainstream view that abnormal returns are primarily risk-based. Instead, they suggest that persistent outperformance may be due to capital concentration, structural inertia, policy distortions, and the survival of inefficient entities. This calls for the development of new asset pricing models that incorporate:\n\nStructural inefficiency and non-ergodic dynamics\n\nRank-based capital dynamics\n\nRent-based valuation in place of marginal utility or productivity\n\nSuch models could offer more realistic frameworks for understanding how capital markets operate in the presence of central bank intervention, passive investing dominance, and technological monopolization.\n\n\n\nThe remainder of the paper is organized as follows:\n\nChapter 2 (Literature Review) situates this study within the broader asset pricing and inequality literature, covering theories of mobility, structural returns, and ETF-induced distortions.\nChapter 3 (Historical Context) summarizes the evolution of market structure post-2008, including the effects of QE and ETF proliferation.\nChapter 4 (Empirical Model) presents the core empirical study in seven modules: research overview, data, benchmark comparison, structural analysis, portfolio strategy, performance evaluation, and robustness checks.\nChapter 5 (Conclusion) reflects on the broader implications of the findings for asset pricing theory, policy intervention, and the future of financial capitalism.",
    "crumbs": [
      "교육",
      "Introduction"
    ]
  },
  {
    "objectID": "1_intro.html#structural-shifts-in-capital-markets",
    "href": "1_intro.html#structural-shifts-in-capital-markets",
    "title": "Introduction",
    "section": "",
    "text": "Over the past two decades, U.S. financial markets have undergone a series of profound structural transformations. These changes—accelerated by post-2008 Federal Reserve interventions, the rise of passive investing, and the dominance of a few mega-cap firms—have gradually undermined the foundational assumptions of neoclassical asset pricing models. Most notably, the classical assumptions of global convexity, representative agents, and arbitrage-free pricing appear increasingly misaligned with empirical realities.\nContemporary market dynamics no longer conform to the canonical vision of systematic risk being priced efficiently. Instead, capital appears to flow disproportionately toward a small set of dominant firms, driven not by marginal productivity or risk-adjusted returns, but by network effects, ETF-driven capital flows, and central bank distortions. These flows exhibit characteristics more consistent with structural rent allocation than with traditional risk-based compensation.",
    "crumbs": [
      "교육",
      "Introduction"
    ]
  },
  {
    "objectID": "1_intro.html#from-risk-pricing-to-rent-extraction",
    "href": "1_intro.html#from-risk-pricing-to-rent-extraction",
    "title": "Introduction",
    "section": "",
    "text": "This paper challenges the notion that financial markets primarily reward investors for bearing systematic risk. Instead, it proposes that post-crisis capital markets have evolved into a system of rent-based pricing, in which certain firms absorb a growing share of capital inflows due to structural advantage rather than economic fundamentals. This environment favors firms that are “too big to fail” (TBTF)—not because they are intrinsically more productive or efficient, but because the market structure allows them to persist at the top through passive flows and low volatility.\nThe study investigates whether stock return distributions in the U.S. stock market now resemble a time-varying mixture of heterogeneous assets under arbitrage-limited dynamics, as opposed to a homogeneous no-arbitrage equilibrium. The market appears to be governed less by efficient risk sharing and more by path-dependent capital accumulation, leading to a form of structural capital lock-in and asymmetric mobility across capital ranks.",
    "crumbs": [
      "교육",
      "Introduction"
    ]
  },
  {
    "objectID": "1_intro.html#stylized-facts-and-hypotheses",
    "href": "1_intro.html#stylized-facts-and-hypotheses",
    "title": "Introduction",
    "section": "",
    "text": "The study is guided by a set of empirical regularities and theoretical conjectures:\n\nReturn Distributions have become increasingly asymmetric and exhibit fat tails concentrated in top-cap stocks.\nCapital Concentration is not only extreme in level but persistent in share and rank.\nTransition Dynamics across size ranks show decreasing upward mobility and increasing lock-in duration for top-cap stocks.\nMarket Efficiency is being eroded by capital polarization, in which capital increasingly accumulates at the top while the rest of the distribution remains stagnant.\nETFs and QE Policies act as reinforcing mechanisms, feeding capital into already dominant firms, thereby reducing entropy and distorting allocative efficiency.",
    "crumbs": [
      "교육",
      "Introduction"
    ]
  },
  {
    "objectID": "1_intro.html#the-dual-distortion-of-market-efficiency",
    "href": "1_intro.html#the-dual-distortion-of-market-efficiency",
    "title": "Introduction",
    "section": "",
    "text": "Importantly, the distortion of allocative efficiency is not limited to the top end of the market. At the lower end of the capitalization spectrum, another structural problem emerges: the survival and persistence of zombie firms—unproductive small-cap companies that continue to operate despite being economically non-viable.\nFueled by prolonged low interest rates, government credit guarantees, and liquidity injections, these firms are shielded from market discipline and allowed to persist. As a result, the small-cap group no longer reflects a competitive set of risky but high-upside firms; rather, it includes a disproportionate share of structurally inefficient entities whose survival biases the return distribution downward and increases tail risk.\nThis zombification effect is likely to depress the average excess return and Sharpe ratio of the small-cap segment, while simultaneously increasing the variance and skewness of returns. Though this paper does not directly model zombie firm identification, the empirical pattern—lower mean, higher volatility, greater downside asymmetry in small-cap returns—is consistent with this interpretation.\nThus, the efficiency erosion in capital markets is dual-channel:\n- Top-heavy concentration reinforces structural dominance and allocative inertia.\n- Bottom-heavy zombification distorts the return-generating process and weakens the innovation–destruction cycle.",
    "crumbs": [
      "교육",
      "Introduction"
    ]
  },
  {
    "objectID": "1_intro.html#the-tbtf-strategy-and-its-paradox",
    "href": "1_intro.html#the-tbtf-strategy-and-its-paradox",
    "title": "Introduction",
    "section": "",
    "text": "Within this environment, the study evaluates a simple yet powerful investment strategy: the TBTF portfolio, which selects the 10 largest U.S. stocks by market capitalization from the full CRSP universe (NYSE, Nasdaq, AMEX), applies a quadratic weighting scheme based on rank, and rebalances at fixed intervals (e.g., monthly).\nEmpirical results show that this strategy has delivered exceptional risk-adjusted performance, particularly after 2010. The strategy outperforms major indices and alternative benchmarks across multiple metrics, including Sharpe ratio, Sortino ratio, Omega ratio, and maximum drawdown—while maintaining low turnover.\nYet the strategy’s success raises a paradox. It is “sadly optimal”:\n- If the strategy continues to succeed, it signals a persistent mispricing and a further decline in market efficiency.\n- If the strategy fails, it suggests a return to competitive capital allocation and improved economic welfare.\nIn either case, it remains optimal from an individual investor’s standpoint, but potentially damaging from a societal perspective.",
    "crumbs": [
      "교육",
      "Introduction"
    ]
  },
  {
    "objectID": "1_intro.html#capital-mobility-lock-in-and-polarization",
    "href": "1_intro.html#capital-mobility-lock-in-and-polarization",
    "title": "Introduction",
    "section": "",
    "text": "A key innovation in this study is the use of percentile-based Markov transition matrices and stationary capital share distributions to analyze capital mobility. The findings indicate that:\n\nTop decile firms exhibit extremely high persistence, acting as near-absorbing states.\nLower decile firms display high volatility and minimal upward transition probability.\nThe stationary distribution of capital shares shows growing convexity over time, resembling a structural Lorenz curve.\nMobility inequality, measured via entropy and upward/downward transition asymmetry, is worsening.\n\nThese features mirror social stratification, where economic agents (firms, in this case) become locked into hierarchical capital classes, resulting in reduced dynamism and allocative efficiency.",
    "crumbs": [
      "교육",
      "Introduction"
    ]
  },
  {
    "objectID": "1_intro.html#implications-and-research-agenda",
    "href": "1_intro.html#implications-and-research-agenda",
    "title": "Introduction",
    "section": "",
    "text": "The findings challenge the mainstream view that abnormal returns are primarily risk-based. Instead, they suggest that persistent outperformance may be due to capital concentration, structural inertia, policy distortions, and the survival of inefficient entities. This calls for the development of new asset pricing models that incorporate:\n\nStructural inefficiency and non-ergodic dynamics\n\nRank-based capital dynamics\n\nRent-based valuation in place of marginal utility or productivity\n\nSuch models could offer more realistic frameworks for understanding how capital markets operate in the presence of central bank intervention, passive investing dominance, and technological monopolization.",
    "crumbs": [
      "교육",
      "Introduction"
    ]
  },
  {
    "objectID": "1_intro.html#structure-of-the-paper",
    "href": "1_intro.html#structure-of-the-paper",
    "title": "Introduction",
    "section": "",
    "text": "The remainder of the paper is organized as follows:\n\nChapter 2 (Literature Review) situates this study within the broader asset pricing and inequality literature, covering theories of mobility, structural returns, and ETF-induced distortions.\nChapter 3 (Historical Context) summarizes the evolution of market structure post-2008, including the effects of QE and ETF proliferation.\nChapter 4 (Empirical Model) presents the core empirical study in seven modules: research overview, data, benchmark comparison, structural analysis, portfolio strategy, performance evaluation, and robustness checks.\nChapter 5 (Conclusion) reflects on the broader implications of the findings for asset pricing theory, policy intervention, and the future of financial capitalism.",
    "crumbs": [
      "교육",
      "Introduction"
    ]
  },
  {
    "objectID": "3_1_history.html",
    "href": "3_1_history.html",
    "title": "Recent History",
    "section": "",
    "text": "This study defines January 1, 2010 as a structural break in the evolution of U.S. financial markets. This point marks a transition from a period of crisis-driven volatility and deregulation to an era characterized by policy-dominated asset inflation, ETF proliferation, and asymmetric capital flows. By segmenting the data at this breakpoint, we aim to assess how financial market performance and capital concentration evolved across two distinct regimes.\nThe dataset spans January 1996 to December 2023, with a monthly frequency. It is segmented as follows:\n\nPre-2010: 1996–2009\n\nPost-2010: 2010–2023\n\nThe segmentation provides a clean empirical framework to evaluate the impact of structural policy interventions and evolving capital dynamics.\n\n\n\n\n\nThe pre-2010 period includes several global shocks and deregulation episodes:\n\n1997 Asian Financial Crisis\nDot-com Bubble and Crash (2000–2001)\n9/11 Terror Attacks (2001)\nRepeal of Glass-Steagall (1999), which contributed to systemic risk buildup\nGlobal Financial Crisis (2008–2009), culminating in the collapse of Lehman Brothers and the implementation of TARP\n\nThis was an era of market discontinuity, high volatility, and risk repricing, where asset allocation was still driven largely by fundamentals and discretionary investing.\n\n\n\nIn contrast, the post-2010 era reflects a policy-driven financial environment, where capital markets were shaped by:\n\nQuantitative Easing (QE1, QE2, QE3)\nPassive flows via ETF proliferation\nTechnological monopolization\nCOVID-19 pandemic and unprecedented stimulus\nAI-fueled equity valuations post-2023\n\nThese changes have amplified top-heavy capital concentration and reinforced inertia in market leadership. Importantly, they enabled the survival of inefficient firms at the lower end (zombie firms), distorting the small-cap return distribution.\n\n\n\n\nAn empirical motivation for the TBTF strategy lies in the historical persistence of top-cap firms. From 2012 to 2023, only 27 unique PERMNOs appeared in the top 10 by market cap. These include:\n\nAAPL, MSFT, AMZN, GOOG, META, NVDA, TSLA\nTraditional blue chips like XOM, JNJ, WMT, JPM, BAC\nHealthcare and consumer giants like UNH, LLY, PG, KO\nFinancials and telcos like BRK, MA, V, T, VZ\n\nThis concentration implies that capital has become locked into a narrow subset of firms, with little room for new entrants—an observation with profound implications for allocative efficiency and market competition.\n\n\n\n\nU.S. equities constitute nearly half of global equity market capitalization.\nThe dominance of U.S. firms in global equity rankings",
    "crumbs": [
      "교육",
      "History",
      "Recent History"
    ]
  },
  {
    "objectID": "3_1_history.html#as-a-structural-break",
    "href": "3_1_history.html#as-a-structural-break",
    "title": "Recent History",
    "section": "",
    "text": "This study defines January 1, 2010 as a structural break in the evolution of U.S. financial markets. This point marks a transition from a period of crisis-driven volatility and deregulation to an era characterized by policy-dominated asset inflation, ETF proliferation, and asymmetric capital flows. By segmenting the data at this breakpoint, we aim to assess how financial market performance and capital concentration evolved across two distinct regimes.\nThe dataset spans January 1996 to December 2023, with a monthly frequency. It is segmented as follows:\n\nPre-2010: 1996–2009\n\nPost-2010: 2010–2023\n\nThe segmentation provides a clean empirical framework to evaluate the impact of structural policy interventions and evolving capital dynamics.",
    "crumbs": [
      "교육",
      "History",
      "Recent History"
    ]
  },
  {
    "objectID": "3_1_history.html#historical-context-of-each-period",
    "href": "3_1_history.html#historical-context-of-each-period",
    "title": "Recent History",
    "section": "",
    "text": "The pre-2010 period includes several global shocks and deregulation episodes:\n\n1997 Asian Financial Crisis\nDot-com Bubble and Crash (2000–2001)\n9/11 Terror Attacks (2001)\nRepeal of Glass-Steagall (1999), which contributed to systemic risk buildup\nGlobal Financial Crisis (2008–2009), culminating in the collapse of Lehman Brothers and the implementation of TARP\n\nThis was an era of market discontinuity, high volatility, and risk repricing, where asset allocation was still driven largely by fundamentals and discretionary investing.\n\n\n\nIn contrast, the post-2010 era reflects a policy-driven financial environment, where capital markets were shaped by:\n\nQuantitative Easing (QE1, QE2, QE3)\nPassive flows via ETF proliferation\nTechnological monopolization\nCOVID-19 pandemic and unprecedented stimulus\nAI-fueled equity valuations post-2023\n\nThese changes have amplified top-heavy capital concentration and reinforced inertia in market leadership. Importantly, they enabled the survival of inefficient firms at the lower end (zombie firms), distorting the small-cap return distribution.",
    "crumbs": [
      "교육",
      "History",
      "Recent History"
    ]
  },
  {
    "objectID": "3_1_history.html#us-top-10-stability-empirical-snapshot",
    "href": "3_1_history.html#us-top-10-stability-empirical-snapshot",
    "title": "Recent History",
    "section": "",
    "text": "An empirical motivation for the TBTF strategy lies in the historical persistence of top-cap firms. From 2012 to 2023, only 27 unique PERMNOs appeared in the top 10 by market cap. These include:\n\nAAPL, MSFT, AMZN, GOOG, META, NVDA, TSLA\nTraditional blue chips like XOM, JNJ, WMT, JPM, BAC\nHealthcare and consumer giants like UNH, LLY, PG, KO\nFinancials and telcos like BRK, MA, V, T, VZ\n\nThis concentration implies that capital has become locked into a narrow subset of firms, with little room for new entrants—an observation with profound implications for allocative efficiency and market competition.",
    "crumbs": [
      "교육",
      "History",
      "Recent History"
    ]
  },
  {
    "objectID": "3_1_history.html#global-top-10-market-cap-stocks",
    "href": "3_1_history.html#global-top-10-market-cap-stocks",
    "title": "Recent History",
    "section": "",
    "text": "U.S. equities constitute nearly half of global equity market capitalization.\nThe dominance of U.S. firms in global equity rankings",
    "crumbs": [
      "교육",
      "History",
      "Recent History"
    ]
  },
  {
    "objectID": "3_1_history.html#appendix",
    "href": "3_1_history.html#appendix",
    "title": "Recent History",
    "section": "Appendix",
    "text": "Appendix\n\n\nA. Key Financial and Policy Events by Period\n\nPre-2010 (1996–2009)\n\n\n\n\n\n\n\n\nDate\nEvent & Description\nImportance\n\n\n\n\n1997-07-02\nAsian Financial Crisis\nVery High\n\n\n1999-01-01\nEuro launched\nHigh\n\n\n1999-11-12\nRepeal of Glass-Steagall\nHigh\n\n\n2000–2001\nDot-com Bubble & Crash\nVery High\n\n\n2001-09-11\n9/11 Attacks\nVery High\n\n\n2008-09-15\nLehman Brothers Collapse\nVery High\n\n\n2008-10-03\nTARP ($700B bailout)\nVery High\n\n\n\n\n\nPost-2010 (2010–2023)\n\n\n\n\n\n\n\n\nDate\nEvent & Description\nImportance\n\n\n\n\n2010–2015\nQE1, QE2, QE3 Implementation & Taper\nVery High\n\n\n2018-01-01\nU.S.–China Trade Tensions\nHigh\n\n\n2020-03-11\nCOVID-19 Declared (Policy + Liquidity Response)\nVery High\n\n\n2023–Present\nRise of AI in Markets (NLP, LLMs)\nHigh",
    "crumbs": [
      "교육",
      "History",
      "Recent History"
    ]
  },
  {
    "objectID": "4_1_overview.html",
    "href": "4_1_overview.html",
    "title": "01 Overview",
    "section": "",
    "text": "This study investigates the structural properties and empirical performance of a “Too Big To Fail” (TBTF) portfolio strategy, defined as a portfolio composed of the largest market capitalization stocks in the U.S. equity market. Using monthly stock-level data from the CRSP universe (NYSE, Nasdaq, AMEX), we examine whether top-ranked stocks—selected purely by market capitalization—consistently outperform others in terms of risk-adjusted returns.\nOur empirical design centers around the hypothesis that structural asymmetries in capital allocation, exacerbated by post-2008 monetary policies and market concentration, result in persistent distortions in return distributions. The TBTF strategy, though simple in construction, appears to capture these structural features with surprising consistency.\nWe treat the year 2010 as a potential structural break, dividing the full sample into two equal-length periods:\n\nPre-2010: January 1996 to December 2009\nPost-2010: January 2010 to December 2023\n\nThis periodization allows us to analyze both long-term stability and post-crisis distortions in the equity return distribution.\n\n\n\nReturn Superiority: The return distributions of top market-cap stocks are structurally superior to those of mid- and small-cap stocks in terms of risk-adjusted performance (e.g., Sharpe, Sortino, Omega ratios).\nPersistence and Absorption: Stocks that reach the top decile of market capitalization exhibit high persistence and low turnover, potentially distorting market competitiveness and reducing allocative efficiency.\nConvex Capital Concentration: The cross-sectional distribution of capital shares among top-ranked stocks is increasingly convex, suggesting long-term structural lock-in effects analogous to wealth inequality (e.g., Lorenz curve analogy).",
    "crumbs": [
      "교육",
      "Model",
      "01 Overview"
    ]
  },
  {
    "objectID": "4_1_overview.html#main-hypotheses",
    "href": "4_1_overview.html#main-hypotheses",
    "title": "01 Overview",
    "section": "",
    "text": "Return Superiority: The return distributions of top market-cap stocks are structurally superior to those of mid- and small-cap stocks in terms of risk-adjusted performance (e.g., Sharpe, Sortino, Omega ratios).\nPersistence and Absorption: Stocks that reach the top decile of market capitalization exhibit high persistence and low turnover, potentially distorting market competitiveness and reducing allocative efficiency.\nConvex Capital Concentration: The cross-sectional distribution of capital shares among top-ranked stocks is increasingly convex, suggesting long-term structural lock-in effects analogous to wealth inequality (e.g., Lorenz curve analogy).",
    "crumbs": [
      "교육",
      "Model",
      "01 Overview"
    ]
  },
  {
    "objectID": "4_1_overview.html#appendix",
    "href": "4_1_overview.html#appendix",
    "title": "01 Overview",
    "section": "Appendix",
    "text": "Appendix\n\n\nSummary of Analytical Components in the TBTF Strategy\n\n\n\n\n\n\n\nComponent\nDescription\n\n\n\n\nLarge vs. Small Stocks\nCompare return distributions and volatility structures\n\n\nTop 10 Stocks\nAnalyze list stability and industry composition (high-tech dominance)\n\n\nTransition Analysis\nEstimate transition probabilities and stationary distributions\n\n\nRisk-Adjusted Performance\nCompare Sharpe, Sortino, Omega ratios against benchmarks (ETFs, indices)",
    "crumbs": [
      "교육",
      "Model",
      "01 Overview"
    ]
  },
  {
    "objectID": "4_3_ff_compare.html",
    "href": "4_3_ff_compare.html",
    "title": "03 Benchmarks",
    "section": "",
    "text": "Code\n# Import Libraries\nimport pandas as pd\nimport numpy as np\n\nimport statsmodels.api as sm # QQ plot\nfrom scipy import stats # relative t-test\n\n# graphics\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Data Frame. Data frequency: Monthly\nstart_date = \"1963-07-01\"\nend_date = \"2023-06-30\"\n\nprint(\"start date:\", start_date)\nprint(\"end date:\", end_date)\n\n\nstart date: 1963-07-01\nend date: 2023-06-30\n\n\n\n\nCode\n#@title Data: Portfolios_Formed_on_ME\n# https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/Data_Library/\n\nimport pandas_datareader as pdr\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning) # FutureWarning 제거\n\n# 'Portfolios_Formed_on_ME', by a Univariate sort on Size (market equity, ME)\n# 3 Potfolios include all NYSE, AMEX, and NASDAQ stocks, but with NYSE breakpoints to divide\n# Size are the bottom 30%, middle 40%, top 30%; quintiles; deciles.\n\npfo_size_raw = pdr.DataReader(\n  name=\"Portfolios_Formed_on_ME\",\n  data_source=\"famafrench\",\n  start=start_date,\n  end=end_date)[0]\n\npfo_size = (pfo_size_raw\n  .reset_index(names=\"date\")\n  .assign(date=lambda x: pd.to_datetime(x[\"date\"].astype(str)))\n  .set_index('date')  # Set the 'date' column as the index\n  .rename(columns=lambda x: x.lower())\n  .rename(columns={'lo 30': 's_30', 'hi 30': 'b_30', 'lo 20': 's_20', 'hi 20': 'b_20', 'lo 10': 's_10', 'hi 10': 'b_10'})\n  .drop(['&lt;= 0'], axis=\"columns\")\n)\n\n# Calculate the average of the small group\n# pfo_size.iloc[:, 8:17].columns.tolist()\npfo_size['s_70'] = pfo_size.iloc[:, 0:2].mean(axis=1)\npfo_size['s_80'] = pfo_size.iloc[:, 3:7].mean(axis=1)\npfo_size['s_90'] = pfo_size.iloc[:, 8:17].mean(axis=1)\n# Drop columns\npfo_size = pfo_size.drop(pfo_size.columns[9:17], axis=1)\npfo_size = pfo_size.drop(pfo_size.columns[4:7], axis=1)\npfo_size = pfo_size.drop(pfo_size.columns[1:2], axis=1)\n\n# Describe\npfo_size.describe().round(2)\n\n\n\n\n\n\n\n\n\ns_30\nb_30\ns_20\nb_20\ns_10\nb_10\ns_70\ns_80\ns_90\n\n\n\n\ncount\n720.00\n720.00\n720.00\n720.00\n720.00\n720.00\n720.00\n720.00\n720.00\n\n\nmean\n1.12\n0.91\n1.10\n0.90\n1.10\n0.88\n1.10\n1.09\n1.08\n\n\nstd\n6.20\n4.33\n6.34\n4.29\n6.35\n4.27\n5.72\n5.58\n5.46\n\n\nmin\n-29.43\n-20.80\n-29.65\n-20.32\n-28.87\n-19.74\n-28.30\n-27.82\n-27.26\n\n\n25%\n-2.37\n-1.57\n-2.41\n-1.56\n-2.25\n-1.53\n-2.18\n-1.96\n-1.92\n\n\n50%\n1.38\n1.22\n1.36\n1.17\n1.25\n1.12\n1.38\n1.41\n1.34\n\n\n75%\n4.70\n3.64\n4.89\n3.54\n4.67\n3.47\n4.57\n4.61\n4.60\n\n\nmax\n26.91\n17.75\n27.54\n18.05\n29.50\n18.06\n24.84\n23.83\n23.08\n\n\n\n\n\n\n\nSummary Statistics for Fama-French Size-Decile Portfolios: Monthly value-weighted excess returns for size-sorted portfolios from July 1963 to June 2023. Excess returns are defined as raw portfolio returns net of the one-month Treasury bill rate.\nTo establish a structural benchmark for the TBTF strategy, we begin by examining the historical performance of size-sorted portfolios constructed by Fama and French. In particular, we compare the top and bottom deciles of market capitalization—commonly denoted as b_10 (large-cap) and s_10 (small-cap)—using monthly return data spanning from July 1963 to June 2023.\nThe data originate from the “Portfolios Formed on Size (ME)” dataset provided by the Ken French Data Library. These portfolios, covering NYSE, Nasdaq, and AMEX stocks, are rebalanced annually using NYSE breakpoints and report monthly value-weighted excess returns—i.e., returns net of the one-month Treasury bill rate (risk-free return).\nFor additional robustness, we construct aggregates (e.g., s_70, s_90) to represent broader small-cap behavior.\n\n\nThis section evaluates whether large-cap portfolios exhibit systematically superior risk-adjusted performance relative to small-cap portfolios. Such a result would support the hypothesis that TBTF-style strategies derive their advantage not from tactical optimization, but from structural features of the cross-sectional return distribution.\n\n\n\nWe compare the s_10 and b_10 portfolios in three dimensions:\n\nDistributional Shape: via QQ-plots and skewness asymmetry\n\nVolatility Profile: through standard deviation comparisons\n\nSharpe Ratio Dynamics: using time-varying rolling Sharpe ratio curves and volatility–mean coordinate plots\n\nUnlike most academic studies that summarize portfolio performance using static points in mean–volatility space, we propose a dynamic framework in which Sharpe ratios are evaluated as time-series objects. This allows us to capture persistent structural asymmetries.\n\n\nWhile static comparisons of mean and volatility offer useful summary insights, they can be misleading in the presence of temporal instability. To capture the time-varying performance profile of small- and large-cap portfolios, we construct rolling Sharpe ratios using annualized excess returns.\nLet \\(\\mu_t\\) and \\(\\sigma_t\\) denote the rolling annualized excess return mean and volatility of a portfolio over a 36-month window. Then, the rolling Sharpe ratio at time \\(t\\) is computed as:\n\\[\n\\text{Sharpe}_t = \\frac{\\mu_t}{\\sigma_t}\n\\]\nWe focus on the bottom and top size deciles: s_10 (small-cap) and b_10 (large-cap).\n\n\n\n\n\nThe b_10 portfolio exhibits consistently lower volatility than s_10, across the full 60-year period.\nDynamic Sharpe ratio visualization reveals that the performance advantage of b_10 is persistent over time, not a byproduct of a particular decade or business cycle.\nQQ-plots show that b_10 excess returns are closer to Gaussian, while s_10 returns display tail asymmetry—characterized by positive skewness and negative tail risk.\n\nThese results suggest that large-cap portfolios, especially those at the very top of the capitalization spectrum, provide more stable and efficient excess return profiles. This supports the structural validity of selecting top-ranked assets by market capitalization in TBTF portfolio construction.\n\nOver the 60-year period, the unconditional average annual Sharpe ratio of b_10 was 0.99, compared to 0.74 for s_10.\nThe time-series of rolling Sharpe ratios reveals that b_10 dominates structurally, not just in isolated windows.\nDuring periods of macroeconomic stress, s_10 portfolios experience sharper volatility spikes, reducing their Sharpe ratios significantly.\n\n\n\nCode\n#@title Time-series of s_10 vs. b_10\n\n# moving average\n# pfo_size_rolling = pfo_size.rolling(window=12).mean()\n\n\n#@title without any overlap between the fiscal years\n# Resample to annual frequency and calculate annual standard deviation\n# groups the monthly data into yearly buckets.\n# annual_std = pfo_size.resample('Y').std() * np.sqrt(12)\n\n# Create a fiscal year column\npfo_size['fiscal_year'] = pfo_size.index.year\npfo_size.loc[pfo_size.index.month &gt;= 7, 'fiscal_year'] = pfo_size.loc[pfo_size.index.month &gt;= 7, 'fiscal_year'] + 1\n\n# Group by fiscal year and calculate annual\nannual_mean = pfo_size.groupby('fiscal_year').mean()*12\nannual_std = pfo_size.groupby('fiscal_year').std() * np.sqrt(12)\n# annual_std.dropna(inplace=True)\n\n#@title Time-series of s_10 vs. b_10\ntemp = annual_mean['s_10'] - annual_mean['b_10']\ntemp.plot()\nplt.axhline(y= temp.mean(), color='r', linestyle='-')\nplt.xlabel('fiscal year')\nplt.ylabel('annual mean difference')\nplt.title('Time-series of s_10 minus b_10 in annual mean')\nplt.show()\nprint('The unconditional mean of the difference is '+str(temp.mean().round(0))+ ' percent, annually')\nprint('\\n')\n\ntemp = annual_std['s_10'] - annual_std['b_10']\ntemp.plot()\nplt.axhline(y= temp.mean(), color='r', linestyle='-')\nplt.xlabel('fiscal year')\nplt.ylabel('annual std difference')\nplt.title('Time-series of s_10 minus b_10 in annual volatility')\nplt.show()\nprint('The unconditional mean of the difference is '+str(temp.mean().round(0))+ ' percent, annually')\nprint('\\n')\n\n# annual_mean and annual_std have the same index\ndf = pd.DataFrame(index=annual_mean.index)\ndf['sr_s_10'] = annual_mean['s_10'] / annual_std['s_10']\ndf['sr_b_10'] = annual_mean['b_10'] / annual_std['b_10']\n\ndf['sr_s_10'].plot()\ndf['sr_b_10'].plot()\nplt.axhline(y= df['sr_s_10'].mean(), color='b', linestyle='dashed')\nplt.axhline(y= df['sr_b_10'].mean(), color='r', linestyle='dashed')\nplt.legend()\nplt.xlabel('fiscal year')\nplt.ylabel('Sharpe Ratio')\nplt.title('Time-series Sharpe Ratios of s_10 and b_10')\nplt.show()\nprint('The unconditional mean Annual Sharpe ratio of s_10 is '+str(df['sr_s_10'].mean().round(2)))\nprint('The unconditional mean Annual Sharpe ratio of b_10 is '+str(df['sr_b_10'].mean().round(2)))\n\n\n\n\n\n\n\n\n\nThe unconditional mean of the difference is 3.0 percent, annually\n\n\n\n\n\n\n\n\n\n\n\nThe unconditional mean of the difference is 6.0 percent, annually\n\n\n\n\n\n\n\n\n\n\n\nThe unconditional mean Annual Sharpe ratio of s_10 is 0.74\nThe unconditional mean Annual Sharpe ratio of b_10 is 0.99\n\n\n\nAnnual excess return difference: s_10 – b_10. The red horizontal line marks the long-run average (~3.0% annually).\nAnnual volatility difference: s_10 – b_10. Large-cap volatility is consistently lower.\nTime-series of 36-month rolling Sharpe ratios for s_10 and b_10. Dashed lines represent the unconditional average for each.\n\nWe also construct a bivariate distribution of annual mean and standard deviation using kernel density estimates:\n\n\nCode\n#@title s_10 vs. b_10 in the volatility-mean coordinate\n\n# Create a list of category names\ncategories = ['s_10', 'b_10']\n\n# Initialize an empty list to store dataframes\ndfs = []\n\n# Iterate through categories and create melted dataframes\nfor category in categories:\n    # Create a temporary dataframe with selected columns and category label\n    temp_mean = annual_mean[[category]].copy()  # Keep the index\n    temp_mean['category'] = category\n    temp_mean.rename(columns={category: 'annual_mean'}, inplace=True)\n\n    temp_std = annual_std[[category]].copy()  # Keep the index\n    temp_std['category'] = category\n    temp_std.rename(columns={category: 'annual_std'}, inplace=True)\n\n    # Merge the temporary dataframes for mean and std, keeping the index\n    temp_df = pd.merge(temp_mean, temp_std, on=['fiscal_year', 'category'])\n\n    # Append the temporary dataframe to the list\n    dfs.append(temp_df)\n\n# Concatenate all the dataframes in the list into a single dataframe, keeping the index\ndf = pd.concat(dfs, ignore_index=False)\n\n# Now create the displot\nsns.displot(\n    data = df,  # Use the reshaped DataFrame\n    x=\"annual_std\",\n    y=\"annual_mean\",\n    hue=\"category\",  # Use the extracted category for hue\n    kind=\"kde\",\n    rug=True,\n)\nplt.xlabel('annual std')\nplt.ylabel('annual mean')\nplt.title('s_10 vs. b_10 in the volatility-mean coordinate')\nplt.show()\n\n\n\n\n\n\n\n\n\nKernel density estimate of annual excess return vs. annual volatility for s_10 and b_10. The separation in volatility–mean space reflects the underlying Sharpe ratio asymmetry.\nThese dynamic plots provide a richer understanding of the persistent performance asymmetry between the smallest and largest decile portfolios—supporting the broader structural thesis underlying TBTF.\n\n\n\n\nIntroduces a time-dynamic framework—Sharpe ratio level curves—to visualize and quantify structural performance asymmetries between size portfolios.\nEstablishes a long-term empirical benchmark against which TBTF performance can be evaluated.\nProvides theoretical and empirical justification for focusing on top-market-cap stocks, beyond simple momentum or value signals.",
    "crumbs": [
      "교육",
      "Model",
      "03 Benchmarks"
    ]
  },
  {
    "objectID": "4_3_ff_compare.html#motivation",
    "href": "4_3_ff_compare.html#motivation",
    "title": "03 Benchmarks",
    "section": "",
    "text": "This section evaluates whether large-cap portfolios exhibit systematically superior risk-adjusted performance relative to small-cap portfolios. Such a result would support the hypothesis that TBTF-style strategies derive their advantage not from tactical optimization, but from structural features of the cross-sectional return distribution.",
    "crumbs": [
      "교육",
      "Model",
      "03 Benchmarks"
    ]
  },
  {
    "objectID": "4_3_ff_compare.html#methodology",
    "href": "4_3_ff_compare.html#methodology",
    "title": "03 Benchmarks",
    "section": "",
    "text": "We compare the s_10 and b_10 portfolios in three dimensions:\n\nDistributional Shape: via QQ-plots and skewness asymmetry\n\nVolatility Profile: through standard deviation comparisons\n\nSharpe Ratio Dynamics: using time-varying rolling Sharpe ratio curves and volatility–mean coordinate plots\n\nUnlike most academic studies that summarize portfolio performance using static points in mean–volatility space, we propose a dynamic framework in which Sharpe ratios are evaluated as time-series objects. This allows us to capture persistent structural asymmetries.\n\n\nWhile static comparisons of mean and volatility offer useful summary insights, they can be misleading in the presence of temporal instability. To capture the time-varying performance profile of small- and large-cap portfolios, we construct rolling Sharpe ratios using annualized excess returns.\nLet \\(\\mu_t\\) and \\(\\sigma_t\\) denote the rolling annualized excess return mean and volatility of a portfolio over a 36-month window. Then, the rolling Sharpe ratio at time \\(t\\) is computed as:\n\\[\n\\text{Sharpe}_t = \\frac{\\mu_t}{\\sigma_t}\n\\]\nWe focus on the bottom and top size deciles: s_10 (small-cap) and b_10 (large-cap).",
    "crumbs": [
      "교육",
      "Model",
      "03 Benchmarks"
    ]
  },
  {
    "objectID": "4_3_ff_compare.html#key-findings",
    "href": "4_3_ff_compare.html#key-findings",
    "title": "03 Benchmarks",
    "section": "",
    "text": "The b_10 portfolio exhibits consistently lower volatility than s_10, across the full 60-year period.\nDynamic Sharpe ratio visualization reveals that the performance advantage of b_10 is persistent over time, not a byproduct of a particular decade or business cycle.\nQQ-plots show that b_10 excess returns are closer to Gaussian, while s_10 returns display tail asymmetry—characterized by positive skewness and negative tail risk.\n\nThese results suggest that large-cap portfolios, especially those at the very top of the capitalization spectrum, provide more stable and efficient excess return profiles. This supports the structural validity of selecting top-ranked assets by market capitalization in TBTF portfolio construction.\n\nOver the 60-year period, the unconditional average annual Sharpe ratio of b_10 was 0.99, compared to 0.74 for s_10.\nThe time-series of rolling Sharpe ratios reveals that b_10 dominates structurally, not just in isolated windows.\nDuring periods of macroeconomic stress, s_10 portfolios experience sharper volatility spikes, reducing their Sharpe ratios significantly.\n\n\n\nCode\n#@title Time-series of s_10 vs. b_10\n\n# moving average\n# pfo_size_rolling = pfo_size.rolling(window=12).mean()\n\n\n#@title without any overlap between the fiscal years\n# Resample to annual frequency and calculate annual standard deviation\n# groups the monthly data into yearly buckets.\n# annual_std = pfo_size.resample('Y').std() * np.sqrt(12)\n\n# Create a fiscal year column\npfo_size['fiscal_year'] = pfo_size.index.year\npfo_size.loc[pfo_size.index.month &gt;= 7, 'fiscal_year'] = pfo_size.loc[pfo_size.index.month &gt;= 7, 'fiscal_year'] + 1\n\n# Group by fiscal year and calculate annual\nannual_mean = pfo_size.groupby('fiscal_year').mean()*12\nannual_std = pfo_size.groupby('fiscal_year').std() * np.sqrt(12)\n# annual_std.dropna(inplace=True)\n\n#@title Time-series of s_10 vs. b_10\ntemp = annual_mean['s_10'] - annual_mean['b_10']\ntemp.plot()\nplt.axhline(y= temp.mean(), color='r', linestyle='-')\nplt.xlabel('fiscal year')\nplt.ylabel('annual mean difference')\nplt.title('Time-series of s_10 minus b_10 in annual mean')\nplt.show()\nprint('The unconditional mean of the difference is '+str(temp.mean().round(0))+ ' percent, annually')\nprint('\\n')\n\ntemp = annual_std['s_10'] - annual_std['b_10']\ntemp.plot()\nplt.axhline(y= temp.mean(), color='r', linestyle='-')\nplt.xlabel('fiscal year')\nplt.ylabel('annual std difference')\nplt.title('Time-series of s_10 minus b_10 in annual volatility')\nplt.show()\nprint('The unconditional mean of the difference is '+str(temp.mean().round(0))+ ' percent, annually')\nprint('\\n')\n\n# annual_mean and annual_std have the same index\ndf = pd.DataFrame(index=annual_mean.index)\ndf['sr_s_10'] = annual_mean['s_10'] / annual_std['s_10']\ndf['sr_b_10'] = annual_mean['b_10'] / annual_std['b_10']\n\ndf['sr_s_10'].plot()\ndf['sr_b_10'].plot()\nplt.axhline(y= df['sr_s_10'].mean(), color='b', linestyle='dashed')\nplt.axhline(y= df['sr_b_10'].mean(), color='r', linestyle='dashed')\nplt.legend()\nplt.xlabel('fiscal year')\nplt.ylabel('Sharpe Ratio')\nplt.title('Time-series Sharpe Ratios of s_10 and b_10')\nplt.show()\nprint('The unconditional mean Annual Sharpe ratio of s_10 is '+str(df['sr_s_10'].mean().round(2)))\nprint('The unconditional mean Annual Sharpe ratio of b_10 is '+str(df['sr_b_10'].mean().round(2)))\n\n\n\n\n\n\n\n\n\nThe unconditional mean of the difference is 3.0 percent, annually\n\n\n\n\n\n\n\n\n\n\n\nThe unconditional mean of the difference is 6.0 percent, annually\n\n\n\n\n\n\n\n\n\n\n\nThe unconditional mean Annual Sharpe ratio of s_10 is 0.74\nThe unconditional mean Annual Sharpe ratio of b_10 is 0.99\n\n\n\nAnnual excess return difference: s_10 – b_10. The red horizontal line marks the long-run average (~3.0% annually).\nAnnual volatility difference: s_10 – b_10. Large-cap volatility is consistently lower.\nTime-series of 36-month rolling Sharpe ratios for s_10 and b_10. Dashed lines represent the unconditional average for each.\n\nWe also construct a bivariate distribution of annual mean and standard deviation using kernel density estimates:\n\n\nCode\n#@title s_10 vs. b_10 in the volatility-mean coordinate\n\n# Create a list of category names\ncategories = ['s_10', 'b_10']\n\n# Initialize an empty list to store dataframes\ndfs = []\n\n# Iterate through categories and create melted dataframes\nfor category in categories:\n    # Create a temporary dataframe with selected columns and category label\n    temp_mean = annual_mean[[category]].copy()  # Keep the index\n    temp_mean['category'] = category\n    temp_mean.rename(columns={category: 'annual_mean'}, inplace=True)\n\n    temp_std = annual_std[[category]].copy()  # Keep the index\n    temp_std['category'] = category\n    temp_std.rename(columns={category: 'annual_std'}, inplace=True)\n\n    # Merge the temporary dataframes for mean and std, keeping the index\n    temp_df = pd.merge(temp_mean, temp_std, on=['fiscal_year', 'category'])\n\n    # Append the temporary dataframe to the list\n    dfs.append(temp_df)\n\n# Concatenate all the dataframes in the list into a single dataframe, keeping the index\ndf = pd.concat(dfs, ignore_index=False)\n\n# Now create the displot\nsns.displot(\n    data = df,  # Use the reshaped DataFrame\n    x=\"annual_std\",\n    y=\"annual_mean\",\n    hue=\"category\",  # Use the extracted category for hue\n    kind=\"kde\",\n    rug=True,\n)\nplt.xlabel('annual std')\nplt.ylabel('annual mean')\nplt.title('s_10 vs. b_10 in the volatility-mean coordinate')\nplt.show()\n\n\n\n\n\n\n\n\n\nKernel density estimate of annual excess return vs. annual volatility for s_10 and b_10. The separation in volatility–mean space reflects the underlying Sharpe ratio asymmetry.\nThese dynamic plots provide a richer understanding of the persistent performance asymmetry between the smallest and largest decile portfolios—supporting the broader structural thesis underlying TBTF.",
    "crumbs": [
      "교육",
      "Model",
      "03 Benchmarks"
    ]
  },
  {
    "objectID": "4_3_ff_compare.html#sharpe-ratio-dynamics",
    "href": "4_3_ff_compare.html#sharpe-ratio-dynamics",
    "title": "03 Benchmarks",
    "section": "",
    "text": "While static comparisons of mean and volatility offer useful summary insights, they can be misleading in the presence of temporal instability. To capture the time-varying performance profile of small- and large-cap portfolios, we construct rolling Sharpe ratios using annualized excess returns.\nLet \\(\\mu_t\\) and \\(\\sigma_t\\) denote the rolling annualized excess return mean and volatility of a portfolio over a 36-month window. Then, the rolling Sharpe ratio at time \\(t\\) is computed as:\n\\[\n\\text{Sharpe}_t = \\frac{\\mu_t}{\\sigma_t}\n\\]\nWe focus on the bottom and top size deciles: s_10 (small-cap) and b_10 (large-cap).",
    "crumbs": [
      "교육",
      "Model",
      "03 Benchmarks"
    ]
  },
  {
    "objectID": "4_3_ff_compare.html#contribution",
    "href": "4_3_ff_compare.html#contribution",
    "title": "03 Benchmarks",
    "section": "",
    "text": "Introduces a time-dynamic framework—Sharpe ratio level curves—to visualize and quantify structural performance asymmetries between size portfolios.\nEstablishes a long-term empirical benchmark against which TBTF performance can be evaluated.\nProvides theoretical and empirical justification for focusing on top-market-cap stocks, beyond simple momentum or value signals.",
    "crumbs": [
      "교육",
      "Model",
      "03 Benchmarks"
    ]
  },
  {
    "objectID": "4_3_ff_compare.html#appendix",
    "href": "4_3_ff_compare.html#appendix",
    "title": "03 Benchmarks",
    "section": "Appendix",
    "text": "Appendix\n\n\nQQ Plot – s_10 vs. b_10\n\n\nCode\n#@title QQ-plot (정규분포 검사)\n# ![](figs/qqplot_s10_b10.png)\n\nbig = pfo_size['b_10']\nsmall = pfo_size['s_10']\n\nimport statsmodels.api as sm\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 6))\n# Calculate the minimum and maximum y-values across both datasets\nmin_y = min(small.min(), big.min())\nmax_y = max(small.max(), big.max())\n\n# QQ plot for 's_low' on the first subplot (ax1)\nsm.qqplot(small, line='s', ax=ax1)\nax1.set_title('Small Portfolio Monthly excess Returns')\nax1.grid(True)\n\n# QQ plot for 'b_low' on the second subplot (ax2)\nsm.qqplot(big, line='s', ax=ax2)\nax2.set_title('Big Portfolio Monthly excess Returns')\nax2.grid(True)\n\n# Set the y-axis limits for both subplots (ax1 and ax2)\nax1.set_ylim([min_y, max_y])\nax2.set_ylim([min_y, max_y])\n# Set x-axis limits of ax2 to be the same as ax1\nxlim = ax1.get_xlim()\nax2.set_xlim(xlim)\n\nplt.tight_layout()  # Adjust spacing between subplots\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n#@title QQ-plot (annualized)\n\nbig = annual_mean['b_10']\nsmall = annual_mean['s_10']\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 6))\n# Calculate the minimum and maximum y-values across both datasets\nmin_y = min(small.min(), big.min())\nmax_y = max(small.max(), big.max())\n\n# QQ plot for 's_low' on the first subplot (ax1)\nsm.qqplot(small, line='s', ax=ax1)\nax1.set_title('QQ Plot of small Portfolio Returns')\nax1.grid(True)\n\n# QQ plot for 'b_low' on the second subplot (ax2)\nsm.qqplot(big, line='s', ax=ax2)\nax2.set_title('QQ Plot of big Portfolio Returns')\nax2.grid(True)\n\n# Set the y-axis limits for both subplots (ax1 and ax2)\nax1.set_ylim([min_y, max_y])\nax2.set_ylim([min_y, max_y])\n# Set x-axis limits of ax2 to be the same as ax1\nxlim = ax1.get_xlim()\nax2.set_xlim(xlim)\n\nplt.tight_layout()  # Adjust spacing between subplots\nplt.show()\n\n\n\n\n\n\n\n\n\nQQ plots comparing sample quantiles of s_10 (left) and b_10 (right) returns to a standard normal distribution. The flatter slope and tighter fit of b_10 indicate lower volatility and greater normality, while s_10 exhibits positive skewness and negative tail risk.\n\nQQ Plot Interpretation: Comparing Small vs. Big Size Portfolios\nTo evaluate the return distribution characteristics of small-cap and large-cap portfolios, we construct QQ plots of returns for the s_10 (smallest decile) and b_10 (largest decile) portfolios. The plots juxtapose sample quantiles against theoretical quantiles from a standard normal distribution. Three notable features emerge:\n\nSlope of the Fitted Line (Volatility Indicator)\nThe slope of the QQ line is significantly flatter for the b_10 portfolio than for the s_10.\nThis reflects lower empirical standard deviation for large-cap returns, consistent with their lower volatility and more stable risk profiles. The slope in a QQ plot corresponds to the ratio of sample to theoretical standard deviation, reinforcing the volatility advantage of big stocks.\nLine–Scatter Fit (Distributional Regularity)\nThe b_10 plot shows a much tighter fit of the scatter points along the reference line, compared to the s_10 portfolio.\nThis implies that large-cap returns conform more closely to the normal distribution. In contrast, the small-cap portfolio exhibits noticeable deviations, suggesting greater skewness, kurtosis, or latent regime switches. The result indicates that large-cap stocks exhibit greater distributional regularity, aligning with their more predictable behavior in large institutional portfolios.\nTail Behavior (Asymmetry in Small-Cap Returns)\nFor s_10, the right tail (positive returns) lies above the line, while the left tail (negative returns) lies below.\nThis pattern suggests positive skewness—i.e., occasional high positive returns but more frequent or severe downside shocks. Such asymmetry is common in small-cap stocks, which may have explosive upside potential but are also subject to default or delisting risk. The departure from symmetry in s_10 strengthens the case for mixture modeling and asymmetric tail analysis in the TBTF framework.\n\n\n\n\nPaired T-test (1963–2023)\n\n\nCode\n#@title Dependent t-test (annualized mean. Small sample)\n\nsmall = annual_mean['s_10']\nbig = annual_mean['b_10']\n\nt_statistic, p_value = stats.ttest_rel(small, big)\nprint(\"Paired t-test results:\")\nprint(\"t-statistic:\", round(t_statistic,3))\nprint(\"p-value:\", round(p_value,3))\n\n# If the p-value is greater than your significance level (0.05),\n# you would fail to reject the null hypothesis (i.e. not enough evidence to suggest a significant difference)\n\npaired_diff = (small - big)\npaired_diff.hist(bins= len(annual_std) )\nplt.axvline(paired_diff.median(), color='red', linestyle='--')\nplt.title('Paired Mean Difference Histogram')\nplt.xlabel('Annual excess return Difference from small size to big size')\nplt.ylabel('Frequency')\nplt.show()\nprint('The dashed line indicates the median of the difference')\n\n\nPaired t-test results:\nt-statistic: 0.988\np-value: 0.327\n\n\n\n\n\n\n\n\n\nThe dashed line indicates the median of the difference\n\n\n\n\nCode\n#@title Dependent t-test (annualized std. Small sample)\n\nsmall = annual_std['s_10']\nbig = annual_std['b_10']\n\nt_statistic, p_value = stats.ttest_rel(small, big)\nprint(\"Paired t-test results:\")\nprint(\"t-statistic:\", round(t_statistic,3))\nprint(\"p-value:\", round(p_value,3))\n\n# If the p-value is greater than your significance level (0.05),\n# you would fail to reject the null hypothesis (i.e. not enough evidence to suggest a significant difference)\n\npaired_diff = (small - big)\npaired_diff.hist(bins= len(annual_std) )\nplt.axvline(paired_diff.median(), color='red', linestyle='--')\nplt.title('Paired Volatility Difference Histogram')\nplt.xlabel('Annual Volatility Difference from small size to big size')\nplt.ylabel('Frequency')\nplt.show()\nprint('The dashed line indicates the median of the difference')\n\n\nPaired t-test results:\nt-statistic: 7.531\np-value: 0.0\n\n\n\n\n\n\n\n\n\nThe dashed line indicates the median of the difference",
    "crumbs": [
      "교육",
      "Model",
      "03 Benchmarks"
    ]
  },
  {
    "objectID": "4_5_strategy.html",
    "href": "4_5_strategy.html",
    "title": "05 Strategy",
    "section": "",
    "text": "Traditional asset pricing frameworks rest on no-arbitrage principles and risk-return tradeoffs, assuming that all assets exist to offer compensation for exposure to priced risks. Under such models, individual asset returns are interpreted as linear functions of sensitivities to systematic risk factors.\nIn contrast, the TBTF (Too Big To Fail) strategy is motivated by a fundamentally different view of financial markets under late-stage capitalism. We argue that the secondary stock market reflects persistent dominance by a small subset of firms whose market power is self-reinforcing. These firms attract new capital not due to risk-efficiency, but due to narrative-driven legitimacy and path-dependent concentration of initial endowment.\nThis motivates a strategy grounded not in diversification or risk exposure, but in capital persistence, market lock-in, and capital hierarchy.\n\n\n\nThe TBTF strategy is constructed under When-What–How framework:\n\nWhen: look-back window (in-sample estimation) & look-forward window (rebalancing frequency)\nWhat: asset universe (e.g. all US-listed stocks) & selection rule (e.g. top-n market cap)\nHow: asset weighting scheme (e.g. convex capital concentration fit)\n\n\n\nOur strategy operates on the full set of U.S. listed common stocks traded on NYSE, NASDAQ, and AMEX. Stocks with non-positive market capitalization are excluded to avoid bankrupt or illiquid firms.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom scipy.stats import skew, kurtosis\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport sqlite3\ntbtf = sqlite3.connect(database=\"../../tbtf.sqlite\")\n\ncrsp = pd.read_sql_query(\n  sql=\"SELECT * FROM crsp\",\n  con=tbtf,\n  parse_dates={\"date\"}\n)\n\n\n\n\n\nEach month, firms are ranked by market capitalization. These rankings define discrete capital states (percentile bins). We focus on the top-decile (state = 10), selecting the top-\\(n\\) firms at time \\(t\\) based on market-cap rank. The baseline is \\(n=10\\), with robustness checks for \\(n \\in \\{5, 20, 30, 50\\}\\).\nA Markovian state transition framework is imposed to capture the temporal dynamics of capital flow between ranked states. The top state exhibits persistent and asymmetric capital retention, supporting the capital lock-in hypothesis.\n\n\n\n\nWe evaluate three portfolio weighting methods:\n\nTBTF (Convex Structural Weighting):\nCapital weights are determined by in-sample estimates of the convex relationship between market-cap rank and capital share. Two specifications are considered:\n\nQuadratic Form: \\(w_i \\propto \\alpha + \\beta r_i + \\gamma r_i^2\\)\nExponential Form: \\(w_i \\propto \\alpha e^{\\beta r_i}\\)\n\nThe exponential form better fits observed capital share concentration and ensures structural monotonicity.\nValue-Weighted (VW):\nProportional to each firm’s market capitalization.\nEqual-Weighted (EQ):\nUniform allocation across all selected assets.\n\n\n\nCode\ndef estimate_exponential_weights(df_in_sample, n=10, state=10):\n    \"\"\"\n    Exponential model 기반으로 특정 state 내 상위 n개 종목의 rank → capshare 관계를 추정.\n    \n    Parameters:\n    - df_in_sample: in-sample CRSP 데이터프레임\n    - n: 각 시점에서 선택할 상위 종목 수\n    - state: 분석 대상 percentile state (e.g., 10 = top 10%)\n\n    Returns:\n    - model_params: {'alpha': α, 'beta': β}\n    - r_squared: 회귀 적합도\n    \"\"\"\n    all_obs = []\n\n    for date, group in df_in_sample[df_in_sample['state'] == state].groupby('date'):\n        group = group.sort_values('mktcap_lag', ascending=False).head(n).copy()\n        if len(group) &lt; n:\n            continue\n\n        group = group.sort_values('mktcap_lag')  # 작은 순으로 정렬\n        group['rank'] = np.arange(1, len(group) + 1)\n        total_cap = group['mktcap_lag'].sum()\n        group['capshare'] = group['mktcap_lag'] / total_cap\n        group['log_capshare'] = np.log(group['capshare'])\n\n        all_obs.append(group[['rank', 'log_capshare']])\n\n    if not all_obs:\n        raise ValueError(\"No valid observations for exponential regression.\")\n\n    df_all = pd.concat(all_obs, ignore_index=True)\n    X = sm.add_constant(df_all['rank'])\n    y = df_all['log_capshare']\n\n    model = sm.OLS(y, X).fit()\n    ln_alpha, beta = model.params\n    alpha = np.exp(ln_alpha)\n\n    return {'alpha': alpha, 'beta': beta}, model.rsquared\n\ndef estimate_quadratic_weights(df_in_sample, n=10, state=10):\n    \"\"\"\n    Rolling in-sample 기간에 대해 top-n 종목에 대한 평균적인 \n    (rank, capshare) 관계를 추정하여 quadratic weight 모델 생성\n    \"\"\"\n    df = df_in_sample[df_in_sample['state'] == state].copy()\n\n    # 각 날짜별로 상위 n개 선택\n    top_n_list = []\n    for date, group in df.groupby('date'):\n        group = group.sort_values('mktcap_lag', ascending=False).head(n)\n        if len(group) &lt; n:\n            continue\n        group = group.sort_values('mktcap_lag')\n        group['rank'] = np.arange(1, len(group) + 1)\n        group['capshare'] = group['mktcap_lag'] / group['mktcap_lag'].sum()\n        top_n_list.append(group[['permno', 'rank', 'capshare']])\n    \n    if not top_n_list:\n        return None, None\n\n    df_all = pd.concat(top_n_list)\n\n    # 회귀 변수 생성\n    df_all['rank_sq'] = df_all['rank'] ** 2\n    X = sm.add_constant(df_all[['rank', 'rank_sq']])\n    y = df_all['capshare']\n\n    # 회귀 수행\n    model = sm.OLS(y, X).fit()\n    \n    return model.params, model.rsquared\n\n\n\n\nCode\ndef plot_quadratic_fit(df_in_sample, n=10, state=10, ax=None):\n    \"\"\"\n    특정 in-sample에서 주어진 state, top-n 종목에 대한\n    (rank, capshare) 산점도와 quadratic regression fitted line 시각화\n    \"\"\"\n    all_obs = []\n\n    for date, group in df_in_sample[df_in_sample['state'] == state].groupby('date'):\n        group = group.sort_values('mktcap_lag', ascending=False).head(n).copy()\n        if len(group) &lt; n:\n            continue\n\n        group = group.sort_values('mktcap_lag')  # rank: 1 = smallest\n        group['rank'] = np.arange(1, len(group) + 1)\n        total_cap = group['mktcap_lag'].sum()\n        group['capshare'] = group['mktcap_lag'] / total_cap\n\n        all_obs.append(group[['rank', 'capshare']])\n\n    if not all_obs:\n        raise ValueError(\"No valid observations for fitting.\")\n\n    df_all = pd.concat(all_obs, ignore_index=True)\n    df_all['rank_sq'] = df_all['rank'] ** 2\n\n    # 회귀\n    X = sm.add_constant(df_all[['rank', 'rank_sq']])\n    y = df_all['capshare']\n    model = sm.OLS(y, X).fit()\n\n    # 추정 계수\n    alpha, beta, gamma = model.params\n\n    # Fitted curve\n    rank_grid = np.linspace(df_all['rank'].min(), df_all['rank'].max(), 100)\n    fitted_curve = alpha + beta * rank_grid + gamma * rank_grid**2\n\n    # 시각화\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(8, 5))\n    ax.scatter(df_all['rank'], df_all['capshare'], alpha=0.4, label='Observed', color='steelblue')\n    ax.plot(rank_grid, fitted_curve, color='darkgreen', linewidth=2.5, label='Quadratic Fit')\n    ax.set_title(f\"Quadratic Fit (state={state}, n={n})\")\n    ax.set_xlabel(\"Rank\")\n    ax.set_ylabel(\"Capital Share\")\n    ax.legend()\n    ax.grid(True)\n\n    return \n\ndef plot_exponential_fit(df_in_sample, n=10, state=10, ax=None):\n    \"\"\"\n    특정 in-sample에서 주어진 state, top-n 종목에 대한\n    (rank, capshare) 산점도와 exponential regression fitted line 시각화\n    \"\"\"\n\n    all_obs = []\n\n    for date, group in df_in_sample[df_in_sample['state'] == state].groupby('date'):\n        group = group.sort_values('mktcap_lag', ascending=False).head(n).copy()\n        if len(group) &lt; n:\n            continue\n\n        group = group.sort_values('mktcap_lag')  # 작은 순서대로 재정렬\n        group['rank'] = np.arange(1, len(group) + 1)\n        total_cap = group['mktcap_lag'].sum()\n        group['capshare'] = group['mktcap_lag'] / total_cap\n\n        all_obs.append(group[['rank', 'capshare']])\n\n    if not all_obs:\n        raise ValueError(\"No valid observations for fitting.\")\n\n    df_all = pd.concat(all_obs, ignore_index=True)\n    df_all['log_capshare'] = np.log(df_all['capshare'])\n\n    # 회귀\n    X = sm.add_constant(df_all['rank'])\n    y = df_all['log_capshare']\n    model = sm.OLS(y, X).fit()\n\n    # 추정 계수\n    ln_alpha, beta = model.params\n    alpha = np.exp(ln_alpha)\n\n    # Fitted curve\n    rank_grid = np.linspace(df_all['rank'].min(), df_all['rank'].max(), 100)\n    fitted_curve = alpha * np.exp(beta * rank_grid)\n\n    # 시각화\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(8, 5))\n    ax.scatter(df_all['rank'], df_all['capshare'], alpha=0.4, label='Observed', color='steelblue')\n    ax.plot(rank_grid, fitted_curve, color='darkred', linewidth=2.5, label='Exponential Fit')\n    ax.set_title(f\"Exponential Fit (state={state}, n={n})\")\n    ax.set_xlabel(\"Rank\")\n    ax.set_ylabel(\"Capital Share\")\n    ax.legend()\n    ax.grid(True)\n\n    return \n\n\n\n\nTo empirically validate the structural weighting functions, we fit both quadratic and exponential models to the in-sample relationship between rank and capital share. The following plots show the cross-sectional fitting results based on a representative rebalance date.\n\n\nCode\ndef split_in_out_sample(df, in_end, in_sample_months=36, out_end=None):\n    \"\"\"\n    주어진 in_end와 in_sample_months, 그리고 선택적으로 out_end를 이용해 \n    in-sample과 out-of-sample 데이터를 분할합니다.\n    \n    Parameters:\n      - df: 전체 데이터프레임 (반드시 'date' 컬럼이 있어야 함)\n      - in_end: in-sample 종료일 (string 또는 datetime)\n      - in_sample_months: in-sample 기간 (월 단위, default 36)\n      - out_end: out-of-sample 종료일 (string 또는 datetime). None인 경우 in_end 이후 전체가 out-of-sample.\n    \n    Returns:\n      - in_sample: [in_end - in_sample_months, in_end] 기간의 데이터프레임\n      - out_sample: (in_end, out_end] 또는 in_end 이후 전체 데이터프레임\n    \"\"\"\n    df = df.copy()\n    df['date'] = pd.to_datetime(df['date'])\n    in_end = pd.to_datetime(in_end)\n    \n    # in-sample 기간 계산\n    in_start = in_end - pd.DateOffset(months=in_sample_months)\n    in_sample = df[(df['date'] &gt;= in_start) & (df['date'] &lt;= in_end)].copy()\n    \n    # out-of-sample 기간 계산\n    if out_end is not None:\n        out_end = pd.to_datetime(out_end)\n        out_sample = df[(df['date'] &gt; in_end) & (df['date'] &lt;= out_end)].copy()\n    else:\n        out_sample = df[df['date'] &gt; in_end].copy()\n    \n    return in_sample, out_sample\n\n\n\n\nCode\nin_end = '2009-12-31'\nin_sample_months = 48\n\nprint('Snapshot at:', in_end)\nprint('Look-back period:', in_sample_months, 'months')\n\ndf_in_sample, _ = split_in_out_sample(crsp, in_end, in_sample_months)\n\n\n\n\nCode\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\nplot_quadratic_fit(df_in_sample, n=10, state=10, ax=axes[0])\nplot_exponential_fit(df_in_sample, n=10, state=10, ax=axes[1])\n\nfig.suptitle(f\"Fitted Capital Share Functions at {in_end}\")\nplt.tight_layout()\n\n\n\n\nCode\nin_end = '2023-12-31'\nin_sample_months = 48\n\nprint('Snapshot at:', in_end)\nprint('Look-back period:', in_sample_months, 'months')\n\ndf_in_sample, _ = split_in_out_sample(crsp, in_end, in_sample_months)\n\n\n\n\nCode\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\nplot_quadratic_fit(df_in_sample, n=10, state=10, ax=axes[0])\nplot_exponential_fit(df_in_sample, n=10, state=10, ax=axes[1])\n\nfig.suptitle(f\"Fitted Capital Share Functions at {in_end}\")\nplt.tight_layout()\n\n\nThese cross-sectional plots confirm the superiority of the exponential fit in capturing the accelerating concentration of capital among top-ranked firms. The difference is especially pronounced in the post-2010 sample, reflecting a structural shift in capital dynamics during the QE era.\nThe exponential fit exhibits stronger monotonicity and structural convexity, particularly in the top-ranked firms, supporting its use in the TBTF weighting scheme.\n\n\n\n\nWe test fixed-interval rebalancing schemes:\n\nMonthly (default)\nQuarterly\nSemiannual\nAnnual\n\nRebalancing frequency directly affects turnover and transaction cost implications. Monthly rebalancing is selected as the baseline to balance responsiveness with frictions.\n\n\n\nFor external comparison, we use:\n\nPre-2010 Index Benchmarks: Dow Jones Industrial Average (^DJI), Nasdaq-100 (^NDX)\nPost-2010 Index ETFs:\n\nDIA (ETF version of DJIA, State Street Corporation ETF. Fund inception: 1998/01/14),\nQQQ (ETF version of NDX. Nasdaq-100 ticker. the Invesco QQQ Trust. Fund inception: 1999/03/10),\nSPY (The SPDR S&P 500 ETF, Fund inception: 1993/01/22),\nVTI (Vanguard Total Stock Market ETF, Fund inception: 2001년 5월 24일),\n\nPost-2010 Style Portfolios: Fama-French ME5 × PRIOR1/3/5, constructed with rolling monthly value- or equal-weighting\n\n\n\n\nThe TBTF strategy is not merely a rule-based selection method. It reflects a structural argument that allocative efficiency in modern markets is compromised by persistent capital concentration. Instead of diversifying away idiosyncratic risk, markets are increasingly dominated by capital lock-in, hierarchy reinforcement, and narrative legitimacy.\nThis framework reconceptualizes the role of financial assets from carriers of risk to vehicles of structural dominance, and evaluates portfolio construction in light of this altered paradigm.",
    "crumbs": [
      "교육",
      "Model",
      "05 Strategy"
    ]
  },
  {
    "objectID": "4_5_strategy.html#strategy-design-assumptions-and-market-philosophy",
    "href": "4_5_strategy.html#strategy-design-assumptions-and-market-philosophy",
    "title": "05 Strategy",
    "section": "",
    "text": "Traditional asset pricing frameworks rest on no-arbitrage principles and risk-return tradeoffs, assuming that all assets exist to offer compensation for exposure to priced risks. Under such models, individual asset returns are interpreted as linear functions of sensitivities to systematic risk factors.\nIn contrast, the TBTF (Too Big To Fail) strategy is motivated by a fundamentally different view of financial markets under late-stage capitalism. We argue that the secondary stock market reflects persistent dominance by a small subset of firms whose market power is self-reinforcing. These firms attract new capital not due to risk-efficiency, but due to narrative-driven legitimacy and path-dependent concentration of initial endowment.\nThis motivates a strategy grounded not in diversification or risk exposure, but in capital persistence, market lock-in, and capital hierarchy.\n\n\n\nThe TBTF strategy is constructed under When-What–How framework:\n\nWhen: look-back window (in-sample estimation) & look-forward window (rebalancing frequency)\nWhat: asset universe (e.g. all US-listed stocks) & selection rule (e.g. top-n market cap)\nHow: asset weighting scheme (e.g. convex capital concentration fit)\n\n\n\nOur strategy operates on the full set of U.S. listed common stocks traded on NYSE, NASDAQ, and AMEX. Stocks with non-positive market capitalization are excluded to avoid bankrupt or illiquid firms.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom scipy.stats import skew, kurtosis\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport sqlite3\ntbtf = sqlite3.connect(database=\"../../tbtf.sqlite\")\n\ncrsp = pd.read_sql_query(\n  sql=\"SELECT * FROM crsp\",\n  con=tbtf,\n  parse_dates={\"date\"}\n)\n\n\n\n\n\nEach month, firms are ranked by market capitalization. These rankings define discrete capital states (percentile bins). We focus on the top-decile (state = 10), selecting the top-\\(n\\) firms at time \\(t\\) based on market-cap rank. The baseline is \\(n=10\\), with robustness checks for \\(n \\in \\{5, 20, 30, 50\\}\\).\nA Markovian state transition framework is imposed to capture the temporal dynamics of capital flow between ranked states. The top state exhibits persistent and asymmetric capital retention, supporting the capital lock-in hypothesis.\n\n\n\n\nWe evaluate three portfolio weighting methods:\n\nTBTF (Convex Structural Weighting):\nCapital weights are determined by in-sample estimates of the convex relationship between market-cap rank and capital share. Two specifications are considered:\n\nQuadratic Form: \\(w_i \\propto \\alpha + \\beta r_i + \\gamma r_i^2\\)\nExponential Form: \\(w_i \\propto \\alpha e^{\\beta r_i}\\)\n\nThe exponential form better fits observed capital share concentration and ensures structural monotonicity.\nValue-Weighted (VW):\nProportional to each firm’s market capitalization.\nEqual-Weighted (EQ):\nUniform allocation across all selected assets.\n\n\n\nCode\ndef estimate_exponential_weights(df_in_sample, n=10, state=10):\n    \"\"\"\n    Exponential model 기반으로 특정 state 내 상위 n개 종목의 rank → capshare 관계를 추정.\n    \n    Parameters:\n    - df_in_sample: in-sample CRSP 데이터프레임\n    - n: 각 시점에서 선택할 상위 종목 수\n    - state: 분석 대상 percentile state (e.g., 10 = top 10%)\n\n    Returns:\n    - model_params: {'alpha': α, 'beta': β}\n    - r_squared: 회귀 적합도\n    \"\"\"\n    all_obs = []\n\n    for date, group in df_in_sample[df_in_sample['state'] == state].groupby('date'):\n        group = group.sort_values('mktcap_lag', ascending=False).head(n).copy()\n        if len(group) &lt; n:\n            continue\n\n        group = group.sort_values('mktcap_lag')  # 작은 순으로 정렬\n        group['rank'] = np.arange(1, len(group) + 1)\n        total_cap = group['mktcap_lag'].sum()\n        group['capshare'] = group['mktcap_lag'] / total_cap\n        group['log_capshare'] = np.log(group['capshare'])\n\n        all_obs.append(group[['rank', 'log_capshare']])\n\n    if not all_obs:\n        raise ValueError(\"No valid observations for exponential regression.\")\n\n    df_all = pd.concat(all_obs, ignore_index=True)\n    X = sm.add_constant(df_all['rank'])\n    y = df_all['log_capshare']\n\n    model = sm.OLS(y, X).fit()\n    ln_alpha, beta = model.params\n    alpha = np.exp(ln_alpha)\n\n    return {'alpha': alpha, 'beta': beta}, model.rsquared\n\ndef estimate_quadratic_weights(df_in_sample, n=10, state=10):\n    \"\"\"\n    Rolling in-sample 기간에 대해 top-n 종목에 대한 평균적인 \n    (rank, capshare) 관계를 추정하여 quadratic weight 모델 생성\n    \"\"\"\n    df = df_in_sample[df_in_sample['state'] == state].copy()\n\n    # 각 날짜별로 상위 n개 선택\n    top_n_list = []\n    for date, group in df.groupby('date'):\n        group = group.sort_values('mktcap_lag', ascending=False).head(n)\n        if len(group) &lt; n:\n            continue\n        group = group.sort_values('mktcap_lag')\n        group['rank'] = np.arange(1, len(group) + 1)\n        group['capshare'] = group['mktcap_lag'] / group['mktcap_lag'].sum()\n        top_n_list.append(group[['permno', 'rank', 'capshare']])\n    \n    if not top_n_list:\n        return None, None\n\n    df_all = pd.concat(top_n_list)\n\n    # 회귀 변수 생성\n    df_all['rank_sq'] = df_all['rank'] ** 2\n    X = sm.add_constant(df_all[['rank', 'rank_sq']])\n    y = df_all['capshare']\n\n    # 회귀 수행\n    model = sm.OLS(y, X).fit()\n    \n    return model.params, model.rsquared\n\n\n\n\nCode\ndef plot_quadratic_fit(df_in_sample, n=10, state=10, ax=None):\n    \"\"\"\n    특정 in-sample에서 주어진 state, top-n 종목에 대한\n    (rank, capshare) 산점도와 quadratic regression fitted line 시각화\n    \"\"\"\n    all_obs = []\n\n    for date, group in df_in_sample[df_in_sample['state'] == state].groupby('date'):\n        group = group.sort_values('mktcap_lag', ascending=False).head(n).copy()\n        if len(group) &lt; n:\n            continue\n\n        group = group.sort_values('mktcap_lag')  # rank: 1 = smallest\n        group['rank'] = np.arange(1, len(group) + 1)\n        total_cap = group['mktcap_lag'].sum()\n        group['capshare'] = group['mktcap_lag'] / total_cap\n\n        all_obs.append(group[['rank', 'capshare']])\n\n    if not all_obs:\n        raise ValueError(\"No valid observations for fitting.\")\n\n    df_all = pd.concat(all_obs, ignore_index=True)\n    df_all['rank_sq'] = df_all['rank'] ** 2\n\n    # 회귀\n    X = sm.add_constant(df_all[['rank', 'rank_sq']])\n    y = df_all['capshare']\n    model = sm.OLS(y, X).fit()\n\n    # 추정 계수\n    alpha, beta, gamma = model.params\n\n    # Fitted curve\n    rank_grid = np.linspace(df_all['rank'].min(), df_all['rank'].max(), 100)\n    fitted_curve = alpha + beta * rank_grid + gamma * rank_grid**2\n\n    # 시각화\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(8, 5))\n    ax.scatter(df_all['rank'], df_all['capshare'], alpha=0.4, label='Observed', color='steelblue')\n    ax.plot(rank_grid, fitted_curve, color='darkgreen', linewidth=2.5, label='Quadratic Fit')\n    ax.set_title(f\"Quadratic Fit (state={state}, n={n})\")\n    ax.set_xlabel(\"Rank\")\n    ax.set_ylabel(\"Capital Share\")\n    ax.legend()\n    ax.grid(True)\n\n    return \n\ndef plot_exponential_fit(df_in_sample, n=10, state=10, ax=None):\n    \"\"\"\n    특정 in-sample에서 주어진 state, top-n 종목에 대한\n    (rank, capshare) 산점도와 exponential regression fitted line 시각화\n    \"\"\"\n\n    all_obs = []\n\n    for date, group in df_in_sample[df_in_sample['state'] == state].groupby('date'):\n        group = group.sort_values('mktcap_lag', ascending=False).head(n).copy()\n        if len(group) &lt; n:\n            continue\n\n        group = group.sort_values('mktcap_lag')  # 작은 순서대로 재정렬\n        group['rank'] = np.arange(1, len(group) + 1)\n        total_cap = group['mktcap_lag'].sum()\n        group['capshare'] = group['mktcap_lag'] / total_cap\n\n        all_obs.append(group[['rank', 'capshare']])\n\n    if not all_obs:\n        raise ValueError(\"No valid observations for fitting.\")\n\n    df_all = pd.concat(all_obs, ignore_index=True)\n    df_all['log_capshare'] = np.log(df_all['capshare'])\n\n    # 회귀\n    X = sm.add_constant(df_all['rank'])\n    y = df_all['log_capshare']\n    model = sm.OLS(y, X).fit()\n\n    # 추정 계수\n    ln_alpha, beta = model.params\n    alpha = np.exp(ln_alpha)\n\n    # Fitted curve\n    rank_grid = np.linspace(df_all['rank'].min(), df_all['rank'].max(), 100)\n    fitted_curve = alpha * np.exp(beta * rank_grid)\n\n    # 시각화\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(8, 5))\n    ax.scatter(df_all['rank'], df_all['capshare'], alpha=0.4, label='Observed', color='steelblue')\n    ax.plot(rank_grid, fitted_curve, color='darkred', linewidth=2.5, label='Exponential Fit')\n    ax.set_title(f\"Exponential Fit (state={state}, n={n})\")\n    ax.set_xlabel(\"Rank\")\n    ax.set_ylabel(\"Capital Share\")\n    ax.legend()\n    ax.grid(True)\n\n    return \n\n\n\n\nTo empirically validate the structural weighting functions, we fit both quadratic and exponential models to the in-sample relationship between rank and capital share. The following plots show the cross-sectional fitting results based on a representative rebalance date.\n\n\nCode\ndef split_in_out_sample(df, in_end, in_sample_months=36, out_end=None):\n    \"\"\"\n    주어진 in_end와 in_sample_months, 그리고 선택적으로 out_end를 이용해 \n    in-sample과 out-of-sample 데이터를 분할합니다.\n    \n    Parameters:\n      - df: 전체 데이터프레임 (반드시 'date' 컬럼이 있어야 함)\n      - in_end: in-sample 종료일 (string 또는 datetime)\n      - in_sample_months: in-sample 기간 (월 단위, default 36)\n      - out_end: out-of-sample 종료일 (string 또는 datetime). None인 경우 in_end 이후 전체가 out-of-sample.\n    \n    Returns:\n      - in_sample: [in_end - in_sample_months, in_end] 기간의 데이터프레임\n      - out_sample: (in_end, out_end] 또는 in_end 이후 전체 데이터프레임\n    \"\"\"\n    df = df.copy()\n    df['date'] = pd.to_datetime(df['date'])\n    in_end = pd.to_datetime(in_end)\n    \n    # in-sample 기간 계산\n    in_start = in_end - pd.DateOffset(months=in_sample_months)\n    in_sample = df[(df['date'] &gt;= in_start) & (df['date'] &lt;= in_end)].copy()\n    \n    # out-of-sample 기간 계산\n    if out_end is not None:\n        out_end = pd.to_datetime(out_end)\n        out_sample = df[(df['date'] &gt; in_end) & (df['date'] &lt;= out_end)].copy()\n    else:\n        out_sample = df[df['date'] &gt; in_end].copy()\n    \n    return in_sample, out_sample\n\n\n\n\nCode\nin_end = '2009-12-31'\nin_sample_months = 48\n\nprint('Snapshot at:', in_end)\nprint('Look-back period:', in_sample_months, 'months')\n\ndf_in_sample, _ = split_in_out_sample(crsp, in_end, in_sample_months)\n\n\n\n\nCode\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\nplot_quadratic_fit(df_in_sample, n=10, state=10, ax=axes[0])\nplot_exponential_fit(df_in_sample, n=10, state=10, ax=axes[1])\n\nfig.suptitle(f\"Fitted Capital Share Functions at {in_end}\")\nplt.tight_layout()\n\n\n\n\nCode\nin_end = '2023-12-31'\nin_sample_months = 48\n\nprint('Snapshot at:', in_end)\nprint('Look-back period:', in_sample_months, 'months')\n\ndf_in_sample, _ = split_in_out_sample(crsp, in_end, in_sample_months)\n\n\n\n\nCode\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\nplot_quadratic_fit(df_in_sample, n=10, state=10, ax=axes[0])\nplot_exponential_fit(df_in_sample, n=10, state=10, ax=axes[1])\n\nfig.suptitle(f\"Fitted Capital Share Functions at {in_end}\")\nplt.tight_layout()\n\n\nThese cross-sectional plots confirm the superiority of the exponential fit in capturing the accelerating concentration of capital among top-ranked firms. The difference is especially pronounced in the post-2010 sample, reflecting a structural shift in capital dynamics during the QE era.\nThe exponential fit exhibits stronger monotonicity and structural convexity, particularly in the top-ranked firms, supporting its use in the TBTF weighting scheme.\n\n\n\n\nWe test fixed-interval rebalancing schemes:\n\nMonthly (default)\nQuarterly\nSemiannual\nAnnual\n\nRebalancing frequency directly affects turnover and transaction cost implications. Monthly rebalancing is selected as the baseline to balance responsiveness with frictions.\n\n\n\nFor external comparison, we use:\n\nPre-2010 Index Benchmarks: Dow Jones Industrial Average (^DJI), Nasdaq-100 (^NDX)\nPost-2010 Index ETFs:\n\nDIA (ETF version of DJIA, State Street Corporation ETF. Fund inception: 1998/01/14),\nQQQ (ETF version of NDX. Nasdaq-100 ticker. the Invesco QQQ Trust. Fund inception: 1999/03/10),\nSPY (The SPDR S&P 500 ETF, Fund inception: 1993/01/22),\nVTI (Vanguard Total Stock Market ETF, Fund inception: 2001년 5월 24일),\n\nPost-2010 Style Portfolios: Fama-French ME5 × PRIOR1/3/5, constructed with rolling monthly value- or equal-weighting\n\n\n\n\nThe TBTF strategy is not merely a rule-based selection method. It reflects a structural argument that allocative efficiency in modern markets is compromised by persistent capital concentration. Instead of diversifying away idiosyncratic risk, markets are increasingly dominated by capital lock-in, hierarchy reinforcement, and narrative legitimacy.\nThis framework reconceptualizes the role of financial assets from carriers of risk to vehicles of structural dominance, and evaluates portfolio construction in light of this altered paradigm.",
    "crumbs": [
      "교육",
      "Model",
      "05 Strategy"
    ]
  },
  {
    "objectID": "4_5_strategy.html#appendix-tbtf-strategy-pipeline",
    "href": "4_5_strategy.html#appendix-tbtf-strategy-pipeline",
    "title": "05 Strategy",
    "section": "Appendix: TBTF Strategy Pipeline",
    "text": "Appendix: TBTF Strategy Pipeline\n\nThis section outlines the modular structure of the TBTF portfolio strategy, from input specification to out-of-sample return generation and performance evaluation. The pipeline is designed to be flexible across different weighting schemes, rebalance frequencies, and strategy parameters.\n\nInputs (Arguments)\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\ndf\nMain input DataFrame (crsp) including fields such as date, permno, mktcap, ret, state, mktcap_lag, etc.\n\n\nstate_level\nTarget capital state to define the asset universe, typically the top decile (e.g., 10).\n\n\ntop_n\nNumber of assets to be selected at each rebalance point (e.g., n ∈ {5, 10, 20, 30, 50}).\n\n\nrebalance_freq\nRebalancing interval (e.g., '1M', '3M', '6M', '12M').\n\n\nweighting_method\nOne of 'equal', 'value', 'quadratic', or 'exponential'.\n\n\nin_sample_period\nEstimation window for in-sample weight calibration (e.g., '2010-01-01' to '2013-12-31').\n\n\nout_sample_period\nEvaluation window for out-of-sample backtesting (e.g., '2014-01-01' to '2023-12-31').\n\n\neta, p\nParameters for CRRA utility (eta) and Omega ratio threshold (p).\n\n\n\n\n\nStrategy Pipeline (Pseudo-code Logic)\n\nStep 1: Data Partitioning & Filtering\nin_sample = df[(df['date'] &gt;= in_sample_start) & (df['date'] &lt;= in_sample_end)]\nout_sample = df[(df['date'] &gt; in_sample_end) & (df['date'] &lt;= out_sample_end)]\nuniverse = df[df['state'] == state_level]\n\nRestrict selection universe to target capital state (e.g., top decile).\n\n\n\nStep 2: In-sample Weight Estimation\nIf weighting_method is 'quadratic' or 'exponential':\n\nEstimate the relationship between within-state rank and capital share using in-sample data:\n\nQuadratic:\n\\[\\text{CapShare}_i = \\alpha + \\beta \\cdot \\text{Rank}_i + \\gamma \\cdot \\text{Rank}_i^2\\]\nExponential:\n\\[\\text{CapShare}_i = \\alpha \\cdot e^{\\beta \\cdot \\text{Rank}_i}\\]\n\nSave estimated coefficients:\ncoefficients = {'alpha': ..., 'beta': ..., 'gamma': ...}\n\n\n\nStep 3: Out-of-sample Portfolio Construction\nFor each rebalance date in the out-sample period:\n\nFilter to target state (state == state_level)\nRank by market cap, select top n\nAssign weights:\n\nEqual: \\(w_i = 1/n\\)\nValue: \\(w_i = \\text{mktcap}_i / \\sum \\text{mktcap}\\)\nQuadratic: apply \\(\\hat{\\alpha}, \\hat{\\beta}, \\hat{\\gamma}\\) to rank\nExponential: apply \\(\\hat{\\alpha}, \\hat{\\beta}\\) to rank\n\nStore [date, permno, weight] for forward return application\n\n\n\nStep 4: Portfolio Return Computation\nAt each evaluation date after rebalance:\n\nMerge forward returns of selected assets\nCompute portfolio return: \\[R_t^{\\text{portfolio}} = \\sum_i w_{i,t} \\cdot r_{i,t}\\]\nConstruct time series of out-of-sample portfolio returns\n\n\n\nStep 5: Turnover Calculation\nFor each rebalance window:\n\nCalculate: \\[\\text{Turnover}_t = \\sum_i |w_{i,t} - w_{i,t-1}|\\]\nUseful for assessing transaction costs and liquidity requirements\n\n\n\nStep 6: Performance Evaluation\nUsing out-of-sample return series, compute:\n\nRisk-Adjusted Metrics:\n\nSharpe ratio, Sortino ratio\nOmega ratio (threshold = p)\nCalmar ratio, maximum drawdown (MDD)\nCRRA utility with risk aversion parameter eta\n\nReturn summary as dictionary:\nmetrics_dict = {\n    'Sharpe': ...,\n    'Sortino': ...,\n    'Omega': ...,\n    'CRRA': ...,\n    'CAGR': ...,\n    'MDD': ...\n}\n\n\n\n\nOutputs\n\n\n\nOutput\nDescription\n\n\n\n\nreturns_df\nTime series of out-of-sample portfolio returns\n\n\nweights_df\nPortfolio composition (weights per date and permno)\n\n\nturnover_df\nTime series of turnover at each rebalance point\n\n\nmetrics_dict\nDictionary of strategy performance metrics\n\n\n\n\n\nModule Lits\n#| label: TBTF Strategy Full Module\n#| warning: false\n#| message: false\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom scipy.stats import skew, kurtosis\n\n# ----------------------------\n# 1. Data Partitioning\n# ----------------------------\ndef split_in_out_sample(df, in_end, in_sample_months=36, out_end=None):\n    in_sample = df[(df['date'] &lt;= in_end)].copy()\n    if out_end:\n        out_sample = df[(df['date'] &gt; in_end) & (df['date'] &lt;= out_end)].copy()\n    else:\n        out_sample = df[(df['date'] &gt; in_end)].copy()\n    return in_sample, out_sample\n\n# ----------------------------\n# 2. Weight Estimation\n# ----------------------------\ndef estimate_exponential_weights(df_in_sample, n=10, state=10):\n    df = df_in_sample[df_in_sample['state'] == state].copy()\n    df['rank'] = df.groupby('date')['mktcap'].rank(ascending=True, method='first')\n    df = df[df['rank'] &lt;= n]\n    df['log_capshare'] = np.log(df['mktcap'] / df.groupby('date')['mktcap'].transform('sum'))\n    X = df[['rank']]\n    X = sm.add_constant(X)\n    y = df['log_capshare']\n    model = sm.OLS(y, X).fit()\n    return {'alpha': np.exp(model.params['const']), 'beta': model.params['rank']}\n\ndef estimate_quadratic_weights(df_in_sample, n=10, state=10):\n    df = df_in_sample[df_in_sample['state'] == state].copy()\n    df['rank'] = df.groupby('date')['mktcap'].rank(ascending=True, method='first')\n    df = df[df['rank'] &lt;= n]\n    df['capshare'] = df['mktcap'] / df.groupby('date')['mktcap'].transform('sum')\n    df['rank_sq'] = df['rank'] ** 2\n    X = df[['rank', 'rank_sq']]\n    X = sm.add_constant(X)\n    y = df['capshare']\n    model = sm.OLS(y, X).fit()\n    return {'alpha': model.params['const'], 'beta': model.params['rank'], 'gamma': model.params['rank_sq']}\n\n# ----------------------------\n# 3. Portfolio Construction\n# ----------------------------\ndef construct_tbtf_exponential(crsp_df, target_state, top_n, params):\n    df = crsp_df[crsp_df['state'] == target_state].copy()\n    df['rank'] = df.groupby('date')['mktcap'].rank(ascending=True, method='first')\n    df = df[df['rank'] &lt;= top_n]\n    df['weight'] = params['alpha'] * np.exp(params['beta'] * df['rank'])\n    df['weight'] = df['weight'] / df.groupby('date')['weight'].transform('sum')\n    return df[['date', 'permno', 'weight']]\n\ndef construct_tbtf_quadratic(crsp_df, target_state, top_n, params):\n    df = crsp_df[crsp_df['state'] == target_state].copy()\n    df['rank'] = df.groupby('date')['mktcap'].rank(ascending=True, method='first')\n    df = df[df['rank'] &lt;= top_n]\n    df['rank_sq'] = df['rank'] ** 2\n    df['weight'] = params['alpha'] + params['beta'] * df['rank'] + params['gamma'] * df['rank_sq']\n    df['weight'] = df['weight'] / df.groupby('date')['weight'].transform('sum')\n    return df[['date', 'permno', 'weight']]\n\n# ----------------------------\n# 4. Return and Turnover Calculation\n# ----------------------------\ndef compute_return_tbtf(crsp, rebalance_dates, weighting_method='exponential', top_n=10, state=10, in_sample_months=36):\n    # Placeholder for full implementation\n    pass\n\ndef compute_return_pfo(crsp, rebalance_dates, weighting_method='vw', top_n=10):\n    # Placeholder for traditional VW or EW method\n    pass\n\ndef compute_turnover(weights_df, rebalance_dates):\n    turnover_list = []\n    prev_weights = None\n    for date in rebalance_dates:\n        w = weights_df[weights_df['date'] == date].set_index('permno')['weight']\n        if prev_weights is not None:\n            turnover = (w - prev_weights).abs().sum()\n            turnover_list.append({'date': date, 'turnover': turnover})\n        prev_weights = w\n    return pd.DataFrame(turnover_list)\n\n# ----------------------------\n# 5. Performance Evaluation\n# ----------------------------\ndef evaluate_performance(returns, eta=3, p=0.01, periods_per_year=12):\n    mu = returns.mean() * periods_per_year\n    sigma = returns.std() * np.sqrt(periods_per_year)\n    sharpe = mu / sigma if sigma != 0 else np.nan\n    sortino = mu / returns[returns &lt; 0].std() if returns[returns &lt; 0].std() != 0 else np.nan\n    omega = (returns[returns &gt; p] - p).sum() / abs((returns[returns &lt;= p] - p).sum())\n    crra_utility = (1 / (1 - eta)) * (1 + returns).apply(lambda x: x**(1 - eta) - 1).mean() if eta != 1 else np.log(1 + returns).mean()\n    return {\n        'Sharpe': sharpe,\n        'Sortino': sortino,\n        'Omega': omega,\n        'CRRA': crra_utility,\n        'Mean': mu,\n        'Volatility': sigma\n    }\n\n# ----------------------------\n# 6. Rebalance Date Utility\n# ----------------------------\ndef get_rebalance_offset(rebalance_freq):\n    offset_map = {'1M': 1, '3M': 3, '6M': 6, '12M': 12}\n    return offset_map.get(rebalance_freq, 1)\n\n# ----------------------------\n# 7. Backtest Pipeline\n# ----------------------------\ndef backtest_pipeline(crsp, in_end, out_end, in_sample_months, rebalance_freq, weighting_method, top_n, state, eta, p):\n    # Placeholder for full backtest execution logic\n    pass\n\n# ----------------------------\n# 8. Visualization (optional)\n# ----------------------------\ndef plot_quadratic_fit(df_in_sample, n=10, state=10):\n    # Placeholder for plot generation\n    pass\n\ndef plot_exponential_fit(df_in_sample, n=10, state=10):\n    # Placeholder for plot generation\n    pass",
    "crumbs": [
      "교육",
      "Model",
      "05 Strategy"
    ]
  },
  {
    "objectID": "4_7_robustness.html",
    "href": "4_7_robustness.html",
    "title": "07 Robustness",
    "section": "",
    "text": "While the previous section focused on the performance outcomes of the TBTF strategy, this section evaluates the stability of those outcomes under variation in core implementation parameters. We test whether the superior performance persists under changes in portfolio size, rebalancing frequency, sample splits, and weighting schemes.\nRather than relying on a single optimized configuration, the TBTF strategy demonstrates structural robustness across plausible alternatives. This not only reinforces the credibility of the results but also supports the practical adaptability of the approach for different institutional contexts.",
    "crumbs": [
      "교육",
      "Model",
      "07 Robustness"
    ]
  },
  {
    "objectID": "4_7_robustness.html#robustness-tests",
    "href": "4_7_robustness.html#robustness-tests",
    "title": "07 Robustness",
    "section": "",
    "text": "While the previous section focused on the performance outcomes of the TBTF strategy, this section evaluates the stability of those outcomes under variation in core implementation parameters. We test whether the superior performance persists under changes in portfolio size, rebalancing frequency, sample splits, and weighting schemes.\nRather than relying on a single optimized configuration, the TBTF strategy demonstrates structural robustness across plausible alternatives. This not only reinforces the credibility of the results but also supports the practical adaptability of the approach for different institutional contexts.",
    "crumbs": [
      "교육",
      "Model",
      "07 Robustness"
    ]
  },
  {
    "objectID": "4_7_robustness.html#sensitivity-to-asset-selection-size-n",
    "href": "4_7_robustness.html#sensitivity-to-asset-selection-size-n",
    "title": "07 Robustness",
    "section": "Sensitivity to Asset Selection Size (\\(n\\))",
    "text": "Sensitivity to Asset Selection Size (\\(n\\))\nWe test the strategy for varying values of \\(n\\), the number of top-ranked stocks selected by market capitalization. Specifically, we compare results for:\n\\[\nn \\in \\{5, 10, 20, 30, 50\\}\n\\]\nThis allows us to examine the marginal contribution of expanding the portfolio beyond the top decile. We evaluate how portfolio volatility, Sharpe ratio, and tail risk change with diversification.\n\n\nCode\nplot_sharpe_vs_n()",
    "crumbs": [
      "교육",
      "Model",
      "07 Robustness"
    ]
  },
  {
    "objectID": "4_7_robustness.html#rebalancing-frequency",
    "href": "4_7_robustness.html#rebalancing-frequency",
    "title": "07 Robustness",
    "section": "Rebalancing Frequency",
    "text": "Rebalancing Frequency\nTo test the effect of trading frequency, we implement the strategy with fixed rebalancing intervals:\n\nMonthly (1M)\nQuarterly (3M)\nSemiannual (6M)\nAnnual (12M)\n\nLower-frequency rebalancing is often preferred by institutional investors due to lower transaction costs. The robustness of TBTF performance under less frequent rebalancing reinforces its practicality.\n\n\nCode\nplot_return_by_rebalance_frequency()",
    "crumbs": [
      "교육",
      "Model",
      "07 Robustness"
    ]
  },
  {
    "objectID": "4_7_robustness.html#sample-period-stability",
    "href": "4_7_robustness.html#sample-period-stability",
    "title": "07 Robustness",
    "section": "Sample Period Stability",
    "text": "Sample Period Stability\nTo avoid look-ahead bias and ensure time-consistent performance, we conduct split-sample out-of-sample tests:\n\nPre-2010 setting:\n\nIn-sample: 1996–1999\nOut-of-sample: 2000–2009\n\nPost-2010 setting:\n\nIn-sample: 2010–2013\nOut-of-sample: 2014–2023\n\n\nWe also explore alternative splits using 6-year and 8-year in-sample windows. All evaluations are based on risk-adjusted metrics such as the Sharpe ratio, maximum drawdown, tail risk, and turnover.\n\n\nCode\ngenerate_oos_performance_table()",
    "crumbs": [
      "교육",
      "Model",
      "07 Robustness"
    ]
  },
  {
    "objectID": "4_7_robustness.html#alternative-weighting-schemes",
    "href": "4_7_robustness.html#alternative-weighting-schemes",
    "title": "07 Robustness",
    "section": "Alternative Weighting Schemes",
    "text": "Alternative Weighting Schemes\nTo evaluate the contribution of the proposed convex weighting methods, we compare performance across:\n\nEqual weighting (EQ): naive benchmark\nMarket cap weighting (VW): proportional to firm size\nQuadratic weighting (TBTF): based on in-sample capital share convexity\nOptional: Inverse volatility weighting\n\nThese variations test whether TBTF’s performance is driven primarily by stock selection or by the shape of the weighting function.\n\n\nCode\nplot_weighting_comparison()",
    "crumbs": [
      "교육",
      "Model",
      "07 Robustness"
    ]
  },
  {
    "objectID": "4_7_robustness.html#summary",
    "href": "4_7_robustness.html#summary",
    "title": "07 Robustness",
    "section": "Summary",
    "text": "Summary\nAcross all specification dimensions—portfolio size, rebalancing frequency, sample period, and weighting approach—the TBTF strategy retains a consistent structural edge. The empirical robustness supports our hypothesis that performance is not merely the result of favorable parameter tuning, but instead reflects a persistent asymmetry in capital concentration and market structure.",
    "crumbs": [
      "교육",
      "Model",
      "07 Robustness"
    ]
  },
  {
    "objectID": "5_conclusion.html",
    "href": "5_conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "Conclusion\nThis study provides empirical evidence that U.S. capital markets have diverged significantly from classical equilibrium assumptions, especially in the aftermath of post-2008 monetary interventions and the rise of passive investment vehicles. Return distributions are no longer symmetric or homogeneous. Instead, they increasingly resemble a mixture of structurally persistent, rank-locked capital flows—a departure from the no-arbitrage paradigm central to traditional asset pricing theory.\nAt the heart of this shift is the “Too Big to Fail” (TBTF) strategy. Despite its simplicity—selecting the top 10 firms by market capitalization and applying a convex weighting scheme—it delivers consistently superior performance in the post-2010 period. This outperformance is not driven by better fundamentals, but rather by capital lock-in, ETF-induced inertia, and policy-enabled distortions. Ironically, the more distorted the market becomes, the more effective the TBTF strategy proves to be.\nYet, this success reveals a deeper problem. The TBTF strategy is “sadly optimal”:\n- If it works, it profits from an increasingly inefficient market.\n- If it fails, it may signal the restoration of competitive capital allocation.\nEither outcome reflects a paradox: individual rationality is rewarded within a system that may be collectively irrational or unsustainable.\nMore broadly, this paper contributes to the growing literature on non-ergodic capital markets, rank-based valuation, and the erosion of allocative efficiency under interventionist policy regimes. It suggests that future asset pricing research must move beyond risk-based compensation models and instead incorporate persistent asymmetries, transitional mobility, and structural sources of rent extraction.\nUltimately, if markets are to fulfill their allocative role in a capitalist economy, a rethinking of capital dynamics, index construction, and the role of passive investment is not optional—it is essential.\nRemember: They may be\n\nToo big to serve"
  },
  {
    "objectID": "4_3_ff_compare.html#results",
    "href": "4_3_ff_compare.html#results",
    "title": "03 Benchmarks",
    "section": "",
    "text": "Over the 60-year period, the unconditional average annual Sharpe ratio of b_10 was 0.99, compared to 0.74 for s_10.\nThe time-series of rolling Sharpe ratios reveals that b_10 dominates structurally, not just in isolated windows.\nDuring periods of macroeconomic stress, s_10 portfolios experience sharper volatility spikes, reducing their Sharpe ratios significantly.\n\n\n\nCode\n#@title Time-series of s_10 vs. b_10\n# ![](figs/annual_mean_diff_s10_b10.png)\n# ![](figs/annual_std_diff_s10_b10.png)\n# ![](figs/sharpe_curve_s10_b10.png)\n# ![](figs/sharpe_plane_s10_b10.png)\n\n# moving average\n# pfo_size_rolling = pfo_size.rolling(window=12).mean()\n\n\n#@title without any overlap between the fiscal years\n# Resample to annual frequency and calculate annual standard deviation\n# groups the monthly data into yearly buckets.\n# annual_std = pfo_size.resample('Y').std() * np.sqrt(12)\n\n# Create a fiscal year column\npfo_size['fiscal_year'] = pfo_size.index.year\npfo_size.loc[pfo_size.index.month &gt;= 7, 'fiscal_year'] = pfo_size.loc[pfo_size.index.month &gt;= 7, 'fiscal_year'] + 1\n\n# Group by fiscal year and calculate annual\nannual_mean = pfo_size.groupby('fiscal_year').mean()*12\nannual_std = pfo_size.groupby('fiscal_year').std() * np.sqrt(12)\n# annual_std.dropna(inplace=True)\n\n#@title Time-series of s_10 vs. b_10\ntemp = annual_mean['s_10'] - annual_mean['b_10']\ntemp.plot()\nplt.axhline(y= temp.mean(), color='r', linestyle='-')\nplt.xlabel('fiscal year')\nplt.ylabel('annual mean difference')\nplt.title('Time-series of s_10 minus b_10 in annual mean')\nplt.show()\nprint('The unconditional mean of the difference is '+str(temp.mean().round(0))+ ' percent, annually')\nprint('\\n')\n\ntemp = annual_std['s_10'] - annual_std['b_10']\ntemp.plot()\nplt.axhline(y= temp.mean(), color='r', linestyle='-')\nplt.xlabel('fiscal year')\nplt.ylabel('annual std difference')\nplt.title('Time-series of s_10 minus b_10 in annual volatility')\nplt.show()\nprint('The unconditional mean of the difference is '+str(temp.mean().round(0))+ ' percent, annually')\nprint('\\n')\n\n# annual_mean and annual_std have the same index\ndf = pd.DataFrame(index=annual_mean.index)\ndf['sr_s_10'] = annual_mean['s_10'] / annual_std['s_10']\ndf['sr_b_10'] = annual_mean['b_10'] / annual_std['b_10']\n\ndf['sr_s_10'].plot()\ndf['sr_b_10'].plot()\nplt.axhline(y= df['sr_s_10'].mean(), color='b', linestyle='dashed')\nplt.axhline(y= df['sr_b_10'].mean(), color='r', linestyle='dashed')\nplt.legend()\nplt.xlabel('fiscal year')\nplt.ylabel('Sharpe Ratio')\nplt.title('Time-series Sharpe Ratios of s_10 and b_10')\nplt.show()\nprint('The unconditional mean Annual Sharpe ratio of s_10 is '+str(df['sr_s_10'].mean().round(2)))\nprint('The unconditional mean annual Sharpe ratio of b_10 is '+str(df['sr_b_10'].mean().round(2)))\n\n\n\n\n\n\n\n\n\nThe unconditional mean of the difference is 3.0 percent, annually\n\n\n\n\n\n\n\n\n\n\n\nThe unconditional mean of the difference is 6.0 percent, annually\n\n\n\n\n\n\n\n\n\n\n\nThe unconditional mean Annual Sharpe ratio of s_10 is 0.74\nThe unconditional mean annual Sharpe ratio of b_10 is 0.99\n\n\n\nAnnual excess return difference: s_10 – b_10. The red horizontal line marks the long-run average (~3.0% annually).\nAnnual volatility difference: s_10 – b_10. Large-cap volatility is consistently lower.\nTime-series of 36-month rolling Sharpe ratios for s_10 and b_10. Dashed lines represent the unconditional average for each.\n\nWe also construct a bivariate distribution of annual mean and standard deviation using kernel density estimates:\n\n\nCode\n#@title s_10 vs. b_10 in the volatility-mean coordinate\n\n# Create a list of category names\ncategories = ['s_10', 'b_10']\n\n# Initialize an empty list to store dataframes\ndfs = []\n\n# Iterate through categories and create melted dataframes\nfor category in categories:\n    # Create a temporary dataframe with selected columns and category label\n    temp_mean = annual_mean[[category]].copy()  # Keep the index\n    temp_mean['category'] = category\n    temp_mean.rename(columns={category: 'annual_mean'}, inplace=True)\n\n    temp_std = annual_std[[category]].copy()  # Keep the index\n    temp_std['category'] = category\n    temp_std.rename(columns={category: 'annual_std'}, inplace=True)\n\n    # Merge the temporary dataframes for mean and std, keeping the index\n    temp_df = pd.merge(temp_mean, temp_std, on=['fiscal_year', 'category'])\n\n    # Append the temporary dataframe to the list\n    dfs.append(temp_df)\n\n# Concatenate all the dataframes in the list into a single dataframe, keeping the index\ndf = pd.concat(dfs, ignore_index=False)\n\n# Now create the displot\nsns.displot(\n    data = df,  # Use the reshaped DataFrame\n    x=\"annual_std\",\n    y=\"annual_mean\",\n    hue=\"category\",  # Use the extracted category for hue\n    kind=\"kde\",\n    rug=True,\n)\nplt.xlabel('annual std')\nplt.ylabel('annual mean')\nplt.title('s_10 vs. b_10 in the volatility-mean coordinate')\nplt.show()\n\n\n\n\n\n\n\n\n\nKernel density estimate of annual excess return vs. annual volatility for s_10 and b_10. The separation in volatility–mean space reflects the underlying Sharpe ratio asymmetry.\nThese dynamic plots provide a richer understanding of the persistent performance asymmetry between the smallest and largest decile portfolios—supporting the broader structural thesis underlying TBTF.",
    "crumbs": [
      "교육",
      "Model",
      "03 Benchmarks"
    ]
  }
]