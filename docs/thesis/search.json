[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Too Big to Fail?",
    "section": "",
    "text": "어려운 세상\nKey highlights\n\nMarket Distortions and Asymmetric Returns: This study provides empirical evidence that post-2008 QE policies, ETF-driven passive flows, and market concentration have contributed to more asymmetric stock return distributions, challenging the conventional no-arbitrage principle in asset pricing.\nThe Paradox of the “Too Big to Fail” (TBTF) Portfolio: A simple strategy selecting the top 10 largest market-cap stocks has outperformed traditional benchmarks, not due to superior fundamentals, but as a byproduct of systemic inefficiencies, wealth concentration, and policy-driven capital allocation.\nImplications for Market Efficiency and Competition: The persistent success of the TBTF strategy suggests deepening inefficiencies in financial markets. If the strategy fails, it may indicate a shift toward more competitive and efficient value discovery, highlighting the paradox of profiting from distorted markets while awaiting their correction.\n\n\nSummary\nThis study empirically investigates whether stock return distributions in the U.S. stock market have become more asymmetric, potentially resembling a mixture of heterogeneous assets under an arbitrage-limited structure rather than a mixture of homogeneous assets under the no-arbitrage principle. This perspective challenges mainstream economic theories that rely on efficient market hypothesis (EMH), the no-arbitrage assumption, and global convexity assumption in a consumer preference or production opportunity model.\nThe study further explores distortions in free-market competition within the U.S. financial markets following the Federal Reserve’s post-2008 quantitative easing (QE) policies and the rise of ETFs as dominant investment vehicles. It hypothesizes that these structural shifts have exacerbated mispricing, increased asymmetric wealth concentration, and intensified extreme wealth polarization, ultimately reducing overall market efficiency. By fueling passive capital flows and reinforcing momentum-driven investment behavior, ETFs have amplified the self-reinforcing cycle of market concentration, where a small number of dominant firms continue to absorb disproportionate market share and capital inflows, further entrenching their position.\nThese distortions provide a structural backdrop for the success of the “Too Big to Fail” (TBTF) portfolio strategy, which consists of the 10 largest market-cap stocks from the U.S. stock market (Nasdaq, NYSE, AMEX) with a monthly rebalancing approach and an internally competitive weighting scheme. Empirical findings indicate that the TBTF portfolio has demonstrated superior risk-adjusted performance across multiple measures, including the Sharpe ratio, Sortino ratio, and Omega ratio. The strategy has been particularly effective in the post-2010 period, where its outperformance has been both substantial and persistent. Additionally, the strategy benefits from low turnover, reducing transaction costs and increasing practical feasibility.\nThis study ultimately highlights a paradox: the TBTF strategy remains “sadly optimal,” as it provides easy profits when it succeeds and signals broader market improvements when it does not. If the strategy continues to thrive, it suggests that market inefficiencies are deepening, whereas its failure could indicate a return to more competitive and efficient financial markets. However, the strategy’s success, particularly in the post-2010 period, may not necessarily reflect superior firm fundamentals. Instead, it may be a direct consequence of structural market imbalances driven by central bank interventions, passive capital flows from ETFs, and the network effects of technological monopolization. These dynamics challenge the traditional view that excess returns reflect rational compensation for risk; rather, they suggest that persistent outperformance may stem from artificial distortions in capital allocation and value discovery mechanisms. This raises concerns about the long-term implications of central bank intervention, wealth concentration, and the erosion of competitive market dynamics.",
    "crumbs": [
      "교육",
      "Too Big to Fail?"
    ]
  },
  {
    "objectID": "03_data.html",
    "href": "03_data.html",
    "title": "Data",
    "section": "",
    "text": "Pooled Panel data\nPanel data\nCRSP from WRDS\nprice data from Yahoo Finance\nFama-French data library\nFRED database\n\n\n\n\n\nIn CRSP monthly stock data, dates like “2010-04-01” typically represent the end-of-month data, such as the market price on 2010-04-30, not the start of the month.\nAdjust CRSP dates (e.g., 2020-01-01) to month-end (e.g., 2020-01-31) to match regular (e.g. Yahoo Finance) resampled dates.\ndf[‘date’] = pd.to_datetime(df[‘date’])+ pd.offsets.MonthEnd(0)\n\n\n\n이전 해 12월 말(예: 2000년 12월) 시점에서 NYSE의 median 시가총액을 기준으로 Small과 Big의 경계선이 결정. 실제로 포트폴리오를 구성하는 시점은 그 다음 해 6월 말 (예: 2001년 6월 말).\n\n6개월의 시간 차이를 둔 것은 실무적으로 연말 결산 데이터와 회계자료(book equity 등)의 발표와 확인, 데이터베이스화 과정이 약 4~6개월 정도 소요되기 때문.\nFama-French의 초기 연구(1992, 1993, 1996년 등)에 따르면, 시가총액(market equity) 기준의 기업 순위(percentile rank)는 대체로 상당히 지속적(persistent)이며, 6개월 이내에 급격하게 변동하는 기업은 상대적으로 적었다고 함.\n\n예시:\n\n이전 해 12월 말에 10 percentile 이상이었고 (당시 큰 편에 속했지만),\n그 다음 해 6월 말에 측정된 시가총액이 NYSE median market equity보다 작다면(즉 50 percentile 이하라면),\n실제로 포트폴리오에 편입될 때는 Small 그룹으로 분류됩니다.\n\n\n\n\nThe segmentation of financial time-series data at 2010-01-01 enables a clear distinction between two fundamentally different economic regimes. The pre-2010 period is characterized by financial deregulation, crises, and heightened volatility, while the post-2010 period reflects policy-driven recovery, evolving geopolitical risks, technological transformations, and the popularization of Exchange-Traded Funds (ETFs). This division enhances the robustness of empirical analysis, allowing for a comparative assessment of portfolio performance across distinct macroeconomic environments.\n\n\nThe dataset analyzed in this study spans 1996-01-01 to 2023-12-31 with a monthly frequency. To assess portfolio performance across different economic conditions, the time series is divided at 2010-01-01, creating two equal sub-periods: - 1996-01-01 to 2009-12-31 (14 years) - 2010-01-01 to 2023-12-31 (14 years)\nThis segmentation aligns with major financial and macroeconomic shifts, particularly the aftermath of the 2008-2009 Global Financial Crisis, which ushered in a new era of monetary policy and market behavior, as well as the rise in popularity and widespread adoption of ETFs.\n\n\n\nThe pre-2010 period was marked by high financial volatility, including the Asian Financial Crisis (1997), Dot-com Bubble (2000-2001), 9/11 attacks (2001), and Global Financial Crisis (2008-2009). These events significantly influenced market structures and investment dynamics.\nConversely, the post-2010 period saw a shift towards policy-driven markets, with extensive Quantitative Easing (QE) programs, U.S.-China trade tensions (2018), the COVID-19 pandemic (2020), the rise of AI-driven market trends (2023-present), and the proliferation of ETFs as a mainstream investment vehicle. This transition underscores the fundamental differences between the two periods in terms of risk exposure, liquidity conditions, technological advancements, and investment strategies.\nBy segmenting the data at 2010-01-01, this study enables a structured analysis of how portfolio performance responds to structurally distinct economic environments, enhancing the robustness of financial research.\n\n\n\n\n\nThe following table outlines major macroeconomic and financial events that influenced global markets during each period. The relative importance of each event is indicated in parentheses (e.g., Very High, High).\n\n\n\n\n\n\n\n\n\n\nDate\nEvent & Description\nImportance\n\n\n\n\n1997-07-02\nAsian Financial Crisis begins: The collapse of the Thai baht triggered widespread currency devaluations and IMF interventions.\nVery High\n\n\n1999-01-01\nIntroduction of the Euro: The launch of the euro as a unified currency, reshaping global trade and financial markets.\nHigh\n\n\n1999-11-12\nGramm-Leach-Bliley Act: Repeal of the Glass-Steagall Act, increasing risk-taking in banking, later contributing to the 2008 financial crisis.\nHigh\n\n\n2000-03-10\nDot-com Bubble Peak: The NASDAQ index reaches its highest point before the bubble bursts.\nVery High\n\n\n2000-04 ~ 2001-Q1\nDot-com Crash: Market correction results in widespread collapse of internet-based companies.\nVery High\n\n\n2001-09-11\n9/11 Terrorist Attacks: Immediate shock to financial markets, heightened global uncertainty.\nVery High\n\n\n2008-09-15\nLehman Brothers Collapse: Key event marking the onset of the Global Financial Crisis.\nVery High\n\n\n2008-10-03\nTARP Bailout Program: U.S. government enacts a $700 billion rescue package to stabilize financial institutions.\nVery High\n\n\n2009-Q2\nEnd of the Great Recession: The NBER officially marks the end of the economic downturn.\nHigh\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nEvent & Description\nImportance\n\n\n\n\n2009-Q3 ~ 2010\nPost-crisis economic recovery: The Federal Reserve initiates QE programs, boosting liquidity and asset prices.\nHigh\n\n\n2012-09-13\nQE3 Announcement: The Fed announces an open-ended bond-buying program.\nHigh\n\n\n2013-12-18\nTapering Policy Begins: The Fed begins scaling back QE measures, causing increased market volatility.\nHigh\n\n\n2015-10-28\nEnd of QE3: The Fed concludes its final round of quantitative easing, signaling policy normalization.\nHigh\n\n\n2018-01-01\nU.S.-China Trade War Escalates: Tariff impositions disrupt global trade, affecting equity and commodity markets.\nVery High\n\n\n2020-03-11\nCOVID-19 Pandemic Declared: The stock market crashes, prompting historic monetary and fiscal stimulus measures.\nVery High\n\n\n2023-Present\nRise of AI in Financial Markets: The mainstream adoption of AI technologies, such as ChatGPT, influences market sentiment and productivity outlook.\nHigh\n\n\n2000s-Present\nProliferation of ETFs: ETFs have grown significantly, offering investors diversified, low-cost investment options, transforming investment strategies.\nHigh\n\n\n\nNote: The Proliferation of ETFs spans both periods but has been particularly impactful in the post-2010 period, reflecting their integration into mainstream investment strategies.",
    "crumbs": [
      "교육",
      "Data"
    ]
  },
  {
    "objectID": "03_data.html#appendix-top-10-permno-시트-샘플",
    "href": "03_data.html#appendix-top-10-permno-시트-샘플",
    "title": "Data",
    "section": "",
    "text": "In CRSP monthly stock data, dates like “2010-04-01” typically represent the end-of-month data, such as the market price on 2010-04-30, not the start of the month.\nAdjust CRSP dates (e.g., 2020-01-01) to month-end (e.g., 2020-01-31) to match regular (e.g. Yahoo Finance) resampled dates.\ndf[‘date’] = pd.to_datetime(df[‘date’])+ pd.offsets.MonthEnd(0)\n\n\n\n이전 해 12월 말(예: 2000년 12월) 시점에서 NYSE의 median 시가총액을 기준으로 Small과 Big의 경계선이 결정. 실제로 포트폴리오를 구성하는 시점은 그 다음 해 6월 말 (예: 2001년 6월 말).\n\n6개월의 시간 차이를 둔 것은 실무적으로 연말 결산 데이터와 회계자료(book equity 등)의 발표와 확인, 데이터베이스화 과정이 약 4~6개월 정도 소요되기 때문.\nFama-French의 초기 연구(1992, 1993, 1996년 등)에 따르면, 시가총액(market equity) 기준의 기업 순위(percentile rank)는 대체로 상당히 지속적(persistent)이며, 6개월 이내에 급격하게 변동하는 기업은 상대적으로 적었다고 함.\n\n예시:\n\n이전 해 12월 말에 10 percentile 이상이었고 (당시 큰 편에 속했지만),\n그 다음 해 6월 말에 측정된 시가총액이 NYSE median market equity보다 작다면(즉 50 percentile 이하라면),\n실제로 포트폴리오에 편입될 때는 Small 그룹으로 분류됩니다.\n\n\n\n\nThe segmentation of financial time-series data at 2010-01-01 enables a clear distinction between two fundamentally different economic regimes. The pre-2010 period is characterized by financial deregulation, crises, and heightened volatility, while the post-2010 period reflects policy-driven recovery, evolving geopolitical risks, technological transformations, and the popularization of Exchange-Traded Funds (ETFs). This division enhances the robustness of empirical analysis, allowing for a comparative assessment of portfolio performance across distinct macroeconomic environments.\n\n\nThe dataset analyzed in this study spans 1996-01-01 to 2023-12-31 with a monthly frequency. To assess portfolio performance across different economic conditions, the time series is divided at 2010-01-01, creating two equal sub-periods: - 1996-01-01 to 2009-12-31 (14 years) - 2010-01-01 to 2023-12-31 (14 years)\nThis segmentation aligns with major financial and macroeconomic shifts, particularly the aftermath of the 2008-2009 Global Financial Crisis, which ushered in a new era of monetary policy and market behavior, as well as the rise in popularity and widespread adoption of ETFs.\n\n\n\nThe pre-2010 period was marked by high financial volatility, including the Asian Financial Crisis (1997), Dot-com Bubble (2000-2001), 9/11 attacks (2001), and Global Financial Crisis (2008-2009). These events significantly influenced market structures and investment dynamics.\nConversely, the post-2010 period saw a shift towards policy-driven markets, with extensive Quantitative Easing (QE) programs, U.S.-China trade tensions (2018), the COVID-19 pandemic (2020), the rise of AI-driven market trends (2023-present), and the proliferation of ETFs as a mainstream investment vehicle. This transition underscores the fundamental differences between the two periods in terms of risk exposure, liquidity conditions, technological advancements, and investment strategies.\nBy segmenting the data at 2010-01-01, this study enables a structured analysis of how portfolio performance responds to structurally distinct economic environments, enhancing the robustness of financial research.",
    "crumbs": [
      "교육",
      "Data"
    ]
  },
  {
    "objectID": "03_data.html#appendix-key-events-in-each-period",
    "href": "03_data.html#appendix-key-events-in-each-period",
    "title": "Data",
    "section": "",
    "text": "The following table outlines major macroeconomic and financial events that influenced global markets during each period. The relative importance of each event is indicated in parentheses (e.g., Very High, High).\n\n\n\n\n\n\n\n\n\n\nDate\nEvent & Description\nImportance\n\n\n\n\n1997-07-02\nAsian Financial Crisis begins: The collapse of the Thai baht triggered widespread currency devaluations and IMF interventions.\nVery High\n\n\n1999-01-01\nIntroduction of the Euro: The launch of the euro as a unified currency, reshaping global trade and financial markets.\nHigh\n\n\n1999-11-12\nGramm-Leach-Bliley Act: Repeal of the Glass-Steagall Act, increasing risk-taking in banking, later contributing to the 2008 financial crisis.\nHigh\n\n\n2000-03-10\nDot-com Bubble Peak: The NASDAQ index reaches its highest point before the bubble bursts.\nVery High\n\n\n2000-04 ~ 2001-Q1\nDot-com Crash: Market correction results in widespread collapse of internet-based companies.\nVery High\n\n\n2001-09-11\n9/11 Terrorist Attacks: Immediate shock to financial markets, heightened global uncertainty.\nVery High\n\n\n2008-09-15\nLehman Brothers Collapse: Key event marking the onset of the Global Financial Crisis.\nVery High\n\n\n2008-10-03\nTARP Bailout Program: U.S. government enacts a $700 billion rescue package to stabilize financial institutions.\nVery High\n\n\n2009-Q2\nEnd of the Great Recession: The NBER officially marks the end of the economic downturn.\nHigh\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nEvent & Description\nImportance\n\n\n\n\n2009-Q3 ~ 2010\nPost-crisis economic recovery: The Federal Reserve initiates QE programs, boosting liquidity and asset prices.\nHigh\n\n\n2012-09-13\nQE3 Announcement: The Fed announces an open-ended bond-buying program.\nHigh\n\n\n2013-12-18\nTapering Policy Begins: The Fed begins scaling back QE measures, causing increased market volatility.\nHigh\n\n\n2015-10-28\nEnd of QE3: The Fed concludes its final round of quantitative easing, signaling policy normalization.\nHigh\n\n\n2018-01-01\nU.S.-China Trade War Escalates: Tariff impositions disrupt global trade, affecting equity and commodity markets.\nVery High\n\n\n2020-03-11\nCOVID-19 Pandemic Declared: The stock market crashes, prompting historic monetary and fiscal stimulus measures.\nVery High\n\n\n2023-Present\nRise of AI in Financial Markets: The mainstream adoption of AI technologies, such as ChatGPT, influences market sentiment and productivity outlook.\nHigh\n\n\n2000s-Present\nProliferation of ETFs: ETFs have grown significantly, offering investors diversified, low-cost investment options, transforming investment strategies.\nHigh\n\n\n\nNote: The Proliferation of ETFs spans both periods but has been particularly impactful in the post-2010 period, reflecting their integration into mainstream investment strategies.",
    "crumbs": [
      "교육",
      "Data"
    ]
  },
  {
    "objectID": "01_intro.html",
    "href": "01_intro.html",
    "title": "Introduction",
    "section": "",
    "text": "현실: CAPM과 MPT는 대표성과 대체성, 유동성을 전제했으나 현실은 다름 - 시장이 systematic risk pricing → structural rent allocation으로 변화 - 시장은 위험 기반 수익률이 아니라 구조적 rent에 대해 보상 중 미래 연구: 구조적 비효율성을 반영한 새로운 자산 가격 모형 설계\n\nStylized Facts (실증 패턴 요약)\nMixture Structure of Market Returns, Return Distribution Decomposition\n\ntime-varying mixture of returns\nPercentile Transition Marix and Stationary Structure\n\nMobility Inequality\nEntropy / Gini index on stationary distribution\nQuantile persistence score, upward/downward mobility imbalance\n\n\nFrom Risk-Based to Rent-Based Pricing\n\nWelfare Implications and Capital Allocation\n\nThe Sadly Optimal TBTF Strategy\n\nTBTF 포트폴리오: 상위 10개 시총 종목 동일 비중 또는 내부 경쟁 가중치\n수익률, 변동성, 샤프비율, Sortino, Omega 비교\n“Win if persists, optimal if fails” – 사회 전체로는 전략의 붕괴가 바람직하나 개인에겐 최적\n\n\nAppendix - Key Events in Each Period - Market concentration - Regular Big vs. Small - Top 10 membership and their average rank, Membership Transition - Stationary distribution within the membership\npercentile 기준. 순위 기반 (order-statistics 기반) - capital concentration on extreme (not middle) - level concentration: - mobility lock-in: 계층 이동성 감소. upward mobility가 단절 (구조적 원인: education, labor market, inheritance 등) - capital polarization: 계층 간 격차 확대 = + divergence - level polarization: 자본주의에서 자본량의 증가비율이 현재 자본량에 비례한다면, 자본량은 기하급수적으로 증가하는 게 당연. (geometrically series, y’=y given y(0)) - share polarization: 총자본량의 증가비율이 현재 총자본량에 비례하여 증가한다고 가정. 각 계층별 capital share가 일정하지 않고 한 곳으로 몰린다면, 몰린 계층의 자본본\nWithin a Market\nAcross Markets",
    "crumbs": [
      "교육",
      "Introduction"
    ]
  },
  {
    "objectID": "02_lit_review.html",
    "href": "02_lit_review.html",
    "title": "Literature Review",
    "section": "",
    "text": "Linear Models for Asset Management - Dual Approaches to Asset Management\n- Limitations of Linear asset pricing Models\n- Practical Asset Management\nTesting Linear Models - Data and Descriptive Statistics\n- Time-Series Split: 2010-01-01: Rationale for choosing this date as a structural break (QE onset, ETF growth).\n- Data Sources: CRSP (stocks), Fama-French (industry classifications), Yahoo Finance (indices/ETFs).\n- Appendix: Timeline of key events (e.g., QE phases, ETF milestones). - Industry Market-Cap Concentration Trends: Time-series analysis using SIC (10) and FF (10, 49) classifications.\n- Industry Beta Dynamics: Evolution of systematic risk by industry (CRSP data).\n- High-Tech Sector Dominance: Focus on chips and software industries (FF 10 classification).\n- Challenging Industry CAPM\n- Single-Factor Model: Fama-MacBeth results show significant unexplained alpha (CRSP, SIC 10).\n- Two-Factor Model: Market premium insignificant, reverse size effect significant (FF 49).\n- Challenging Linear Factor Models\n- Factor Selection: Highlight multicollinearity (mkt_excess, macro_bm) and limited explanatory power.\n- Lasso Regression: Apply regularization to select predictors for FF 10 industry portfolio excess returns.\n- Challenging Betting Against Beta\n- Out-of-Sample Performance: Optimal assets have betas near 1, not high or low (SIC 10 portfolios).\n- Challenging the traditional Mean-Variance weight Optimization\n- Extreme Weights: Unconstrained MV yields impractical results; constraints (e.g., no short sales) added.\n- Passive vs. Active: Passive value-weighted portfolios outperform MV with lower costs.\n- Challenging the Parametric Mean-Variance Weight Optimization\n- Bayesian update : Black and Litterman (1992) - Momentum and Size Tilting: Brandt et al. (2009) approach underperforms passive benchmarks.",
    "crumbs": [
      "교육",
      "Literature Review"
    ]
  },
  {
    "objectID": "02_lit_review.html#dual-approaches-to-asset-management",
    "href": "02_lit_review.html#dual-approaches-to-asset-management",
    "title": "Literature Review",
    "section": "Dual Approaches to Asset Management",
    "text": "Dual Approaches to Asset Management\nIn economics, a limited resource allocation involves relative price vectors (\\(P\\)) and quantity vectors (\\(Q\\)), often constrained by a structural inner product:\n\\[P \\cdot Q = \\text{constant}\\]\nWith a convex or concave scalar function of \\(P\\) or \\(Q\\) which are complete and compact Euclidean spaces, this structure enables convex optimization tools. Duality in convex optimization posits that solving for \\(Q\\) given \\(P\\) (primal problem) or \\(P\\) given \\(Q\\) (dual problem) yields equivalent results under linear constraints, reflecting the Hahn-Banach theorem’s guarantee of consistent linear functionals.\n\nAsset Weight Optimization:\n\nGoal: Given \\(P\\) (joint distribution of returns), find \\(Q\\) (asset weights) to optimize an objective (e.g., maximize Sharpe ratio).\nCritique: Modern Portfolio Theory (MPT) assumes elliptical distributions and time-separable utility (e.g., CRRA), which are unrealistic. Non-stationary or fat-tailed distributions (e.g., Cauchy) render moment estimates unreliable, and utility may not reflect diminishing marginal returns for future wealth.\n\nAsset Pricing Optimization:\n\nGoal: Given \\(Q\\) (future cash flows), find \\(P\\) (state prices) to satisfy no-arbitrage conditions via the Euler equation.\nCritique: If risk-return tradeoffs break down (as with mega-caps), linear factor models derived from local approximations fail, undermining theoretical predictions.",
    "crumbs": [
      "교육",
      "Literature Review"
    ]
  },
  {
    "objectID": "02_lit_review.html#limitations-of-linear-asset-pricing-models",
    "href": "02_lit_review.html#limitations-of-linear-asset-pricing-models",
    "title": "Literature Review",
    "section": "Limitations of Linear Asset Pricing Models",
    "text": "Limitations of Linear Asset Pricing Models\nTraditional asset pricing theories operate on the fundamental premise that expected returns directly compensate for systematic risk exposure. However, the empirical reality of top-performing mega-cap stocks presents a profound contradiction: these entities often display disproportionate returns relative to their risk profiles. Consider Apple, which despite having relatively high beta compared to other mega-caps, has generated returns that consistently exceed what would be predicted by its systematic risk exposure alone.\n\nStatistical Limitations\nThe conventional approach to addressing anomalies involves extending traditional asset pricing models by incorporating additional factors. These extensions represent a fundamentally inadequate methodology as they merely perpetuate the linear structure of existing frameworks while appending explanatory variables. The standard multi-factor model takes the form:\n\\[E(R_i) = R_f + \\beta_{i,1}F_1 + \\beta_{i,2}F_2 + ... + \\beta_{i,n}F_n + \\alpha_i\\]\nWhere \\(E(R_i)\\) represents expected returns, \\(R_f\\) is the risk-free rate, \\(\\beta_{i,j}\\) are factor loadings, and \\(F_j\\) are risk factors. The proposed extensions for capturing phenomena such as network effects, winner-take-all dynamics, and regulatory capture simply add terms to this equation.\nThis approach has several critical flaws related to both model specification and parameter estimation.\nLinear additive models inherently assume factor orthogonality, normal distribution of returns, and stable risk premiums—assumptions violated by the complex, non-linear relationships between institutional protection, market power, and returns. These models rely on the core statistical assumptions:\n\n\\(E(\\varepsilon_i) = 0\\) (zero-mean residuals)\n\\(Cov(F_j, \\varepsilon_i) = 0\\) (factors uncorrelated with residuals)\n\\(Cov(F_j, F_k) = 0\\) for \\(j \\neq k\\) (orthogonal factors)\n\\(\\varepsilon_i \\sim N(0, \\sigma^2)\\) (normally distributed residuals)\n\nYet empirical tests would consistently reject these assumptions for mega-cap returns. A particularly problematic issue is multicollinearity between factors. When adding variables such as institutional protection, regulatory capture, and network effects to explain mega-cap performance, these factors often exhibit high correlation amongst themselves. This multicollinearity leads to unstable coefficient estimates, inflated standard errors, and ultimately, unreliable inference. For instance, a firm’s market power is often highly correlated with its regulatory influence, making it difficult to disentangle their individual effects.\nMoreover, adding qualitative variables like “network effects” to statistical models is particularly vulnerable to p-hacking, as these constructs can be operationalized in numerous ways. The improvement in in-sample fitting (\\(R^2\\)) often comes at the expense of out-of-sample prediction accuracy—a fundamental manifestation of the bias-variance tradeoff in mean squared error:\n\\[MSE(prediction) = Bias^2 + Variance + Irreducible\\_Error\\]\nAdding complex factors to capture mega-cap outperformance typically increases model complexity, which may reduce in-sample bias but simultaneously increases estimation variance. This increased variance occurs because more complex models with additional parameters require more data for stable estimation and are more prone to overfitting noise rather than signal. This relationship is particularly problematic for stock returns, which violate ergodicity assumptions necessary for consistent parameter estimation.\nFurthermore, traditional frameworks remain epistemologically constrained by their reliance on backward-looking statistical relationships. Consider the standard time-series approach to estimating factor models:\n\\[R_{it} = \\alpha_i + \\beta_{i,MKT}R_{mt} + \\beta_{i,SMB}SMB_t + \\beta_{i,HML}HML_t + \\varepsilon_{it}\\]\nThis specification assumes parameter stability over time (\\(\\beta_{i,j,t} = \\beta_{i,j,t+1}\\)), an assumption violated when institutional dynamics create structural breaks. Microsoft’s evolving regulatory landscape demonstrates this challenge—its systematic risk profile has fundamentally changed as its market and political power have increased, rendering historical beta estimates increasingly irrelevant for forward-looking predictions.\n\n\nThe Inverse Relationship Between Risk and Return for Dominant Firms\nThe conventional understanding of systematic risk requires fundamental reconceptualization. The Capital Asset Pricing Model posits:\n\\[E(R_i) = R_f + \\beta_i(E(R_m) - R_f)\\]\nWhere \\(\\beta_i = \\frac{Cov(R_i, R_m)}{Var(R_m)} = \\rho_{i,m}\\frac{\\sigma_i}{\\sigma_m}\\) measures systematic risk exposure. This can be decomposed into correlation (\\(\\rho_{i,m}\\)) and relative volatility components (\\(\\frac{\\sigma_i}{\\sigma_m}\\)).\nAn empirical analysis of market data reveals a critical paradox for mega-cap stocks versus non-mega-cap stocks. Both categories typically exhibit similar correlation coefficients with the market (\\(\\rho_{i,m} \\approx 1\\)), indicating strong co-movement with overall market trends. Similarly, their expected returns (\\(E(R_i)\\)) are often comparable or even favor mega-caps. However, the crucial distinction emerges in their volatility profiles: mega-cap stocks consistently demonstrate lower idiosyncratic volatility (\\(\\sigma_i^{mega} &lt; \\sigma_i^{non-mega}\\)) due to their institutional protection, diversified revenue streams, and market power.\nThis creates a fundamental contradiction in the risk-return paradigm. When calculating the Sharpe ratio or other risk-adjusted return metrics:\n\\[Sharpe_i = \\frac{E(R_i) - R_f}{\\sigma_i}\\]\nMega-cap stocks consistently outperform on a risk-adjusted basis. This superior risk-adjusted performance cannot be reconciled with traditional asset pricing theory, which predicts that lower risk should be associated with lower returns. The empirical reality suggests the opposite for market dominant stocks:\n\\[\\frac{E(R_i^{mega}) - R_f}{\\sigma_i^{mega}} &gt; \\frac{E(R_i^{non-mega}) - R_f}{\\sigma_i^{non-mega}}\\]\nDespite comparable raw returns and market correlations, the risk-adjusted outperformance of mega-cap stocks directly contradicts the foundational risk-return trade-off of asset pricing theory. Amazon exemplifies this phenomenon—its volatility has decreased relative to smaller competitors as its market dominance has increased, yet its returns have remained exceptional, generating superior risk-adjusted performance that cannot be explained by traditional models.\n\n\nMarket Efficiency and Structural Advantages\nMarket efficiency assumptions underlying traditional models presuppose:\n\\[E[R_{i,t+1} | \\Omega_t] := E_t[R_{i,t+1}]\\]\nWhere \\(\\Omega_t\\) represents the information set available at time \\(t\\), and \\(E_t[\\cdot]\\) denotes expectation conditional on information at time \\(t\\). However, the persistence of abnormal returns for dominant firms suggests either:\n\nInformation about institutional advantages is not fully incorporated in prices\nThese advantages create structural market inefficiencies that cannot be arbitraged away\n\nGoogle’s digital advertising dominance illustrates these dynamics. Its network effects create increasing returns to scale that can be modeled as:\n\\[Profit_t = f(MarketShare_{t-1})^{\\gamma}\\]\nWhere \\(\\gamma &gt; 1\\) indicates increasing returns to scale. Traditional asset pricing models assume competitive markets where \\(\\gamma \\leq 1\\), making them structurally incapable of capturing the valuation implications of dominant market positions.\nThe fundamental inadequacy of traditional models becomes evident when examining their predictive accuracy for top market-cap stocks. A comparison of realized returns versus CAPM-predicted returns for top 10 stocks from 2010-2020 shows systematic underestimation:\n\\[\\alpha_i:= E(R_i) - [R_f + \\beta_i(R_m - R_f)] &gt; 0 \\text{ for top 10 stocks}\\]\nThis persistent alpha cannot be explained as compensation for omitted risk factors, as these firms typically enjoy reduced risk through institutional protection and market dominance.\nThis theoretical crisis demands not incremental additions to existing models but a comprehensive reconceptualization of capital market dynamics. The failure to accurately predict returns for dominant firms reflects not merely model misspecification but a systematic failure to capture the fundamental nature of contemporary capitalism, where returns increasingly flow not as compensation for risk-bearing but as rents extracted through structural market advantages.\nA more appropriate specification might acknowledge the non-linear relationship between institutional position and returns:\n\\[E(R_i) = R_f + \\beta_i(R_m - R_f) + g(MarketPower_i, InstitutionalProtection_i)\\]\nWhere \\(g(\\cdot)\\) is a non-linear function capturing the complex relationship between market power, institutional protection, and returns—a relationship that fundamentally contradicts the risk-return paradigm underlying traditional asset pricing theory.",
    "crumbs": [
      "교육",
      "Literature Review"
    ]
  },
  {
    "objectID": "02_lit_review.html#practical-asset-management",
    "href": "02_lit_review.html#practical-asset-management",
    "title": "Literature Review",
    "section": "Practical Asset Management",
    "text": "Practical Asset Management\n\nImplications of Model Failure for Asset Management\nAsset management involves making investment decisions to achieve financial objectives, such as maximizing returns or minimizing risk, typically by constructing portfolios based on expected returns and risk estimates derived from asset pricing models. However, the inadequacy of traditional models when applied to dominant firms—those “superstar” companies that capture disproportionate market value—creates significant practical challenges:\n\nBreakdown of Market Efficiency Assumptions\nTraditional models assume markets are efficient, with asset prices reflecting all available information. Yet, dominant firms consistently generate abnormal returns that persist over time, suggesting inefficiencies tied to structural advantages—like brand loyalty or data monopolies—that markets fail to fully price in. For asset managers, this means traditional models underestimate the value of these firms, leading to underweighting in portfolios.\nInverse Risk-Return Dynamics\nModels like the CAPM posit a linear relationship where higher risk (measured as beta) yields higher expected returns. For dominant firms, however, the opposite often holds: their entrenched positions reduce idiosyncratic risk—through diversified revenue streams or regulatory buffers—while delivering superior returns. This inversion misleads asset managers into overestimating the risk of mega-cap stocks and underestimating their return potential, skewing portfolio optimization.\nFactor Instability and Omission\nMulti-factor models, such as those based on Fama-French factors, depend on stable relationships between risk factors (e.g., size, value) and returns. Yet, the market power of dominant firms evolves dynamically—through technological innovation or policy shifts—causing factor loadings to fluctuate. Moreover, these models lack factors that explicitly capture rents from structural advantages, leaving asset managers with incomplete tools to assess true risk-adjusted performance.\nInability to Model Non-Linear Effects\nThe linear structure of traditional models cannot accommodate the non-linear dynamics of contemporary capitalism, such as increasing returns to scale or tipping points in market dominance. For example, a firm like Amazon benefits from self-reinforcing network effects that amplify its returns beyond what risk-based models predict, rendering traditional allocations ineffective.\n\nThese shortcomings result in portfolios that miss out on the concentrated returns of dominant firms, which increasingly drive market performance. Asset managers relying on outdated frameworks risk underperformance in a landscape where structural advantages, not risk exposure, dictate success.\n\n\nA Data-Driven Alternative: Stochastic Dominance\nTo address these challenges, asset management must shift toward empirical, data-driven approaches that eschew the restrictive assumptions of traditional models. One such method is stochastic dominance, which evaluates assets by comparing their entire return distributions rather than relying on summary statistics like mean and variance. This approach offers a practical way to identify investments that align with the empirical realities of contemporary markets.\n\nHow It Works\n\nFirst-Order Stochastic Dominance (FSD): Asset A dominates Asset B if its cumulative distribution function (CDF) lies below B’s for all return levels, meaning A offers higher returns across all outcomes.\n\nSecond-Order Stochastic Dominance (SSD): For risk-averse investors, A dominates B if the area under A’s CDF (its integral) is less than B’s up to any point, indicating a better risk-return tradeoff.\n\nAdvantages\n\nRobustness: Unlike mean-variance optimization, stochastic dominance requires no assumptions about return distributions (e.g., normality), making it suitable for markets with fat tails or skewness—common in mega-cap stock performance.\n\nEmpirical Focus: By analyzing historical return distributions, it captures the real-world impact of structural advantages, such as the persistent outperformance of firms like Apple or Microsoft.\n\nFlexibility: It accommodates diverse investor risk preferences without specifying a utility function, broadening its applicability.\n\nPractical Implementation\nAsset managers can apply stochastic dominance by:\n\nHistorical Simulations: Using past return data to estimate CDFs and calculate the likelihood of one asset dominating another.\n\nResampling: Employing techniques like bootstrapping to test dominance under varying market conditions, enhancing robustness.\n\nChallenges\n\nData Requirements: Accurate CDF estimation demands extensive historical data, which may be scarce for newer firms or asset classes.\n\nNon-Stationarity: Return distributions shift over time—due to regulatory changes or competitive disruptions—limiting the reliability of historical dominance.\n\nPortfolio Complexity: Extending stochastic dominance to multi-asset portfolios is computationally intensive, often requiring simplifications or heuristic rules.\n\n\nDespite these hurdles, stochastic dominance provides a rigorous framework that sidesteps the theoretical pitfalls of traditional models. It allows asset managers to focus on observable performance, directly addressing the rent extraction that defines dominant firms.\n\n\nComplementary Tools\nWhile stochastic dominance serves as a strong foundation, asset managers can bolster their strategies with additional tools to capture the nuances of contemporary markets:\n\nFundamental Analysis\nDetailed assessments of a firm’s financials, competitive moat (e.g., patents, scale), and industry trends can reveal structural advantages missed by quantitative models. For instance, analyzing Tesla’s innovation pipeline or Alphabet’s data ecosystem offers insights into their sustained dominance.\nMachine Learning\nAdvanced algorithms can detect non-linear patterns in returns or predict shifts in market power, though they require careful tuning to avoid overfitting and ensure interpretability.\nLiquidity Considerations\nMega-cap stocks typically offer high liquidity, reducing transaction costs and making them practical choices for large portfolios—a factor stochastic dominance alone might not prioritize.\n\nMoreover, the concentration of returns in a few dominant firms suggests that a focused investment strategy—overweighting current mega-caps or scouting emerging leaders—may outperform broad diversification. However, identifying future dominants demands foresight into technological, regulatory, and economic trends, adding complexity to the process.\n\n\nConclusion\nIn a world where returns stem from structural market advantages rather than risk-bearing, traditional asset pricing models fail to guide effective asset management. Their inability to predict the performance of dominant firms—rooted in flawed assumptions about efficiency, risk, and linearity—leads to misallocated portfolios that undervalue mega-cap opportunities. A data-driven approach using stochastic dominance offers a compelling alternative, leveraging empirical return distributions to identify superior investments without theoretical baggage. While challenges like data demands and non-stationarity persist, combining this method with fundamental analysis and modern tools equips asset managers to navigate contemporary capitalism. By embracing flexibility and empiricism, asset management can better capture the rents that define today’s markets, delivering superior outcomes in an era of concentrated dominance.",
    "crumbs": [
      "교육",
      "Literature Review"
    ]
  },
  {
    "objectID": "04_model.html",
    "href": "04_model.html",
    "title": "Model",
    "section": "",
    "text": "The TBTF Strategy\n\nLarge vs. Small Stocks: Analyze return distribution asymmetry and volatility differences over time.\n\nTop 10 Stocks: List membership (e.g., permno) and stability, dominated by high-tech (Nasdaq chips/software).\n\nTop 10 Transition Analysis: Transition matrix and stationary distribution (three states: top, mid, out).\n\nTBTF Performance: Compare risk-adjusted returns (Sharpe, Sortino, Omega) to market indices/ETFs.\n\n\n패널 데이터 상의 개인별 소득의 로그 (log-income) 가 AR(1) 과정을 따르며, 이 과정이 시간이 지남에 따라 두 개의 상태(regime) 간 전환되는 Markov-switching AR(1) 구조라고 가정하면, 상태 전이 행렬 (transition matrix) 를 포함한\n모형 파라미터를 추정 방법 - Expectation-Maximization (EM) 알고리즘 - Hamilton 필터와 같은 비선형 시계열 필터링 기법\n아래에서는 2-state Markov-switching AR(1) 모형을 panel data 상황에 맞춰 수식화하고, 상태 전이 행렬 추정 절차를 설명합니다.\n\n\n\n\n\n패널 데이터에서 개인 \\(i\\)의 시점 \\(t\\)의 로그 소득 \\(y_{it} = \\log(\\text{income}_{it})\\)가 다음과 같은 구조를 따른다고 가정합니다:\n\\[\ny_{it} = \\mu_{s_{it}} + \\phi_{s_{it}} y_{i,t-1} + \\epsilon_{it}, \\quad \\epsilon_{it} \\sim \\mathcal{N}(0, \\sigma_{s_{it}}^2)\n\\]\n여기서: - \\(s_{it} \\in \\{1, 2\\}\\): 시점 \\(t\\)에서 개인 \\(i\\)의 은닉 상태 (hidden state) (예: “저성장 상태”, “고성장 상태”) - 각 상태는 상태별 AR(1) 계수와 평균, 분산을 가짐\n\n\n\n\n개별 개인마다 상태가 전이되지만, 상태 전이 확률은 전체적으로 공통적이라고 가정:\n\\[\nP =\n\\begin{bmatrix}\np_{11} & p_{12} \\\\\np_{21} & p_{22}\n\\end{bmatrix}, \\quad \\text{where } p_{jk} = \\mathbb{P}(s_{it} = k \\mid s_{i,t-1} = j)\n\\]\n예:\n- \\(p_{11}\\) = 상태 1 (저성장)이 지속될 확률\n- \\(p_{12}\\) = 상태 1에서 2로 전이될 확률\n- …\n\n\n\n\nEM 알고리즘을 적용하여 다음을 추정: - 상태 전이 확률 \\(P\\) - 상태별 AR(1) 파라미터 \\(\\mu_1, \\mu_2, \\phi_1, \\phi_2, \\sigma_1^2, \\sigma_2^2\\) - 각 시점의 상태 posterior 확률 \\(\\mathbb{P}(s_{it} = k \\mid \\text{observed data})\\)\n\n\n\n초기 상태 전이행렬 \\(P^{(0)}\\)\n초기 파라미터 \\((\\mu_k^{(0)}, \\phi_k^{(0)}, \\sigma_k^{(0)})\\) for \\(k=1,2\\)\n\n\n\n\nHamilton filter 또는 Forward-Backward algorithm을 통해 다음을 계산:\n\n\\(\\gamma_{it}^{(k)} = \\mathbb{P}(s_{it} = k \\mid \\text{data})\\): 상태 k에 있을 posterior 확률\n\\(\\xi_{it}^{(j,k)} = \\mathbb{P}(s_{i,t-1} = j, s_{it} = k \\mid \\text{data})\\): 상태 전이 joint posterior\n\n\n\n\n\n\n\\[\np_{jk} = \\frac{\\sum_{i,t} \\xi_{it}^{(j,k)}}{\\sum_{i,t} \\sum_{k'} \\xi_{it}^{(j,k')}}\n\\]\n즉, 상태 \\(j \\to k\\)로의 전이가 얼마나 자주 발생했는지를 전체 joint posterior 기반으로 집계하여 전이 확률로 추정합니다.\n\n\n\n예를 들어 상태 \\(k\\)에 대해서:\n\n\\(\\phi_k, \\mu_k\\): \\(\\gamma_{it}^{(k)}\\)를 가중치로 사용한 weighted regression\n\\(\\sigma_k^2\\): weighted residual variance\n\n\n\n\n\n\n\n\n\n\n\n공통 전이행렬: 모든 개인에 대해 상태 전이 확률은 동일하다고 가정 (\\(P\\)는 공통)\n개인별 상이한 초기 분포: \\(\\mathbb{P}(s_{i1})\\)은 개인별로 추정하거나 공통으로 가정\n데이터가 충분히 길지 않은 경우, 시간 단위(\\(T\\))는 상태 추정을 어렵게 만들 수 있어 개인 간 정보 풀링이 중요\n\n\n\n\n\n예를 들어 다음과 같은 전이행렬이 있다고 가정:\n\\[\nP = \\begin{bmatrix}\n0.95 & 0.05 \\\\\n0.10 & 0.90\n\\end{bmatrix}\n\\]\n이 경우: - 상태 1 (저성장)은 매우 안정적으로 지속됨 (95%) - 상태 2 (고성장)은 상대적으로 덜 안정적이며, 10% 확률로 저성장으로 전이\n이 전이확률은 위 EM 알고리즘 M-step에서 \\(\\xi_{it}^{(j,k)}\\)를 평균하여 추정됩니다.\n\n\n\n\n패널 데이터를 대상으로 이 모형을 추정하려면 다음과 같은 라이브러리를 활용할 수 있습니다:\n\nstatsmodels.tsa.regime_switching.MarkovRegression (단일 시계열에 한정)\npyhsmm 또는 hmmlearn: HMM 기반 EM 추정\nlinearmodels.panel + custom EM 알고리즘: 패널 구조에 맞춘 추정",
    "crumbs": [
      "교육",
      "Model"
    ]
  },
  {
    "objectID": "04_model.html#모형-구조-개인-i의-로그-소득",
    "href": "04_model.html#모형-구조-개인-i의-로그-소득",
    "title": "Model",
    "section": "",
    "text": "패널 데이터에서 개인 \\(i\\)의 시점 \\(t\\)의 로그 소득 \\(y_{it} = \\log(\\text{income}_{it})\\)가 다음과 같은 구조를 따른다고 가정합니다:\n\\[\ny_{it} = \\mu_{s_{it}} + \\phi_{s_{it}} y_{i,t-1} + \\epsilon_{it}, \\quad \\epsilon_{it} \\sim \\mathcal{N}(0, \\sigma_{s_{it}}^2)\n\\]\n여기서: - \\(s_{it} \\in \\{1, 2\\}\\): 시점 \\(t\\)에서 개인 \\(i\\)의 은닉 상태 (hidden state) (예: “저성장 상태”, “고성장 상태”) - 각 상태는 상태별 AR(1) 계수와 평균, 분산을 가짐",
    "crumbs": [
      "교육",
      "Model"
    ]
  },
  {
    "objectID": "04_model.html#상태-전이-행렬-공통-가정",
    "href": "04_model.html#상태-전이-행렬-공통-가정",
    "title": "Model",
    "section": "",
    "text": "개별 개인마다 상태가 전이되지만, 상태 전이 확률은 전체적으로 공통적이라고 가정:\n\\[\nP =\n\\begin{bmatrix}\np_{11} & p_{12} \\\\\np_{21} & p_{22}\n\\end{bmatrix}, \\quad \\text{where } p_{jk} = \\mathbb{P}(s_{it} = k \\mid s_{i,t-1} = j)\n\\]\n예:\n- \\(p_{11}\\) = 상태 1 (저성장)이 지속될 확률\n- \\(p_{12}\\) = 상태 1에서 2로 전이될 확률\n- …",
    "crumbs": [
      "교육",
      "Model"
    ]
  },
  {
    "objectID": "04_model.html#추정-방법-em-알고리즘-baumwelch-계열",
    "href": "04_model.html#추정-방법-em-알고리즘-baumwelch-계열",
    "title": "Model",
    "section": "",
    "text": "EM 알고리즘을 적용하여 다음을 추정: - 상태 전이 확률 \\(P\\) - 상태별 AR(1) 파라미터 \\(\\mu_1, \\mu_2, \\phi_1, \\phi_2, \\sigma_1^2, \\sigma_2^2\\) - 각 시점의 상태 posterior 확률 \\(\\mathbb{P}(s_{it} = k \\mid \\text{observed data})\\)\n\n\n\n초기 상태 전이행렬 \\(P^{(0)}\\)\n초기 파라미터 \\((\\mu_k^{(0)}, \\phi_k^{(0)}, \\sigma_k^{(0)})\\) for \\(k=1,2\\)\n\n\n\n\nHamilton filter 또는 Forward-Backward algorithm을 통해 다음을 계산:\n\n\\(\\gamma_{it}^{(k)} = \\mathbb{P}(s_{it} = k \\mid \\text{data})\\): 상태 k에 있을 posterior 확률\n\\(\\xi_{it}^{(j,k)} = \\mathbb{P}(s_{i,t-1} = j, s_{it} = k \\mid \\text{data})\\): 상태 전이 joint posterior\n\n\n\n\n\n\n\\[\np_{jk} = \\frac{\\sum_{i,t} \\xi_{it}^{(j,k)}}{\\sum_{i,t} \\sum_{k'} \\xi_{it}^{(j,k')}}\n\\]\n즉, 상태 \\(j \\to k\\)로의 전이가 얼마나 자주 발생했는지를 전체 joint posterior 기반으로 집계하여 전이 확률로 추정합니다.\n\n\n\n예를 들어 상태 \\(k\\)에 대해서:\n\n\\(\\phi_k, \\mu_k\\): \\(\\gamma_{it}^{(k)}\\)를 가중치로 사용한 weighted regression\n\\(\\sigma_k^2\\): weighted residual variance",
    "crumbs": [
      "교육",
      "Model"
    ]
  },
  {
    "objectID": "04_model.html#실용적인-가정-및-제약",
    "href": "04_model.html#실용적인-가정-및-제약",
    "title": "Model",
    "section": "",
    "text": "공통 전이행렬: 모든 개인에 대해 상태 전이 확률은 동일하다고 가정 (\\(P\\)는 공통)\n개인별 상이한 초기 분포: \\(\\mathbb{P}(s_{i1})\\)은 개인별로 추정하거나 공통으로 가정\n데이터가 충분히 길지 않은 경우, 시간 단위(\\(T\\))는 상태 추정을 어렵게 만들 수 있어 개인 간 정보 풀링이 중요",
    "crumbs": [
      "교육",
      "Model"
    ]
  },
  {
    "objectID": "04_model.html#간단한-예시",
    "href": "04_model.html#간단한-예시",
    "title": "Model",
    "section": "",
    "text": "예를 들어 다음과 같은 전이행렬이 있다고 가정:\n\\[\nP = \\begin{bmatrix}\n0.95 & 0.05 \\\\\n0.10 & 0.90\n\\end{bmatrix}\n\\]\n이 경우: - 상태 1 (저성장)은 매우 안정적으로 지속됨 (95%) - 상태 2 (고성장)은 상대적으로 덜 안정적이며, 10% 확률로 저성장으로 전이\n이 전이확률은 위 EM 알고리즘 M-step에서 \\(\\xi_{it}^{(j,k)}\\)를 평균하여 추정됩니다.",
    "crumbs": [
      "교육",
      "Model"
    ]
  },
  {
    "objectID": "04_model.html#python-추정-예제-제공-가능",
    "href": "04_model.html#python-추정-예제-제공-가능",
    "title": "Model",
    "section": "",
    "text": "패널 데이터를 대상으로 이 모형을 추정하려면 다음과 같은 라이브러리를 활용할 수 있습니다:\n\nstatsmodels.tsa.regime_switching.MarkovRegression (단일 시계열에 한정)\npyhsmm 또는 hmmlearn: HMM 기반 EM 추정\nlinearmodels.panel + custom EM 알고리즘: 패널 구조에 맞춘 추정",
    "crumbs": [
      "교육",
      "Model"
    ]
  }
]