[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Too Big to Fail?",
    "section": "",
    "text": "Too big to fail? More like too big to be good, too big for justice, and too big to not exploit.\n\n\nStructural Distortion and Distributional Asymmetry\nPost-crisis liquidity regimes and passive flows have created persistent advantages for dominant firms. The result is a market characterized by asymmetric return distributions, rank inertia, and the erosion of marginal risk pricing.\nThe TBTF Strategy as a Structural Probe\nA simple long-only portfolio of the top 10 firms, weighted convexly, consistently outperforms across risk-adjusted metrics. Its success functions not as evidence of market efficiency, but as a diagnostic of structural misallocation.\nParadox of Optimality in a Broken System\nTBTF is optimal precisely when markets are not. If it performs, it exposes deep inefficiencies; if it fails, it may herald a return to allocative justice. Either way, the investor wins, while society bears the risk of institutionalized stagnation.\nImplications for Theory and Policy\nThe findings call for asset pricing models that move beyond risk–return tradeoffs and incorporate rank-based valuation, non-ergodic dynamics, and mobility constraints. Policymakers must confront the unintended consequences of index-driven capital allocation.",
    "crumbs": [
      "Apps",
      "Too Big to Fail?"
    ]
  },
  {
    "objectID": "index.html#key-highlights",
    "href": "index.html#key-highlights",
    "title": "Too Big to Fail?",
    "section": "",
    "text": "Too big to fail? More like too big to be good, too big for justice, and too big to not exploit.\n\n\nStructural Distortion and Distributional Asymmetry\nPost-crisis liquidity regimes and passive flows have created persistent advantages for dominant firms. The result is a market characterized by asymmetric return distributions, rank inertia, and the erosion of marginal risk pricing.\nThe TBTF Strategy as a Structural Probe\nA simple long-only portfolio of the top 10 firms, weighted convexly, consistently outperforms across risk-adjusted metrics. Its success functions not as evidence of market efficiency, but as a diagnostic of structural misallocation.\nParadox of Optimality in a Broken System\nTBTF is optimal precisely when markets are not. If it performs, it exposes deep inefficiencies; if it fails, it may herald a return to allocative justice. Either way, the investor wins, while society bears the risk of institutionalized stagnation.\nImplications for Theory and Policy\nThe findings call for asset pricing models that move beyond risk–return tradeoffs and incorporate rank-based valuation, non-ergodic dynamics, and mobility constraints. Policymakers must confront the unintended consequences of index-driven capital allocation.",
    "crumbs": [
      "Apps",
      "Too Big to Fail?"
    ]
  },
  {
    "objectID": "4_8_implementation.html",
    "href": "4_8_implementation.html",
    "title": "08 Implementation",
    "section": "",
    "text": "This section summarizes the technical implementation details of the TBTF strategy, including programming environment, file structure, and reproducibility instructions. The entire empirical workflow is based on Quarto and organized into modular .qmd documents with embedded Python code blocks. This ensures seamless rendering of figures and tables alongside text, and supports reproducible research.",
    "crumbs": [
      "Apps",
      "Model",
      "08 Implementation"
    ]
  },
  {
    "objectID": "4_8_implementation.html#implementation-notes",
    "href": "4_8_implementation.html#implementation-notes",
    "title": "08 Implementation",
    "section": "",
    "text": "This section summarizes the technical implementation details of the TBTF strategy, including programming environment, file structure, and reproducibility instructions. The entire empirical workflow is based on Quarto and organized into modular .qmd documents with embedded Python code blocks. This ensures seamless rendering of figures and tables alongside text, and supports reproducible research.",
    "crumbs": [
      "Apps",
      "Model",
      "08 Implementation"
    ]
  },
  {
    "objectID": "4_8_implementation.html#programming-environment",
    "href": "4_8_implementation.html#programming-environment",
    "title": "08 Implementation",
    "section": "2 Programming Environment",
    "text": "2 Programming Environment\nAll analysis was conducted using Python 3.x. Core packages used include:\n\nData Processing: pandas, numpy\nStatistical Modeling: statsmodels, scipy, sklearn\nVisualization: matplotlib, seaborn, plotly\nRendering: quarto, jupyter, ipykernel\n\nAdditional utilities like joblib and multiprocessing were used for robustness tests and performance acceleration.",
    "crumbs": [
      "Apps",
      "Model",
      "08 Implementation"
    ]
  },
  {
    "objectID": "4_8_implementation.html#document-structure",
    "href": "4_8_implementation.html#document-structure",
    "title": "08 Implementation",
    "section": "3 Document Structure",
    "text": "3 Document Structure\nThe analysis is broken into modular .qmd files, each corresponding to a specific stage of the empirical framework. These files are rendered either as part of the full manuscript or as standalone reports.\n\n\n\n\n\n\n\n\nQMD File\nDescription\nRelated Section\n\n\n\n\n02_data.qmd\nLoad and clean CRSP & FF data, construct derived variables\nSection 2 – Data\n\n\n03_ff_compare.qmd\nFF size decile analysis, Sharpe dynamics comparison\nSection 3 – FF Benchmark\n\n\n04_structure.qmd\nCapital convexity, mixture modeling, Markov transition structure\nSection 4 – Structure\n\n\n05_strategy.qmd\nTBTF logic: What–How–When framework, weight estimation\nSection 5 – Strategy\n\n\n06_performance.qmd\nPerformance comparison: distribution, price level, metrics\nSection 6 – Performance\n\n\n07_robustness.qmd\nRobustness checks: size, frequency, weights, sample splits\nSection 7 – Robustness\n\n\n\nAll figures and tables are embedded directly within these .qmd documents using Quarto-native fig-cap and tbl-cap options.",
    "crumbs": [
      "Apps",
      "Model",
      "08 Implementation"
    ]
  },
  {
    "objectID": "4_8_implementation.html#output-artifacts",
    "href": "4_8_implementation.html#output-artifacts",
    "title": "08 Implementation",
    "section": "4 Output Artifacts",
    "text": "4 Output Artifacts\nThe results are organized in structured folders and linked automatically to each section:\n\nfigs/: Figures (e.g., PNG/HTML via matplotlib or plotly)\ntables/: Summary tables and evaluation metrics (CSV or HTML)\n\nEach artifact is generated at render time and versioned with appropriate suffixes. Example:\n\nsharpe_curve_s10_b10.png, return_dist_tbtf_post2010.png\nperformance_summary_pre2010.csv, weight_table_exp_fit.html",
    "crumbs": [
      "Apps",
      "Model",
      "08 Implementation"
    ]
  },
  {
    "objectID": "4_8_implementation.html#reproducibility",
    "href": "4_8_implementation.html#reproducibility",
    "title": "08 Implementation",
    "section": "5 Reproducibility",
    "text": "5 Reproducibility\nTo reproduce the full pipeline:\n\nClone the project repository and install dependencies:\n\npip install -r requirements.txt\n\nDownload CRSP/Compustat data via WRDS (institutional access required)\nEdit configuration or .qmd arguments if needed\nRender the full manuscript or individual parts:\n\nquarto render\nOptional automation scripts using make, snakemake, or papermill are available for batch processing.",
    "crumbs": [
      "Apps",
      "Model",
      "08 Implementation"
    ]
  },
  {
    "objectID": "4_8_implementation.html#appendix",
    "href": "4_8_implementation.html#appendix",
    "title": "08 Implementation",
    "section": "6 Appendix",
    "text": "6 Appendix\n\n\n6.1 requirements.txt\npandas&gt;=1.5\nnumpy&gt;=1.22\nscikit-learn&gt;=1.3\nmatplotlib&gt;=3.6\nseaborn&gt;=0.12\nstatsmodels&gt;=0.13\nscipy&gt;=1.10\nplotly&gt;=5.10\nquarto&gt;=1.3",
    "crumbs": [
      "Apps",
      "Model",
      "08 Implementation"
    ]
  },
  {
    "objectID": "4_6_performance.html",
    "href": "4_6_performance.html",
    "title": "06 Performance",
    "section": "",
    "text": "Code\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport pandas as pd\nimport sqlite3\ncons = sqlite3.connect(database=\"../../tbtf.sqlite\")\n\ncrsp = pd.read_sql_query(\n  sql=\"SELECT * FROM crsp\",\n  con=cons,\n  parse_dates={\"date\"}\n)",
    "crumbs": [
      "Apps",
      "Model",
      "06 Performance"
    ]
  },
  {
    "objectID": "4_6_performance.html#out-of-sample-evaluation-framework",
    "href": "4_6_performance.html#out-of-sample-evaluation-framework",
    "title": "06 Performance",
    "section": "1 Out-of-Sample Evaluation Framework",
    "text": "1 Out-of-Sample Evaluation Framework\nThe TBTF strategy relies on a structurally motivated weighting scheme that reflects not only asset-specific characteristics but also investor preferences toward risk concentration and downside protection. Two key parameters govern this framework:\n\nRisk aversion parameter (\\(\\eta\\)):\nWe employ a CRRA-type (Constant Relative Risk Aversion) transformation in the exponential weighting function, where asset weights are proportional to\n\\[\n  w_i \\propto \\exp\\left(\\eta \\cdot \\hat{r}_i\\right).\n  \\]\nHere, \\(\\hat{r}_i\\) denotes the estimated expected return for asset \\(i\\) based on its in-sample historical performance, typically computed as the average excess return over the past 36 months. This formulation captures the intuition that investors tend to allocate more capital to assets with higher perceived profitability, especially under risk-averse preferences.\nThe parameter \\(\\eta &gt; 0\\) reflects the degree of risk aversion or, equivalently, the strength of capital concentration toward assets with higher expected returns. A higher \\(\\eta\\) induces more aggressive overweighting of top-ranked stocks, thereby mimicking the structural capital lock-in observed in real-world markets. In this study, we set \\(\\eta = 3\\), which strikes a balance between diversification and concentration, and is broadly consistent with utility curvature in standard asset pricing and consumption–savings models.\nTail probability threshold (\\(p\\)) for Omega ratio:\nTo quantify downside risk, we use the Omega ratio, which captures the full shape of the return distribution:\n\\[\n\\Omega(p) = \\frac{\\int_p^\\infty (1 - F(r)) \\, dr}{\\int_{-\\infty}^p F(r) \\, dr}.\n\\]\nThe threshold \\(p\\) defines the return level below which outcomes are considered losses. We set \\(p = 0.01\\), corresponding to a 1% monthly return threshold—commonly adopted as the minimum acceptable return (MAR) in institutional risk management. This conservative cutoff captures meaningful left-tail risk while avoiding excessive sensitivity to extreme outliers.\n\nThe strategy is trained on a rolling 36-month in-sample window to estimate optimal exponential weights across the top decile (state = 10) based on lagged market capitalization. Portfolios are rebalanced quarterly (rebalance_freq='3M') to reflect the long-term stickiness of capital among dominant firms, with structural filtering applied via \\(\\eta = 3\\) and \\(p = 0.01\\).\nTo rigorously test the strategy’s robustness and economic significance, we conduct fully out-of-sample evaluations over two distinct macro-financial regimes:\nresults_pre = tbtf.backtest_pipeline(\n    crsp=crsp,\n    in_end='1999-12-31',\n    out_end='2009-12-31',\n    in_sample_months=36,\n    rebalance_freq='3M',\n    weighting_method='exponential',\n    top_n=10,\n    state=10,\n    eta=3,\n    p=0.01\n)\n\nresults_post = tbtf.backtest_pipeline(\n    crsp=crsp,\n    in_end='2013-12-31',\n    out_end='2023-12-31',\n    in_sample_months=36,\n    rebalance_freq='3M',\n    weighting_method='exponential',\n    top_n=10,\n    state=10,\n    eta=3,\n    p=0.01\n)\n\n\nCode\n# Out-of-sample Investment\n# 10 years in pre-2010 from 2000-01\n# 10 years in post-2010 from 2004-01\n\nimport sys\nimport os\n# 현재 경로 기준으로 상위 디렉토리로 경로 추가\nsys.path.append(os.path.abspath('../..'))\n\nimport tbtf\n\nresults_pre = tbtf.backtest_pipeline(\n    crsp=crsp,\n    in_end='1999-12-31',\n    out_end='2009-12-31',\n    in_sample_months=36,\n    rebalance_freq='3M',\n    weighting_method='exponential',\n    top_n=10,\n    state=10,\n    eta=3,\n    p=0.01\n)\n\nresults_post = tbtf.backtest_pipeline(\n    crsp=crsp,\n    in_end='2013-12-31',\n    out_end='2023-12-31',\n    in_sample_months=36,\n    rebalance_freq='3M',\n    weighting_method='exponential',\n    top_n=10,\n    state=10,\n    eta=3,\n    p=0.01\n)\n\n# TBTF returns in the out-of-sample\ntbtf_returns_pre2010 = results_pre['returns'].set_index('date')['portfolio_return']\ntbtf_returns_post2010 = results_post['returns'].set_index('date')['portfolio_return']\n\n\n[START] backtest_pipeline 호출됨\n[INFO] Total sample: 1996-12-31 00:00:00 to 2009-12-31 00:00:00\n[INFO] Total Out-of-sample: 2000-01-31 00:00:00 to 2009-12-31 00:00:00\n[DONE] Total returns recorded: 39\n[START] backtest_pipeline 호출됨\n[INFO] Total sample: 2010-12-31 00:00:00 to 2023-12-31 00:00:00\n[INFO] Total Out-of-sample: 2014-01-31 00:00:00 to 2023-12-31 00:00:00\n[DONE] Total returns recorded: 39\n\n\n\n\nCode\nff3 = pd.read_sql_query(\n  sql=\"SELECT * FROM ff3\",\n  con=cons,\n  parse_dates={\"date\"}\n)\n# 날짜를 datetime 형식으로 변환\nff3['date'] = pd.to_datetime(ff3['date'])\n# 인덱스 지정\nff3.set_index('date', inplace=True)\n\n# Pre-2010: 2000–2009\nmarket_returns_pre2010 = ff3.loc['2000-01-31':'2009-12-31', 'mkt_ret']\n# Post-2010: 2014–2023\nmarket_returns_post2010 = ff3.loc['2014-01-31':'2023-12-31', 'mkt_ret']",
    "crumbs": [
      "Apps",
      "Model",
      "06 Performance"
    ]
  },
  {
    "objectID": "4_6_performance.html#comparative-distributional-analysis-pre-2010-vs.-post-2010",
    "href": "4_6_performance.html#comparative-distributional-analysis-pre-2010-vs.-post-2010",
    "title": "06 Performance",
    "section": "2 Comparative Distributional Analysis: Pre-2010 vs. Post-2010",
    "text": "2 Comparative Distributional Analysis: Pre-2010 vs. Post-2010\nTo assess the structural evolution of market efficiency and capital concentration, we compare the TBTF portfolio’s out-of-sample return distributions against the Fama-French market portfolio return, denoted as mkt_ret. This return series is sourced directly from the Fama-French data library and corresponds to the aggregate U.S. stock market return including dividends.\nThe comparison is conducted across two distinct macro-financial regimes:\n\nPre-2010 period: 2000–2009 (trained on data up to 1999)\nPost-2010 period: 2014–2023 (trained on data up to 2013)\n\n\nCode\ntbtf.plot_return_distribution(tbtf_returns_pre2010, market_returns_pre2010, period_label='pre-2010')\ntbtf.plot_return_distribution(tbtf_returns_post2010, market_returns_post2010, period_label='post-2010')\n\n\n\n\n\n\n\nMonthly Return Distribution of TBTF vs. Market (Pre-2010 vs. Post-2010)\n\n\n\n\n\n\n\n\nDistributional Shifts from this comparative analysis are as follows:\n\nIn the pre-2010 window, the TBTF and market portfolios exhibit similar interquartile ranges (IQR), indicating comparable volatility levels. However, the mean return is higher for the market, suggesting that TBTF underperformed not due to risk but due to return inefficiency. TBTF’s distribution is relatively symmetric, while the market shows mild left-skewness. Notably, TBTF exhibits high kurtosis, indicating heavier tails and more frequent extreme outcomes, both positive and negative.\nIn the post-2010 period, the relationship reverses. While the market portfolio’s standard deviation declines, TBTF not only achieves a higher mean return, but also exhibits superior interquartile positioning—especially in the median and upper quartile. The lower quartile remains similar across both, indicating equivalent downside frequency, but TBTF captures the upside more effectively. In terms of shape, the market distribution becomes more symmetric, whereas TBTF turns left-skewed, reflecting an asymmetric probability mass concentrated just above the loss threshold—consistent with convex return dynamics and infrequent but large outperformance events.\nIn summary, the roles of the two distributions are structurally inverted across regimes. Pre-2010, TBTF resembled a symmetrical bell-shaped curve with fat tails, while the market was mildly skewed. Post-2010, the market distribution becomes tighter and more symmetric, whereas TBTF becomes more asymmetric and wider, capturing structural convexity in return outcomes. This shift aligns with a regime transition in capital flows and market microstructure following the QE era.\n\n\nCode\ntbtf.plot_price_level(tbtf_returns_pre2010, market_returns_pre2010, start_date='2000-01-01', end_date='2009-12-31')\ntbtf.plot_price_level(tbtf_returns_post2010, market_returns_post2010, start_date='2004-01-01', end_date='2023-12-31')\n\n\n\n\n\n\n\nPrice Level Comparison: TBTF vs. Market (Pre-2010 vs. Post-2010)\n\n\n\n\n\n\n\n\n\n\nCode\npre_index = pd.read_sql_query(\n  sql=\"SELECT * FROM pre_index\",\n  con=cons,\n  parse_dates={\"date\"}\n)\n\npost_index = pd.read_sql_query(\n  sql=\"SELECT * FROM post_index\",\n  con=cons,\n  parse_dates={\"date\"}\n)\n\npost_ff = pd.read_sql_query(\n  sql=\"SELECT * FROM post_ff\",\n  con=cons,\n  parse_dates={\"date\"}\n)",
    "crumbs": [
      "Apps",
      "Model",
      "06 Performance"
    ]
  },
  {
    "objectID": "4_6_performance.html#other-benchmark-comparison",
    "href": "4_6_performance.html#other-benchmark-comparison",
    "title": "06 Performance",
    "section": "3 Other Benchmark Comparison",
    "text": "3 Other Benchmark Comparison\nTo further contextualize the performance of the TBTF strategy, we compare it against a wide set of benchmark portfolios, including major U.S. equity indices and Fama-French style-sorted portfolios. Each comparison is presented as a paired figure showing the monthly return distribution (left) and the cumulative price level evolution (right).\n\n\nCode\ntbtf.plot_benchmark_comparison(tbtf_returns_pre2010, pre_index, period_label='pre-2010')\n\n\n\n\n\nTBTF vs. Benchmarks (Pre-2010)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ntbtf.plot_benchmark_comparison(tbtf_returns_post2010, post_index, period_label='post-2010')\ntbtf.plot_benchmark_comparison(tbtf_returns_post2010, post_ff, period_label='post-2010 (FF)')\n\n\n\n\n\nTBTF vs. Benchmarks (Post-2010)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.1 Pre-2010: Market Dominance and Diversification\nIn the pre-2010 sample, TBTF underperforms all major benchmarks both in terms of average monthly returns and cumulative price levels:\n\nCompared to the Dow Jones Industrial Average (^DJI) and the Nasdaq 100 (^NDX), TBTF lags significantly in mean returns, with its average return even falling below zero. This underperformance is visible in both the return histogram and the flattened price trajectory.\nThe shape of the return distributions further illustrates this divergence. While ^DJI displays a concentrated bell-shaped curve reflecting strong diversification benefits across sectors, TBTF’s distribution is wider and flatter, and ^NDX’s distribution appears even flatter and fatter-tailed—consistent with its tech-heavy composition and lower effective diversification.\nThese findings imply that during this regime, the structural capital lock-in emphasized by TBTF was not yet rewarded, and sectoral diversification still played a dominant role in driving returns.\n\n\n\n3.2 Post-2010: Structural Convexity and Breakaway Performance\nPost-2010, the TBTF strategy decisively outperforms all benchmark portfolios, not only in cumulative performance but also in distributional characteristics:\n\nMost strikingly, TBTF’s return distribution exhibits bi-modality, a rare phenomenon in empirical asset returns. Both modes are positive, with the second, larger mode emerging after recovering from a brief period of negative returns during late 2021 to early 2022. This distributional structure reflects a convex outcome space where moderate and large gains dominate.\nDIA, which tracks the 30 blue-chip constituents of ^DJI, displays a symmetric but left-skewed return profile and a relatively muted price level curve. By contrast, TBTF rapidly diverges post-2019, widening the performance gap year by year.\nQQQ, representing the Nasdaq 100 ETF, initially tracks TBTF closely between 2014 and 2020. However, since 2021, TBTF begins to consistently outperform QQQ, suggesting a regime shift where capital dominance surpasses even tech momentum.\nSPY, representing the broader S&P 500, sits between DIA and QQQ in both return level and distributional shape. Its distribution approximates a classic symmetric bell curve, but lacks the convexity and upside concentration observed in TBTF.\nThe return distribution of SPY is unimodal and centered, offering a useful contrast to the dual-modal, right-tilted structure of TBTF.\n\n\n\n3.3 Post-2010: Fama-French Portfolios (ME × PRIOR)\nWe also include value-weighted and equal-weighted Fama-French 3x2 portfolios constructed from top 20% market cap stocks (ME5) sorted by prior return (PRIOR1–5):\n\nAmong value-weighted portfolios, PRIOR5 (ff_b) performs best, followed by PRIOR3 (ff_m) and PRIOR1 (ff_s). This gradient suggests a standard momentum pattern. However, TBTF outpaces all of them, with its price level evolving geometrically, while FF portfolios show only linear growth trends over the same horizon.\nAmong equal-weighted portfolios, performance is more muted. Notably, ff_e_s (equal-weighted ME5 × PRIOR1) performs slightly better than its value-weighted counterpart, suggesting that in low-momentum groups, idiosyncratic selection beats size dominance.\nYet again, none of the FF portfolios, whether value- or equal-weighted, come close to TBTF’s performance, emphasizing that TBTF’s structural logic—built on capital persistence and lock-in rather than factor exposure—offers a fundamentally different return engine.\n\n\n\n3.4 Interpretation\nThe above comparisons reinforce the hypothesis that post-2010 capital markets have undergone a structural realignment. In this new regime, traditional sources of diversification and factor exposure lose explanatory power, while the concentration of capital into a select few entities generates persistent excess returns. The TBTF strategy captures this realignment directly through its construction, thereby achieving outperformance not through luck or timing, but by aligning itself with the new architecture of capital allocation in the post-QE world.",
    "crumbs": [
      "Apps",
      "Model",
      "06 Performance"
    ]
  },
  {
    "objectID": "4_6_performance.html#risk-adjusted-performance-metrics",
    "href": "4_6_performance.html#risk-adjusted-performance-metrics",
    "title": "06 Performance",
    "section": "4 Risk-Adjusted Performance Metrics",
    "text": "4 Risk-Adjusted Performance Metrics\nTo benchmark the performance of the TBTF strategy against traditional market indices, we compute a suite of risk-adjusted performance metrics over both the pre-2010 and post-2010 periods. These include:\n\nSharpe ratio: Mean excess return normalized by standard deviation.\nSortino ratio: Return per unit of downside deviation, focusing on losses.\nOmega ratio: A distribution-sensitive metric measuring the ratio of gains above a threshold to losses below it.\nMaximum drawdown: Largest peak-to-trough loss during the sample period.\nExpected CRRA utility: A model-based expected utility under constant relative risk aversion.\n\nThe final metric—Expected CRRA Utility—provides a theoretically grounded welfare measure, widely used in academic finance. Unlike Sharpe or Sortino ratios, CRRA utility reflects higher-order moments of the return distribution, such as skewness and kurtosis, which are especially relevant for TBTF’s left-skewed but convex payoff structure. For a CRRA coefficient \\(\\gamma &gt; 0\\), utility is computed as:\n\\[\n\\mathbb{E}[U(W)] = \\frac{1}{1 - \\gamma} \\cdot \\mathbb{E}\\left[(1 + r)^{1 - \\gamma}\\right],\n\\]\nwhere \\(r\\) is the gross monthly return. We set \\(\\gamma = 3\\) to reflect moderate risk aversion, consistent with our strategy design.\nTables below report the performance metrics for selected portfolios, sorted by Expected CRRA Utility.\n\n4.1 Pre-2010 Period\n\n\nCode\nreturns_dict_pre2010 = {\n    'TBTF': tbtf_returns_pre2010,\n    'DJI': pre_index['^DJI'],\n    'NDX': pre_index['^NDX']\n}\n\n#| tbl-cap: \"Risk-Adjusted Performance Comparison (Pre-2010)\"\ntbtf.generate_performance_table(returns_dict_pre2010).sort_values(by='Expected CRRA', ascending=False)\n\n\n\n\n\n\n\n\n\nAnnualized Return\nAnnualized Volatility\nSharpe Ratio\nSortino Ratio\nOmega Ratio\nMax Drawdown\nCalmar Ratio\nExpected CRRA\nFisher Skewness\nPearson Skewness\nExcess Kurtosis\n\n\nPortfolio\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDJI\n0.053029\n0.159515\n0.405245\n0.536194\n0.764380\n-0.492970\n0.107571\n0.002087\n-0.611944\n-0.246797\n1.035444\n\n\nNDX\n0.088080\n0.300315\n0.433556\n0.638719\n1.026011\n-0.810698\n0.108647\n-0.000878\n-0.249061\n-0.162229\n0.519681\n\n\nTBTF\n-0.102382\n0.180976\n-0.504393\n-0.693212\n0.398926\n-0.312157\n-0.327984\n-0.011923\n-0.194323\n0.078361\n0.463931\n\n\n\n\n\n\n\nThe risk-adjusted performance metrics for the pre-2010 period reveal that the TBTF strategy significantly underperforms relative to traditional benchmarks such as the Dow Jones Industrial Average (DJI) and Nasdaq 100 (NDX). While both benchmarks deliver positive annualized returns—5.3% for DJI and 8.8% for NDX—TBTF produces a negative average return of -10.2%, resulting in uniformly negative values for all performance ratios.\n\nSharpe and Sortino ratios for TBTF are both negative, indicating that the strategy failed to compensate for volatility and downside risk during this period.\nThe Omega ratio of TBTF (0.40) is substantially lower than that of NDX (1.03) and DJI (0.76), further emphasizing poor downside-adjusted performance.\nMaximum drawdown, surprisingly, is less severe for TBTF (-31.2%) than for NDX (-81.1%), though this is largely due to TBTF’s failure to experience large upswings, not resilience in downturns.\nThe Expected CRRA utility for TBTF is negative (-0.0119), in stark contrast to the positive values for DJI (+0.0021). This indicates that, for a moderately risk-averse investor (\\(\\gamma = 3\\)), TBTF would have been strictly dominated by holding a benchmark index.\nIn terms of distribution shape, TBTF displays near-zero skewness and moderate excess kurtosis, suggesting symmetric but fat-tailed returns. This is consistent with the earlier observation that pre-2010 TBTF returns resembled a bell curve with heavy tails, whereas NDX was left-skewed and more extreme in both tails.\n\nTaken together, these results indicate that during the pre-QE, industrial-cycle-driven market regime, the structural logic behind TBTF—capital lock-in and size dominance—did not translate into superior returns. Instead, diversification (as in DJI) and tech momentum (as in NDX) dominated portfolio performance.\n\n\n4.2 Interpretation: Post-2010 Period\n\n\nCode\nreturns_dict_post2010 = {\n    'TBTF': tbtf_returns_post2010,\n    'DIA': post_index['DIA'],\n    'QQQ': post_index['QQQ'],\n    'SPY': post_index['SPY'],\n    'VTI': post_index['VTI']\n}\n\n#| tbl-cap: \"Risk-Adjusted Performance Comparison (Post-2010)\"\ntbtf.generate_performance_table(returns_dict_post2010).sort_values(by='Expected CRRA', ascending=False)\n\n\n\n\n\n\n\n\n\nAnnualized Return\nAnnualized Volatility\nSharpe Ratio\nSortino Ratio\nOmega Ratio\nMax Drawdown\nCalmar Ratio\nExpected CRRA\nFisher Skewness\nPearson Skewness\nExcess Kurtosis\n\n\nPortfolio\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTBTF\n0.358215\n0.211433\n1.568884\n2.209493\n2.169813\n-0.210497\n1.701760\n0.021408\n-0.352156\n0.054055\n1.127887\n\n\nQQQ\n0.169477\n0.178377\n0.971665\n1.612117\n1.241137\n-0.330703\n0.512476\n0.010306\n-0.192792\n-0.303277\n0.010333\n\n\nSPY\n0.109163\n0.148966\n0.772762\n1.143819\n0.975512\n-0.247979\n0.440211\n0.006722\n-0.344475\n-0.344606\n0.541203\n\n\nVTI\n0.108100\n0.153839\n0.746852\n1.077633\n0.974996\n-0.256668\n0.421165\n0.006514\n-0.355191\n-0.254563\n0.720072\n\n\nDIA\n0.096274\n0.145753\n0.705438\n1.085427\n0.913413\n-0.231042\n0.416694\n0.005855\n-0.171162\n-0.164886\n0.948737\n\n\n\n\n\n\n\nThe post-2010 performance metrics tell a dramatically different story. In this regime—characterized by sustained quantitative easing, historically low interest rates, and capital concentration—the TBTF strategy not only outperforms all benchmark portfolios in raw returns, but does so decisively across all risk-adjusted dimensions.\n\nAnnualized return for TBTF reaches 35.8%, more than twice that of QQQ (16.9%) and over three times that of SPY, VTI, and DIA (all below 11%).\nThe Sharpe ratio of TBTF (1.57) is by far the highest, indicating a return per unit of volatility unmatched by any benchmark. More notably, the Sortino ratio (2.21) and Omega ratio (2.17) reveal TBTF’s asymmetric protection against downside, a signature feature of its convex payoff structure.\nMaximum drawdown is relatively modest at -21.0%, despite the high return level, and the Calmar ratio (1.70) reflects this efficiency. By comparison, QQQ suffers a deeper drawdown (-33.1%) with only a third of the Calmar ratio.\nCrucially, the Expected CRRA utility for TBTF is +0.0214, more than double that of QQQ (+0.0103), and significantly above other diversified portfolios (SPY: +0.0067, VTI: +0.0065, DIA: +0.0059). For a moderately risk-averse investor (\\(\\gamma = 3\\)), TBTF represents a clear welfare-maximizing choice.\nFrom a distributional perspective, TBTF shows mild negative skewness (Fisher: -0.35) and positive excess kurtosis (1.13), consistent with earlier evidence of bimodality and tail concentration. In contrast, SPY and VTI are more symmetric but exhibit thinner tails, which limits their upside potential.\nQQQ, while historically strong, demonstrates a flattened distribution with lower Omega and Sortino ratios, suggesting diminished downside protection relative to TBTF in recent years.\n\nThese results reinforce the view that the post-2010 market environment systematically rewarded capital dominance over traditional diversification or momentum strategies. TBTF’s success is not just statistically significant, but economically meaningful across every major dimension of portfolio evaluation—returns, risk, asymmetry, drawdowns, and investor utility.",
    "crumbs": [
      "Apps",
      "Model",
      "06 Performance"
    ]
  },
  {
    "objectID": "4_6_performance.html#drawdown-and-turnover",
    "href": "4_6_performance.html#drawdown-and-turnover",
    "title": "06 Performance",
    "section": "5 Drawdown and Turnover",
    "text": "5 Drawdown and Turnover\nIn this section, we evaluate the downside risk exposure and implementation feasibility of the TBTF strategy. Two key dimensions are analyzed:\n\nMaximum Drawdown: Measures the largest peak-to-trough decline in cumulative return over the evaluation period.\nPortfolio Turnover: Assesses the degree of rebalancing required, with lower turnover implying lower transaction costs and higher implementability.\n\n\n5.1 Drawdown Dynamics\n\nWhile TBTF suffered deeper losses than ^DJI during the 2008 crisis, its post-2010 drawdown profile proved significantly more resilient than traditional style portfolios and benchmark ETFs, with a rapid recovery trajectory by 2023.\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\ndef compute_drawdown(return_series: pd.Series) -&gt; pd.Series:\n    cumulative = (1 + return_series).cumprod()\n    peak = cumulative.cummax()\n    drawdown = (cumulative - peak) / peak\n    return drawdown\n\ndef plot_drawdown(tbtf_returns_raw, benchmark_df, title=\"Drawdown Profile\"):\n    plt.figure(figsize=(10, 4))\n\n    # --- TBTF 처리 ---\n    if isinstance(tbtf_returns_raw, pd.DataFrame):\n        if 'date' in tbtf_returns_raw.columns:\n            tbtf_returns_raw = tbtf_returns_raw.set_index(pd.to_datetime(tbtf_returns_raw['date']))\n        tbtf_returns = tbtf_returns_raw.select_dtypes(include='number').iloc[:, 0]\n    elif isinstance(tbtf_returns_raw, pd.Series):\n        tbtf_returns = tbtf_returns_raw\n    else:\n        raise TypeError(\"tbtf_returns_raw must be DataFrame or Series.\")\n\n    tbtf_returns.index = pd.to_datetime(tbtf_returns.index)\n    tbtf_drawdown = compute_drawdown(tbtf_returns)\n    plt.plot(tbtf_drawdown.index, tbtf_drawdown.values, label='TBTF', color='black', linewidth=2)\n\n    # --- 벤치마크 DataFrame 처리 ---\n    if 'date' in benchmark_df.columns:\n        benchmark_df = benchmark_df.set_index(pd.to_datetime(benchmark_df['date']))\n    else:\n        benchmark_df.index = pd.to_datetime(benchmark_df.index)\n\n    benchmark_df = benchmark_df.select_dtypes(include='number')  # 숫자만\n\n    for col in benchmark_df.columns:\n        series = benchmark_df[col].reindex(tbtf_returns.index)\n        if series.isna().all():\n            continue  # 완전히 비어있으면 skip\n        drawdown = compute_drawdown(series)\n        plt.plot(drawdown.index, drawdown.values, label=col, alpha=0.7)\n\n    # --- x축 날짜 포맷 ---\n    ax = plt.gca()\n    ax.xaxis.set_major_locator(mdates.YearLocator())\n    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n    plt.xticks(rotation=45)\n\n    # --- 마무리 ---\n    plt.title(title)\n    plt.ylabel(\"Drawdown\")\n    plt.xlabel(\"Date\")\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()\n\n\n\n5.1.1 Pre-2010 Period (2000–2009)\n\n\nCode\nplot_drawdown(results_pre['returns'], pre_index, title=\"Drawdown Profile (Pre-2010)\")\n\n\n\n\n\n\n\n\n\nDuring the first decade, the TBTF strategy underperformed the Dow Jones Industrial Average (^DJI) in terms of drawdown resilience. While TBTF, ^DJI, and ^NDX all exhibited relatively similar patterns prior to the 2008 Global Financial Crisis, the drawdown plot reveals that TBTF experienced a sharper decline during the crisis episode. This suggests that TBTF’s concentrated exposure to the largest-cap firms provided limited diversification benefit during systemic tail events, particularly in contrast to the more balanced composition of the ^DJI.\n\n\n5.1.2 Post-2010 Period (2014–2023)\n\n\nCode\nplot_drawdown(results_post['returns'], post_index, title=\"Drawdown Profile (Post-2010)\")\nplot_drawdown(results_post['returns'], post_ff, title=\"Drawdown Profile (Post-2010)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn contrast, during the second decade, TBTF demonstrated superior drawdown performance relative to all benchmark ETFs, particularly until late 2021. The strategy maintained shallow drawdowns through several volatility cycles, including the COVID-19 shock and inflation-driven rate adjustments, reflecting its structural bias toward persistent market leaders.\nHowever, in late 2021, TBTF experienced a notable drawdown phase, closely mirrored by the QQQ, indicating common exposure to high-growth tech stocks. Despite this synchronized decline, both TBTF and QQQ rebounded strongly by 2023, surpassing prior peaks.\nNotably, the Fama-French style-sorted portfolios (ME×PRIOR) exhibited deeper and more persistent drawdowns throughout the post-2010 period. These portfolios consistently underperformed TBTF in drawdown magnitude, highlighting the limitations of conventional size- and momentum-based constructions in capturing the structural dominance embedded in TBTF’s selection mechanism.\n\n\n\n5.2 TBTF Turnover\n\n\nCode\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\ndef plot_turnover(turnover_df, title=\"TBTF Turnover Over Time\"):\n    df = turnover_df.copy()\n    df['rebalance_date'] = pd.to_datetime(df['rebalance_date'])\n    df.set_index('rebalance_date', inplace=True)\n\n    plt.figure(figsize=(10, 3))\n    plt.plot(df.index, df['turnover'], marker='o', color='darkgreen', linewidth=1.5)\n\n    # x축 날짜 format\n    ax = plt.gca()\n    ax.xaxis.set_major_locator(mdates.YearLocator())\n    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n    plt.xticks(rotation=45)\n\n    plt.title(title)\n    plt.ylabel(\"Turnover (sum of abs weight changes)\")\n    plt.xlabel(\"Rebalance Date\")\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()\n\n\n\n\nCode\n# Pre-2010 turnover plot\nplot_turnover(results_pre['turnover'].iloc[:-1], title=\"TBTF Turnover (Pre-2010)\")\n\n# Post-2010 turnover plot\nplot_turnover(results_post['turnover'].iloc[:-1], title=\"TBTF Turnover (Post-2010)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe TBTF strategy rebalances quarterly, yet maintains a remarkably steady turnover profile across both subperiods, highlighting its structural persistence and practical implementability.\n\n\n\nPeriod\nMean Turnover\nStd Dev\n\n\n\n\nPre-2010\n0.22\n0.12\n\n\nPost-2010\n0.15\n0.10\n\n\n\nThis low and stable turnover suggests that, although the strategy updates weights every three months, the actual reallocation of capital is limited. This is particularly meaningful given TBTF’s concentrated structure: the top 10 stocks by market capitalization.\nTo further explore the internal mechanics of turnover, we examine the group-rank positions of stocks entering and exiting the portfolio:\n\nThe average max_diff_group_rank is approximately 1–2, indicating that stocks removed from the TBTF portfolio were typically ranked 9th or 10th before exit.\n\nThe average min_diff_group_rank is approximately 3–3.5, suggesting that newly entering stocks typically displaced members ranked around 6th or 7th.\n\nThis pattern implies a core–fringe structure within the TBTF portfolio:\nthe top-ranked firms (1st–5th) exhibit strong persistence, while changes are concentrated at the margin of the selection, reducing both instability and turnover-related costs.",
    "crumbs": [
      "Apps",
      "Model",
      "06 Performance"
    ]
  },
  {
    "objectID": "4_6_performance.html#implications",
    "href": "4_6_performance.html#implications",
    "title": "06 Performance",
    "section": "6 Implications",
    "text": "6 Implications\nThe TBTF strategy consistently delivers strong out-of-sample performance, characterized by high risk-adjusted returns, stable drawdowns, and low turnover. These patterns persist across both pre- and post-2010 market regimes and cannot be explained by conventional size or momentum factors.\nWhat distinguishes TBTF is not merely statistical outperformance but its reflection of a persistent capital hierarchy, in which dominant firms retain market share through institutional mechanisms rather than pure productivity or risk-taking.\nWhereas traditional models explain return premia through exposure to priced risk, TBTF accumulates return through insulation from risk—via passive flows, platform effects, and embedded expectations. In this sense, market power does not demand a premium—it neutralizes the need for one.\nThis invites a reframing of modern asset pricing:\n&gt; Return without risk is no longer a puzzle, but a feature of financial capitalism.\nReaders seeking formal modeling of regime persistence and capital asymmetry are referred to Section 4. Here, we have focused on empirical validation, leaving latent structure estimation to the structural section.\nIn the following section, we assess whether TBTF’s performance is robust to alternative specifications: selection cutoffs, rebalancing intervals, weighting schemes, and estimation windows. This stress-testing is essential to distinguish true structural effects from parameter-driven artifacts.",
    "crumbs": [
      "Apps",
      "Model",
      "06 Performance"
    ]
  },
  {
    "objectID": "4_4_structure.html",
    "href": "4_4_structure.html",
    "title": "04 Structure",
    "section": "",
    "text": "This section decomposes the structural mechanisms underlying the TBTF strategy across two economic regimes—pre-2010 and post-2010. By comparing these periods, we identify changes in distributional composition, capital concentration, and rank mobility that reflect broader shifts in financial market dynamics following systemic interventions and structural innovation—each of which challenges conventional assumptions in asset pricing and market efficiency.\nCode\nimport numpy as np\nimport statsmodels.api as sm\nfrom scipy.stats import norm\nfrom sklearn.mixture import GaussianMixture\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# CRSP dataframe import\nimport pandas as pd\nimport sqlite3\ncon = sqlite3.connect(database=\"../../tbtf.sqlite\")\n\ncrsp = pd.read_sql_query(\n  sql=\"SELECT * FROM crsp\",\n  con=con,\n  parse_dates={\"date\"}\n)\ncrsp.head()\n\n\n\n\n\n\n\n\n\npermno\ndate\nmktcap\nret\nprimaryexch\nsiccd\nindustry\nmktcap_lag\nret_excess\nstate\nstate_lag\n\n\n\n\n0\n10001\n1996-01-31\n20.814125\n-0.026667\nQ\n4925\nUtilities\n21.384375\n-0.030967\n3\n0\n\n\n1\n10001\n1996-02-29\n21.099250\n0.013699\nQ\n4925\nUtilities\n20.814125\n0.009799\n3\n3\n\n\n2\n10001\n1996-03-31\n21.899480\n0.036423\nQ\n4925\nUtilities\n21.099250\n0.032523\n3\n3\n\n\n3\n10001\n1996-04-30\n20.348063\n-0.070840\nQ\n4925\nUtilities\n21.899480\n-0.075440\n2\n3\n\n\n4\n10001\n1996-05-31\n19.915125\n-0.021277\nQ\n4925\nUtilities\n20.348063\n-0.025477\n2\n2",
    "crumbs": [
      "Apps",
      "Model",
      "04 Structure"
    ]
  },
  {
    "objectID": "4_4_structure.html#mixture-distribution-decomposition",
    "href": "4_4_structure.html#mixture-distribution-decomposition",
    "title": "04 Structure",
    "section": "1 Mixture Distribution Decomposition",
    "text": "1 Mixture Distribution Decomposition\n\n1.1 Core Question:\n\nIs the market portfolio return better explained as a mixture of distinct structural groups, such as top decile vs. the rest, and how have their mixture weights changed across regimes?\n\n\n\n1.2 Framework & Method:\n\nWe define two return-generating groups:\n\nGroup A: Top 10% stocks by market cap (state = 10)\nGroup B: All other listed stocks (states &lt; 10)\n\nAt each month \\(t\\), compute value-weighted returns:\n\\[\nR_t^{\\text{market}} = w_t \\cdot R_t^{(10)} + (1 - w_t) \\cdot R_t^{(&lt;10)}\n\\]\nwhere \\(w_t\\) is the capital share of Group A—i.e., the mixture weight.\n\n\n\n1.3 Interpretation\nThe empirical results from the mixture decomposition provide compelling evidence of a structural shift in how market returns are generated and distributed. Unlike traditional interpretations that focus on regime switching (e.g., bull vs. bear), our analysis reveals a persistent cross-sectional asymmetry—notably between the top decile and the remaining 90% of firms by market capitalization.\n\n\nCode\n# df_mix\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 1. Value-Weighted Return 계산 함수 정의\ndef value_weighted_return(df, value_col='mktcap_lag', return_col='ret'):\n    weighted = df[return_col] * df[value_col]\n    total_weight = df[value_col].sum()\n    return weighted.sum() / total_weight if total_weight &gt; 0 else np.nan\n\n# 2. Group별 수익률 계산 및 mixture 분해\n# 결과 저장용 리스트\nresults = []\n\n# 월별 반복\nfor date, group in crsp.groupby('date'):\n    top_group = group[group['state'] == 10]\n    rest_group = group[(group['state'] &lt; 10) & (group['state'] &gt; 0)]\n\n    if len(top_group) == 0 or len(rest_group) == 0:\n        continue\n\n    # 개별 그룹의 value-weighted return 계산\n    r_top = value_weighted_return(top_group)\n    r_rest = value_weighted_return(rest_group)\n    \n    # 전체 시장 포트폴리오 value-weighted return 계산\n    r_total = value_weighted_return(group[group['state'] &gt; 0])\n\n    # mixture weight (자본 비중)\n    w_top = top_group['mktcap_lag'].sum() / group[group['state'] &gt; 0]['mktcap_lag'].sum()\n\n    results.append({\n        'date': date,\n        'r_top': r_top,\n        'r_rest': r_rest,\n        'r_total': r_total,\n        'w_top': w_top,\n        'w_rest': 1 - w_top,\n        'r_predicted': w_top * r_top + (1 - w_top) * r_rest\n    })\n\ndf_mix = pd.DataFrame(results).sort_values('date')\n\ndf_mix.head()\n\n\n\n\n\n\n\n\n\ndate\nr_top\nr_rest\nr_total\nw_top\nw_rest\nr_predicted\n\n\n\n\n0\n1996-01-31\n0.034183\n-0.003783\n0.026941\n0.809244\n0.190756\n0.026941\n\n\n1\n1996-02-29\n0.014810\n0.027623\n0.017185\n0.814608\n0.185392\n0.017185\n\n\n2\n1996-03-31\n0.009841\n0.017373\n0.011246\n0.813420\n0.186580\n0.011246\n\n\n3\n1996-04-30\n0.017836\n0.056573\n0.025221\n0.809359\n0.190641\n0.025221\n\n\n4\n1996-05-31\n0.026211\n0.034577\n0.027858\n0.803161\n0.196839\n0.027858\n\n\n\n\n\n\n\n\n1.3.1 Capital Lock-In and Mixture Weight Dynamics\n\n\nCode\n# Time-series Capital Weight of Top 10% (State = 10)\nplt.plot(df_mix['date'], df_mix['w_top'])\nplt.title('Time-series Capital Weight of Top 10% (State = 10)')\nplt.xlabel('Date')\nplt.ylabel('Top 10% Capital Weight')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe time-series evolution of \\(w_t\\)—the value-weighted capital share of the top 10% stocks—offers compelling evidence of persistent and intensifying capital lock-in.\n\nBetween 1996 and 2001, \\(w_t\\) rose rapidly from below 0.75 to a peak of approximately 0.88, reflecting the early stages of mega-cap dominance during the dot-com boom.\nAfter peaking, it declined gradually, reaching just below 0.80 by 2006, indicating a temporary rebalancing of capital across ranks.\nFrom 2006 to 2009, \\(w_t\\) climbed again, stabilizing around 0.83 during the financial crisis—a period marked by concentrated flight to safety and the early impact of Federal Reserve interventions.\nA modest decline occurred post-crisis, with \\(w_t\\) dipping to around 0.78 in 2011.\nSince then, however, the capital share of the top 10% has exhibited near-monotonic increases with remarkably low volatility, except for a brief interruption during the COVID-19 shock in 2020.\nBy the end of 2023, \\(w_t\\) had reached levels exceeding 0.85, with persistent upward momentum and minimal fluctuations.\n\nThis structural pattern indicates not merely high concentration but capital inertia—a state where capital ceases to reallocate dynamically and instead becomes entrenched within a quasi-permanent elite. Capital flows no longer reflect dynamic responses to fundamentals or risk signals, but instead conform to structural entrenchment supported by indexation, ETF flows, and policy-driven yield compression. Capital lock-in is not only persistent but self-reinforcing, reflecting a transition from competitive allocation to institutionalized dominance.\nc.f. Compared to Fama-French’s traditional use of NYSE-only breakpoints, our percentile-based method—spanning NYSE + Nasdaq + AMEX—captures the true cross-sectional concentration of market power, including modern platform firms (e.g., AAPL, MSFT, NVDA) that disproportionately shape returns in the post-crisis era.\n\n\n1.3.2 Unconditional Return Distributions: Top vs. Rest\n\n\nCode\n# Unconditional Return Distribution: Actual vs. Estimated Mixture\n# GMM의 간소화 버전\" 또는 \"semi-parametric mixture modeling\n\n# 샘플 수\nobs = len(df_mix)\n\n# 각 그룹의 수익률 평균 및 표준편차\nr_top_mean = df_mix['r_top'].mean()\nr_top_std = df_mix['r_top'].std()\nr_rest_mean = df_mix['r_rest'].mean()\nr_rest_std = df_mix['r_rest'].std()\nw_top_mean = df_mix['w_top'].mean()\n\n# 실제 및 mixture 수익률\nr_total = df_mix['r_total']\nr_predicted = df_mix['r_predicted']\n\n# 히스토그램과 PDF x축 범위\nxmin = min(r_total.min(), r_predicted.min())\nxmax = max(r_total.max(), r_predicted.max())\nx = np.linspace(xmin, xmax, 1000)\n\n# PDF 계산\npdf_top = norm.pdf(x, loc=r_top_mean, scale=r_top_std)\npdf_rest = norm.pdf(x, loc=r_rest_mean, scale=r_rest_std)\npdf_mix = w_top_mean * pdf_top + (1 - w_top_mean) * pdf_rest\n\n# Plot\nplt.figure(figsize=(10, 6))\nbins = np.linspace(xmin, xmax, 50)\n\n# 실제 수익률 히스토그램\nplt.hist(r_total, bins=bins, alpha=0.3, label='Actual Total Return', color='steelblue', density=True)\n\n# 각 컴포넌트 PDF\nplt.plot(x, pdf_top, linestyle='-', color='red', label='Top 10% Estimated PDF')\nplt.plot(x, pdf_rest, linestyle='-', color='blue', label='Bottom 90% Estimated PDF')\n\n# 혼합 정규 분포 PDF\nplt.plot(x, pdf_mix, linestyle='--', color='black', linewidth=2, label='Mixture PDF (Estimated)')\n\nplt.title(\"Unconditional Return Distribution: Actual vs. Estimated Mixture\")\nplt.xlabel(\"Monthly Return\")\nplt.ylabel(\"Density\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Unconditional Return Distribution by Period and Capital Rank Group\n\n# 요약 통계 (Pre-2010 vs Post-2010 비교 등)\ndf_mix['period'] = np.where(df_mix['date'] &lt; '2010-01-01', 'Pre-2010', 'Post-2010')\n\n# 데이터 재구조화: long format\ndf_long = pd.melt(\n    df_mix,\n    id_vars=['date', 'period'],\n    value_vars=['r_top', 'r_rest'],\n    var_name='Group',\n    value_name='Monthly Return'\n)\n\n# Group 이름 정리\ndf_long['Group'] = df_long['Group'].replace({\n    'r_top': 'Top 10%',\n    'r_rest': 'Bottom 90%'\n})\n\n# Boxplot\nplt.figure(figsize=(10, 6))\nsns.boxplot(\n    data=df_long,\n    x='period',\n    y='Monthly Return',\n    hue='Group',\n    palette='Set2'\n)\nplt.title('Unconditional Return Distribution by Period and Capital Rank Group')\nplt.axhline(0, color='gray', linestyle='--', linewidth=1)\nplt.ylabel('Monthly Return')\nplt.xlabel('Period')\nplt.legend(title='Group', loc='lower left')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nBoxplot visualizations reveal that in the post-2010 regime:\n\nThe top decile (r_top) has higher median returns and a narrower interquartile range than the bottom 90% (r_rest).\nOutliers on the downside virtually disappear for r_top, while they persist—especially on both tails—for r_rest.\nThe return distribution of r_top becomes increasingly compact, upward-shifted, and volatility-suppressed, resembling a managed financial product rather than a random equity basket.\n\nThese patterns imply that risk-adjusted return asymmetry is not just about mean differences, but about structural volatility suppression and downside risk elimination at the top.\n\n\n\n1.3.3 Gaussian Mixture Results: Two-Component Fit\n\n\nCode\n# GMM (k=2) for Top 10% and for Remining 90% \nimport os\nos.environ['LOKY_MAX_CPU_COUNT'] = '4'  # 사용하려는 코어 수를 직접 지정\n\n# 각 그룹의 return 시리즈\ntop_returns = df_mix['r_top'].dropna().values.reshape(-1, 1)\nrest_returns = df_mix['r_rest'].dropna().values.reshape(-1, 1)\n\n# GMM 적합 함수\ndef fit_gmm(data, n_components=2):\n    gmm = GaussianMixture(n_components=n_components, random_state=42)\n    gmm.fit(data)\n    return gmm\n\ngmm_top = fit_gmm(top_returns, n_components=2)\ngmm_rest = fit_gmm(rest_returns, n_components=2)\n\ndef print_gmm_summary(gmm, label):\n    print(f\"\\nGMM summary for {label}\")\n    for i, (w, mu, var) in enumerate(zip(gmm.weights_, gmm.means_, gmm.covariances_)):\n        print(f\" Component {i+1}: weight={w:.3f}, mean={mu[0]:.4f}, std={np.sqrt(var[0][0]):.4f}\")\n\nprint_gmm_summary(gmm_top, 'Top 10%')\nprint_gmm_summary(gmm_rest, 'Remaining 90%')\n\n\n\nGMM summary for Top 10%\n Component 1: weight=0.290, mean=-0.0307, std=0.0437\n Component 2: weight=0.710, mean=0.0267, std=0.0327\n\nGMM summary for Remaining 90%\n Component 1: weight=0.340, mean=-0.0295, std=0.0750\n Component 2: weight=0.660, mean=0.0193, std=0.0404\n\n\nThe Gaussian Mixture Models (GMMs) estimated separately on r_top and r_rest uncover two key structural regimes:\n\nBoth groups exhibit bimodal structure, with one cluster capturing low-mean, high-volatility returns, and the other positive-mean, low-volatility returns.\nHowever, for the top decile, the low-return regime has a lower weight and tighter variance, suggesting that even when shocks occur, they are attenuated within this elite group.\nIn contrast, the bottom 90% remains exposed to broader volatility regimes, maintaining a larger spread of possible outcomes.\n\n\n\n1.3.4 Testing Internal Heterogeneity in Top Decile\n\n\nCode\n# k Selection by AIC vs. BIC\n\nX = df_mix['r_top'].dropna().values.reshape(-1, 1)\n\naic_scores = []\nbic_scores = []\nks = range(1, 21)\n\nfor k in ks:\n    gmm = GaussianMixture(n_components=k, random_state=42)\n    gmm.fit(X)\n    aic_scores.append(gmm.aic(X))\n    bic_scores.append(gmm.bic(X))\n\n# Plot AIC vs BIC to visualize optimal k\nplt.plot(ks, aic_scores, label='AIC')\nplt.plot(ks, bic_scores, label='BIC')\nplt.xlabel('Number of Components (k)')\nplt.ylabel('Information Criterion')\nplt.legend()\nplt.title('Model Selection: AIC vs BIC')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nTo test whether the top decile itself contains further latent substructures, we estimate GMMs with increasing number of components (\\(k = 1, 2, ..., 20\\)). The result is striking:\n\nBoth AIC and BIC increase monotonically in \\(k\\), indicating that no additional latent components significantly improve model fit.\nThe most parsimonious and interpretable solution is obtained at \\(k = 2\\) or even \\(k = 1\\), reinforcing the view that a single dominant structural cluster governs return generation at the top.\n\nThis suggests that return asymmetry may not stem from overlapping regimes or transitory factors, but rather from the structural dominance of a statistically persistent subgroup of mega-cap firms.\n\n\n\n1.4 Summary Interpretation\nTogether, these results imply that:\n\nThe top-decile portfolio behaves like a capital sink—absorbing flows while insulating itself from market volatility.\nMarket return is no longer the outcome of dispersed competition, but rather converges to a structurally locked elite, supported by policy, passive flows, and indexation mechanics.\nThis undermines the classical view of a dynamic, risk-compensated market and instead points toward structural rent extraction by the few.",
    "crumbs": [
      "Apps",
      "Model",
      "04 Structure"
    ]
  },
  {
    "objectID": "4_4_structure.html#capital-share-convexity-quadratic-regression-evolution",
    "href": "4_4_structure.html#capital-share-convexity-quadratic-regression-evolution",
    "title": "04 Structure",
    "section": "2 Capital Share Convexity: Quadratic Regression Evolution",
    "text": "2 Capital Share Convexity: Quadratic Regression Evolution\n\n2.1 Core Question:\n\nHas capital become more concentrated in the top-ranked firms over time?\n\n\n\n2.2 Framework & Method:\n\nCross-sectional regression at each \\(t\\):\n\\[\n\\text{CapShare}_{i,t} = \\alpha_t + \\beta_t \\cdot \\text{Rank}_i + \\gamma_t \\cdot \\text{Rank}_i^2 + \\varepsilon_{i,t}\n\\]\nwhere \\(\\text{Rank}_i\\) is percentile-based with state 1 = bottom 10%, …, state 10 = top 10%.\nThe convexity of capital distribution is captured by \\(\\gamma_t\\):\n\n\\(\\gamma_t &gt; 0\\) implies increasing convexity, or accelerated capital share at the top.\n\nTime-series \\(\\{\\gamma_t\\}\\) is visualized to show evolution of capital inequality.\n\n\n\n2.3 Interpretation\n\n\nCode\n# Time-Series of Capital Share Convexity\n\n# 1. 각 날짜별로 state별 mktcap 합산 → CapShare_i 생성\ndef compute_capshare(df):\n    df = df.copy()\n    cap_by_state = df.groupby('state')['mktcap'].sum()\n    total_cap = cap_by_state.sum()\n    df['cap_share'] = df['state'].map(cap_by_state / total_cap)\n    return df\n\n# 2. cross-sectional regression에서 사용될 state별 요약 데이터 생성\ndef compute_quadratic_coefficients(df):\n    results = []\n    for date, group in df.groupby('date'):\n        # 유효한 state만 사용 (state 0은 제외)\n        group = group[group['state'] &gt; 0].copy()\n        cap_by_state = group.groupby('state')['mktcap'].sum()\n        total_cap = cap_by_state.sum()\n        capshare = cap_by_state / total_cap\n\n        # x = state (1~10), y = capshare\n        x = capshare.index.values\n        y = capshare.values\n\n        X = np.column_stack([x, x**2])\n        X = sm.add_constant(X)\n        model = sm.OLS(y, X).fit()\n\n        results.append({\n            'date': date,\n            'alpha': model.params[0],\n            'beta': model.params[1],\n            'gamma': model.params[2],\n            'r_squared': model.rsquared\n        })\n\n    return pd.DataFrame(results)\n\n\ndef plot_gamma_time_series(gamma_df):\n    plt.figure(figsize=(12, 6))\n    plt.plot(gamma_df['date'], gamma_df['gamma'], label='Gamma (Quadratic Term)', color='darkred')\n\n    # 기준선들\n    plt.axhline(0, color='gray', linestyle='--', linewidth=1)\n    plt.axvline(pd.to_datetime(\"2010-01-01\"), color='black', linestyle='--', linewidth=1.2, label='2010 Breakpoint')\n    plt.title('Time-Series of Capital Share Convexity (γₜ)')\n    plt.xlabel('Date')\n    plt.ylabel('Gamma Coefficient (Convexity)')\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()\n\ncrsp_filtered = crsp[crsp['state'] &gt; 0].copy()\n\ngamma_df = compute_quadratic_coefficients(crsp_filtered)\n\nplot_gamma_time_series(gamma_df)\n\n\n\n\n\n\n\n\n\nThe time-series of the capital share convexity coefficient, \\(\\gamma_t\\), estimated via cross-sectional quadratic regressions, reveals a striking and persistent structure.\n\n\nCode\ngamma_df.drop('date', axis=1).describe()\n\n\n\n\n\n\n\n\n\nalpha\nbeta\ngamma\nr_squared\n\n\n\n\ncount\n336.000000\n336.000000\n336.000000\n336.000000\n\n\nmean\n0.234058\n-0.154610\n0.018605\n0.669198\n\n\nstd\n0.010872\n0.006820\n0.000692\n0.015498\n\n\nmin\n0.215743\n-0.169420\n0.017445\n0.635521\n\n\n25%\n0.225711\n-0.160178\n0.018074\n0.656935\n\n\n50%\n0.231705\n-0.152995\n0.018444\n0.672377\n\n\n75%\n0.242551\n-0.149454\n0.019180\n0.681382\n\n\nmax\n0.257679\n-0.143156\n0.020107\n0.695882\n\n\n\n\n\n\n\nAcross the full sample period (1996–2023), \\(\\gamma_t\\) remains consistently positive and tightly bounded, with:\n\nA mean value of approximately 0.0186,\n\nA narrow interquartile range: [0.0181, 0.0192],\n\nA low standard deviation of 0.0007, and\n\nR-squared values averaging 0.67, indicating robust fit quality across time.\n\nThis remarkable temporal consistency suggests that the top ranks in the market-cap distribution have maintained a structurally convex advantage—absorbing disproportionately higher capital relative to their rank in nearly every month over the past three decades.\nA closer examination shows that:\n\nPrior to 2010, there was mild downward drift in \\(\\gamma_t\\), consistent with episodic rebalancing and potential capital redistribution (e.g., during the post-dotcom or pre-GFC phase).\nHowever, post-2010, \\(\\gamma_t\\) exhibits a slight but stable upward trend, indicating reinforced convexity—a steeper capital concentration curve among the top-ranked firms.\n\nThis empirical curvature echoes the shape of a discrete Lorenz curve, where \\(\\gamma_t\\) can be interpreted as a “capital inequality curvature index”. In this framework, increases in \\(\\gamma_t\\) correspond to a more extreme dominance of top ranks, consistent with both the capital lock-in and transition persistence mechanisms documented elsewhere in this study.\nImportantly, the tight dispersion and monotonic stabilization of \\(\\gamma_t\\) provide indirect but compelling support for the out-of-sample consistency of capital-weight-based strategies. The fact that the quadratic relationship between rank and capshare is not only convex but also time-invariant implies that a convex weighting scheme calibrated in-sample may remain optimal (or near-optimal) out-of-sample.\nThis further reinforces the central hypothesis of the TBTF framework: top capital positions are not merely episodic outcomes but persistent structural anchors in modern financial markets.",
    "crumbs": [
      "Apps",
      "Model",
      "04 Structure"
    ]
  },
  {
    "objectID": "4_4_structure.html#persistence-and-lock-in-dynamics",
    "href": "4_4_structure.html#persistence-and-lock-in-dynamics",
    "title": "04 Structure",
    "section": "3 Persistence and Lock-In Dynamics",
    "text": "3 Persistence and Lock-In Dynamics\n좋습니다. 아래는 4_4_structure.qmd의 “Persistence and Lock-In Dynamics” 섹션 중, Markov transition matrix 추정 방식을 실제 구현된 10×10 matrix 기반으로 수정 및 보완한 내용입니다. 핵심은 다음 두 가지입니다:\n\n원래 open system에서 state = 0(탈락 또는 미관측 상태)을 포함한 11×11 matrix는 완전히 닫힌 체계(closed system)가 아님\n본 분석은 실용적 접근으로 state = 1–10 범위에만 집중한 pseudo-closed system으로 제한하고, 이에 따른 해석상의 제약은 추후 확장 연구로 남긴다는 코멘트를 명시",
    "crumbs": [
      "Apps",
      "Model",
      "04 Structure"
    ]
  },
  {
    "objectID": "4_4_structure.html#persistence-and-lock-in-dynamics-1",
    "href": "4_4_structure.html#persistence-and-lock-in-dynamics-1",
    "title": "04 Structure",
    "section": "4 Persistence and Lock-In Dynamics",
    "text": "4 Persistence and Lock-In Dynamics\n\n4.1 Core Question:\n\nIs capital mobility declining, with firms increasingly locked in to top positions?\n\n\n\n4.2 Framework & Method\n\nStates are defined as decile ranks of market capitalization:\n\nState 10: Top 10% (≥ 90th percentile)\nStates 6–9: Upper middle 40% (50–90 percentile)\nStates 1–5: Bottom 50%\nState 0: Delisted or unranked (excluded from main estimation)\n\nUsing CRSP panel data, we construct an empirical first-order Markov transition matrix:\n\\[\nP_{ij} = \\Pr(\\text{state}_{t+1} = j \\mid \\text{state}_t = i)\n\\]\nImportant Adjustment:\n\nThe original system includes state 0, representing delisting or unranked stocks.\nHowever, to focus on the relative dynamics of ranked firms and avoid structural imbalance caused by market entry/exit, we restrict the analysis to states 1–10, forming a pseudo-closed system.\nThis restriction implicitly conditions on survival and omits net capital flow into or out of the system.\nA more comprehensive treatment including open-system dynamics (e.g., absorbing states, boundary flows) is deferred to future work.\n\nWe estimate:\n\nThe 10×10 transition matrix \\(P_{ij}\\) among ranked states only.\nDiagonal elements (\\(P_{ii}\\)): Represent state persistence or retention probability.\nOff-diagonal elements: Capture mobility dynamics, including upward or downward shifts.\nThe stationary distribution \\(\\pi\\) is computed via eigen decomposition:\n\\[\n\\pi' P = \\pi', \\quad \\sum_i \\pi_i = 1\n\\]\n\nAll transition counts are computed from actual firm-level changes in decile state based on monthly data, pre-processed to include both listing and delisting effects via augmented panel construction (crsp_trunc).\n\n\n\n4.3 Interpretation\n\n\nCode\nimport sys\nimport os\n# 현재 경로 기준으로 상위 디렉토리로 경로 추가\nsys.path.append(os.path.abspath('../..'))\n\nimport tbtf_data\n\nff3 = tbtf_data.load_ff3_factors(start_date=crsp['date'].min(), end_date=crsp['date'].max())\ncrsp_trunc = tbtf_data.crsp_truncation(crsp,ff3)\n# crsp_full = tbtf_data.crsp_lifecycle(crsp_trunc)\n\n\n\n\nCode\n# ================================================\n# transition_matrix : pd.DataFrame (10x10)\n# stationary_series : pd.Series (length 10)\n# ================================================\n\n# old-living만 선택 (full lifecycle이 없거나 right-truncated)\n#crsp_valid = crsp_full[crsp_full['lifetype'] == 'old-living'].copy()\n\n#print(\"The number of stocks from state 0 to any states:\", (crsp_trunc['state_lag'] == 0).sum())\n#print(\"The number of stocks from any states to state 0:\", (crsp_trunc['state'] == 0).sum())\n#print(\"The number of Total In-flow stocks during the sample period:\", (crsp_trunc['state_lag'] == 0).sum()-(crsp_trunc['state'] == 0).sum())\n\n# Transition matrix 계산 시점에서 filtering\n# 조건: state_lag ∈ [1, 10] AND state ∈ [1, 10]\ncrsp_valid = crsp_trunc.query(\"state &gt; 0 and state_lag &gt; 0\").copy()\n\n# Compute empirical transition counts\n# (state_lag → state) pair frequency table\ntransition_counts = (\n    crsp_valid\n    .groupby(['state_lag', 'state'])\n    .size()\n    .unstack(fill_value=0)\n    .sort_index()\n    .reindex(columns=range(1, 11), fill_value=0)  # Ensure all states 0 to 10 are present\n)\n\n# Convert counts to row-wise probabilities\n# Each row represents conditional distribution: P(next state | current state)\ntransition_matrix = transition_counts.div(transition_counts.sum(axis=1), axis=0)\n\n# Compute stationary distribution (long-run equilibrium distribution over states)\n# Method: Use eigen-decomposition of the transpose of the transition matrix\neigvals, eigvecs = np.linalg.eig(transition_matrix.T.values)\nstationary = np.real(eigvecs[:, np.isclose(eigvals, 1)])\n\n# Normalize to sum to 1\nstationary = stationary[:, 0]\nstationary_dist = stationary / stationary.sum()\n\n# Format for output\ntransition_matrix.index.name = 'From State'\ntransition_matrix.columns.name = 'To State'\n\nstationary_series = pd.Series(stationary_dist, index=transition_matrix.columns, name='Stationary Dist.')\n\n# Markov Transition Matrix Heatmap (States 0–10)\nimport seaborn as sns\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(\n    transition_matrix,\n    annot=True,\n    fmt=\".3f\",\n    cmap=\"YlGnBu\",\n    linewidths=0.5,\n    cbar_kws={'label': 'Transition Probability'},\n    xticklabels=[f\"To {i}\" for i in transition_matrix.columns],\n    yticklabels=[f\"From {i}\" for i in transition_matrix.index]\n)\nplt.title(\"Markov Transition Matrix Heatmap (States 0–10)\")\nplt.xlabel(\"Next Period State\")\nplt.ylabel(\"Current Period State\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Long-Run Stationary Distribution by State (Excl. Exit)\n\n# Step 4: Compute stationary distribution\n# Method: Use eigen decomposition for the transpose of the transition matrix\neigvals, eigvecs = np.linalg.eig(transition_matrix.T.values)\nstationary = np.real(eigvecs[:, np.isclose(eigvals, 1)])\nstationary = stationary[:, 0]\nstationary_dist = stationary / stationary.sum()\n\n# Prepare final transition matrix and stationary distribution\ntransition_matrix.index.name = 'From State'\ntransition_matrix.columns.name = 'To State'\nstationary_series = pd.Series(stationary_dist, index=transition_matrix.columns, name=\"Stationary Dist.\")\n\n# State 1부터 10까지의 stationary probability\nstationary_series_nonzero = stationary_series.iloc[:]\n\n# Plot\nplt.figure(figsize=(8, 5))\nsns.barplot(\n    x=stationary_series_nonzero.index.astype(str),\n    y=stationary_series_nonzero.values,\n    color='steelblue',\n    edgecolor='black'\n)\n\n# Annotate each bar with its value\nfor i, val in enumerate(stationary_series_nonzero.values):\n    plt.text(i, val + 0.001, f\"{val:.3f}\", ha='center', va='bottom', fontsize=10)\n\n# Plot labels and formatting\nplt.title(\"Long-Run Stationary Distribution by State (Excl. Exit)\", fontsize=14)\nplt.xlabel(\"State (Market Cap Rank)\", fontsize=12)\nplt.ylabel(\"Stationary Probability\", fontsize=12)\nplt.xticks(rotation=0)\nplt.ylim(0, stationary_series_nonzero.max() + 0.01)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe estimated Markov transition matrix and stationary distribution, constructed using only ranked states (States 1–10), reveal a structurally persistent yet more symmetric capital mobility structure than previously observed in open-system models that included State 0. By excluding delisted or unranked firms, this framework focuses on rank-to-rank transitions within the surviving universe of firms and captures a distinct form of intra-market hierarchy.\nHigh Persistence at the Extremes\nThe system continues to exhibit strong persistence at both ends of the distribution. In particular, the top decile (State 10) shows a self-transition probability of 0.972, implying an expected duration of over 36 months. This persistence confirms that once firms enter the top decile, they tend to remain there, reinforcing the notion that the highest tier functions as a quasi-absorbing state.\nSimilarly, the bottom decile (State 1) displays a self-transition probability of 0.931, also associated with prolonged durations. This bilateral persistence at the extremes suggests a structural form of capital stratification, where firms, once classified into the highest or lowest decile, tend to stay put. These two tails behave like boundary layers of the rank system, characterized by entrenchment rather than fluid mobility.\nMobility in the Middle\nIn contrast, the middle states—especially States 4 and 5—display the lowest persistence, each with a diagonal transition probability near 0.79. These intermediate ranks represent the most unstable region of the system, acting as a transitory buffer between the stable top and bottom groups. Firms in these states face greater uncertainty and classification volatility, neither entrenched like the top-tier incumbents nor consistently relegated like small-cap firms.\nThis suggests that mid-cap firms are most exposed to competitive reshuffling. Their trajectories remain open-ended, with material probabilities of upward or downward movement depending on performance shocks or relative repricing, but with limited protection from persistent identity.\nAbsence of Exit/Entry Mechanisms\nIn this formulation, the Markov chain excludes transitions into or out of State 0. As such, delisting and new entries are not explicitly modeled. This simplification defines a pseudo-closed system designed to examine intra-system mobility rather than open-market turnover. The exclusion of exit risk makes the estimated transition probabilities purely conditional on being within the ranked universe, ideal for studying relative capital stickiness but less suited for estimating attrition or survivorship.\nThis design choice removes the asymmetry introduced by non-reversible states such as delisting or IPO-based entry, which would otherwise distort the symmetry of the estimated chain. While this comes at the cost of completeness, it offers clarity in isolating the structural dynamics of persistence, volatility, and mobility within the active market.\nStationary Distribution: Skewed Toward Lower Deciles\nThe estimated stationary distribution across ranked states displays a gradual but consistent decline from lower to higher ranks. State 1 exhibits the highest long-run probability, at approximately 16 percent, while State 10 stabilizes at around 8 percent. This pattern suggests that lower-ranked firms are more prevalent in the long run, at least in terms of count, even though they may not dominate capital or influence.\nTwo forces likely drive this skew: first, a disproportionately high entry frequency into the lower deciles, often from new listings or small restructurings; and second, a limited degree of upward mobility from these initial positions. The bottom deciles, while not formally absorbing, act as a sticky floor—consistently replenished by inflows yet rarely exited by transitions to higher states.\nThe implication is that mobility across ranks is asymmetric even within the active market. Structural regeneration happens predominantly at the bottom, while upward reallocation remains limited. This reinforces the interpretation of the capital market not as a fluid mobility engine, but as a system with hierarchical layers, each marked by different persistence dynamics and long-run outcomes.",
    "crumbs": [
      "Apps",
      "Model",
      "04 Structure"
    ]
  },
  {
    "objectID": "4_4_structure.html#summary-table",
    "href": "4_4_structure.html#summary-table",
    "title": "04 Structure",
    "section": "5 Summary Table",
    "text": "5 Summary Table\n\n\n\n\n\n\n\n\nStructural Channel\nMechanism\nInterpretation\n\n\n\n\nMixture\nReturn as weighted sum of 2 subgroups\nTop decile increasingly drives aggregate performance\n\n\nConvexity\nCapital share as function of rank\nLorenz-type concentration intensifies over time\n\n\nTransition\nMarkov dynamics of rank-state mobility\nTop and Exit states are quasi-absorbing, path-dependent\n\n\n\n\nTop-decile persistence reveals a form of quasi-absorption, where firms not only rise to the top but stay there disproportionately longer — effectively becoming “Too Big to Exit.”\nMiddle-rank volatility suggests structural fragility, with firms unable to stabilize or ascend — reflecting a “Fragile Middle” that lacks lasting competitive foothold.\nLower-rank saturation reflects systemic bottlenecks, where inflows into low deciles are not matched by upward mobility — indicating a “sticky floor” rather than an active ladder.\nThe long-run distribution reinforces this structure: capital is not symmetrically mobile, but increasingly trapped in persistent states. This challenges foundational assumptions in representative-agent models, suggesting instead that capital flows in financial markets obey a hierarchical and inertial logic, not a fluid or ergodic one.",
    "crumbs": [
      "Apps",
      "Model",
      "04 Structure"
    ]
  },
  {
    "objectID": "4_2_data.html",
    "href": "4_2_data.html",
    "title": "02 Data Preprocessing",
    "section": "",
    "text": "This section describes the data used to construct and evaluate the TBTF portfolio strategy. We utilize multiple sources to enable a comprehensive analysis of market structure, return distributions, and benchmark comparisons. The primary dataset is drawn from CRSP, with supplemental reference portfolios and index returns from Fama-French and Yahoo Finance, respectively.",
    "crumbs": [
      "Apps",
      "Model",
      "02 Data Preprocessing"
    ]
  },
  {
    "objectID": "4_2_data.html#primary-data-crsp-monthly-stock-file",
    "href": "4_2_data.html#primary-data-crsp-monthly-stock-file",
    "title": "02 Data Preprocessing",
    "section": "1 Primary Data: CRSP Monthly Stock File",
    "text": "1 Primary Data: CRSP Monthly Stock File\nWe use the Center for Research in Security Prices (CRSP) monthly stock files from January 1996 to December 2023. The CRSP universe includes all listed common stocks on the NYSE, Nasdaq, and AMEX exchanges. Key variables include:\n\nPERMNO: Unique stock identifier\n\nDate: Monthly observation date\n\nMarket Capitalization: Shares outstanding × price\n\nprimaryexch : primary exchange\nSIC Code: Optional industry classification\n\n\n1.1 Market-Cap Based Valuation Perspective\nUnlike fixed-income instruments, public equities derive their economic value primarily from their tradeability. While dividends contribute to realized returns, they are secondary to a stock’s market capitalization path, which reflects its ability to attract and retain capital under uncertainty.\nWe therefore adopt a market cap–based valuation approach, using capitalization time series as the core proxy for equity value and flow dynamics.\n\n\n1.2 Delisting Treatment: Terminal Value = Zero\nWhen a stock is delisted:\n\nIts market capitalization becomes zero, and its final gross return is set to 0, implying net return of –1.\n\n\\[ R_{\\text{gross}} = 0, \\quad R_{\\text{net}} = -1 \\]\nThis reflects the reality that delisted equities become practically untradeable and lose all resale value for investors. In contrast to fixed-income instruments, equities have no residual recovery claim.\n\n\n1.3 IPO Treatment: External Entry State\n\nNewly listed stocks (IPOs) are initially placed in an external state (state 0).\nFrom their first full month after IPO:\n\nTheir market capitalization is ranked within the universe.\nThey are assigned to a decile group (state 1–10) based on market cap percentile.\n\n\n\n\n1.4 Survivorship Bias Adjustment\nTo avoid survivorship bias:\n\nWe retain all stocks, including those that were delisted during the sample period.\nThis prevents the overestimation of returns and misrepresentation of capital mobility that arises from survivor-only datasets.\n\n\n\n1.5 State Classification Framework\nFor transition matrix estimation and mixture modeling (Section 4), we define the following percentile-based state mapping:\n\n\n\n\n\n\n\n\nState\nMarket Cap Percentile Range\nDescription\n\n\n\n\n10\n90%–100%\nTop 10% (largest-cap stocks)\n\n\n9\n80%–90%\n\n\n\n…\n…\n\n\n\n1\n0%–10%\nBottom 10% (smallest-cap stocks)\n\n\n0\nExternal\nDelisted stocks (absorbing state) or IPOs before full inclusion\n\n\n\nThis unconventional but analytically convenient mapping allows for intuitive modeling of:\n\nUpward mobility as a transition to higher state numbers\nCapital lock-in at the top (persistence in State 10)\nExit fragility for lower states (transition to State 0)\n\n\n\n1.6 Structural Implication\nThis mapping enables a semi-absorbing, non-reversible Markov transition model, where:\n\nDelisted stocks enter the absorbing state (0) permanently.\nStocks that remain listed transition among states 1–10, based on their market cap percentile ranks at each time \\(t\\).\nNew entrants (IPOs) begin in state 0 and join the ranked universe from their second month onward.\n\nThis design forms the empirical backbone for transition matrix analysis, mixture decomposition, and long-run mobility asymmetry in Section 4.",
    "crumbs": [
      "Apps",
      "Model",
      "02 Data Preprocessing"
    ]
  },
  {
    "objectID": "4_2_data.html#secondary-data-sources",
    "href": "4_2_data.html#secondary-data-sources",
    "title": "02 Data Preprocessing",
    "section": "2 Secondary Data Sources",
    "text": "2 Secondary Data Sources\n\n2.1 Fama-French Research Portfolios\nFrom the Ken French Data Library, we collect the following datasets:\n\nME10 Decile Portfolios: Used to compare return characteristics between small-cap (s_10) and large-cap (b_10) portfolios.\nME × PRIOR Portfolios: Cross-sorted on size (ME) and prior 12-month return momentum. We focus on:\n\nME5 PRIOR5: Large size, high momentum\nME5 PRIOR3: Mid size, moderate momentum\nME5 PRIOR1: Mid size, low momentum\n\n\nThese portfolios are value-weighted, rebalanced annually using NYSE breakpoints, and provided as monthly returns.\n\n\n2.2 Index and ETF Benchmarks\nTo compare TBTF performance with market-traded investment vehicles, we incorporate the following index and ETF data:\n\nPre-2010 Indices (from Yahoo Finance):\n\n^NDX (Nasdaq 100 Index)\n^DJI (Dow Jones Industrial Average)\n\nPost-2010 ETFs:\n\nVTI: Vanguard Total Stock Market ETF (inception: 2001)\nSPY: SPDR S&P 500 ETF (inception: 1993)\nQQQ: Invesco Nasdaq-100 ETF (inception: 1999)\nDIA: SPDR Dow Jones Industrial Average ETF (inception: 1998)\n\n\nThe inclusion of these instruments enables direct comparison with passive investment strategies commonly available to retail and institutional investors.",
    "crumbs": [
      "Apps",
      "Model",
      "02 Data Preprocessing"
    ]
  },
  {
    "objectID": "4_2_data.html#appendix",
    "href": "4_2_data.html#appendix",
    "title": "02 Data Preprocessing",
    "section": "3 Appendix",
    "text": "3 Appendix\n\n\n3.1 Data Availability and Frequency\n\n\n\n\n\n\n\n\n\nDataset\nRaw data Frequency\nPeriod Covered\nSource\n\n\n\n\nFama-French ME Decile\nMonthly\n1963–2023\nKen French Data Library\n\n\nCRSP Monthly Stock File\nMonthly\n1996–2023\nWRDS/CRSP\n\n\nFama-French ME × PRIOR\nMonthly\n2010–2023\nKen French Data Library\n\n\nDIA, QQQ, SPY, VTI (ETFs)\nDaily\n2010–2023\nYahoo Finance\n\n\nDJIA, NDX (Price Indices)\nDaily\n1996–2009\nYahoo Finance\n\n\n\n\nPooled Panel data\n\nFama-French data library\nFRED database\n\nPanel data\n\nCRSP from WRDS\nYahoo Finance",
    "crumbs": [
      "Apps",
      "Model",
      "02 Data Preprocessing"
    ]
  },
  {
    "objectID": "3_3_history_survivorship.html",
    "href": "3_3_history_survivorship.html",
    "title": "Equity Lifecycle",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\n\nimport sqlite3\ncon = sqlite3.connect(database=\"../../tbtf.sqlite\")\n\ncrsp = pd.read_sql_query(\n  sql=\"SELECT * FROM crsp\",\n  con=con,\n  parse_dates={\"date\"}\n)\n\nimport sys\nimport os\n# 현재 경로 기준으로 상위 디렉토리로 경로 추가\nsys.path.append(os.path.abspath('../..'))\n\nimport tbtf_data\n\nff3 = tbtf_data.load_ff3_factors(start_date=crsp['date'].min(), end_date=crsp['date'].max())\ncrsp_trunc = tbtf_data.crsp_truncation(crsp,ff3)\ncrsp_full = tbtf_data.crsp_lifecycle(crsp_trunc)\n\nprint(crsp_full.head())\n\n\n   permno       date     mktcap       ret primaryexch  siccd   industry  \\\n0   10001 1996-01-31  20.814125 -0.026667           Q   4925  Utilities   \n1   10001 1996-02-29  21.099250  0.013699           Q   4925  Utilities   \n2   10001 1996-03-31  21.899480  0.036423           Q   4925  Utilities   \n3   10001 1996-04-30  20.348063 -0.070840           Q   4925  Utilities   \n4   10001 1996-05-31  19.915125 -0.021277           Q   4925  Utilities   \n\n   mktcap_lag  ret_excess  state  state_lag   lifetype  life  \n0   21.384375   -0.030967      3          0  old-death   260  \n1   20.814125    0.009799      3          3  old-death   260  \n2   21.099250    0.032523      3          3  old-death   260  \n3   21.899480   -0.075440      2          3  old-death   260  \n4   20.348063   -0.025477      2          2  old-death   260",
    "crumbs": [
      "Apps",
      "History",
      "Equity Lifecycle"
    ]
  },
  {
    "objectID": "3_3_history_survivorship.html#data-pre-processing",
    "href": "3_3_history_survivorship.html#data-pre-processing",
    "title": "Equity Lifecycle",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\n\nimport sqlite3\ncon = sqlite3.connect(database=\"../../tbtf.sqlite\")\n\ncrsp = pd.read_sql_query(\n  sql=\"SELECT * FROM crsp\",\n  con=con,\n  parse_dates={\"date\"}\n)\n\nimport sys\nimport os\n# 현재 경로 기준으로 상위 디렉토리로 경로 추가\nsys.path.append(os.path.abspath('../..'))\n\nimport tbtf_data\n\nff3 = tbtf_data.load_ff3_factors(start_date=crsp['date'].min(), end_date=crsp['date'].max())\ncrsp_trunc = tbtf_data.crsp_truncation(crsp,ff3)\ncrsp_full = tbtf_data.crsp_lifecycle(crsp_trunc)\n\nprint(crsp_full.head())\n\n\n   permno       date     mktcap       ret primaryexch  siccd   industry  \\\n0   10001 1996-01-31  20.814125 -0.026667           Q   4925  Utilities   \n1   10001 1996-02-29  21.099250  0.013699           Q   4925  Utilities   \n2   10001 1996-03-31  21.899480  0.036423           Q   4925  Utilities   \n3   10001 1996-04-30  20.348063 -0.070840           Q   4925  Utilities   \n4   10001 1996-05-31  19.915125 -0.021277           Q   4925  Utilities   \n\n   mktcap_lag  ret_excess  state  state_lag   lifetype  life  \n0   21.384375   -0.030967      3          0  old-death   260  \n1   20.814125    0.009799      3          3  old-death   260  \n2   21.099250    0.032523      3          3  old-death   260  \n3   21.899480   -0.075440      2          3  old-death   260  \n4   20.348063   -0.025477      2          2  old-death   260",
    "crumbs": [
      "Apps",
      "History",
      "Equity Lifecycle"
    ]
  },
  {
    "objectID": "3_3_history_survivorship.html#life-type-of-firms",
    "href": "3_3_history_survivorship.html#life-type-of-firms",
    "title": "Equity Lifecycle",
    "section": "2 Life Type of Firms",
    "text": "2 Life Type of Firms\n\n\nCode\n# crsp_full[crsp_full['permno']==90848] \n# permno, 제대로된 lifetype, 현재 lifetype, 현재 life, 입력 crsp_trunc 레코드가 2개 존재함\n# 30330, old-death, lifetype=old-death,life=2, first record date = sample_start\n# 14252, shell, lifetype= normal, life=2, sample_start &lt; first record date &lt; sample_end\n# 13010, shell, lifetype= normal, life=2, sample_start &lt; first record date &lt; sample_end\n# 90848, shell, lifetype= normal, life=2, sample_start &lt; first record date &lt; sample_end\n# Refco Inc, 90848\n\n# permno, 제대로된 lifetype, 현재 lifetype, 현재 life, 이 경우는 입력 crsp_trunc 레코드가 1개만 존재함\n# 15139, young-living, lifetype= shell, life=1, first record date = sample_end\n# 16815\n# crsp[crsp['permno']==14475] # Super Micro Computer (SMCI)라는 회사만 2019 상장폐지 후 2020년 재상장시 Permno (14475)가 유지되었다\n\nresult = crsp_full.groupby('lifetype')['permno'].nunique()\nprint(result)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Group by permno to get unique records for plotting (1 row per permno)\npermno_summary = (\n    crsp_full.sort_values(\"date\")\n    .groupby(\"permno\", as_index=False)\n    .first()\n    [[\"permno\", \"industry\", \"lifetype\", \"life\", \"state\"]]\n)\n\n\nlifetype\nnormal          5860\nold-death       5882\nold-living      1021\nshell             10\nyoung-living    3000\nName: permno, dtype: int64",
    "crumbs": [
      "Apps",
      "History",
      "Equity Lifecycle"
    ]
  },
  {
    "objectID": "3_3_history_survivorship.html#life-distribution-for-each-lifetype",
    "href": "3_3_history_survivorship.html#life-distribution-for-each-lifetype",
    "title": "Equity Lifecycle",
    "section": "3 Life Distribution for Each Lifetype",
    "text": "3 Life Distribution for Each Lifetype\n\n\nCode\n# FacetGrid 생성\ng = sns.FacetGrid(permno_summary, col=\"industry\", col_wrap=4, height=4, sharex=False, sharey=False)\n\n# industry 이름 리스트\nindustry_names = g.col_names\n\n# 각 subplot에 대해 데이터 분할 및 그리기\nfor ax, industry in zip(g.axes.flat, industry_names):\n    data = permno_summary[permno_summary['industry'] == industry]\n    for lifetype, lifetype_data in data.groupby('lifetype'):\n        ax.hist(lifetype_data['life'], bins=20, alpha=0.7, label=lifetype)\n    ax.set_title(industry)\n    ax.legend()\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Apps",
      "History",
      "Equity Lifecycle"
    ]
  },
  {
    "objectID": "3_3_history_survivorship.html#firm-counts-per-industry-for-each-lifetype",
    "href": "3_3_history_survivorship.html#firm-counts-per-industry-for-each-lifetype",
    "title": "Equity Lifecycle",
    "section": "4 Firm counts per Industry for Each Lifetype",
    "text": "4 Firm counts per Industry for Each Lifetype\n\n\nCode\n# 주요 lifetype만 사용\nmain_lifetypes = ['old-living', 'old-death', 'young-living', 'normal']\nfiltered_data = permno_summary[permno_summary['lifetype'].isin(main_lifetypes)]\n\n# industry 순서: old-death 기준\nindustry_order = (\n    filtered_data[filtered_data['lifetype'] == 'old-death']['industry']\n    .value_counts()\n    .index.tolist()\n)\n\n# 전체 industry 목록 확보\nall_industries = filtered_data['industry'].unique().tolist()\n\n# industry에 대한 색상 팔레트 생성\npalette_colors = sns.color_palette(\"tab20\", len(all_industries))\npalette = dict(zip(all_industries, palette_colors))\n\n# y축 최대값: old-death 기준\ny_max = (\n    filtered_data[filtered_data['lifetype'] == 'old-death']['industry']\n    .value_counts()\n    .max()\n)\n\n# 서브플롯 설정\nfig, axes = plt.subplots(2, 2, figsize=(16, 10))\naxes = axes.flatten()\n\nfor i, lifetype in enumerate(main_lifetypes):\n    ax = axes[i]\n    subset = filtered_data[filtered_data['lifetype'] == lifetype]\n    sns.countplot(\n        data=subset,\n        x='industry',\n        hue='industry',\n        order=industry_order,\n        palette=palette,\n        ax=ax\n    )\n    ax.set_title(f\"Firm counts by Industry for Lifetype={lifetype}\")\n    ax.set_ylabel(\"Count\")\n    ax.set_ylim(0, y_max + 5)\n    ax.tick_params(axis='x', rotation=45)\n    ax.set_xlabel(\"Industry\")\n\n# 여분 subplot 제거\nfor j in range(len(main_lifetypes), len(axes)):\n    fig.delaxes(axes[j])\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Apps",
      "History",
      "Equity Lifecycle"
    ]
  },
  {
    "objectID": "3_3_history_survivorship.html#newly-listed-firms-per-initial-decile-by-industry",
    "href": "3_3_history_survivorship.html#newly-listed-firms-per-initial-decile-by-industry",
    "title": "Equity Lifecycle",
    "section": "5 Newly listed Firms per Initial Decile by Industry",
    "text": "5 Newly listed Firms per Initial Decile by Industry\n\n\nCode\n# in-sample 신규 상장된 permno\nin_sample_types = ['normal', 'young-living', 'shell']\nin_sample_df = permno_summary[permno_summary['lifetype'].isin(in_sample_types)]\n\n# 산업 순서 정의\nindustry_order = ['Manufacturing', 'Finance', 'Services', 'Retail', 'Transportation',\n                  'Mining', 'Wholesale', 'Utilities', 'Construction', 'Agriculture',\n                  'Public', 'Missing']\n\n# subplot 설정\nn_cols = 4\nn_rows = (len(industry_order) + n_cols - 1) // n_cols\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 10), sharey=True)\naxes = axes.flatten()\n\nfor i, industry in enumerate(industry_order):\n    ax = axes[i]\n    subset = in_sample_df[in_sample_df['industry'] == industry]\n    if subset.empty:\n        ax.set_title(industry)\n        ax.axis('off')\n        continue\n    sns.countplot(data=subset, x='state', ax=ax, order=sorted(subset['state'].dropna().unique()), color='skyblue')\n    ax.set_title(industry)\n    ax.set_xlabel('Initial Decile')\n    ax.set_ylabel('Permno Counts')\n    ax.tick_params(axis='x', rotation=0)\n\n# 남는 subplot 제거\nfor j in range(i + 1, len(axes)):\n    fig.delaxes(axes[j])\n\nfig.suptitle(\"Newly listed Firms per Initial Decile by Industry\", fontsize=16)\nfig.tight_layout(rect=[0, 0, 1, 0.96])\nplt.show()",
    "crumbs": [
      "Apps",
      "History",
      "Equity Lifecycle"
    ]
  },
  {
    "objectID": "3_3_history_survivorship.html#life-per-of-initial-decile-by-industry-full-cycle-firms",
    "href": "3_3_history_survivorship.html#life-per-of-initial-decile-by-industry-full-cycle-firms",
    "title": "Equity Lifecycle",
    "section": "6 Life per of Initial Decile by Industry (Full cycle Firms)",
    "text": "6 Life per of Initial Decile by Industry (Full cycle Firms)\n\n\nCode\n# 필터링: lifetype이 'normal'인 경우만\nnormal_df = permno_summary[permno_summary['lifetype'] == 'normal'].copy()\n\n# 산업 순서 지정\nindustry_order = ['Manufacturing', 'Finance', 'Services', 'Retail', 'Transportation', 'Mining',\n                  'Wholesale', 'Utilities', 'Construction', 'Agriculture', 'Public', 'Missing']\n\n# state와 life의 관계를 박스플롯으로 각 industry별 subplot 생성\nn_cols = 4\nn_rows = 3\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 10), sharey=True)\n\nfor ax, industry in zip(axes.flat, industry_order):\n    data = normal_df[normal_df['industry'] == industry]\n    if not data.empty and data['state'].nunique() &gt; 0:\n        sns.boxplot(data=data, x='state', y='life', ax=ax, order=sorted(data['state'].unique()))\n        ax.set_title(industry)\n    else:\n        ax.set_visible(False)\n\n    ax.set_xlabel('Initial Decile')\n    ax.set_ylabel('Life (Months)')\n\n# 빈 subplot 제거\nfor j in range(len(industry_order), len(axes.flat)):\n    fig.delaxes(axes.flat[j])\n\nfig.suptitle(\"Life per of Initial Decile by Industry (Lifetype=Normal)\", fontsize=16)\nplt.tight_layout(rect=[0, 0, 1, 0.96])\nplt.show()",
    "crumbs": [
      "Apps",
      "History",
      "Equity Lifecycle"
    ]
  },
  {
    "objectID": "3_3_history_survivorship.html#median-life-per-of-initial-decile-by-industry-full-lifecycle-firms",
    "href": "3_3_history_survivorship.html#median-life-per-of-initial-decile-by-industry-full-lifecycle-firms",
    "title": "Equity Lifecycle",
    "section": "7 Median Life per of Initial Decile by Industry (Full lifecycle Firms)",
    "text": "7 Median Life per of Initial Decile by Industry (Full lifecycle Firms)\n\n\nCode\n# lifetype이 'normal'인 permno만 필터링\nnormal_df = permno_summary[permno_summary['lifetype'] == 'normal'].copy()\n\n# subplot 순서 지정\nindustry_order = ['Manufacturing', 'Finance', 'Services', 'Retail', 'Transportation',\n                  'Mining', 'Wholesale', 'Utilities', 'Construction',\n                  'Agriculture', 'Public', 'Missing']\n\n# 산업과 state별로 median life 계산\ngrouped = normal_df.groupby(['industry', 'state'])['life'].median().reset_index()\n\n# plotting\nn_cols = 4\nn_rows = 3\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 10), sharey=True)\naxes = axes.flatten()\n\nfor i, industry in enumerate(industry_order):\n    ax = axes[i]\n    data = grouped[grouped['industry'] == industry]\n    if not data.empty:\n        sns.barplot(data=data, x='state', y='life', hue='state', ax=ax, palette='Blues_d', legend=False)\n        ax.set_title(industry)\n        ax.set_xlabel(\"Initial Decile\")\n        ax.set_ylabel(\"Median Life (Month)\")\n    else:\n        ax.set_visible(False)\n\nfig.suptitle(\"Median Life per Initial State by Industry (Lifetype=Normal)\", fontsize=16)\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Apps",
      "History",
      "Equity Lifecycle"
    ]
  },
  {
    "objectID": "3_3_history_survivorship.html#the-number-of-continuously-alive-firms-per-initial-state-by-industry-lifetype-old-living",
    "href": "3_3_history_survivorship.html#the-number-of-continuously-alive-firms-per-initial-state-by-industry-lifetype-old-living",
    "title": "Equity Lifecycle",
    "section": "8 The number of “continuously alive” firms per Initial State by Industry (lifetype = Old-Living)",
    "text": "8 The number of “continuously alive” firms per Initial State by Industry (lifetype = Old-Living)\n\n\nCode\n# Filter only 'old-living' lifetype\nold_living_data = permno_summary[permno_summary['lifetype'] == 'old-living']\n\n# Define industry order\nindustry_order = [\n    \"Manufacturing\", \"Finance\", \"Services\", \"Retail\", \"Transportation\",\n    \"Mining\", \"Wholesale\", \"Utilities\", \"Construction\",\n    \"Agriculture\", \"Public\", \"Missing\"\n]\n\n# Set up the subplot grid\nfig, axes = plt.subplots(3, 4, figsize=(16, 10), sharey=True)\naxes = axes.flatten()\n\nfor i, industry in enumerate(industry_order):\n    ax = axes[i]\n    data = old_living_data[old_living_data[\"industry\"] == industry]\n    if not data.empty:\n        state_counts = data[\"state\"].value_counts().sort_index()\n        bar_data = pd.DataFrame({\"state\": state_counts.index, \"count\": state_counts.values})\n        sns.barplot(data=bar_data, x=\"state\", y=\"count\", ax=ax, color=\"skyblue\", order=sorted(bar_data[\"state\"].unique()))\n    ax.set_title(industry)\n    ax.set_xlabel(\"Initial Decile\")\n    ax.set_ylabel(\"Permno Count\")\n    ax.set_xticks(range(10))\n    ax.set_xticklabels(range(1, 11))\n\n# Remove any unused subplots\nfor j in range(i + 1, len(axes)):\n    fig.delaxes(axes[j])\n\nplt.suptitle(\"Permno Count per Initial State By Industry (lifetype = Old-Living)\", fontsize=16)\nplt.tight_layout(rect=[0, 0, 1, 0.96])\nplt.show()",
    "crumbs": [
      "Apps",
      "History",
      "Equity Lifecycle"
    ]
  },
  {
    "objectID": "3_3_history_survivorship.html#the-number-of-1-month-shell-firms-per-initial-state-by-industry-lifetype-shell",
    "href": "3_3_history_survivorship.html#the-number-of-1-month-shell-firms-per-initial-state-by-industry-lifetype-shell",
    "title": "Equity Lifecycle",
    "section": "9 The number of “1-month shell” firms per Initial State by Industry (lifetype = shell)",
    "text": "9 The number of “1-month shell” firms per Initial State by Industry (lifetype = shell)\n\n\nCode\n# Filter only 'shell' lifetype\nshell_data = permno_summary[permno_summary['lifetype'] == 'shell'].copy()\n\n# Define industry order\nindustry_order = [\n    \"Manufacturing\", \"Finance\", \"Services\", \"Retail\", \"Transportation\",\n    \"Mining\", \"Wholesale\", \"Utilities\", \"Construction\",\n    \"Agriculture\", \"Public\", \"Missing\"\n]\n\n# Set up the subplot grid\nfig, axes = plt.subplots(3, 4, figsize=(16, 10), sharey=True)\naxes = axes.flatten()\n\nfor i, industry in enumerate(industry_order):\n    ax = axes[i]\n    data = shell_data[shell_data[\"industry\"] == industry]\n    if not data.empty:\n        # Count state values and convert to sorted DataFrame\n        bar_data = (\n            data.groupby(\"state\")\n            .size()\n            .reindex(range(1, 11), fill_value=0)\n            .reset_index(name=\"count\")\n        )\n        sns.barplot(data=bar_data, x=\"state\", y=\"count\", ax=ax, color=\"skyblue\")\n        ax.set_xticks(range(0, 10))\n        ax.set_xticklabels(range(1, 11))\n    ax.set_title(industry)\n    ax.set_xlabel(\"Initial Decile\")\n    ax.set_ylabel(\"Permno Count\")\n\n# Remove any unused subplots\nfor j in range(i + 1, len(axes)):\n    fig.delaxes(axes[j])\n\nplt.suptitle(\"Permno Count per Initial State by Industry (lifetype = Shell)\", fontsize=16)\nplt.tight_layout(rect=[0, 0, 1, 0.96])\nplt.show()",
    "crumbs": [
      "Apps",
      "History",
      "Equity Lifecycle"
    ]
  },
  {
    "objectID": "3_1_history.html",
    "href": "3_1_history.html",
    "title": "Recent History",
    "section": "",
    "text": "This study defines January 1, 2010 as a structural break in the evolution of U.S. financial markets. This point marks a transition from a period of crisis-driven volatility and deregulation to an era characterized by policy-dominated asset inflation, ETF proliferation, and asymmetric capital flows. By segmenting the data at this breakpoint, we aim to assess how financial market performance and capital concentration evolved across two distinct regimes.\nThe dataset spans January 1996 to December 2023, with a monthly frequency. It is segmented as follows:\n\nPre-2010: 1996–2009\n\nPost-2010: 2010–2023\n\nThe segmentation provides a clean empirical framework to evaluate the impact of structural policy interventions and evolving capital dynamics.",
    "crumbs": [
      "Apps",
      "History",
      "Recent History"
    ]
  },
  {
    "objectID": "3_1_history.html#as-a-structural-break",
    "href": "3_1_history.html#as-a-structural-break",
    "title": "Recent History",
    "section": "",
    "text": "This study defines January 1, 2010 as a structural break in the evolution of U.S. financial markets. This point marks a transition from a period of crisis-driven volatility and deregulation to an era characterized by policy-dominated asset inflation, ETF proliferation, and asymmetric capital flows. By segmenting the data at this breakpoint, we aim to assess how financial market performance and capital concentration evolved across two distinct regimes.\nThe dataset spans January 1996 to December 2023, with a monthly frequency. It is segmented as follows:\n\nPre-2010: 1996–2009\n\nPost-2010: 2010–2023\n\nThe segmentation provides a clean empirical framework to evaluate the impact of structural policy interventions and evolving capital dynamics.",
    "crumbs": [
      "Apps",
      "History",
      "Recent History"
    ]
  },
  {
    "objectID": "3_1_history.html#historical-context-of-each-period",
    "href": "3_1_history.html#historical-context-of-each-period",
    "title": "Recent History",
    "section": "2 Historical Context of Each Period",
    "text": "2 Historical Context of Each Period\n\n2.1 Pre-2010: Volatility, Deregulation, and Crisis\nThe pre-2010 period includes several global shocks and deregulation episodes:\n\n1997 Asian Financial Crisis\nDot-com Bubble and Crash (2000–2001)\n9/11 Terror Attacks (2001)\nRepeal of Glass-Steagall (1999), which contributed to systemic risk buildup\nGlobal Financial Crisis (2008–2009), culminating in the collapse of Lehman Brothers and the implementation of TARP\n\nThis was an era of market discontinuity, high volatility, and risk repricing, where asset allocation was still driven largely by fundamentals and discretionary investing.\n\n\n2.2 Post-2010: QE Regime and Capital Lock-In\nIn contrast, the post-2010 era reflects a policy-driven financial environment, where capital markets were shaped by:\n\nQuantitative Easing (QE1, QE2, QE3)\nPassive flows via ETF proliferation\nTechnological monopolization\nCOVID-19 pandemic and unprecedented stimulus\nAI-fueled equity valuations post-2023\n\nThese changes have amplified top-heavy capital concentration and reinforced inertia in market leadership. Importantly, they enabled the survival of inefficient firms at the lower end (zombie firms), distorting the small-cap return distribution.\n\n\n2.3 US Top 10 Stability: Empirical Snapshot\nAn empirical motivation for the TBTF strategy lies in the historical persistence of top-cap firms. From 2012 to 2023, only 27 unique PERMNOs appeared in the top 10 by market cap. These include:\n\nAAPL, MSFT, AMZN, GOOG, META, NVDA, TSLA\nTraditional blue chips like XOM, JNJ, WMT, JPM, BAC\nHealthcare and consumer giants like UNH, LLY, PG, KO\nFinancials and telcos like BRK, MA, V, T, VZ\n\nThis concentration implies that capital has become locked into a narrow subset of firms, with little room for new entrants—an observation with profound implications for allocative efficiency and market competition.\n\n\n2.4 Global Top 10 Market-Cap Stocks\n\nU.S. equities constitute nearly half of global equity market capitalization.\nThe dominance of U.S. firms in global equity rankings",
    "crumbs": [
      "Apps",
      "History",
      "Recent History"
    ]
  },
  {
    "objectID": "3_1_history.html#market-adaptation",
    "href": "3_1_history.html#market-adaptation",
    "title": "Recent History",
    "section": "4 Market Adaptation",
    "text": "4 Market Adaptation\n\nWhy did ETF flows accelerate toward market-cap-weighted indices after 2010?\nWhy did the largest firms begin to dominate both flows and returns?\nWhy did the size premium—long treated as a cross-sectional regularity—reverse so abruptly?\nWhy did the TBTF strategy begin to outperform so persistently after 2010?\n\nThese are not merely empirical anomalies. They signal a deeper structural transformation in how modern capital markets internalize crisis, encode institutional memory, and reshape investment behavior. The post-2008 financial order did not simply recalibrate asset valuations—it rewrote the underlying logic by which survivability, scale, and systemic legitimacy are recognized and rewarded.\nIn the years following the crisis, capital allocation in U.S. markets underwent a subtle but profound transition. This shift was not driven by changes in earnings expectations or interest rate paths alone. Rather, it reflected a deeper reconfiguration in the way investors interpreted institutional risk, firm durability, and long-term capital positioning. At the center of this shift was a redefinition of duration—no longer as sensitivity to discount rates, but as a heuristic for projected survival within the investment ecosystem.\nTraditionally, equity duration refers to the maturity-weighted sensitivity of expected cash flows to discount rate changes—a core concept in valuation models. Yet post-crisis dynamics revealed that investors began treating duration less as a metric of interest rate exposure, and more as a proxy for institutional endurance. The relevant question was no longer “What is this firm worth?” but “Will this firm continue to exist in future index compositions?” In effect, duration became behavioral: an estimate of structural permanence rather than financial maturity.\nThis behavioral shift was not irrational. Rather, it reflects a regime-level Bayesian updating process. The coordinated policy response to the crisis—Federal Reserve liquidity facilities, bailouts, TARP, and subsequent QE programs—signaled to investors that size now implied protection. Survival was no longer a function of profitability or innovation, but of systemic embeddedness. Market participants inferred a new asymmetry: certain firms, by virtue of their scale, had become too institutionalized to fail.\nETFs and other passive vehicles further reinforced this inference. Because index-linked strategies allocate capital based on lagged market capitalization, persistence at the top of the distribution became a structural input to future flows. This generated a recursive loop: survival secured inclusion; inclusion secured flows; flows secured survival. This feedback mechanism did not reward expected productivity, but rather historical presence and structural resilience. As such, we observe the emergence of a duration-weighted survival premium, distinct from classical notions of risk premia.\nIn this context, the TBTF strategy is not a market inefficiency but an adaptation to structure. It is a behavioral rationalization of post-crisis inference: capital flows to where failure is structurally unlikely. Outperformance no longer results from superior stock-picking, but from alignment with the attractor dynamics of institutional permanence.\nTraditional pricing models—CAPM, APT, and size/value factor models—assume frictionless reallocation, homogenous expectations, and competitive turnover. But these assumptions collapse in a market where survivability itself becomes the core priced asset. Post-2010 capital flows reveal a system where market cap is not the outcome of valuation—it is a reflexive signal of duration legitimacy.\nThus, the post-2008 period should not merely be interpreted as the era of monetary intervention or ETF proliferation. It marks the onset of a new selection regime, where duration displaces alpha, and continuity substitutes for expected upside. The rational investor, under this structure, does not seek transitory outperformance but perpetual investability.\n\nValuation shifted from pricing expected return to pricing expected duration.\nNot, “How much will this firm return?”\nBut, “How long will this firm remain investable?”",
    "crumbs": [
      "Apps",
      "History",
      "Recent History"
    ]
  },
  {
    "objectID": "3_1_history.html#the-rise-of-duration-based-capital-allocation",
    "href": "3_1_history.html#the-rise-of-duration-based-capital-allocation",
    "title": "Recent History",
    "section": "5 The Rise of Duration-Based Capital Allocation",
    "text": "5 The Rise of Duration-Based Capital Allocation\nFollowing the 2008 financial crisis, investor behavior in U.S. equity markets underwent a structural reorientation. Classical asset pricing frameworks—rooted in equilibrium assumptions and short-term risk–return optimization—became increasingly insufficient to account for new dynamics in capital flows, including the surge of ETF investments into large-cap indices, the reversal of the size premium, and the persistent outperformance of top-capitalization portfolios. These developments suggest a shift in the decision-making framework: from optimizing short-term efficiency to navigating long-term survivability under asymmetric institutional conditions.\nThis behavioral realignment reflects what can be described as long-horizon structure learning. The extraordinary scale and nature of post-crisis interventions—ranging from quantitative easing to implicit guarantees for systemically important institutions—prompted a regime-level updating of expectations. Rather than adjusting beliefs about firm-specific cash flows or macroeconomic risk premia, investors internalized a more abstract signal: that scale now conferred structural protection. Market participants inferred that certain firms would persist not because they outperformed on fundamentals, but because their collapse would be institutionally intolerable.\nIn this context, the conventional concept of equity duration—defined in discounted cash flow models as the maturity-weighted average of future earnings—underwent a conceptual transformation. It evolved into a proxy for institutional persistence. Post-2010 flows into passive vehicles, particularly market-cap-weighted ETFs, reflected not only a belief in informational efficiency but also a structural inference: that survival, rather than performance, had become the key determinant of long-run valuation. Allocation decisions became reflexive—past index inclusion predicted future capital inflows, and those flows further entrenched the firm’s systemic position. TBTF firms thus ceased to be merely large; they became narrative fixtures, anchoring investor expectations within an uncertain regime.\nThis adaptive behavior is not readily explained by conventional cognitive biases. Instead, it aligns more closely with evolutionary theories of decision-making. Under uncertainty, agents are better served by favoring persistent structures over transient signals. Humans are cognitively predisposed to detect long-run patterns, assign value to endurance, and avoid volatility through structural heuristics. Investors, acting within this framework, did not merely chase safety—they responded to embedded institutional signals of continuity. What might appear as herding is, in this view, an expression of rationality calibrated to survival under systemic constraint.\nSuch a structural reading of investor behavior challenges the core assumptions of many foundational asset pricing models. The CAPM and APT, for example, presuppose a world of frictionless adjustment, symmetric information, and rational expectations over marginal return distributions. Yet, the sustained outperformance of TBTF strategies post-2010 suggests that markets do not allocate capital solely based on marginal risk–return tradeoffs. Instead, they increasingly reward assets positioned to endure within structurally reinforced hierarchies. Similarly, factor models built on cross-sectional dispersion now conflate fundamental characteristics with recursive index mechanics, institutional design, and regulatory asymmetries.\nIn this regime, equilibrium is no longer a normative ideal. It is a structural attractor—an emergent state stabilized not by efficiency, but by feedback loops between market cap, passive allocation, and policy memory. The system does not reward productivity; it rewards persistence. What emerges is a revised empirical hypothesis: that capital flows in the post-crisis era reflect beliefs about institutional durability and regime resilience, rather than expectations about marginal outperformance.\nThis shift calls for new forms of empirical validation. Tests might include analyzing the sensitivity of ETF flows to prior survivorship during crisis periods, modeling index-induced path dependence in return generation, or tracing the co-evolution of firm rank and capital allocation through recursive reinforcement. Such analyses would illuminate the degree to which capital now flows according to expectations of structural durability, rather than redistributive efficiency.\nUltimately, TBTF strategies do not function as classical arbitrage mechanisms. They align with the evolving architecture of financial survivability. In this context, market capitalization is no longer an output of valuation—it is a recursive input to it. Passive investment vehicles do not passively mirror the market—they help manufacture it. Investors are not merely optimizing—they are adapting to a stratified topology. The assets that persist are not necessarily the most innovative or productive, but the most embedded. What drives capital allocation today is not short-term alpha, but long-run legitimacy.",
    "crumbs": [
      "Apps",
      "History",
      "Recent History"
    ]
  },
  {
    "objectID": "3_1_history.html#appendix",
    "href": "3_1_history.html#appendix",
    "title": "Recent History",
    "section": "3 Appendix",
    "text": "3 Appendix\n\n\n3.1 A. Key Financial and Policy Events by Period\n\n3.1.1 Pre-2010 (1996–2009)\n\n\n\n\n\n\n\n\nDate\nEvent & Description\nImportance\n\n\n\n\n1997-07-02\nAsian Financial Crisis\nVery High\n\n\n1999-01-01\nEuro launched\nHigh\n\n\n1999-11-12\nRepeal of Glass-Steagall\nHigh\n\n\n2000–2001\nDot-com Bubble & Crash\nVery High\n\n\n2001-09-11\n9/11 Attacks\nVery High\n\n\n2008-09-15\nLehman Brothers Collapse\nVery High\n\n\n2008-10-03\nTARP ($700B bailout)\nVery High\n\n\n\n\n\n3.1.2 Post-2010 (2010–2023)\n\n\n\n\n\n\n\n\nDate\nEvent & Description\nImportance\n\n\n\n\n2010–2015\nQE1, QE2, QE3 Implementation & Taper\nVery High\n\n\n2018-01-01\nU.S.–China Trade Tensions\nHigh\n\n\n2020-03-11\nCOVID-19 Declared (Policy + Liquidity Response)\nVery High\n\n\n2023–Present\nRise of AI in Markets (NLP, LLMs)\nHigh",
    "crumbs": [
      "Apps",
      "History",
      "Recent History"
    ]
  },
  {
    "objectID": "1_intro.html",
    "href": "1_intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Over the past two decades, U.S. financial markets have undergone profound structural transformations. These shifts—accelerated by post-2008 Federal Reserve interventions, the rise of passive investing, and the concentration of capital among a few mega-cap firms—have undermined core assumptions of neoclassical asset pricing models. The long-held tenets of global convexity, representative agents, and arbitrage-free pricing increasingly diverge from empirical realities.\nModern capital markets no longer reflect a system in which risk is efficiently priced and capital is dynamically allocated. Instead, capital appears to flow persistently toward a small subset of dominant firms—not due to marginal productivity or higher expected returns, but through network effects, index-induced flows, and macro-policy distortions. These dynamics are more consistent with rent extraction than with equilibrium-based compensation for risk.",
    "crumbs": [
      "Apps",
      "Introduction"
    ]
  },
  {
    "objectID": "1_intro.html#structural-shifts-in-capital-markets",
    "href": "1_intro.html#structural-shifts-in-capital-markets",
    "title": "Introduction",
    "section": "",
    "text": "Over the past two decades, U.S. financial markets have undergone profound structural transformations. These shifts—accelerated by post-2008 Federal Reserve interventions, the rise of passive investing, and the concentration of capital among a few mega-cap firms—have undermined core assumptions of neoclassical asset pricing models. The long-held tenets of global convexity, representative agents, and arbitrage-free pricing increasingly diverge from empirical realities.\nModern capital markets no longer reflect a system in which risk is efficiently priced and capital is dynamically allocated. Instead, capital appears to flow persistently toward a small subset of dominant firms—not due to marginal productivity or higher expected returns, but through network effects, index-induced flows, and macro-policy distortions. These dynamics are more consistent with rent extraction than with equilibrium-based compensation for risk.",
    "crumbs": [
      "Apps",
      "Introduction"
    ]
  },
  {
    "objectID": "1_intro.html#from-risk-pricing-to-rent-extraction",
    "href": "1_intro.html#from-risk-pricing-to-rent-extraction",
    "title": "Introduction",
    "section": "2 From Risk Pricing to Rent Extraction",
    "text": "2 From Risk Pricing to Rent Extraction\nThis paper challenges the foundational belief that financial markets reward systematic risk-bearing. Instead, we argue that the post-crisis financial ecosystem has evolved into a system of rank-based rent allocation, where a handful of firms absorb disproportionate capital flows due to structural advantages embedded in the market itself.\nWe empirically test whether U.S. stock return distributions now resemble a time-varying mixture of heterogeneous capital states under arbitrage limits, rather than a homogeneous no-arbitrage equilibrium. The evidence points to a market governed by path-dependent capital lock-in, asymmetric mobility, and structural concentration—a dynamic far removed from the efficient frontier.",
    "crumbs": [
      "Apps",
      "Introduction"
    ]
  },
  {
    "objectID": "1_intro.html#stylized-facts-and-empirical-hypotheses",
    "href": "1_intro.html#stylized-facts-and-empirical-hypotheses",
    "title": "Introduction",
    "section": "3 Stylized Facts and Empirical Hypotheses",
    "text": "3 Stylized Facts and Empirical Hypotheses\nThis inquiry is guided by the following empirical regularities:\n\nReturn Distributions have become increasingly asymmetric and heavy-tailed, especially among top-cap stocks.\nCapital Concentration is extreme, persistent, and exhibits rank-lock dynamics.\nTransition Probabilities across market-cap quantiles show declining upward mobility and growing absorption at the top.\nMarket Efficiency is eroded by polarization, in which capital accumulates at the top while smaller firms stagnate or survive artificially.\nPolicy and Index Flows (via QE and ETFs) act as systemic amplifiers of these trends, reinforcing incumbency and reducing entropy in allocation.",
    "crumbs": [
      "Apps",
      "Introduction"
    ]
  },
  {
    "objectID": "1_intro.html#the-dual-erosion-of-market-efficiency",
    "href": "1_intro.html#the-dual-erosion-of-market-efficiency",
    "title": "Introduction",
    "section": "4 The Dual Erosion of Market Efficiency",
    "text": "4 The Dual Erosion of Market Efficiency\nThe erosion of allocative efficiency is twofold:\n\nAt the top, capital becomes trapped in dominant firms through structural lock-in, reducing competitive discipline.\nAt the bottom, zombie firms—unproductive entities sustained by liquidity, not fundamentals—persist and bias the return distribution.\n\nWhile we do not formally identify zombie firms, we observe that the small-cap segment exhibits lower average returns, higher variance, and greater downside skewness, consistent with capital misallocation and reduced creative destruction. Together, these effects form a dual-channel breakdown in market efficiency.",
    "crumbs": [
      "Apps",
      "Introduction"
    ]
  },
  {
    "objectID": "1_intro.html#the-tbtf-strategy-and-its-structural-paradox",
    "href": "1_intro.html#the-tbtf-strategy-and-its-structural-paradox",
    "title": "Introduction",
    "section": "5 The TBTF Strategy and Its Structural Paradox",
    "text": "5 The TBTF Strategy and Its Structural Paradox\nWithin this environment, we evaluate a simple but powerful portfolio: the “Too Big to Fail” (TBTF) strategy. It selects the ten largest firms by market cap from the CRSP universe, applies a convex (quadratic or exponential) weighting scheme, and rebalances at fixed intervals.\nDespite its simplicity, the TBTF portfolio delivers persistent and superior risk-adjusted performance, especially in the post-2010 regime. It outperforms standard indices and academic benchmarks across multiple metrics while maintaining low turnover. Its success, however, is paradoxical:\n\nThe strategy is optimal only if the market is inefficient.\nIf it fails, society wins. If it works, structural distortion persists.\n\nThis “sadly optimal” paradox reveals that investors can rationally profit from a system that is collectively irrational.",
    "crumbs": [
      "Apps",
      "Introduction"
    ]
  },
  {
    "objectID": "1_intro.html#capital-mobility-lock-in-and-structural-polarization",
    "href": "1_intro.html#capital-mobility-lock-in-and-structural-polarization",
    "title": "Introduction",
    "section": "6 Capital Mobility, Lock-In, and Structural Polarization",
    "text": "6 Capital Mobility, Lock-In, and Structural Polarization\nTo uncover the deeper structure, we construct percentile-based Markov transition matrices and stationary capital share distributions. Our findings indicate:\n\nTop-decile firms behave as near-absorbing states, showing extreme persistence.\nLow-decile firms exhibit volatility and minimal upward mobility.\nThe capital distribution grows increasingly convex over time, resembling a structural Lorenz curve.\nMobility inequality is worsening, consistent with a capital-based caste system.\n\nThese features parallel social stratification: the market mimics a hierarchy in which firms are locked into persistent capital classes, undermining both innovation and allocative justice.",
    "crumbs": [
      "Apps",
      "Introduction"
    ]
  },
  {
    "objectID": "1_intro.html#broader-implications-and-research-agenda",
    "href": "1_intro.html#broader-implications-and-research-agenda",
    "title": "Introduction",
    "section": "7 Broader Implications and Research Agenda",
    "text": "7 Broader Implications and Research Agenda\nThis study contributes to the asset pricing literature by highlighting that persistent outperformance may arise not from risk-bearing, but from capital lock-in, institutional frictions, and policy distortions. It motivates a new generation of asset pricing models that integrate:\n\nNon-ergodic dynamics and structural path dependence\nRank-based valuation and transition asymmetry\nRent-based pricing in lieu of marginal productivity\n\nSuch models would better capture how capital markets behave in a world dominated by passive flows, interventionist policy, and endogenous concentration.",
    "crumbs": [
      "Apps",
      "Introduction"
    ]
  },
  {
    "objectID": "1_intro.html#structure-of-the-paper",
    "href": "1_intro.html#structure-of-the-paper",
    "title": "Introduction",
    "section": "8 Structure of the Paper",
    "text": "8 Structure of the Paper\nThe remainder of the paper proceeds as follows:\n\nChapter 2 (Literature Review) situates this study within the literature on asset pricing, inequality, and ETF-induced capital distortions.\nChapter 3 (Historical Context) reviews the evolution of U.S. market structure post-2008, with emphasis on monetary policy and index construction.\nChapter 4 (Empirical Study) provides detailed analysis across seven modules: research framing, data, benchmark construction, structural modeling, strategy implementation, performance evaluation, and robustness testing.\nChapter 5 (Conclusion) reflects on the theoretical, empirical, and societal implications of a world in which TBTF is optimal.",
    "crumbs": [
      "Apps",
      "Introduction"
    ]
  },
  {
    "objectID": "2_lit_review.html",
    "href": "2_lit_review.html",
    "title": "Literature Review",
    "section": "",
    "text": "Modern finance rests on the principle that asset returns compensate for exposure to systematic risk. Foundational models such as the Capital Asset Pricing Model (CAPM), the Fama-French factor models, and the Betting Against Beta (BAB) framework formalize this idea under assumptions of frictionless trading, rational expectations, and arbitrage-free equilibria.\nIn CAPM, expected returns scale linearly with market beta. The Fama-French Three- and Five-Factor models extend this by incorporating size, value, investment, and profitability factors (Fama and French 1993, 2015). Alternative models such as Black–Litterman (Black and Litterman 1992) or Bayesian tilting (Brandt, Santa-Clara, and Valkanov 2009) adjust this equilibrium using investor views. Despite their theoretical elegance, these frameworks rely heavily on convex market structure, homogeneous expectations, and efficient capital mobility — conditions increasingly absent in post-crisis financial markets.",
    "crumbs": [
      "Apps",
      "Literature Review"
    ]
  },
  {
    "objectID": "2_lit_review.html#classical-foundations-of-risk-based-asset-pricing",
    "href": "2_lit_review.html#classical-foundations-of-risk-based-asset-pricing",
    "title": "Literature Review",
    "section": "",
    "text": "Modern finance rests on the principle that asset returns compensate for exposure to systematic risk. Foundational models such as the Capital Asset Pricing Model (CAPM), the Fama-French factor models, and the Betting Against Beta (BAB) framework formalize this idea under assumptions of frictionless trading, rational expectations, and arbitrage-free equilibria.\nIn CAPM, expected returns scale linearly with market beta. The Fama-French Three- and Five-Factor models extend this by incorporating size, value, investment, and profitability factors (Fama and French 1993, 2015). Alternative models such as Black–Litterman (Black and Litterman 1992) or Bayesian tilting (Brandt, Santa-Clara, and Valkanov 2009) adjust this equilibrium using investor views. Despite their theoretical elegance, these frameworks rely heavily on convex market structure, homogeneous expectations, and efficient capital mobility — conditions increasingly absent in post-crisis financial markets.",
    "crumbs": [
      "Apps",
      "Literature Review"
    ]
  },
  {
    "objectID": "2_lit_review.html#empirical-challenges-and-structural-frictions",
    "href": "2_lit_review.html#empirical-challenges-and-structural-frictions",
    "title": "Literature Review",
    "section": "2 Empirical Challenges and Structural Frictions",
    "text": "2 Empirical Challenges and Structural Frictions\n\n2.1 Factor Models and Fragility Under Structural Shifts\nThe empirical performance of multi-factor models is often weak out-of-sample. Applying Fama-MacBeth regressions (Fama and MacBeth 1973) to industry or quantile portfolios reveals unstable betas, multicollinearity among explanatory variables, and residual alpha persistence (Fama and French 1997). Recent replication studies (e.g., Hou, Xue, and Zhang (2020)) show that most anomalies lack robustness, particularly when controlling for micro-cap bias and rebalancing frictions.\nEven advanced selection methods, such as Lasso or Ridge-regularized regressions (Kozak, Nagel, and Santosh 2020), identify few stable predictors. These results suggest that the dominant source of return variation may lie outside the domain of classical factor structures.\n\n\n2.2 Nonlinearity and the Failure of Beta as a Pricing Kernel\nThe BAB model posits that low-beta portfolios offer higher Sharpe ratios after adjusting for leverage constraints (Frazzini and Pedersen 2014). However, empirical tests using SIC-based portfolios reveal a non-monotonic relationship: optimal risk-adjusted returns often cluster around beta ≈ 1, contradicting the implication of continuous beta-based mispricing. This points to risk inertia rather than reward for bearing volatility.\n\n\n2.3 Practical Limitations of Optimization-Based Portfolios\nMean-Variance optimization (Markowitz 1952) and its extensions frequently fail in practice. Portfolio weights are unstable and overfit, leading to high turnover and inferior real-world performance (DeMiguel, Garlappi, and Uppal 2009). Constrained variants reduce instability but underperform simple passive benchmarks like SPY or QQQ.\n\n\n2.4 Post-2010 Failures of Predictive Allocation\nDynamic asset allocation models using Bayesian tilting or factor timing (Brandt, Santa-Clara, and Valkanov 2009) show limited edge post-2010. In regimes dominated by non-Gaussian returns, policy shocks, and arbitrage limits, these models fail to anticipate structural capital movements or benefit from persistent factor exposures.",
    "crumbs": [
      "Apps",
      "Literature Review"
    ]
  },
  {
    "objectID": "2_lit_review.html#structural-interpretations-rent-extraction-and-capital-lock-in",
    "href": "2_lit_review.html#structural-interpretations-rent-extraction-and-capital-lock-in",
    "title": "Literature Review",
    "section": "3 Structural Interpretations: Rent Extraction and Capital Lock-in",
    "text": "3 Structural Interpretations: Rent Extraction and Capital Lock-in\nA growing literature interprets return generation as increasingly governed by non-risk factors such as capital scale, policy insulation, and rank persistence:\n\nLarge banks exhibit lower risk-adjusted returns, suggestive of implicit guarantees (Gandhi and Lustig 2015).\nSmall-cap segments contain a rising share of “zombie” firms — structurally unproductive entities sustained by policy support (Acharya et al. 2024).\nETF flows bias price discovery toward large-cap incumbents (Jiang, Vayanos, and Zheng 2020; Glosten, Li, and Zhang 2021).\nConcentration of capital and returns parallels macro trends in labor share decline and superstar firm dominance (Autor et al. 2020).\n\nThese findings frame market returns not as compensation for risk, but as rents accruing to structural position within the capital hierarchy.",
    "crumbs": [
      "Apps",
      "Literature Review"
    ]
  },
  {
    "objectID": "2_lit_review.html#positioning-tbtf-within-the-literature",
    "href": "2_lit_review.html#positioning-tbtf-within-the-literature",
    "title": "Literature Review",
    "section": "4 Positioning TBTF within the Literature",
    "text": "4 Positioning TBTF within the Literature\nThis study contributes to this emerging paradigm by introducing:\n\nRank-based capital lock-in frameworks via percentile transition matrices, inspired by social mobility models.\nA low-turnover, high-performance TBTF strategy, rooted in capital persistence rather than information efficiency.\nA critique of post-QE capital dynamics, where ETF-driven flows reinforce rank-based inequality in financial markets.\n\nIn contrast to conventional pricing models assuming rapid mean-reversion and marginal arbitrage, the TBTF strategy illustrates how capital stratification can sustain excess returns through structural mechanisms rather than informational edges.\nIts continued success does not reject modern finance—it reflects its breakdown under structural stress, necessitating models that account for capital rigidity, transition asymmetry, and institutional drift.",
    "crumbs": [
      "Apps",
      "Literature Review"
    ]
  },
  {
    "objectID": "3_2_history_nyse.html",
    "href": "3_2_history_nyse.html",
    "title": "History of NYSE ME",
    "section": "",
    "text": "The NYSE continues to exhibit substantial market capitalization concentration. Since 2010 — and more sharply since 2016 — the very largest firms have distanced themselves even from the rest of the top 5%, highlighting the structural importance of tail dynamics in financial markets. Any realistic asset pricing model must account for this persistent and extreme upper-tail asymmetry.\nCode\nimport pandas as pd\nimport pandas_datareader.data as pdr\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning) # FutureWarning 제거\n\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\n# Define time period\nstart_date = \"1996-01-01\"\nend_date = \"2024-12-31\"\n\n# Fetch Fama-French ME_Breakpoints data (NYSE percentile breakpoints)\nbreakpoints_raw = pdr.DataReader(\n    name=\"ME_Breakpoints\",\n    data_source=\"famafrench\",\n    start=start_date,\n    end=end_date\n)[0]\n\n# Extract percentile labels from column names (e.g., \"(0, 5)\" -&gt; \"5\")\ndef extract_upper_bound(label):\n    if isinstance(label, str) and \"(\" in label:\n        try:\n            return str(int(label.split(\",\")[1].replace(\")\", \"\").strip()))\n        except Exception:\n            return label\n    elif isinstance(label, tuple):\n        return str(label[1])\n    return str(label)\n\n# Rename columns to only use upper percentile values\ncolumns_to_rename = {col: extract_upper_bound(col) for col in breakpoints_raw.columns if col != 'Count'}\nbreakpoints = breakpoints_raw.rename(columns=columns_to_rename)\n\n# Normalize ME values by number of firms (Count) to get \"per firm\" values\nfor col in breakpoints.columns:\n    if col != 'Count':\n        breakpoints[col] = breakpoints[col] / breakpoints['Count']\n\n# Print average firm count over the period\navg_count = int(breakpoints['Count'].mean())\nprint(f\"Average number of NYSE firms from {start_date} to {end_date}: {avg_count}\")\n\n\nAverage number of NYSE firms from 1996-01-01 to 2024-12-31: 1386",
    "crumbs": [
      "Apps",
      "History",
      "History of NYSE ME"
    ]
  },
  {
    "objectID": "3_2_history_nyse.html#nyse-market-equity-breakpoints-long-term-dynamics",
    "href": "3_2_history_nyse.html#nyse-market-equity-breakpoints-long-term-dynamics",
    "title": "History of NYSE ME",
    "section": "1 NYSE Market Equity Breakpoints: Long-Term Dynamics",
    "text": "1 NYSE Market Equity Breakpoints: Long-Term Dynamics\nThe Fama-French dataset ME_Breakpoints provides monthly percentile breakpoints for market equity (ME), computed only from NYSE stocks. These breakpoints span from the 5th to the 100th percentile and are calculated based on market capitalization (price times shares outstanding, in millions of USD) at month-end. Importantly, closed-end funds and REITs are excluded, and only firms with CRSP share codes 10 or 11 and valid price/share data are included.\nThis file investigates the evolution of market concentration in the NYSE based on these ME breakpoints, emphasizing the dynamics in the upper tail of the distribution, particularly the top 5% of firms.",
    "crumbs": [
      "Apps",
      "History",
      "History of NYSE ME"
    ]
  },
  {
    "objectID": "3_2_history_nyse.html#breakpoint-time-series-per-firm-19962024",
    "href": "3_2_history_nyse.html#breakpoint-time-series-per-firm-19962024",
    "title": "History of NYSE ME",
    "section": "2 Breakpoint Time Series per Firm (1996–2024)",
    "text": "2 Breakpoint Time Series per Firm (1996–2024)\nWe plot the NYSE ME breakpoints divided by the number of firms each month (“per firm”) from 1996 to 2024. The results reveal two distinct phases:\n\nPre-2010: A cyclical pattern dominates, consistent with broader economic expansions and contractions. For instance, the 2000–2001 tech bubble and the 2008 global financial crisis exhibit clear signals of expansion and collapse.\nPost-2010: A structural break is visible. Especially since 2016, the average ME per firm in the top percentile (100%) exhibits a sharp and persistent upward trend.\n\nThis long-term trend implies a sustained capital lock-in within a small number of mega-cap firms, increasingly distanced from the rest of the NYSE universe.\n\n\nCode\n# Plot selected percentile breakpoints over time\nselected_percentiles = ['80', '85', '90', '95', '100']\nbreakpoints[selected_percentiles].plot(figsize=(10, 5))\n\nplt.legend(title='Percentile')\nplt.ylabel('Market Equity (in millions) per firm')\nplt.title('NYSE ME Breakpoints (Per Firm Basis)')\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Apps",
      "History",
      "History of NYSE ME"
    ]
  },
  {
    "objectID": "3_2_history_nyse.html#cross-sectional-concentration-at-the-tail",
    "href": "3_2_history_nyse.html#cross-sectional-concentration-at-the-tail",
    "title": "History of NYSE ME",
    "section": "3 Cross-Sectional Concentration at the Tail",
    "text": "3 Cross-Sectional Concentration at the Tail\nTo better understand the shape of the right tail, we visualize the percentile distribution at the most recent observation (2024-12). The result is striking: while the ME per firm grows gradually between percentiles 5 to 95, a dramatic jump occurs at the 100th percentile.\nThis highlights that the concentration of market value within the top 5% is extreme, and the very last percentile alone contains firms with ME per firm often an order of magnitude greater than those in the 95th percentile.\n\n\nCode\ndef plot_breakpoints_at_end(df, count_col='Count', start_pct=0, end_pct=None, title_suffix=''):\n    \"\"\"\n    Plot breakpoints at the last available date.\n    \n    Parameters:\n        df: DataFrame with percentile columns and 'Count'\n        count_col: name of the column representing number of firms (default: 'Count')\n        start_pct: starting index for column slice (e.g., -20 for top 20 percentiles)\n        end_pct: ending index for column slice (default: None means till the end)\n        title_suffix: string appended to plot title\n    \"\"\"\n    # Select data at last date\n    last_row = df.tail(1).drop(columns=[count_col])\n    \n    # Slice desired percentile columns\n    selected_columns = last_row.columns[start_pct:end_pct]\n    y_data = pd.to_numeric(last_row[selected_columns].values.flatten(), errors='coerce')\n    \n    # Plot\n    plt.figure(figsize=(8, 4))\n    plt.plot(selected_columns, y_data, marker='o')\n    plt.xticks(rotation=45, ha='right')\n    plt.xlabel('Percentile')\n    plt.ylabel('ME Breakpoints (in millions per firm)')\n    plt.title(f'NYSE ME Breakpoints at {df.index[-1]} {title_suffix}')\n    plt.tight_layout()\n    plt.grid(True)\n    plt.show()\n\n# 전체 percentile 구간 시각화\nplot_breakpoints_at_end(breakpoints, start_pct=0, title_suffix='(Full Range)')\n\n# 상위 3개 빼고 시각화 (5~90)\nplot_breakpoints_at_end(breakpoints, start_pct=-20, end_pct=-2, title_suffix='(upto Top 20 Percentiles)')\n\n# 가장 극단적인 상위 3개만 (90, 95, 100 만)\nplot_breakpoints_at_end(breakpoints, start_pct=-3, title_suffix='(Top 3 Percentiles)')",
    "crumbs": [
      "Apps",
      "History",
      "History of NYSE ME"
    ]
  },
  {
    "objectID": "3_2_history_nyse.html#time-series-of-the-tail-ratio-100th-95th-percentile",
    "href": "3_2_history_nyse.html#time-series-of-the-tail-ratio-100th-95th-percentile",
    "title": "History of NYSE ME",
    "section": "4 Time Series of the Tail Ratio: 100th / 95th Percentile",
    "text": "4 Time Series of the Tail Ratio: 100th / 95th Percentile\nTo quantify tail concentration dynamics over time, we construct a monthly time series of the ME per firm ratio between the 100th and 95th percentiles. This ratio serves as a tail index for how dominant the very largest firms are, even among the elite.\nThe time series reveals the following:\n\n1996–2001: Rapid escalation during the dot-com boom, with the ratio peaking above 20.\n2003–2008: Stabilization around ~13.\n2009: A brief post-crisis surge back above 18.\n2010–2016: A sharp decline and plateau near 7, indicating relative equality among top-tier firms.\nPost-2016: A gradual resurgence in the ratio, reflecting renewed concentration at the very top.\n\n\n\nCode\n# Calculate the ME per firm ratio: 100th / 95th percentile\nratio_series = breakpoints['100'] / breakpoints['95']\n\n# Convert PeriodIndex to DatetimeIndex for plotting\nratio_series.index = ratio_series.index.to_timestamp()\n\n# Plot both raw and log-transformed ratio\nplt.figure(figsize=(12, 5))\n\nax = plt.gca()\nax.xaxis.set_major_locator(mdates.YearLocator(2))\nax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\nax.tick_params(axis='x', rotation=45) # x축 눈금 회전 추가\n\n# Raw ratio\nplt.plot(ratio_series.index, ratio_series, marker='o')\nplt.title('ME per Firm Ratio: 100th / 95th Percentile')\nplt.xlabel('Date')\nplt.ylabel('Ratio (ME[100] / ME[95])')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Calculate the ME per firm ratio: 95th / 50th percentile\nratio_series = breakpoints['95'] / breakpoints['50']\n\n# Convert PeriodIndex to DatetimeIndex for plotting\nratio_series.index = ratio_series.index.to_timestamp()\n\n# Plot both raw and log-transformed ratio\nplt.figure(figsize=(12, 5))\n\n# Raw ratio\nplt.plot(ratio_series.index, ratio_series, marker='o')\nplt.title('ME per Firm Ratio: 95th / 50th Percentile')\nplt.xlabel('Date')\nplt.ylabel('Ratio (ME[95] / ME[50])')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Apps",
      "History",
      "History of NYSE ME"
    ]
  },
  {
    "objectID": "3_2_history_nyse.html#broader-context-nyse-and-the-top-5",
    "href": "3_2_history_nyse.html#broader-context-nyse-and-the-top-5",
    "title": "History of NYSE ME",
    "section": "5 Broader Context: NYSE and the Top 5%",
    "text": "5 Broader Context: NYSE and the Top 5%\nIt is critical to underscore that Fama-French breakpoints are calculated using only NYSE stocks. Despite the rise of Nasdaq dominance in recent decades, the NYSE remains the foundation for constructing breakpoints in academic asset pricing.\nThe breakpoints for:\n\nMarket Equity (ME): monthly, based on NYSE stocks with viable price and share data.\nBook-to-Market (BE/ME): annually, using BE from t-1 and ME from December of t-1.\nPrior 2–12 month Return: monthly, requiring CRSP price and return data.\n\nThese indicators, especially in the upper tail, are overwhelmingly driven by the top 5% of NYSE firms — roughly 60–70 firms. These companies exert outsized influence on asset pricing, portfolio construction, and market dynamics.",
    "crumbs": [
      "Apps",
      "History",
      "History of NYSE ME"
    ]
  },
  {
    "objectID": "4_1_overview.html",
    "href": "4_1_overview.html",
    "title": "01 Overview",
    "section": "",
    "text": "This study investigates the structural properties and empirical performance of a “Too Big To Fail” (TBTF) portfolio strategy, defined as a portfolio composed of the largest market capitalization stocks in the U.S. equity market. Using monthly stock-level data from the CRSP universe (NYSE, Nasdaq, AMEX), we examine whether top-ranked stocks—selected purely by market capitalization—consistently outperform others in terms of risk-adjusted returns.\nOur empirical design centers around the hypothesis that structural asymmetries in capital allocation, exacerbated by post-2008 monetary policies and market concentration, result in persistent distortions in return distributions. The TBTF strategy, though simple in construction, appears to capture these structural features with surprising consistency.\nWe treat the year 2010 as a potential structural break, dividing the full sample into two equal-length periods:\nThis periodization allows us to analyze both long-term stability and post-crisis distortions in the equity return distribution.",
    "crumbs": [
      "Apps",
      "Model",
      "01 Overview"
    ]
  },
  {
    "objectID": "4_1_overview.html#main-hypotheses",
    "href": "4_1_overview.html#main-hypotheses",
    "title": "01 Overview",
    "section": "1 Main Hypotheses",
    "text": "1 Main Hypotheses\n\nReturn Superiority: The return distributions of top market-cap stocks are structurally superior to those of mid- and small-cap stocks in terms of risk-adjusted performance (e.g., Sharpe, Sortino, Omega ratios).\nPersistence and Absorption: Stocks that reach the top decile of market capitalization exhibit high persistence and low turnover, potentially distorting market competitiveness and reducing allocative efficiency.\nConvex Capital Concentration: The cross-sectional distribution of capital shares among top-ranked stocks is increasingly convex, suggesting long-term structural lock-in effects analogous to wealth inequality (e.g., Lorenz curve analogy).",
    "crumbs": [
      "Apps",
      "Model",
      "01 Overview"
    ]
  },
  {
    "objectID": "4_1_overview.html#appendix",
    "href": "4_1_overview.html#appendix",
    "title": "01 Overview",
    "section": "2 Appendix",
    "text": "2 Appendix\n\n\n2.1 Summary of Analytical Components in the TBTF Strategy\n\n\n\n\n\n\n\nComponent\nDescription\n\n\n\n\nLarge vs. Small Stocks\nCompare return distributions and volatility structures\n\n\nTop 10 Stocks\nAnalyze list stability and industry composition (high-tech dominance)\n\n\nTransition Analysis\nEstimate transition probabilities and stationary distributions\n\n\nRisk-Adjusted Performance\nCompare Sharpe, Sortino, Omega ratios against benchmarks (ETFs, indices)",
    "crumbs": [
      "Apps",
      "Model",
      "01 Overview"
    ]
  },
  {
    "objectID": "4_3_ff_compare.html",
    "href": "4_3_ff_compare.html",
    "title": "03 Size Benchmark",
    "section": "",
    "text": "Code\n# Import Libraries\nimport pandas as pd\nimport numpy as np\n\nimport statsmodels.api as sm # QQ plot\nfrom scipy import stats # relative t-test\n\n# graphics\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Data Frame. Data frequency: Monthly\nstart_date = \"1963-07-01\"\nend_date = \"2023-06-30\"\n\nprint(\"start date:\", start_date)\nprint(\"end date:\", end_date)\n\n\nstart date: 1963-07-01\nend date: 2023-06-30\nCode\n#@title Data: Portfolios_Formed_on_ME\n# https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/Data_Library/\n\nimport pandas_datareader as pdr\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning) # FutureWarning 제거\n\n# 'Portfolios_Formed_on_ME', by a Univariate sort on Size (market equity, ME)\n# 3 Potfolios include all NYSE, AMEX, and NASDAQ stocks, but with NYSE breakpoints to divide\n# Size are the bottom 30%, middle 40%, top 30%; quintiles; deciles.\n\npfo_size_raw = pdr.DataReader(\n  name=\"Portfolios_Formed_on_ME\",\n  data_source=\"famafrench\",\n  start=start_date,\n  end=end_date)[0]\n\npfo_size = (pfo_size_raw\n  .reset_index(names=\"date\")\n  .assign(date=lambda x: pd.to_datetime(x[\"date\"].astype(str)))\n  .set_index('date')  # Set the 'date' column as the index\n  .rename(columns=lambda x: x.lower())\n  .rename(columns={'lo 30': 's_30', 'hi 30': 'b_30', 'lo 20': 's_20', 'hi 20': 'b_20', 'lo 10': 's_10', 'hi 10': 'b_10'})\n  .drop(['&lt;= 0'], axis=\"columns\")\n)\n\n# Calculate the average of the small group\n# pfo_size.iloc[:, 8:17].columns.tolist()\npfo_size['s_70'] = pfo_size.iloc[:, 0:2].mean(axis=1)\npfo_size['s_80'] = pfo_size.iloc[:, 3:7].mean(axis=1)\npfo_size['s_90'] = pfo_size.iloc[:, 8:17].mean(axis=1)\n# Drop columns\npfo_size = pfo_size.drop(pfo_size.columns[9:17], axis=1)\npfo_size = pfo_size.drop(pfo_size.columns[4:7], axis=1)\npfo_size = pfo_size.drop(pfo_size.columns[1:2], axis=1)\n\n# Describe\npfo_size.describe().round(2)\n\n\n\n\n\n\n\n\n\ns_30\nb_30\ns_20\nb_20\ns_10\nb_10\ns_70\ns_80\ns_90\n\n\n\n\ncount\n720.00\n720.00\n720.00\n720.00\n720.00\n720.00\n720.00\n720.00\n720.00\n\n\nmean\n1.12\n0.91\n1.10\n0.90\n1.10\n0.88\n1.10\n1.09\n1.08\n\n\nstd\n6.20\n4.33\n6.34\n4.29\n6.35\n4.27\n5.72\n5.58\n5.46\n\n\nmin\n-29.43\n-20.80\n-29.65\n-20.32\n-28.87\n-19.74\n-28.30\n-27.82\n-27.26\n\n\n25%\n-2.37\n-1.57\n-2.41\n-1.56\n-2.25\n-1.53\n-2.18\n-1.96\n-1.92\n\n\n50%\n1.38\n1.22\n1.36\n1.17\n1.25\n1.12\n1.38\n1.41\n1.34\n\n\n75%\n4.70\n3.64\n4.89\n3.54\n4.67\n3.47\n4.57\n4.61\n4.60\n\n\nmax\n26.91\n17.75\n27.54\n18.05\n29.50\n18.06\n24.84\n23.83\n23.08\nSummary Statistics for Fama-French Size-Decile Portfolios: Monthly value-weighted excess returns for size-sorted portfolios from July 1963 to June 2023. Excess returns are defined as raw portfolio returns net of the one-month Treasury bill rate.\nTo establish a structural benchmark for the TBTF strategy, we begin by examining the historical performance of size-sorted portfolios constructed by Fama and French. In particular, we compare the top and bottom deciles of market capitalization—commonly denoted as b_10 (large-cap) and s_10 (small-cap)—using monthly return data spanning from July 1963 to June 2023.\nThe data originate from the “Portfolios Formed on Size (ME)” dataset provided by the Ken French Data Library. These portfolios, covering NYSE, Nasdaq, and AMEX stocks, are rebalanced annually using NYSE breakpoints and report monthly value-weighted excess returns—i.e., returns net of the one-month Treasury bill rate (risk-free return).\nFor additional robustness, we construct aggregates (e.g., s_70, s_90) to represent broader small-cap behavior.",
    "crumbs": [
      "Apps",
      "Model",
      "03 Size Benchmark"
    ]
  },
  {
    "objectID": "4_3_ff_compare.html#motivation",
    "href": "4_3_ff_compare.html#motivation",
    "title": "03 Size Benchmark",
    "section": "1 Motivation",
    "text": "1 Motivation\nThis section evaluates whether large-cap portfolios exhibit systematically superior risk-adjusted performance relative to small-cap portfolios. Such a result would support the hypothesis that TBTF-style strategies derive their advantage not from tactical optimization, but from structural features of the cross-sectional return distribution.",
    "crumbs": [
      "Apps",
      "Model",
      "03 Size Benchmark"
    ]
  },
  {
    "objectID": "4_3_ff_compare.html#methodology",
    "href": "4_3_ff_compare.html#methodology",
    "title": "03 Size Benchmark",
    "section": "2 Methodology",
    "text": "2 Methodology\nWe compare the s_10 and b_10 portfolios in three dimensions:\n\nDistributional Shape: via QQ-plots and skewness asymmetry\n\nVolatility Profile: through standard deviation comparisons\n\nSharpe Ratio Dynamics: using time-varying rolling Sharpe ratio curves and volatility–mean coordinate plots\n\nUnlike most academic studies that summarize portfolio performance using static points in mean–volatility space, we propose a dynamic framework in which Sharpe ratios are evaluated as time-series objects. This allows us to capture persistent structural asymmetries.\n\n2.1 Sharpe Ratio Dynamics\nWhile static comparisons of mean and volatility offer useful summary insights, they can be misleading in the presence of temporal instability. To capture the time-varying performance profile of small- and large-cap portfolios, we construct rolling Sharpe ratios using annualized excess returns.\nLet \\(\\mu_t\\) and \\(\\sigma_t\\) denote the rolling annualized excess return mean and volatility of a portfolio over a 36-month window. Then, the rolling Sharpe ratio at time \\(t\\) is computed as:\n\\[\n\\text{Sharpe}_t = \\frac{\\mu_t}{\\sigma_t}\n\\]\nWe focus on the bottom and top size deciles: s_10 (small-cap) and b_10 (large-cap).",
    "crumbs": [
      "Apps",
      "Model",
      "03 Size Benchmark"
    ]
  },
  {
    "objectID": "4_3_ff_compare.html#key-findings",
    "href": "4_3_ff_compare.html#key-findings",
    "title": "03 Size Benchmark",
    "section": "3 Key Findings",
    "text": "3 Key Findings\n\nThe b_10 portfolio exhibits consistently lower volatility than s_10, across the full 60-year period.\nDynamic Sharpe ratio visualization reveals that the performance advantage of b_10 is persistent over time, not a byproduct of a particular decade or business cycle.\nQQ-plots show that b_10 excess returns are closer to Gaussian, while s_10 returns display tail asymmetry—characterized by positive skewness and negative tail risk.\n\nThese results suggest that large-cap portfolios, especially those at the very top of the capitalization spectrum, provide more stable and efficient excess return profiles. This supports the structural validity of selecting top-ranked assets by market capitalization in TBTF portfolio construction.\n\nOver the 60-year period, the unconditional average annual Sharpe ratio of b_10 was 0.99, compared to 0.74 for s_10.\nThe time-series of rolling Sharpe ratios reveals that b_10 dominates structurally, not just in isolated windows.\nDuring periods of macroeconomic stress, s_10 portfolios experience sharper volatility spikes, reducing their Sharpe ratios significantly.\n\n\n\nCode\n#@title Time-series of s_10 vs. b_10\n\n# moving average\n# pfo_size_rolling = pfo_size.rolling(window=12).mean()\n\n\n#@title without any overlap between the fiscal years\n# Resample to annual frequency and calculate annual standard deviation\n# groups the monthly data into yearly buckets.\n# annual_std = pfo_size.resample('Y').std() * np.sqrt(12)\n\n# Create a fiscal year column\npfo_size['fiscal_year'] = pfo_size.index.year\npfo_size.loc[pfo_size.index.month &gt;= 7, 'fiscal_year'] = pfo_size.loc[pfo_size.index.month &gt;= 7, 'fiscal_year'] + 1\n\n# Group by fiscal year and calculate annual\nannual_mean = pfo_size.groupby('fiscal_year').mean()*12\nannual_std = pfo_size.groupby('fiscal_year').std() * np.sqrt(12)\n# annual_std.dropna(inplace=True)\n\n#@title Time-series of s_10 vs. b_10\ntemp = annual_mean['s_10'] - annual_mean['b_10']\ntemp.plot()\nplt.axhline(y= temp.mean(), color='r', linestyle='-')\nplt.xlabel('fiscal year')\nplt.ylabel('annual mean difference')\nplt.title('Time-series of s_10 minus b_10 in annual mean')\nplt.show()\nprint('The unconditional mean of the difference is '+str(temp.mean().round(0))+ ' percent, annually')\nprint('\\n')\n\ntemp = annual_std['s_10'] - annual_std['b_10']\ntemp.plot()\nplt.axhline(y= temp.mean(), color='r', linestyle='-')\nplt.xlabel('fiscal year')\nplt.ylabel('annual std difference')\nplt.title('Time-series of s_10 minus b_10 in annual volatility')\nplt.show()\nprint('The unconditional mean of the difference is '+str(temp.mean().round(0))+ ' percent, annually')\nprint('\\n')\n\n# annual_mean and annual_std have the same index\ndf = pd.DataFrame(index=annual_mean.index)\ndf['sr_s_10'] = annual_mean['s_10'] / annual_std['s_10']\ndf['sr_b_10'] = annual_mean['b_10'] / annual_std['b_10']\n\ndf['sr_s_10'].plot()\ndf['sr_b_10'].plot()\nplt.axhline(y= df['sr_s_10'].mean(), color='b', linestyle='dashed')\nplt.axhline(y= df['sr_b_10'].mean(), color='r', linestyle='dashed')\nplt.legend()\nplt.xlabel('fiscal year')\nplt.ylabel('Sharpe Ratio')\nplt.title('Time-series Sharpe Ratios of s_10 and b_10')\nplt.show()\nprint('The unconditional mean Annual Sharpe ratio of s_10 is '+str(df['sr_s_10'].mean().round(2)))\nprint('The unconditional mean Annual Sharpe ratio of b_10 is '+str(df['sr_b_10'].mean().round(2)))\n\n\n\n\n\n\n\n\n\nThe unconditional mean of the difference is 3.0 percent, annually\n\n\n\n\n\n\n\n\n\n\n\nThe unconditional mean of the difference is 6.0 percent, annually\n\n\n\n\n\n\n\n\n\n\n\nThe unconditional mean Annual Sharpe ratio of s_10 is 0.74\nThe unconditional mean Annual Sharpe ratio of b_10 is 0.99\n\n\n\nAnnual excess return difference: s_10 – b_10. The red horizontal line marks the long-run average (~3.0% annually).\nAnnual volatility difference: s_10 – b_10. Large-cap volatility is consistently lower.\nTime-series of 36-month rolling Sharpe ratios for s_10 and b_10. Dashed lines represent the unconditional average for each.\n\nWe also construct a bivariate distribution of annual mean and standard deviation using kernel density estimates:\n\n\nCode\n#@title s_10 vs. b_10 in the volatility-mean coordinate\n\n# Create a list of category names\ncategories = ['s_10', 'b_10']\n\n# Initialize an empty list to store dataframes\ndfs = []\n\n# Iterate through categories and create melted dataframes\nfor category in categories:\n    # Create a temporary dataframe with selected columns and category label\n    temp_mean = annual_mean[[category]].copy()  # Keep the index\n    temp_mean['category'] = category\n    temp_mean.rename(columns={category: 'annual_mean'}, inplace=True)\n\n    temp_std = annual_std[[category]].copy()  # Keep the index\n    temp_std['category'] = category\n    temp_std.rename(columns={category: 'annual_std'}, inplace=True)\n\n    # Merge the temporary dataframes for mean and std, keeping the index\n    temp_df = pd.merge(temp_mean, temp_std, on=['fiscal_year', 'category'])\n\n    # Append the temporary dataframe to the list\n    dfs.append(temp_df)\n\n# Concatenate all the dataframes in the list into a single dataframe, keeping the index\ndf = pd.concat(dfs, ignore_index=False)\n\n# Now create the displot\nsns.displot(\n    data = df,  # Use the reshaped DataFrame\n    x=\"annual_std\",\n    y=\"annual_mean\",\n    hue=\"category\",  # Use the extracted category for hue\n    kind=\"kde\",\n    rug=True,\n)\nplt.xlabel('annual std')\nplt.ylabel('annual mean')\nplt.title('s_10 vs. b_10 in the volatility-mean coordinate')\nplt.show()\n\n\n\n\n\n\n\n\n\nKernel density estimate of annual excess return vs. annual volatility for s_10 and b_10. The separation in volatility–mean space reflects the underlying Sharpe ratio asymmetry.\nThese dynamic plots provide a richer understanding of the persistent performance asymmetry between the smallest and largest decile portfolios—supporting the broader structural thesis underlying TBTF.",
    "crumbs": [
      "Apps",
      "Model",
      "03 Size Benchmark"
    ]
  },
  {
    "objectID": "4_3_ff_compare.html#contribution",
    "href": "4_3_ff_compare.html#contribution",
    "title": "03 Size Benchmark",
    "section": "4 Contribution",
    "text": "4 Contribution\n\nIntroduces a time-dynamic framework—Sharpe ratio level curves—to visualize and quantify structural performance asymmetries between size portfolios.\nEstablishes a long-term empirical benchmark against which TBTF performance can be evaluated.\nProvides theoretical and empirical justification for focusing on top-market-cap stocks, beyond simple momentum or value signals.",
    "crumbs": [
      "Apps",
      "Model",
      "03 Size Benchmark"
    ]
  },
  {
    "objectID": "4_3_ff_compare.html#appendix",
    "href": "4_3_ff_compare.html#appendix",
    "title": "03 Size Benchmark",
    "section": "5 Appendix",
    "text": "5 Appendix\n\n\n5.1 QQ Plot – s_10 vs. b_10\n\n\nCode\n#@title QQ-plot (정규분포 검사)\n# ![](figs/qqplot_s10_b10.png)\n\nbig = pfo_size['b_10']\nsmall = pfo_size['s_10']\n\nimport statsmodels.api as sm\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 6))\n# Calculate the minimum and maximum y-values across both datasets\nmin_y = min(small.min(), big.min())\nmax_y = max(small.max(), big.max())\n\n# QQ plot for 's_low' on the first subplot (ax1)\nsm.qqplot(small, line='s', ax=ax1)\nax1.set_title('Small Portfolio Monthly excess Returns')\nax1.grid(True)\n\n# QQ plot for 'b_low' on the second subplot (ax2)\nsm.qqplot(big, line='s', ax=ax2)\nax2.set_title('Big Portfolio Monthly excess Returns')\nax2.grid(True)\n\n# Set the y-axis limits for both subplots (ax1 and ax2)\nax1.set_ylim([min_y, max_y])\nax2.set_ylim([min_y, max_y])\n# Set x-axis limits of ax2 to be the same as ax1\nxlim = ax1.get_xlim()\nax2.set_xlim(xlim)\n\nplt.tight_layout()  # Adjust spacing between subplots\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n#@title QQ-plot (annualized)\n\nbig = annual_mean['b_10']\nsmall = annual_mean['s_10']\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 6))\n# Calculate the minimum and maximum y-values across both datasets\nmin_y = min(small.min(), big.min())\nmax_y = max(small.max(), big.max())\n\n# QQ plot for 's_low' on the first subplot (ax1)\nsm.qqplot(small, line='s', ax=ax1)\nax1.set_title('QQ Plot of small Portfolio Returns')\nax1.grid(True)\n\n# QQ plot for 'b_low' on the second subplot (ax2)\nsm.qqplot(big, line='s', ax=ax2)\nax2.set_title('QQ Plot of big Portfolio Returns')\nax2.grid(True)\n\n# Set the y-axis limits for both subplots (ax1 and ax2)\nax1.set_ylim([min_y, max_y])\nax2.set_ylim([min_y, max_y])\n# Set x-axis limits of ax2 to be the same as ax1\nxlim = ax1.get_xlim()\nax2.set_xlim(xlim)\n\nplt.tight_layout()  # Adjust spacing between subplots\nplt.show()\n\n\n\n\n\n\n\n\n\nQQ plots comparing sample quantiles of s_10 (left) and b_10 (right) returns to a standard normal distribution. The flatter slope and tighter fit of b_10 indicate lower volatility and greater normality, while s_10 exhibits positive skewness and negative tail risk.\n\n5.1.1 QQ Plot Interpretation: Comparing Small vs. Big Size Portfolios\nTo evaluate the return distribution characteristics of small-cap and large-cap portfolios, we construct QQ plots of returns for the s_10 (smallest decile) and b_10 (largest decile) portfolios. The plots juxtapose sample quantiles against theoretical quantiles from a standard normal distribution. Three notable features emerge:\n\nSlope of the Fitted Line (Volatility Indicator)\nThe slope of the QQ line is significantly flatter for the b_10 portfolio than for the s_10.\nThis reflects lower empirical standard deviation for large-cap returns, consistent with their lower volatility and more stable risk profiles. The slope in a QQ plot corresponds to the ratio of sample to theoretical standard deviation, reinforcing the volatility advantage of big stocks.\nLine–Scatter Fit (Distributional Regularity)\nThe b_10 plot shows a much tighter fit of the scatter points along the reference line, compared to the s_10 portfolio.\nThis implies that large-cap returns conform more closely to the normal distribution. In contrast, the small-cap portfolio exhibits noticeable deviations, suggesting greater skewness, kurtosis, or latent regime switches. The result indicates that large-cap stocks exhibit greater distributional regularity, aligning with their more predictable behavior in large institutional portfolios.\nTail Behavior (Asymmetry in Small-Cap Returns)\nFor s_10, the right tail (positive returns) lies above the line, while the left tail (negative returns) lies below.\nThis pattern suggests positive skewness—i.e., occasional high positive returns but more frequent or severe downside shocks. Such asymmetry is common in small-cap stocks, which may have explosive upside potential but are also subject to default or delisting risk. The departure from symmetry in s_10 strengthens the case for mixture modeling and asymmetric tail analysis in the TBTF framework.\n\n\n\n\n5.2 Paired T-test (1963–2023)\n\n\nCode\n#@title Dependent t-test (annualized mean. Small sample)\n\nsmall = annual_mean['s_10']\nbig = annual_mean['b_10']\n\nt_statistic, p_value = stats.ttest_rel(small, big)\nprint(\"Paired t-test results:\")\nprint(\"t-statistic:\", round(t_statistic,3))\nprint(\"p-value:\", round(p_value,3))\n\n# If the p-value is greater than your significance level (0.05),\n# you would fail to reject the null hypothesis (i.e. not enough evidence to suggest a significant difference)\n\npaired_diff = (small - big)\npaired_diff.hist(bins= len(annual_std) )\nplt.axvline(paired_diff.median(), color='red', linestyle='--')\nplt.title('Paired Mean Difference Histogram')\nplt.xlabel('Annual excess return Difference from small size to big size')\nplt.ylabel('Frequency')\nplt.show()\nprint('The dashed line indicates the median of the difference')\n\n\nPaired t-test results:\nt-statistic: 0.988\np-value: 0.327\n\n\n\n\n\n\n\n\n\nThe dashed line indicates the median of the difference\n\n\n\n\nCode\n#@title Dependent t-test (annualized std. Small sample)\n\nsmall = annual_std['s_10']\nbig = annual_std['b_10']\n\nt_statistic, p_value = stats.ttest_rel(small, big)\nprint(\"Paired t-test results:\")\nprint(\"t-statistic:\", round(t_statistic,3))\nprint(\"p-value:\", round(p_value,3))\n\n# If the p-value is greater than your significance level (0.05),\n# you would fail to reject the null hypothesis (i.e. not enough evidence to suggest a significant difference)\n\npaired_diff = (small - big)\npaired_diff.hist(bins= len(annual_std) )\nplt.axvline(paired_diff.median(), color='red', linestyle='--')\nplt.title('Paired Volatility Difference Histogram')\nplt.xlabel('Annual Volatility Difference from small size to big size')\nplt.ylabel('Frequency')\nplt.show()\nprint('The dashed line indicates the median of the difference')\n\n\nPaired t-test results:\nt-statistic: 7.531\np-value: 0.0\n\n\n\n\n\n\n\n\n\nThe dashed line indicates the median of the difference",
    "crumbs": [
      "Apps",
      "Model",
      "03 Size Benchmark"
    ]
  },
  {
    "objectID": "4_5_strategy.html",
    "href": "4_5_strategy.html",
    "title": "05 Strategy",
    "section": "",
    "text": "Traditional asset pricing frameworks rest on no-arbitrage principles and risk-return tradeoffs, assuming that all assets exist to offer compensation for exposure to priced risks. Under such models, individual asset returns are interpreted as linear functions of sensitivities to systematic risk factors.\nIn contrast, the TBTF (Too Big To Fail) strategy is motivated by a fundamentally different view of financial markets under late-stage capitalism. We argue that the secondary stock market reflects persistent dominance by a small subset of firms whose market power is self-reinforcing. These firms attract new capital not due to risk-efficiency, but due to narrative-driven legitimacy and path-dependent concentration of initial endowment.\nThis motivates a strategy grounded not in diversification or risk exposure, but in capital persistence, market lock-in, and capital hierarchy.",
    "crumbs": [
      "Apps",
      "Model",
      "05 Strategy"
    ]
  },
  {
    "objectID": "4_5_strategy.html#motivation-and-market-philosophy",
    "href": "4_5_strategy.html#motivation-and-market-philosophy",
    "title": "05 Strategy",
    "section": "",
    "text": "Traditional asset pricing frameworks rest on no-arbitrage principles and risk-return tradeoffs, assuming that all assets exist to offer compensation for exposure to priced risks. Under such models, individual asset returns are interpreted as linear functions of sensitivities to systematic risk factors.\nIn contrast, the TBTF (Too Big To Fail) strategy is motivated by a fundamentally different view of financial markets under late-stage capitalism. We argue that the secondary stock market reflects persistent dominance by a small subset of firms whose market power is self-reinforcing. These firms attract new capital not due to risk-efficiency, but due to narrative-driven legitimacy and path-dependent concentration of initial endowment.\nThis motivates a strategy grounded not in diversification or risk exposure, but in capital persistence, market lock-in, and capital hierarchy.",
    "crumbs": [
      "Apps",
      "Model",
      "05 Strategy"
    ]
  },
  {
    "objectID": "4_5_strategy.html#portfolio-selection-rule",
    "href": "4_5_strategy.html#portfolio-selection-rule",
    "title": "05 Strategy",
    "section": "2 Portfolio Selection Rule",
    "text": "2 Portfolio Selection Rule\nThe TBTF strategy is constructed under When-What–How framework:\n\nWhen: look-back window (in-sample estimation) & look-forward window (rebalancing frequency)\nWhat: asset universe (e.g. all US-listed stocks) & selection rule (e.g. top-n market cap)\nHow: asset weighting scheme (e.g. convex capital concentration fit)\n\n\n2.1 Asset Universe\nOur strategy operates on the full set of U.S. listed common stocks traded on NYSE, NASDAQ, and AMEX. Stocks with non-positive market capitalization are excluded to avoid bankrupt or illiquid firms.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport sqlite3\ncon = sqlite3.connect(database=\"../../tbtf.sqlite\")\n\ncrsp = pd.read_sql_query(\n  sql=\"SELECT * FROM crsp\",\n  con=con,\n  parse_dates={\"date\"}\n)\n\n\n\n\n2.2 Ranking and State Formation\nEach month, firms are ranked by market capitalization. These rankings define discrete capital states (percentile bins). We focus on the top-decile (state = 10), selecting the top-\\(n\\) firms at time \\(t\\) based on market-cap rank. The baseline is \\(n=10\\), with robustness checks for \\(n \\in \\{5, 7, 10, 15, 20\\}\\).\nA Markovian state transition framework is imposed to capture the temporal dynamics of capital flow between ranked states. The top state exhibits persistent and asymmetric capital retention, supporting the capital lock-in hypothesis.",
    "crumbs": [
      "Apps",
      "Model",
      "05 Strategy"
    ]
  },
  {
    "objectID": "4_5_strategy.html#asset-weighting-schemes",
    "href": "4_5_strategy.html#asset-weighting-schemes",
    "title": "05 Strategy",
    "section": "3 Asset Weighting Schemes",
    "text": "3 Asset Weighting Schemes\nWe evaluate several portfolio weighting methods:\n\nTBTF (Convex Structural Weighting):\nCapital weights are determined by in-sample estimates of the convex relationship between market-cap rank and capital share. For example, specifications can be :\n\nQuadratic Form: \\(w_i \\propto \\alpha + \\beta r_i + \\gamma r_i^2\\)\nExponential Form: \\(w_i \\propto \\alpha e^{\\beta r_i}\\)\nsee Section 8 for more\n\nNote that the exponential form ensures structural monotonicity.\nValue-Weighted (VW):\nProportional to each firm’s market capitalization.\nEqual-Weighted (EQ):\nUniform allocation across all selected assets.\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 현재 경로 기준으로 상위 디렉토리로 경로 추가\nimport sys, os\nsys.path.append(os.path.abspath('../..'))\n\nimport tbtf\n\n\nTo empirically validate the structural weighting functions, we fit several models to the in-sample relationship between rank and capital share. The following plots show the cross-sectional fitting results based on a representative rebalance date.\n\n\nCode\nin_end = '2009-12-31'\nin_sample_months = 48\n\nprint('Snapshot at:', in_end)\nprint('Look-back period:', in_sample_months, 'months')\n\ndf_in_sample, _ = tbtf.split_in_out_sample(crsp, in_end, in_sample_months)\n\n\nSnapshot at: 2009-12-31\nLook-back period: 48 months\n\n\n\n\nCode\nfig, axes = plt.subplots(3, 2, figsize=(14, 15))\ntbtf.plot_quadratic_fit(df_in_sample, n=10, state=10, ax=axes[0, 0])\ntbtf.plot_exponential_fit(df_in_sample, n=10, state=10, ax=axes[0, 1])\ntbtf.plot_mean_fit(df_in_sample, n=10, state=10, ax=axes[1, 0])\ntbtf.plot_loess_fit(df_in_sample, n=10, state=10, ax=axes[1, 1])\ntbtf.plot_bayes_fit(df_in_sample, n=10, state=10, ax=axes[2, 0])\ntbtf.plot_spline_fit(df_in_sample, n=10, state=10, ax=axes[2, 1])\nfig.suptitle(f\"Fitted Capital Share Functions at {in_end}\", fontsize=16)\nplt.tight_layout()\n\n\n\n\n\nEmpirical Fit at 2009-12-31\n\n\n\n\n\n\nCode\nin_end = '2023-12-31'\nin_sample_months = 48\n\nprint('Snapshot at:', in_end)\nprint('Look-back period:', in_sample_months, 'months')\n\ndf_in_sample, _ = tbtf.split_in_out_sample(crsp, in_end, in_sample_months)\n\n\nSnapshot at: 2023-12-31\nLook-back period: 48 months\n\n\n\n\nCode\nfig, axes = plt.subplots(3, 2, figsize=(14, 15))\ntbtf.plot_quadratic_fit(df_in_sample, n=10, state=10, ax=axes[0, 0])\ntbtf.plot_exponential_fit(df_in_sample, n=10, state=10, ax=axes[0, 1])\ntbtf.plot_mean_fit(df_in_sample, n=10, state=10, ax=axes[1, 0])\ntbtf.plot_loess_fit(df_in_sample, n=10, state=10, ax=axes[1, 1])\ntbtf.plot_bayes_fit(df_in_sample, n=10, state=10, ax=axes[2, 0])\ntbtf.plot_spline_fit(df_in_sample, n=10, state=10, ax=axes[2, 1])\nfig.suptitle(f\"Fitted Capital Share Functions at {in_end}\", fontsize=16)\nplt.tight_layout()\n\n\n\n\n\nEmpirical Fit at 2023-12-31",
    "crumbs": [
      "Apps",
      "Model",
      "05 Strategy"
    ]
  },
  {
    "objectID": "4_5_strategy.html#rebalancing-frequency",
    "href": "4_5_strategy.html#rebalancing-frequency",
    "title": "05 Strategy",
    "section": "4 Rebalancing Frequency",
    "text": "4 Rebalancing Frequency\nWe test fixed-interval rebalancing schemes:\n\nMonthly (default)\nQuarterly\nSemiannual\nAnnual\n\nRebalancing frequency directly affects turnover and transaction cost implications. Monthly rebalancing is selected as the baseline to balance responsiveness with frictions.",
    "crumbs": [
      "Apps",
      "Model",
      "05 Strategy"
    ]
  },
  {
    "objectID": "4_5_strategy.html#benchmark-portfolios",
    "href": "4_5_strategy.html#benchmark-portfolios",
    "title": "05 Strategy",
    "section": "5 Benchmark Portfolios",
    "text": "5 Benchmark Portfolios\nFor external comparison, we use:\n\nPre-2010 Index Benchmarks: Dow Jones Industrial Average (^DJI), Nasdaq-100 (^NDX)\nPost-2010 Index ETFs:\n\nDIA (ETF version of DJIA, State Street Corporation ETF. Fund inception: 1998/01/14),\nQQQ (ETF version of NDX. Nasdaq-100 ticker. the Invesco QQQ Trust. Fund inception: 1999/03/10),\nSPY (The SPDR S&P 500 ETF, Fund inception: 1993/01/22),\nVTI (Vanguard Total Stock Market ETF, Fund inception: 2001년 5월 24일),\n\nPost-2010 Style Portfolios: Fama-French ME5 × PRIOR1/3/5, constructed with rolling monthly value- or equal-weighting",
    "crumbs": [
      "Apps",
      "Model",
      "05 Strategy"
    ]
  },
  {
    "objectID": "4_5_strategy.html#strategic-rationale",
    "href": "4_5_strategy.html#strategic-rationale",
    "title": "05 Strategy",
    "section": "6 Strategic Rationale",
    "text": "6 Strategic Rationale\nThe TBTF strategy is not merely a rule-based selection method. It reflects a structural argument that allocative efficiency in modern markets is compromised by persistent capital concentration. Instead of diversifying away idiosyncratic risk, markets are increasingly dominated by capital lock-in, hierarchy reinforcement, and narrative legitimacy.\nThis framework reconceptualizes the role of financial assets from carriers of risk to vehicles of structural dominance, and evaluates portfolio construction in light of this altered paradigm.",
    "crumbs": [
      "Apps",
      "Model",
      "05 Strategy"
    ]
  },
  {
    "objectID": "4_5_strategy.html#tbtf-strategy-pipeline",
    "href": "4_5_strategy.html#tbtf-strategy-pipeline",
    "title": "05 Strategy",
    "section": "7 TBTF Strategy Pipeline",
    "text": "7 TBTF Strategy Pipeline\nThis appendix section outlines the modular structure of the TBTF portfolio strategy, from input specification to out-of-sample return generation and performance evaluation. The pipeline is designed to be flexible across different weighting schemes, rebalance frequencies, and strategy parameters.\n\n7.1 Inputs (Arguments)\n\n\n\n\n\n\n\nArgument\nDescription\n\n\n\n\ndf\nMain input DataFrame (crsp) including fields such as date, permno, mktcap, ret, state, mktcap_lag, etc.\n\n\nstate_level\nTarget capital state to define the asset universe, typically the top decile (e.g., 10).\n\n\ntop_n\nNumber of assets to be selected at each rebalance point (e.g., n ∈ {5, 10, 20, 30, 50}).\n\n\nrebalance_freq\nRebalancing interval (e.g., '1M', '3M', '6M', '12M').\n\n\nweighting_method\nOne of 'equal', 'value', 'quadratic', or 'exponential'.\n\n\nin_sample_period\nEstimation window for in-sample weight calibration (e.g., '2010-01-01' to '2013-12-31').\n\n\nout_sample_period\nEvaluation window for out-of-sample backtesting (e.g., '2014-01-01' to '2023-12-31').\n\n\neta, p\nParameters for CRRA utility (eta) and Omega ratio threshold (p).\n\n\n\n\n\n7.2 Strategy Pipeline (Pseudo-code Logic)\n\n7.2.1 Step 1: Data Partitioning & Filtering\nin_sample = df[(df['date'] &gt;= in_sample_start) & (df['date'] &lt;= in_sample_end)]\nout_sample = df[(df['date'] &gt; in_sample_end) & (df['date'] &lt;= out_sample_end)]\nuniverse = df[df['state'] == state_level]\n\nRestrict selection universe to target capital state (e.g., top decile).\n\n\n\n7.2.2 Step 2: In-sample Weight Estimation\nIf weighting_method is 'quadratic' or 'exponential':\n\nEstimate the relationship between within-state rank and capital share using in-sample data:\n\nQuadratic:\n\\[\\text{CapShare}_i = \\alpha + \\beta \\cdot \\text{Rank}_i + \\gamma \\cdot \\text{Rank}_i^2\\]\nExponential:\n\\[\\text{CapShare}_i = \\alpha \\cdot e^{\\beta \\cdot \\text{Rank}_i}\\]\n\nSave estimated coefficients:\ncoefficients = {'alpha': ..., 'beta': ..., 'gamma': ...}\n\n\n\n7.2.3 Step 3: Out-of-sample Portfolio Construction\nFor each rebalance date in the out-sample period:\n\nFilter to target state (state == state_level)\nRank by market cap, select top n\nAssign weights:\n\nEqual: \\(w_i = 1/n\\)\nValue: \\(w_i = \\text{mktcap}_i / \\sum \\text{mktcap}\\)\nQuadratic: apply \\(\\hat{\\alpha}, \\hat{\\beta}, \\hat{\\gamma}\\) to rank\nExponential: apply \\(\\hat{\\alpha}, \\hat{\\beta}\\) to rank\n\nStore [date, permno, weight] for forward return application\n\n\n\n7.2.4 Step 4: Portfolio Return Computation\nAt each evaluation date after rebalance:\n\nMerge forward returns of selected assets\nCompute portfolio return: \\[R_t^{\\text{portfolio}} = \\sum_i w_{i,t} \\cdot r_{i,t}\\]\nConstruct time series of out-of-sample portfolio returns\n\n\n\n7.2.5 Step 5: Turnover Calculation\nFor each rebalance window:\n\nCalculate: \\[\\text{Turnover}_t = \\sum_i |w_{i,t} - w_{i,t-1}|\\]\nUseful for assessing transaction costs and liquidity requirements\n\n\n\n7.2.6 Step 6: Performance Evaluation\nUsing out-of-sample return series, compute:\n\nRisk-Adjusted Metrics:\n\nSharpe ratio, Sortino ratio\nOmega ratio (threshold = p)\nCalmar ratio, maximum drawdown (MDD)\nCRRA utility with risk aversion parameter eta\n\nReturn summary as dictionary:\nmetrics_dict = {\n    'Sharpe': ...,\n    'Sortino': ...,\n    'Omega': ...,\n    'CRRA': ...,\n    'CAGR': ...,\n    'MDD': ...\n}\n\n\n\n\n7.3 Outputs\n\n\n\nOutput\nDescription\n\n\n\n\nreturns_df\nTime series of out-of-sample portfolio returns\n\n\nweights_df\nPortfolio composition (weights per date and permno)\n\n\nturnover_df\nTime series of turnover at each rebalance point\n\n\nmetrics_dict\nDictionary of strategy performance metrics\n\n\n\n\n\n7.4 Module Lists\n#| label: TBTF Strategy Full Module\n#| warning: false\n#| message: false\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\n\n# ----------------------------\n# 1. Data Partitioning\n# ----------------------------\ndef split_in_out_sample(df, in_end, in_sample_months=36, out_end=None):\n    in_sample = df[(df['date'] &lt;= in_end)].copy()\n    if out_end:\n        out_sample = df[(df['date'] &gt; in_end) & (df['date'] &lt;= out_end)].copy()\n    else:\n        out_sample = df[(df['date'] &gt; in_end)].copy()\n    return in_sample, out_sample\n\n# ----------------------------\n# 2. Weight Estimation\n# ----------------------------\ndef estimate_exponential_weights(df_in_sample, n=10, state=10):\n    # Placeholder for full implementation\n    pass\n\ndef estimate_quadratic_weights(df_in_sample, n=10, state=10):\n    # Placeholder for full implementation\n    pass\n# ----------------------------\n# 3. Portfolio Construction\n# ----------------------------\ndef construct_tbtf_exponential(crsp_df, target_state, top_n, params):\n    # Placeholder for full implementation\n    pass\n\ndef construct_tbtf_quadratic(crsp_df, target_state, top_n, params):\n    # Placeholder for full implementation\n    pass\n\n# ----------------------------\n# 4. Return and Turnover Calculation\n# ----------------------------\ndef compute_return_tbtf(crsp, rebalance_dates, weighting_method='exponential', top_n=10, state=10, in_sample_months=36):\n    # Placeholder for full implementation\n    pass\n\ndef compute_return_pfo(crsp, rebalance_dates, weighting_method='vw', top_n=10):\n    # Placeholder for traditional VW or EW method\n    pass\n\ndef compute_turnover(weights_df, rebalance_dates):\n    # Placeholder for full implementation\n    pass\n\n# ----------------------------\n# 5. Performance Evaluation\n# ----------------------------\ndef evaluate_performance(returns, eta=3, p=0.01, periods_per_year=12):\n    # Placeholder for full implementation\n    pass\n\n# ----------------------------\n# 6. Rebalance Date Utility\n# ----------------------------\ndef get_rebalance_offset(rebalance_freq):\n    # Placeholder for full implementation\n    pass\n\n# ----------------------------\n# 7. Backtest Pipeline\n# ----------------------------\ndef backtest_pipeline(crsp, in_end, out_end, in_sample_months, rebalance_freq, weighting_method, top_n, state, eta, p):\n    # Placeholder for full backtest execution logic\n    pass\n\n# ----------------------------\n# 8. Visualization (optional)\n# ----------------------------\ndef plot_quadratic_fit(df_in_sample, n=10, state=10):\n    # Placeholder for plot generation\n    pass\n\ndef plot_exponential_fit(df_in_sample, n=10, state=10):\n    # Placeholder for plot generation\n    pass",
    "crumbs": [
      "Apps",
      "Model",
      "05 Strategy"
    ]
  },
  {
    "objectID": "4_5_strategy.html#sec-appendix",
    "href": "4_5_strategy.html#sec-appendix",
    "title": "05 Strategy",
    "section": "8 Appendix: Alternative Fitting Methods for Capital Share Functions",
    "text": "8 Appendix: Alternative Fitting Methods for Capital Share Functions\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\nFunctional Form\nStatistical Interpretation\nFlexibility\nStability\nInterpretability\nComplexity\n\n\n\n\nMean-Based\nStepwise function\nGroup-wise Sample Mean\nLow\nLow\nHigh\nVery Low\n\n\nLOESS\nLocal smoothing\nNonparametric Local Regression\nVery High\nMedium\nMedium\nModerate\n\n\nBayesian\nGlobal shrinkage estimator\nHierarchical Bayesian Model\nMedium\nHigh\nMedium\nModerate–High\n\n\nSpline\nSmooth piecewise polynomial\nSemi-parametric Regression\nHigh\nMedium\nMedium\nModerate\n\n\n\n\n8.1 Mean-Based Estimation (1st Moment per Rank)\nDefinition\nFor each rank \\(r \\in \\{1,\\dots,10\\}\\), we estimate the average capital share over the in-sample period: \\[\n\\hat{w}_r = \\frac{1}{T} \\sum_{t=1}^{T} w_{r,t}\n\\]\nStatistical Structure\n\nA fully nonparametric approach, treating each rank as a discrete category.\nEquivalent to a fixed effect model without regressors, estimating the group-wise sample mean.\n\nTheoretical Interpretation\n\nThis is a form of empirical histogram fitting, relying only on the sample mean of observed capital shares.\nThe method does not require distributional assumptions and treats the sample average as representative, regardless of time variation or non-stationarity.\n\nAdvantages\n\nSimple, intuitive, and computationally efficient.\nHighly flexible due to the absence of structural assumptions.\n\nLimitations\n\nVery sensitive to variance and outliers in the capital share distribution at each rank.\nNo extrapolation or smoothing is applied across ranks.\n\n\n\n8.2 LOESS (Locally Weighted Scatterplot Smoothing)\nDefinition\nCapital share is estimated via local regression, using a kernel-weighted average of nearby observations: \\[\n\\hat{w}_r = \\sum_{j} K\\left(\\frac{r - j}{h}\\right) w_j\n\\] - \\(K(\\cdot)\\) is a kernel function (e.g., tricube); \\(h\\) controls the bandwidth and smoothness.\nStatistical Structure\n\nA classic nonparametric regression method.\nLOESS typically uses local polynomial regressions (order 0, 1, or 2) for smoothing.\n\nTheoretical Interpretation\n\nConstructs the overall rank–capshare curve as a composition of local approximations, rather than a single global function.\nHighly sensitive to the bias-variance tradeoff depending on bandwidth \\(h\\).\n\nAdvantages\n\nEffectively captures local irregularities in the rank–capshare mapping.\nVery flexible, with no functional form assumptions.\n\nLimitations\n\nPoor extrapolation performance at boundary ranks (e.g., rank 1 and 10).\nBandwidth selection has a significant effect on smoothing behavior.\n\n\n\n8.3 Bayesian Smoothing (Hierarchical Empirical Bayes)\nDefinition\nEach rank-specific capital share \\(w_r\\) is modeled as a noisy observation from a latent distribution: \\[\nw_r \\sim \\mathcal{N}(\\theta_r, \\sigma_r^2), \\quad \\theta_r \\sim \\mathcal{N}(\\mu, \\tau^2)\n\\]\nStatistical Structure\n\nA hierarchical Bayesian model that allows ranks to share strength through a common prior.\nIn the empirical Bayes setting, the hyperparameters \\((\\mu, \\tau^2)\\) are estimated from data.\n\nTheoretical Interpretation\n\nStabilizes estimates for ranks with high variance by shrinking them toward the global prior.\nRanks near the center are barely adjusted, while outlier ranks are pulled closer to the mean.\n\nAdvantages\n\nProduces stable estimates even under noisy or volatile capital share data.\nAllows for uncertainty quantification via posterior distributions.\n\nLimitations\n\nSensitive to prior or hyperparameter specification.\nImplementation can be more complex than standard nonparametric methods.\n\n\n\n8.4 Spline Smoothing (Piecewise Polynomial with Smooth Joints)\nDefinition\nThe rank–capshare function is approximated as a linear combination of spline basis functions: \\[\n\\hat{w}_r = \\sum_{k} \\beta_k B_k(r)\n\\] - \\(B_k(r)\\) are spline basis functions (e.g., B-splines or natural splines).\nStatistical Structure\n\nA semi-parametric regression approach.\nThe model is similar to linear regression but extended via basis transformation.\n\nTheoretical Interpretation\n\nBalances global fitting and local flexibility by using piecewise polynomial segments with smooth joints.\nThe number and placement of knots (and the degree of polynomials) determine flexibility.\n\nAdvantages\n\nProduces smooth and differentiable fits across the rank domain.\nExtrapolation is partially feasible, especially when using natural splines.\n\nLimitations\n\nPerformance is highly dependent on the selection of knots.\nCan overfit if too many basis functions are used.",
    "crumbs": [
      "Apps",
      "Model",
      "05 Strategy"
    ]
  },
  {
    "objectID": "4_7_robustness.html",
    "href": "4_7_robustness.html",
    "title": "07 Robustness",
    "section": "",
    "text": "While the previous section focused on the performance outcomes of the TBTF strategy, this section evaluates the stability of those outcomes under variation in core implementation parameters. We test whether the superior performance persists under changes in portfolio size, rebalancing frequency, weighting schemes, and look-back windows.\nRather than relying on a single optimized configuration, the TBTF strategy demonstrates structural robustness across plausible alternatives. This not only reinforces the credibility of the results, but also supports the practical adaptability of the approach for different institutional contexts.\nAll robustness tests are performed on the post-2010 period, with out-of-sample investment beginning on 2010-01-01. The portfolio is trained using a 48-month rolling window (in_sample=48M) and evaluated through 2023-12-31.\nCode\nimport pandas as pd\nimport sqlite3\ncons = sqlite3.connect(database=\"../../tbtf.sqlite\")\n\ncrsp = pd.read_sql_query(\n  sql=\"SELECT * FROM crsp\",\n  con=cons,\n  parse_dates={\"date\"}\n)\nCode\n# Out-of-sample Investment\nimport sys\nimport os\n# 현재 경로 기준으로 상위 디렉토리로 경로 추가\nsys.path.append(os.path.abspath('../..'))\n\nimport tbtf\n\n\n# robust check 대상 파라미터 설정\ntop_ns = [5, 7, 10, 15, 20]\nrebalance_freqs = ['3M', '6M', '12M']\nweighting_methods = ['exponential', 'quadratic', 'value', 'equal']\n\n# 기본 설정 정리\nin_end = '2009-12-31'         # in-sample 끝\nout_end = '2023-12-31'        # out-of-sample 끝\nin_sample_months = 48         # in-sample 기간 (예: 4년)\neta = 3                       # CRRA 계수\np = 0.01                      # Omega Ratio threshold\nstate = 10\n\n\nfrom joblib import Parallel, delayed\n\ndef run_single_backtest(method, n, freq):\n    result = tbtf.backtest_pipeline(\n        crsp=crsp,\n        in_end=in_end,\n        out_end=out_end,\n        in_sample_months=in_sample_months,\n        rebalance_freq=freq,\n        weighting_method=method,\n        top_n=n,\n        state=state,\n        eta=eta,\n        p=p\n    )\n    perf = result['performance']\n    perf.update({'n': n, 'rebalance_freq': freq, 'weighting_method': method})\n\n    turnover_df = result['turnover']\n    total_turnover = turnover_df.iloc[:-1]['turnover'].sum() if len(turnover_df) &gt; 1 else np.nan\n\n    return perf, {'n': n, 'rebalance_freq': freq, 'weighting_method': method, 'total_turnover': total_turnover}\nCode\nfrom itertools import product\n\nparam_grid = list(product(weighting_methods, top_ns, rebalance_freqs))\n\n# 병렬 실행\nresults = Parallel(n_jobs=-1)(delayed(run_single_backtest)(method, n, freq) for method, n, freq in param_grid)\n\n# Robustness check 결과 DataFrame 생성\nperformance_df = pd.DataFrame([r[0] for r in results])\nturnover_summary_df = pd.DataFrame([r[1] for r in results])",
    "crumbs": [
      "Apps",
      "Model",
      "07 Robustness"
    ]
  },
  {
    "objectID": "4_7_robustness.html#by-weighting-method",
    "href": "4_7_robustness.html#by-weighting-method",
    "title": "07 Robustness",
    "section": "1 By Weighting Method",
    "text": "1 By Weighting Method\nWe begin by comparing the four weighting schemes: exponential, quadratic, value-weighted, and equal-weighted portfolios. The results show that TBTF’s convex weighting yields consistently superior risk-adjusted performance, particularly in terms of expected CRRA utility and downside-sensitive metrics like the Omega ratio. We set the CRRA parmater \\(\\eta = 3\\) to reflect moderate risk aversion and \\(p = 0.01\\) to evaluate downside risk via the Omega Ratio at the minimum 1% monthly threshold.\n\n\nCode\nmetrics = ['Expected CRRA', 'Annualized Return', 'Sharpe Ratio', 'Sortino Ratio', 'Calmar Ratio', 'Omega Ratio', 'Max Drawdown']\nperformance_df.groupby('weighting_method')[metrics].median().round(3).sort_values(by='Expected CRRA', ascending=False)\n\n\n\n\n\n\nGood Performance Metrics by Weighting Method\n\n\n\nExpected CRRA\nAnnualized Return\nSharpe Ratio\nSortino Ratio\nCalmar Ratio\nOmega Ratio\nMax Drawdown\n\n\nweighting_method\n\n\n\n\n\n\n\n\n\n\n\nexponential\n0.020\n0.051\n0.766\n1.444\n0.430\n2.155\n-0.134\n\n\nquadratic\n0.020\n0.052\n0.779\n1.443\n0.433\n2.182\n-0.132\n\n\nvalue\n0.020\n0.050\n0.770\n1.454\n0.435\n2.147\n-0.134\n\n\nequal\n0.018\n0.045\n0.727\n1.326\n0.382\n1.967\n-0.135\n\n\n\n\n\n\n\n\n\nCode\nmetrics = ['Annualized Volatility', 'Pearson Skewness', 'Excess Kurtosis']\nperformance_df.groupby('weighting_method')[metrics].median().round(3).sort_values(by='Annualized Volatility', ascending=True)\n\n\n\n\n\n\nBad Performance Metric by Weighting Method\n\n\n\nAnnualized Volatility\nPearson Skewness\nExcess Kurtosis\n\n\nweighting_method\n\n\n\n\n\n\n\nequal\n0.064\n-0.341\n0.307\n\n\nexponential\n0.068\n-0.240\n0.752\n\n\nvalue\n0.068\n-0.144\n0.524\n\n\nquadratic\n0.069\n-0.214\n0.791\n\n\n\n\n\n\n\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#sns.violinplot(data=performance_df, x='weighting_method', y='Annualized Volatility')\nsns.boxplot(data=performance_df, x='weighting_method', y='Sharpe Ratio')\nplt.title(\"Metric Distribution by Weighting Method\")\nplt.show()\n\n\n\n\n\n\n\n\n\nThese results suggest that nonlinear weighting schemes are critical to capturing the TBTF premium. Among them, the quadratic method slightly outperforms exponential weighting across most performance metrics, including the highest median Omega ratio (2.182) and lowest median max drawdown (−13.2%). Given that both exponential and quadratic weighting schemes yield nearly identical results under the median of CRRA-based metrics, the exponential weighting—though rooted in CRRA-type concentration—offers no clear advantage over the quadratic scheme, which continues to strike a robust balance between concentration and diversification.",
    "crumbs": [
      "Apps",
      "Model",
      "07 Robustness"
    ]
  },
  {
    "objectID": "4_7_robustness.html#by-asset-selection-size-top_n",
    "href": "4_7_robustness.html#by-asset-selection-size-top_n",
    "title": "07 Robustness",
    "section": "2 By Asset Selection Size (top_n)",
    "text": "2 By Asset Selection Size (top_n)\nNext, we vary the number of selected stocks, ranging from 5 to 20. The results clearly show that performance declines as top_n increases, confirming that capital concentration—not mere size exposure—is the key driver.\n\n\nCode\nperformance_df.pivot_table(\n    values='Sharpe Ratio',\n    index='weighting_method',\n    columns='n',\n    aggfunc='mean'\n)\n\n\n\n\n\n\nSharpe Ratio by Weighting Method and Top-n\n\n\nn\n5\n7\n10\n15\n20\n\n\nweighting_method\n\n\n\n\n\n\n\n\n\nequal\n0.608791\n0.627588\n0.605765\n0.584674\n0.585974\n\n\nexponential\n0.636665\n0.648066\n0.622787\n0.604382\n0.602568\n\n\nquadratic\n0.612236\n0.642290\n0.630014\n0.610713\n0.597321\n\n\nvalue\n0.616514\n0.633672\n0.615023\n0.608970\n0.605151\n\n\n\n\n\n\n\n\n\nCode\nperformance_df.pivot_table(\n    values='Expected CRRA',\n    index='weighting_method',\n    columns='n'\n).round(4)\n\n\n\n\n\n\nExpected CRRA Utility across Weighting Methods and Top-n\n\n\nn\n5\n7\n10\n15\n20\n\n\nweighting_method\n\n\n\n\n\n\n\n\n\nequal\n0.0168\n0.0167\n0.0141\n0.0119\n0.0110\n\n\nexponential\n0.0175\n0.0174\n0.0154\n0.0135\n0.0125\n\n\nquadratic\n0.0167\n0.0172\n0.0156\n0.0138\n0.0127\n\n\nvalue\n0.0166\n0.0168\n0.0151\n0.0139\n0.0130\n\n\n\n\n\n\n\nVarying the number of selected stocks from 5 to 20 reveals a consistent trend: risk-adjusted performance tends to decline once the portfolio exceeds \\(n=7\\).\nThis pattern implies that the TBTF premium is not a gradual function of portfolio size, but instead concentrated at the extreme upper tail of the capital distribution. The declining marginal benefit of including additional assets reflects the structural nature of capital lock-in—only the very largest firms systematically attract persistent investor flows, benefit from narrative insulation, and receive reinforcement through index inclusion.\nNotably, traditional weighting schemes (Value or Equal) consistently underperform compared to TBTF-based weightings (Exponential or Quadratic) when \\(n &lt; 15\\), highlighting the importance of embedding size asymmetry in portfolio construction.",
    "crumbs": [
      "Apps",
      "Model",
      "07 Robustness"
    ]
  },
  {
    "objectID": "4_7_robustness.html#rebalancing-frequency-and-turnover-tradeoff",
    "href": "4_7_robustness.html#rebalancing-frequency-and-turnover-tradeoff",
    "title": "07 Robustness",
    "section": "3 Rebalancing Frequency and Turnover Tradeoff",
    "text": "3 Rebalancing Frequency and Turnover Tradeoff\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# 원하는 순서로 정렬\nrebalance_order = ['3M', '6M', '12M']\nn_order = sorted(performance_df['n'].unique(), reverse=True)  # n=5,7,10,15,20 등 오름차순\n\n# Pivot and reorder\npivot_sharpe = (\n    performance_df.pivot_table(\n        values='Sharpe Ratio',\n        index='n',\n        columns='rebalance_freq'\n    )\n    .loc[n_order, rebalance_order]  # y축 (n), x축 (rebalance_freq) 순서 정렬\n    .round(2)\n)\n\n# Pivot을 위한 사용자 정의 함수\ndef best_weighting_by_sharpe(df):\n    idx = df.groupby(['n', 'rebalance_freq'])['Sharpe Ratio'].idxmax()\n    best_df = df.loc[idx, ['n', 'rebalance_freq', 'weighting_method']]\n    pivot = best_df.pivot(index='n', columns='rebalance_freq', values='weighting_method')\n\n    # 시각화-friendly 정렬\n    pivot = pivot.loc[sorted(pivot.index, reverse=True), ['3M', '6M', '12M']]\n    return pivot\n\nbest_weighting_pivot = best_weighting_by_sharpe(performance_df)\n\n# 결과 출력\n# display(best_weighting_pivot)\n\nplt.figure(figsize=(8, 5))\n# sns.heatmap(pivot_sharpe, annot=True, cmap=\"YlGnBu\", cbar_kws={'label': 'Sharpe Ratio'})\nsns.heatmap(\n    pivot_sharpe,  # 기존 numeric table\n    annot=best_weighting_pivot.values,  # 텍스트는 scheme 이름으로\n    fmt='',  # 숫자 포맷 아님\n    cmap=\"YlGnBu\",\n    cbar_kws={'label': 'Sharpe Ratio'}\n)\nplt.title(\"Best Weighting Method by Sharpe Ratio\")\nplt.xlabel(\"Rebalancing Frequency\")\nplt.ylabel(\"Number of Selected Assets (n)\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nSharpe Ratio Heatmap (Top-n vs Rebalancing Frequency)\n\n\n\n\nWe next examine the impact of rebalancing frequency—specifically, quarterly (3M), semiannual (6M), and annual (12M)—on risk-adjusted performance and underlying portfolio structure. As shown in the heatmap above, quarterly rebalancing tends to produce higher Sharpe ratios across most configurations. However, this benefit comes at the cost of substantially higher turnover. We find that shorter rebalancing intervals better preserve the structural core of TBTF portfolios. Quarterly updates maintain capital concentration and produce stable weight dynamics, while longer horizons may induce drift and dilute structural persistence—especially at larger n.\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.boxplot(data=turnover_summary_df, x='weighting_method', y='total_turnover', hue='rebalance_freq')\nplt.title(\"Turnover Distribution by Weighting and Frequency\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nTotal Turnover across Parameters\n\n\n\n\n\n\nCode\nsns.boxplot(\n    data=turnover_summary_df,\n    x='weighting_method',\n    y='total_turnover',\n    hue='n'  # Top-n 구성 자산 수\n)\nplt.title(\"Turnover Distribution by Weighting and Top-n\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nTotal Turnover across Parameters\n\n\n\n\nTo assess the practical implementability of the TBTF strategy, we examine the total turnover across different weighting schemes and rebalancing frequencies. As illustrated in the boxplot above, turnover levels vary meaningfully by configuration, offering insights into the structural dynamics of each method.\nTBTF weighting methods—particularly under less frequent rebalancing (e.g., quarterly or semiannual)—consistently exhibit lower interquartile ranges of turnover, indicating a high degree of structural stability in portfolio composition. In contrast, equal weighting yields the widest dispersion, reflecting more volatile asset entry and reallocation patterns due to its lack of embedded capital sensitivity.\nAcross all weighting methods, turnover responds strongly to changes in rebalancing frequency. Reducing the frequency from quarterly to semiannual, and from semiannual to annual, results in approximately a twofold decline in average turnover at each step. This regularity highlights a core tradeoff: finer rebalancing intervals enable more timely tracking of short-term shifts but also introduce greater portfolio churn.\nA noteworthy exception emerges within the TBTF schemes: quadratic weighting produces higher turnover when moving from semiannual to annual rebalancing—a reversal of the pattern observed in all other methods. This suggests that the quadratic weighting’s greater sensitivity to initial rank–capital structure may amplify reallocations over longer horizons, especially if the top decile membership becomes more volatile at lower rebalancing frequencies.\nTaken together, these findings suggest that TBTF-based weighting offers the most stable tradeoff between capital concentration and transaction frictions. In particular, rebalancing at a semiannual or quarterly frequency appears to strike a practical balance, preserving capital lock-in dynamics while keeping turnover at manageable levels.\nWe further analyze turnover as a function of the number of selected stocks, visualized in the “Turnover Distribution by Weighting and Top-n” plot. Here, TBTF, value-weighted, and equal-weighted portfolios all exhibit their most stable and lowest turnover levels at \\(n = 10\\) and \\(n = 20\\). Interestingly, turnover does not decline monotonically as the number of holdings increases. For example, moving from \\(n = 10\\) to \\(n = 15\\) slightly increases total turnover before falling again at \\(n = 20\\). This non-monotonicity suggests that portfolio stability is not simply a function of size, but also of how concentration thresholds interact with the rank dynamics of capital allocation.\n\n\nCode\n# Merge performance & turnover\nmerged_df = pd.merge(performance_df, turnover_summary_df,\n                     on=['n', 'rebalance_freq', 'weighting_method'])\n\n#| fig-cap: \"Performance Metric vs. Turnover Tradeoff\"\nsns.scatterplot(data=merged_df,\n                x='total_turnover',\n                y='Expected CRRA',\n                hue='weighting_method',\n                style='rebalance_freq')\nplt.title(\"Performance vs. Turnover Tradeoff\")\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nFinally, we visualize the performance-turnover tradeoff, showing how each strategy balances return and implementation frictions:\nThe figure above visualizes the tradeoff between expected CRRA utility and total turnover across all strategy configurations. Each point represents a unique combination of n, rebalance_freq, and weighting_method.\nThe resulting plot reveals distinct horizontal clusters by rebalancing frequency, especially under the post-2010 regime. This banding effect is most likely driven by regime-specific shocks—particularly the COVID-19 crash, which had sharply divergent effects depending on the timing of portfolio adjustment.\nTo further analyze this, we compute the expected CRRA per unit of turnover (i.e., \\(\\frac{\\text{Expected CRRA}}{\\text{Total Turnover}}\\)) across time subperiods. During non-crisis periods, 6M and 12M configurations yield comparable ratios, with 3M performing slightly lower due to higher turnover. However, during the COVID-19 period, the 6M rebalancing interval dominates, suggesting that it hit a timing sweet spot—being responsive enough to adapt, but not overly reactive like 3M or sluggish like 12M.\nThese findings underscore the importance of considering macro-structural shocks in assessing turnover efficiency. While quarterly rebalancing ensures persistence and structural alignment, semiannual updates may offer better resilience under nonlinear market stress, at least in crisis regimes.",
    "crumbs": [
      "Apps",
      "Model",
      "07 Robustness"
    ]
  },
  {
    "objectID": "4_7_robustness.html#summary-of-robustness",
    "href": "4_7_robustness.html#summary-of-robustness",
    "title": "07 Robustness",
    "section": "4 Summary of Robustness",
    "text": "4 Summary of Robustness\nAcross a broad set of configurations, the TBTF strategy exhibits strong structural robustness. Its performance remains resilient to changes in portfolio size, weighting method, and rebalancing frequency, indicating that its effectiveness does not hinge on a narrow set of parameter choices.\nThe consistent superiority of nonlinear weighting schemes—particularly exponential and quadratic—underscores that the TBTF effect stems not merely from large-cap exposure, but from deliberate capital concentration. At the same time, the diminishing performance observed as the number of included assets increases confirms that the strategy draws its strength from a small group of dominant firms at the top of the capital hierarchy.\nIn implementation terms, both quarterly and semiannual rebalancing strike workable tradeoffs. Quarterly updates better preserve structural lock-in, while semiannual schedules offer greater resilience during regime shocks, such as the COVID-19 episode. Turnover remains moderate in both cases, especially under quadratic weighting, which achieves stability without compromising returns.\nTaken together, the evidence suggests that TBTF is neither an artifact of tuning nor a fragile anomaly. Rather, it is a reflection of deeper structural features—platform dominance, passive reinforcement, and institutional persistence—that increasingly govern capital allocation in modern markets.",
    "crumbs": [
      "Apps",
      "Model",
      "07 Robustness"
    ]
  },
  {
    "objectID": "5_conclusion.html",
    "href": "5_conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "This study provides robust empirical evidence that U.S. capital markets have diverged from the classical ideals of allocative efficiency, particularly in the post-2008 era of quantitative easing and the dominance of passive capital flows. Asset returns no longer follow symmetric, risk-compensated distributions; instead, they increasingly resemble a mixture of structurally persistent, rank-locked flows governed by capital incumbency and index-based reinforcement.\nAt the core of this transformation is the “Too Big to Fail” (TBTF) strategy. Simple in design yet profound in implication, the strategy selects the top decile firms by market capitalization and applies a convex weighting function inspired by utility-theoretic and structural concentration principles. Its empirical outperformance—especially after 2010—is not rooted in fundamentals or superior information, but rather in passive lock-in, narrative insulation, and macro-policy distortions.\nYet, this success raises a sobering paradox:\nThe TBTF strategy is “sadly optimal.”\n\nIf it continues to outperform, it profits from a system that rewards scale over efficiency, entrenchment over innovation.\n\nIf it falters, it may signal a long-overdue reversion to competitive capital allocation—a welcome but unlikely shift under current institutional dynamics.\n\nIn either case, the implications are clear. The strategy thrives not because markets are efficient, but because they are systematically tilted toward power and persistence. The mechanisms that once ensured discipline through arbitrage and competition now appear weakened—replaced by self-reinforcing structures of capital inertia.\nThis paper contributes to a growing literature on non-ergodic market dynamics, rank-based valuation frameworks, and the erosion of allocative efficiency under interventionist regimes. It calls for a fundamental reorientation in asset pricing theory: away from marginal risk pricing and toward a framework that incorporates persistence, asymmetry, and rent extraction as first-order forces in financial economics.\nUltimately, if financial markets are to reclaim their role as allocative institutions rather than passive amplifiers of incumbent power, the rethinking of capital indexing, passive vehicle design, and structural fairness is not merely desirable—it is necessary.\n\nToo big to fail?\nMore like: too big to compete, too big to move, and too big to serve.",
    "crumbs": [
      "Apps",
      "Conclusion"
    ]
  }
]